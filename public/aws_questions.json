[
  {
    "question": "CertyIQ\nA company collects data for temperature, humidity, and atmospheric pressure in cities across multiple continents.\nThe average volume of data that the company collects from each site daily is 500 G",
    "options": {
      "B": "Upload the data from each site to an S3 bucket in the closest Region. Use S3 Cross-Region Replication to",
      "A": "Turn on S3 Transfer Acceleration on the destination S3 bucket. Use multipart uploads to directly upload site",
      "C": "Schedule AWS Snowball Edge Storage Optimized device jobs daily to transfer data from each site to the",
      "D": "Upload the data from each site to an Amazon EC2 instance in the closest Region. Store the data in an"
    },
    "answer": "A",
    "explanation": "The correct answer is A because it directly addresses the requirements of speed, minimal operational\ncomplexity, and leveraging existing high-speed internet connections. S3 Transfer Acceleration utilizes AWS's\nglobally distributed edge locations to optimize data transfer speeds into an S3 bucket. Multipart uploads\nenhance reliability and speed, especially for large files (500 GB daily). This method avoids the complexity of\nmanaging intermediate buckets, EC2 instances, or physical devices like Snowball Edge.\nOption B introduces unnecessary complexity with multiple S3 buckets and cross-region replication,\nincreasing management overhead and costs. While replication handles data transfer, Transfer Acceleration is\ndesigned specifically for speed optimization in direct uploads.\nOption C is unsuitable because AWS Snowball Edge is intended for environments with limited or no internet\nconnectivity. Given the high-speed internet connection available at each site, Snowball Edge adds\nunnecessary logistical complexity and delays.\nOption D involves managing EC2 instances, EBS volumes, and snapshots, significantly increasing operational\noverhead. Transferring EBS snapshots is also not an optimized method for data aggregation into S3 compared\nto direct S3 uploads, especially regarding speed and cost. The described method is more appropriate for\ndisaster recovery of complete systems rather than daily data aggregation.\nIn summary, S3 Transfer Acceleration with multipart uploads is the most efficient and straightforward\nsolution for quickly aggregating data from global sites into a single S3 bucket, aligning with the requirements\nfor speed, minimal operational overhead, and leveraging existing high-speed internet.\nRelevant links:\nAmazon S3 Transfer Acceleration: https://aws.amazon.com/s3/transfer-acceleration/\nAmazon S3 Multipart Upload Overview:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html",
    "links": [
      "https://aws.amazon.com/s3/transfer-acceleration/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html"
    ]
  },
  {
    "question": "CertyIQ\nA company needs the ability to analyze the log files of its proprietary application. The logs are stored in JSON\nformat in an Amazon S3 bucket. Queries will be simple and will run on-demand. A solutions architect needs to\nperform the analysis with minimal changes to the existing architecture.\nWhat should the solutions architect do to meet these requirements with the LEAST amount of operational\noverhead?",
    "options": {
      "A": "Amazon Redshift: Redshift requires setting up and managing a data warehouse cluster. This involves",
      "B": "Amazon CloudWatch Logs: CloudWatch Logs are best suited for real-time monitoring and centralized",
      "C": "Use Amazon Athena directly with Amazon S3 to run the queries as needed.",
      "D": "AWS Glue and Amazon EMR: This solution is an overkill. AWS Glue is for ETL and data cataloging, and EMR"
    },
    "answer": "C",
    "explanation": "The best solution is C. Use Amazon Athena directly with Amazon S3 to run the queries as needed.\nHere's why:\nMinimal Operational Overhead: Athena is serverless. It eliminates the need to provision or manage\ninfrastructure. It also allows you to query data directly in S3, which is a major advantage as the logs are\nalready stored there.\nCost-Effectiveness: Athena charges based on the amount of data scanned per query. Since the queries are\non-demand and presumably infrequent, this pay-per-query model is the most cost-effective option.\nSimplicity: Athena allows direct querying of JSON data stored in S3 using standard SQL. The log format is\nalready compatible.\nLet's examine why the other options are less ideal:\nA. Amazon Redshift: Redshift requires setting up and managing a data warehouse cluster. This involves\nprovisioning resources, handling scaling, and performing ETL (Extract, Transform, Load) to move the JSON\ndata from S3 into Redshift. This adds significant operational overhead and cost compared to Athena.\nB. Amazon CloudWatch Logs: CloudWatch Logs are best suited for real-time monitoring and centralized\nlogging, not for complex analytical queries. While CloudWatch Logs Insights exists, its query language isn't\nSQL, and it's not designed for ad-hoc analysis of JSON files stored elsewhere (like S3). Migrating the logs\nwould also be required.\nD. AWS Glue and Amazon EMR: This solution is an overkill. AWS Glue is for ETL and data cataloging, and EMR\nis for big data processing using frameworks like Spark. While suitable for very large and complex data\nanalysis scenarios, it introduces unnecessary complexity and operational overhead for the stated\nrequirements of simple, on-demand queries. It also has high cost compared to using AWS Athena.\nTherefore, Athena aligns best with the requirement of minimal operational overhead and allows simple SQL\nqueries on JSON logs stored in S3, making it the superior choice.\nAuthoritative Links:\nAmazon Athena: https://aws.amazon.com/athena/\nAmazon S3: https://aws.amazon.com/s3/",
    "links": [
      "https://aws.amazon.com/athena/",
      "https://aws.amazon.com/s3/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses AWS Organizations to manage multiple AWS accounts for different departments. The\nmanagement account has an Amazon S3 bucket that contains project reports. The company wants to limit access\nto this S3 bucket to only users of accounts within the organization in AWS Organizations.\nWhich solution meets these requirements with the LEAST amount of operational overhead?",
    "options": {
      "A": "Here's why:",
      "B": "Create an organizational unit (OU) for each department. Add the aws:PrincipalOrgPaths global condition key",
      "C": "Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization, LeaveOrganization, and",
      "D": "When applied, the S3 bucket will only allow"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's why:\nOption A leverages the aws:PrincipalOrgID global condition key in the S3 bucket policy. This condition key\ndirectly references the AWS Organizations organization ID. When applied, the S3 bucket will only allow\naccess from AWS accounts that belong to the specified organization. This approach offers the least\noperational overhead because it's a simple, direct configuration within the S3 bucket policy, automatically\nencompassing all current and future accounts within the organization without requiring ongoing updates.\nOption B, using aws:PrincipalOrgPaths, involves creating organizational units (OUs) for each department. While\nit provides more granular control based on OU membership, it necessitates managing and updating the S3\nbucket policy whenever OUs change or accounts are moved, increasing operational overhead. It's unnecessary\ncomplexity for the stated requirement of granting access to all accounts within the entire organization.\nOption C, using CloudTrail to monitor organizational changes and then updating the S3 bucket policy, is\nunnecessarily complex and creates significant operational overhead. It requires implementing a custom\nsolution to react to CloudTrail events and programmatically modify the S3 bucket policy, which is prone to\nerrors and maintenance issues. It is also not a real-time mechanism; there would be a delay between the\norganizational event and the policy update.\nOption D, tagging users and using aws:PrincipalTag, is suitable for controlling access based on individual user\nattributes, not organizational membership. Applying and managing tags for each user across multiple\naccounts within the organization adds significant operational overhead and is not the appropriate tool for this\nspecific requirement. Furthermore, tagging users across multiple accounts and keeping those tags consistent\nintroduces administrative challenges.\nTherefore, option A provides the most efficient and least complex solution for limiting S3 bucket access to\nonly users of accounts within the organization, by directly referencing the organization ID in the S3 bucket\npolicy, minimizing the manual intervention and operational overhead.\nRelevant links for further research:\nAWS Organizations Condition Keys\nControlling Access to S3 Buckets\nAWS Organizations Overview",
    "links": []
  },
  {
    "question": "CertyIQ\nAn application runs on an Amazon EC2 instance in a VP",
    "options": {
      "C": "It is overkill for this",
      "A": "Create a gateway VPC endpoint to the S3 bucket.",
      "B": "Stream the logs to Amazon CloudWatch Logs. Export the logs to the S3 bucket: While CloudWatch Logs",
      "D": "Create an Amazon API Gateway API with a private link to access the S3 endpoint: This involves creating"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Create a gateway VPC endpoint to the S3 bucket.\nHere's why:\nGateway VPC Endpoints for S3: Gateway VPC endpoints provide private connectivity between your VPC and\nS3 without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection.\nThis ensures that traffic between your EC2 instance and S3 remains within the AWS network.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html\nHow it works: When you create a gateway VPC endpoint for S3, a route is automatically added to your VPC's\nroute table. This route directs traffic destined for S3 to the endpoint instead of the internet. The EC2 instance,\nusing its IAM role permissions to access S3, can now reach the S3 bucket privately.\nLet's analyze the other options:\nB. Stream the logs to Amazon CloudWatch Logs. Export the logs to the S3 bucket: While CloudWatch Logs\nis useful for centralized logging, exporting logs from CloudWatch Logs to S3 still requires connectivity to S3.\nIt doesn't inherently solve the private connectivity requirement. Furthermore, it adds unnecessary complexity\nand cost.\nC. Create an instance profile on Amazon EC2 to allow S3 access: An instance profile (IAM role) grants\npermissions to the EC2 instance to access S3, but it doesn't establish private network connectivity. The EC2\ninstance still needs a way to reach S3 over the network, and without a gateway endpoint, it would need\ninternet access.\nD. Create an Amazon API Gateway API with a private link to access the S3 endpoint: This involves creating\nan API Gateway and configuring it to use a VPC endpoint service (PrivateLink). While PrivateLink can provide\nprivate connectivity, it is typically used for exposing services running within a VPC to other VPCs or on-\npremises networks, not for a simple EC2 to S3 communication within the same VPC. It is overkill for this\nscenario and adds significant complexity. Additionally, API Gateway adds latency and cost where the simple\ngateway endpoint of option A suffices.",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is hosting a web application on AWS using a single Amazon EC2 instance that stores user-uploaded\ndocuments in an Amazon EBS volume. For better scalability and availability, the company duplicated the\narchitecture and created a second EC2 instance and EBS volume in another Availability Zone, placing both behind\nan Application Load Balancer. After completing this change, users reported that, each time they refreshed the\nwebsite, they could see one subset of their documents or the other, but never all of the documents at the same\ntime.\nWhat should a solutions architect propose to ensure users see all of their documents at once?",
    "options": {
      "A": "Copy the data so both EBS volumes contain all the documents",
      "B": "Configure the Application Load Balancer to direct a user to the server with the documents",
      "C": "Copy the data from both EBS volumes to Amazon EFS. Modify the application to save new documents to",
      "D": "Configure the Application Load Balancer to send the request to both servers. Return each document from"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it addresses the core issue of data consistency across multiple instances.\nThe users are experiencing inconsistent document views because each EC2 instance has a separate EBS\nvolume, and data isn't synchronized between them.\nOption A, copying data between EBS volumes, is a short-term fix that becomes increasingly difficult to\nmanage as the data grows and changes. It doesn't provide a scalable or reliable long-term solution. Consider\nthe overhead involved in constantly syncing large EBS volumes.\nOption B, using the Application Load Balancer to direct users to the server containing the documents, isn't\nfeasible. The load balancer isn't aware of which server holds which documents. It would require complex\nsession management based on document ownership, which is error-prone and inefficient.\nOption D, sending requests to both servers simultaneously, won't resolve the issue. It might even exacerbate\nthe problem by showing fragmented document sets more frequently. There would be significant overhead in\nmerging results from multiple servers and the application would need to manage this complexity.\nOption C, migrating to Amazon EFS, provides a centralized, shared file system accessible by both EC2\ninstances. EFS ensures data consistency and allows both instances to see the same set of documents. By\ncopying the existing data to EFS and modifying the application to use EFS for storage, all users will access\nthe same data, resolving the inconsistency. EFS is designed for this exact scenario - providing shared storage\nfor multiple compute instances.\nAmazon EFS is well-suited for web applications that require persistent, shared storage. It eliminates the need\nfor data replication and synchronization across instances. It also offers scalability and performance to\naccommodate growing data needs.\nTherefore, migrating to Amazon EFS is the most effective and scalable solution for ensuring users see all their\ndocuments simultaneously.\nFurther reading:\nAmazon EFS: https://aws.amazon.com/efs/\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
    "links": [
      "https://aws.amazon.com/efs/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses NFS to store large video files in on-premises network attached storage. Each video file ranges in\nsize from 1 MB to 500 G",
    "options": {
      "B": "Create an AWS Snowball Edge job. Receive a Snowball Edge device on premises. Use the Snowball Edge",
      "A": "Create an S3 bucket. Create an IAM role that has permissions to write to the S3 bucket. Use the AWS CLI to",
      "C": "Deploy an S3 File Gateway on premises. Create a public service endpoint to connect to the S3 File Gateway.",
      "D": "Set up an AWS Direct Connect connection between the on-premises network and AWS. Deploy an S3 File"
    },
    "answer": "B",
    "explanation": "The correct solution is B: Using AWS Snowball Edge.\nHere's a detailed justification:\nThe key requirements are migrating 70 TB of video files to S3 quickly while minimizing network bandwidth\nusage.\nOption A (AWS CLI): Copying 70 TB over the internet using the AWS CLI would be extremely slow and\nconsume a significant amount of network bandwidth. This contradicts the requirements.\nOption B (AWS Snowball Edge): AWS Snowball Edge is designed for transferring large amounts of data\noffline. AWS ships a physical device (Snowball Edge) to the company. The company then copies the data onto\nthe device locally. Once the transfer is complete, the device is shipped back to AWS, where the data is\nuploaded to S3. This avoids network bandwidth usage and enables a much faster transfer compared to online\nmethods for this volume of data. The 70 TB size falls within the typical Snowball Edge capacity. This aligns\nperfectly with the requirements of minimizing bandwidth and ensuring a swift transfer.\nOption C (S3 File Gateway with Public Service Endpoint): S3 File Gateway would cache the data before\nuploading it to S3. While it provides an NFS interface, it still requires transferring the data over the internet,\nwhich contradicts the requirement of minimizing network bandwidth. A public service endpoint still means\ndata traversing the public internet. Furthermore, introducing File Gateway for a one-time migration adds\nunnecessary complexity.\nOption D (S3 File Gateway with AWS Direct Connect): While Direct Connect provides a dedicated, faster\nconnection to AWS, setting it up solely for a one-time migration is an overkill and adds significant cost and\ncomplexity. It doesn't eliminate the need to transfer 70 TB of data over a network.\nTherefore, AWS Snowball Edge is the optimal solution because it directly addresses the constraints of large\ndata volume and limited network bandwidth. It is significantly faster than transferring over the internet and\navoids ongoing network usage.\nSupporting concepts and links:\nAWS Snowball Edge: AWS Snowball Edge is a data migration and edge computing device that comes in\nvarious configurations, including Storage Optimized. It allows customers to transfer large amounts of data\ninto and out of AWS without relying on network connectivity.\nhttps://aws.amazon.com/snowball/\nData Migration to AWS: AWS provides several services for data migration, each suitable for different\nscenarios. Snowball Edge is typically used for large-scale migrations when network bandwidth is limited.\nhttps://aws.amazon.com/migration/\nFinal Answer: The final answer is $\\boxed B $",
    "links": [
      "https://aws.amazon.com/snowball/",
      "https://aws.amazon.com/migration/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an application that ingests incoming messages. Dozens of other applications and microservices\nthen quickly consume these messages. The number of messages varies drastically and sometimes increases\nsuddenly to 100,000 each second. The company wants to decouple the solution and increase scalability.\nWhich solution meets these requirements?",
    "options": {
      "A": "Persist the messages to Amazon Kinesis Data Analytics. Configure the consumer applications to read and",
      "B": "Configure the consumer applications to read from",
      "C": "Write the messages to Amazon Kinesis Data Streams with a single shard. Use an AWS Lambda function to",
      "D": "Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with multiple Amazon"
    },
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the best solution, along with supporting concepts and links:\nThe scenario demands a highly scalable and decoupled message ingestion and consumption system. Let's\nanalyze why option D, using Amazon SNS and SQS, best fulfills these requirements.\nOption D (SNS and SQS) utilizes the publish-subscribe pattern offered by SNS and the message queuing\ncapabilities of SQS. The ingestion application publishes messages to an SNS topic. Multiple SQS queues\nsubscribe to this topic. Each consumer application then pulls messages from its own dedicated SQS queue.\nThis provides excellent decoupling because the ingestion application is unaware of the consumers, and the\nconsumers are isolated from each other. This isolation is crucial for scalability; if one consumer fails or\nbecomes overloaded, it doesn't impact the others or the ingestion process. SQS provides buffering for\nmessages, preventing data loss during surges.\nSNS is designed for high-throughput, enabling it to handle the peak load of 100,000 messages per second.\nSQS, being a fully managed queuing service, automatically scales to handle the message volume. The\ncombination ensures messages are reliably delivered to all subscribed consumers without overwhelming\nthem. SQS also facilitates asynchronous processing, allowing consumers to process messages at their own\npace.\nNow, let's look at why the other options are less suitable.\nOption A (Kinesis Data Analytics) is designed for real-time data processing and analytics, not primarily for\ndecoupling and distribution to multiple consumers. While it can process messages, it's not as efficient for\nsimply fanning them out to many downstream applications.\nOption B (EC2 Auto Scaling) addresses the scaling of the ingestion application, but not the decoupling or\nscaling of the message delivery to consumers. It doesn't solve the problem of distributing messages\neffectively to multiple, independent consumers.\nOption C (Kinesis Data Streams and DynamoDB) introduces unnecessary complexity. Kinesis Data Streams\ncan handle high throughput, but using a single shard would create a bottleneck. DynamoDB as an\nintermediary storage adds latency and complexity compared to the direct delivery from SQS. Furthermore,\npolling DynamoDB would not scale nearly as efficiently as SQS queues.\nIn summary, option D is the most appropriate choice because it directly addresses the requirements for\ndecoupling, scalability, and reliable message delivery to multiple consumers using services specifically\ndesigned for these purposes.\nSupporting Links:\nAmazon SNS: https://aws.amazon.com/sns/\nAmazon SQS: https://aws.amazon.com/sqs/\nDecoupled Architecture on AWS: https://aws.amazon.com/solutions/guidance/decoupling-applications-aws/",
    "links": [
      "https://aws.amazon.com/sns/",
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/solutions/guidance/decoupling-applications-aws/"
    ]
  },
  {
    "question": "CertyIQ\nA company is migrating a distributed application to AWS. The application serves variable workloads. The legacy\nplatform consists of a primary server that coordinates jobs across multiple compute nodes. The company wants to\nmodernize the application with a solution that maximizes resiliency and scalability.\nHow should a solutions architect design the architecture to meet these requirements?",
    "options": {
      "A": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement",
      "B": "Here's why:",
      "C": "Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an",
      "D": "Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Here's why:\nThe problem statement emphasizes resiliency, scalability, and variable workloads. Option B best addresses\nthese requirements.\nAmazon SQS: Using SQS decouples the primary server (job coordinator) from the compute nodes. This\npromotes resiliency. If the primary server fails, the messages in the queue persist, and processing can resume\nwhen a new primary server is available. https://aws.amazon.com/sqs/\nEC2 Auto Scaling Group: Placing the compute nodes in an Auto Scaling group provides scalability and high\navailability. Auto Scaling automatically adjusts the number of EC2 instances based on demand. This ensures\nthat the application can handle variable workloads efficiently. https://aws.amazon.com/autoscaling/\nScaling based on Queue Size: Configuring Auto Scaling to scale based on the size of the SQS queue is\ncrucial. As the queue grows (more jobs are waiting), Auto Scaling will launch more EC2 instances to process\nthe jobs. Conversely, if the queue shrinks, Auto Scaling can terminate instances to reduce costs. This reactive,\nqueue-driven scaling aligns directly with the variable workload requirement.\nOption A is incorrect because scheduled scaling doesn't react to real-time workload changes. It scales\ninstances at predefined times regardless of the actual queue size. While useful for predictable workload\npatterns, it's less effective for variable workloads.\nOption C is incorrect because CloudTrail is an auditing service, not a job queue. Furthermore, scaling based on\nthe primary server's load doesn't directly reflect the number of waiting jobs. If the primary server is\noverloaded, that is a symptom of a problem, not the core driver for scaling compute nodes.\nOption D is incorrect because EventBridge (formerly CloudWatch Events) is typically used for event-driven\narchitectures, not as a direct queue for jobs in this scenario. Also, scaling based on the load of the compute\nnodes becomes a lagging metric. The queue length provides earlier indication that scaling is needed.\nFurthermore, managing the primary server within the auto-scaling group might be more complex than\nnecessary and is less resilient compared to the option leveraging SQS.",
    "links": [
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/autoscaling/"
    ]
  },
  {
    "question": "CertyIQ\nA company is running an SMB file server in its data center. The file server stores large files that are accessed\nfrequently for the first few days after the files are created. After 7 days the files are rarely accessed.\nThe total data size is increasing and is close to the company's total storage capacity. A solutions architect must\nincrease the company's available storage space without losing low-latency access to the most recently accessed\nfiles. The solutions architect must also provide file lifecycle management to avoid future storage issues.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.",
      "B": "Create an Amazon S3 File Gateway to extend the company's storage space. Create an S3 Lifecycle policy to",
      "C": "Create an Amazon FSx for Windows File Server file system to extend the company's storage space.",
      "D": "Install a utility on each user's computer to access Amazon S3. Create an S3 Lifecycle policy to transition the"
    },
    "answer": "B",
    "explanation": "The correct solution is B, which involves using Amazon S3 File Gateway and S3 Lifecycle policies. Here's why:\nS3 File Gateway: This service provides a seamless way to extend on-premises file storage to Amazon S3\nwithout requiring significant application changes. It presents a local file system interface to the on-premises\nSMB server, which then transparently caches frequently accessed data locally while storing the full dataset\nin S3. This addresses the requirement for low-latency access to recent files.\nhttps://aws.amazon.com/storagegateway/file-gateway/\nS3 Lifecycle Policies: These policies automate the process of moving less frequently accessed data to lower-\ncost storage tiers such as S3 Glacier Deep Archive after a specified period (7 days in this case). This\naddresses the requirement for file lifecycle management and helps to reduce storage costs without deleting\nthe data. S3 Glacier Deep Archive is suitable for long-term archival where retrieval times of several hours are\nacceptable. https://aws.amazon.com/s3/storage-classes/\nOption A is incorrect because AWS DataSync is primarily a data migration tool, not a continuous storage\nextension solution. It would require ongoing manual or scripted execution to move files after 7 days, which is\nless efficient than a managed solution like S3 File Gateway with lifecycle policies.\nOption C is not ideal because Amazon FSx for Windows File Server is a fully managed Windows file server in\nthe cloud. While it provides scalable storage, it doesn't directly address the need to seamlessly integrate with\nthe existing on-premises SMB server and leverage cheaper archival storage. It would be better suited as a\ncomplete replacement to the on-premise server.\nOption D is incorrect because it involves manually installing a utility on each user's computer. It is complex to\nmanage and prone to user error. Additionally, while S3 Lifecycle policies are used, S3 Glacier Flexible\nRetrieval is a more expensive storage option than Glacier Deep Archive for archival purposes. This is not an\nideal solution.\nIn summary, option B provides the most cost-effective and manageable solution for extending on-premises\nfile storage to the cloud, maintaining low-latency access to recent files, and implementing automated\nlifecycle management.",
    "links": [
      "https://aws.amazon.com/storagegateway/file-gateway/",
      "https://aws.amazon.com/s3/storage-classes/"
    ]
  },
  {
    "question": "CertyIQ\nA company is building an ecommerce web application on AWS. The application sends information about new\norders to an Amazon API Gateway REST API to process. The company wants to ensure that orders are processed in\nthe order that they are received.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Use an API Gateway integration to publish a message to an Amazon Simple Notification Service (Amazon",
      "B": "Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO",
      "C": "Use an API Gateway authorizer to block any requests while the application processes an order.",
      "D": "Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS)"
    },
    "answer": "B",
    "explanation": "The correct answer is B because it directly addresses the requirement of processing orders in the order they\nare received. Here's why:\nFIFO (First-In, First-Out) Queues: Amazon SQS FIFO queues guarantee that messages are retrieved from the\nqueue in the exact order they were placed. This is crucial for order processing, where the sequence of events\nmatters. https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-\nqueues.html\nAPI Gateway Integration: Integrating API Gateway with SQS allows you to seamlessly send order data\nreceived through the API directly into the FIFO queue.\nLambda Function Invocation: Configuring the SQS queue to invoke a Lambda function on message arrival\nensures that each order is automatically processed as it becomes available in the queue.\nOption A is incorrect because Amazon SNS is a publish/subscribe service, not a queuing service. It's designed\nfor broadcasting messages to multiple subscribers simultaneously, not for preserving the order of messages.\nThis makes SNS unsuitable for ordered processing.\nOption C is incorrect because API Gateway authorizers are used for authentication and authorization, not for\ncontrolling the processing order of requests. Blocking requests with an authorizer might prevent processing\naltogether, but won't guarantee correct sequencing.\nOption D is incorrect because Amazon SQS standard queues do not guarantee message order. While they aim\nfor best-effort ordering, messages can sometimes be delivered out of sequence, which violates the stated\nrequirement. https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-\nqueues.html\nIn summary, using an API Gateway integrated with an SQS FIFO queue and a Lambda function ensures that\norder information is captured in the correct sequence and then processed sequentially, meeting the\napplication's specific requirements. This approach leverages the ordered message delivery capability of FIFO\nqueues to maintain data integrity within the ecommerce application's workflow.",
    "links": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-"
    ]
  },
  {
    "question": "CertyIQ\nA company has an application that runs on Amazon EC2 instances and uses an Amazon Aurora database. The EC2\ninstances connect to the database by using user names and passwords that are stored locally in a file. The\ncompany wants to minimize the operational overhead of credential management.\nWhat should a solutions architect do to accomplish this goal?",
    "options": {
      "A": "Use AWS Secrets Manager. Turn on automatic rotation.",
      "B": "Use AWS Systems Manager Parameter Store. Turn on automatic rotation.",
      "C": "Create an Amazon S3 bucket to store objects that are encrypted with an AWS Key Management Service",
      "D": "Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume for each EC2 instance. Attach the"
    },
    "answer": "A",
    "explanation": "The best solution is to use AWS Secrets Manager with automatic rotation. Here's why:\nSecrets Manager Purpose: AWS Secrets Manager is specifically designed for securely storing and managing\nsecrets, like database credentials. It eliminates the need to hardcode or store secrets within application code\nor configuration files.\nReduced Operational Overhead: By using Secrets Manager, the company centralizes credential management,\nwhich simplifies the process of updating, rotating, and auditing secrets. This reduces the operational burden\nassociated with managing credentials manually on each EC2 instance.\nAutomatic Rotation: Secrets Manager's automatic rotation feature automates the process of changing\ncredentials regularly without application downtime. This enhances security by limiting the window of\nopportunity for compromised credentials to be exploited.\nIAM Integration: Secrets Manager integrates with AWS Identity and Access Management (IAM) to control\naccess to secrets, ensuring that only authorized EC2 instances or roles can retrieve the database credentials.\nAuditing and Monitoring: Secrets Manager provides auditing capabilities through AWS CloudTrail, allowing\nthe company to track secret access and modifications.\nWhy other options are less suitable:\nAWS Systems Manager Parameter Store: While Parameter Store can store sensitive information, it's\nprimarily designed for storing configuration data and not for managing secrets with rotation capabilities.\nWhile Parameter Store offers SecureString parameters, it lacks the robust rotation features of Secrets\nManager.\nAmazon S3: Storing credentials in an S3 bucket, even if encrypted, is not the best practice for secret\nmanagement. It requires custom logic to retrieve and manage the credentials, which increases operational\noverhead and doesn't provide automatic rotation.\nEncrypted EBS Volume: Storing credentials on encrypted EBS volumes attached to each EC2 instance offers\nsome level of security, but it doesn't centralize credential management or provide automatic rotation\ncapabilities. It also requires managing separate volumes for each instance.\nIn summary, AWS Secrets Manager with automatic rotation is the most efficient and secure way to manage\ndatabase credentials for EC2 instances connecting to Aurora, minimizing operational overhead and enhancing\nsecurity posture.\nAuthoritative Links:\nAWS Secrets Manager: https://aws.amazon.com/secrets-manager/\nRotate AWS Secrets Manager secrets:\nhttps://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html",
    "links": [
      "https://aws.amazon.com/secrets-manager/",
      "https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html"
    ]
  },
  {
    "question": "CertyIQ\nA global company hosts its web application on Amazon EC2 instances behind an Application Load Balancer (ALB).\nThe web application has static data and dynamic data. The company stores its static data in an Amazon S3 bucket.\nThe company wants to improve performance and reduce latency for the static data and dynamic data. The\ncompany is using its own domain name registered with Amazon Route 53.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Create an Amazon CloudFront distribution that has the S3 bucket and the ALB as origins. Configure Route 53",
      "B": "This allows CloudFront to cache the responses when possible, further improving",
      "C": "Create an Amazon CloudFront distribution that has the S3 bucket as an origin. Create an AWS Global",
      "D": "Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator"
    },
    "answer": "A",
    "explanation": "The best solution is to use CloudFront with both the S3 bucket (for static content) and the ALB (for dynamic\ncontent) as origins and then point Route 53 to the CloudFront distribution. This approach efficiently leverages\nCloudFront's caching capabilities to reduce latency and improve performance for both types of content\nglobally.\nHere's why other options are less optimal:\nOption B & D: While Global Accelerator can improve performance, it primarily focuses on TCP and UDP traffic,\nnot HTTP/HTTPS, which are used by web applications. Also, Global Accelerator is an overkill for static content\nserved from S3 and adds unnecessary complexity. Global Accelerator is most useful for improving the\nperformance of applications with users distributed globally where network congestion or routing issues can\nimpact performance. Furthermore, managing separate domain names, as suggested in option D, would\ncomplicate the web application's architecture and potentially lead to inconsistent user experience.\nOption C: Using Global Accelerator as a front for CloudFront is not a typical use case. CloudFront is already a\nglobal content delivery network (CDN) designed to optimize content delivery. Global Accelerator's benefits\nwould be redundant.\nOption A directly addresses the requirements by:\n1. Improving Performance and Reducing Latency: CloudFront caches static content from the S3\nbucket at edge locations globally, bringing data closer to users and reducing latency.\n2. Serving Dynamic Content: CloudFront can also be configured to forward requests for dynamic\ncontent to the ALB. This allows CloudFront to cache the responses when possible, further improving\nperformance.\n3. Using Existing Domain Name: Route 53 is configured to route traffic to the CloudFront distribution,\nwhich means you can continue using your existing domain name.\nAuthoritative Links:\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nAmazon S3: https://aws.amazon.com/s3/\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\nAWS Global Accelerator: https://aws.amazon.com/global-accelerator/\nAmazon Route 53: https://aws.amazon.com/route53/",
    "links": [
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
      "https://aws.amazon.com/global-accelerator/",
      "https://aws.amazon.com/route53/"
    ]
  },
  {
    "question": "CertyIQ\nA company performs monthly maintenance on its AWS infrastructure. During these maintenance activities, the\ncompany needs to rotate the credentials for its Amazon RDS for MySQL databases across multiple AWS Regions.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Store the credentials as secrets in AWS Secrets Manager. Use multi-Region secret replication for the",
      "B": "Use the RDS API to rotate the secrets.",
      "C": "Store the credentials in an Amazon S3 bucket that has server-side encryption (SSE) enabled. Use Amazon",
      "D": "Encrypt the credentials as secrets by using AWS Key Management Service (AWS KMS) multi-Region"
    },
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A is the best solution for rotating RDS for MySQL database\ncredentials across multiple AWS Regions with the least operational overhead:\nOption A leverages AWS Secrets Manager's built-in capabilities for secret management and rotation across\nRegions. Secrets Manager is designed specifically for storing and managing secrets like database\ncredentials. Its multi-Region secret replication feature automatically replicates secrets to specified Regions,\nensuring consistency. The automatic rotation feature allows you to define a rotation schedule, where Secrets\nManager handles the process of generating new credentials and updating them in the database, significantly\nreducing manual intervention and operational overhead.\nOption B is incorrect because AWS Systems Manager Parameter Store, while capable of storing secrets,\ndoesn't natively support multi-Region secret replication or automated rotation in the same streamlined\nmanner as Secrets Manager. Implementing similar functionality with Parameter Store would require custom\nscripting and automation, increasing operational complexity.\nOption C, using S3 and Lambda, involves more manual setup and management. While S3 can store encrypted\ndata, it doesn't inherently offer secret rotation capabilities. The Lambda function would need to be custom-\ncoded to generate new credentials, update the database, and update the secret in S3, adding to operational\noverhead and potential error points.\nOption D, using DynamoDB and Lambda with KMS encryption, is the most complex. DynamoDB global tables\nprovide replication, but managing the encryption with KMS and implementing secret rotation logic within\nLambda, including calling the RDS API, adds significant overhead. KMS is mainly for encryption key\nmanagement, not secret rotation.\nTherefore, option A provides the most straightforward and automated solution for rotating RDS database\ncredentials across multiple Regions due to Secrets Manager's features like built-in rotation and multi-region\nreplication, minimizing operational overhead.\nRelevant Links:\nAWS Secrets Manager: https://aws.amazon.com/secrets-manager/\nSecrets Manager Multi-Region Secrets:\nhttps://docs.aws.amazon.com/secretsmanager/latest/userguide/replication.html\nSecrets Manager Rotation: https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-\nsecrets.html",
    "links": [
      "https://aws.amazon.com/secrets-manager/",
      "https://docs.aws.amazon.com/secretsmanager/latest/userguide/replication.html",
      "https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an ecommerce application on Amazon EC2 instances behind an Application Load Balancer. The\ninstances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group\nscales based on CPU utilization metrics. The ecommerce application stores the transaction data in a MySQL 8.0\ndatabase that is hosted on a large EC2 instance.\nThe database's performance degrades quickly as application load increases. The application handles more read\nrequests than write transactions. The company wants a solution that will automatically scale the database to meet\nthe demand of unpredictable read workloads while maintaining high availability.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Amazon Redshift is a data warehouse service optimized for analytical workloads, not transactional",
      "B": "Amazon RDS with a Single-AZ deployment does not guarantee high availability. Although reader instances",
      "C": "Use Amazon Aurora with a Multi-AZ deployment. Configure Aurora Auto Scaling with Aurora Replicas.",
      "D": "Amazon ElastiCache for Memcached is an in-memory caching service, not a database. It is not suitable for"
    },
    "answer": "C",
    "explanation": "The correct solution is to use Amazon Aurora with a Multi-AZ deployment and configure Aurora Auto Scaling\nwith Aurora Replicas. Here's why:\nScalability: Aurora Auto Scaling automatically adds or removes Aurora Replicas in response to changes in\napplication load. This addresses the need to scale the database to meet unpredictable read workloads.\nRead-Heavy Workloads: Aurora Replicas are designed for read operations. By directing read traffic to these\nreplicas, the primary database instance is relieved, improving overall performance.\nHigh Availability: A Multi-AZ deployment ensures that the database remains available even if there is an\ninfrastructure failure in one Availability Zone. Aurora automatically fails over to a replica in another\nAvailability Zone.\nAurora's Compatibility: Aurora is compatible with MySQL and PostgreSQL, allowing for easier migration from\nthe existing MySQL database.\nOther options are suboptimal because:\nA. Amazon Redshift is a data warehouse service optimized for analytical workloads, not transactional\nworkloads.\nB. Amazon RDS with a Single-AZ deployment does not guarantee high availability. Although reader instances\ncan be added, a single-AZ configuration for the primary instance presents a single point of failure.\nD. Amazon ElastiCache for Memcached is an in-memory caching service, not a database. It is not suitable for\nstoring transactional data.\nSupporting Links:\nAmazon Aurora: https://aws.amazon.com/rds/aurora/\nAurora Auto Scaling:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Performance.html\nAurora Replicas:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.ReadReplicas.html\nAmazon RDS Multi-AZ: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html",
    "links": [
      "https://aws.amazon.com/rds/aurora/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Performance.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.ReadReplicas.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html"
    ]
  },
  {
    "question": "CertyIQ\nA company recently migrated to AWS and wants to implement a solution to protect the traffic that flows in and out\nof the production VP",
    "options": {
      "C": "Authoritative links:",
      "A": "Use Amazon GuardDuty for traffic inspection and traffic filtering in the production VPC.",
      "B": "Use Traffic Mirroring to mirror traffic from the production VPC for traffic inspection and filtering."
    },
    "answer": "C",
    "explanation": "The correct answer is C, using AWS Network Firewall. Here's a detailed justification:\nThe requirement is to replicate on-premises traffic inspection and filtering functionalities within AWS for a\nproduction VPC. Amazon GuardDuty (A) is a threat detection service that monitors for malicious activity and\nunauthorized behavior but doesn't provide inline traffic inspection and filtering based on custom rules. Traffic\nMirroring (B) duplicates network traffic for analysis, but it doesn't provide filtering capabilities itself. You\nwould need a separate inspection appliance to receive and process the mirrored traffic, adding complexity\nand cost. AWS Firewall Manager (D) centrally manages firewall rules across multiple accounts and VPCs; it\ndoesn't inspect traffic directly or provide inspection capabilities. It works with AWS WAF, AWS Shield\nAdvanced, and AWS Network Firewall.\nAWS Network Firewall (C), on the other hand, is a managed network firewall service that provides essential\nprotection for your VPCs. It allows you to define rules for inspecting and filtering network traffic based on\ncriteria such as source and destination IP addresses, ports, and protocols. This precisely aligns with the\ncompany's need to implement traffic flow inspection and filtering, mimicking the capabilities of their on-\npremises inspection server. Network Firewall operates at the network layer (Layer 3 and Layer 4) and\napplication layer (Layer 7), allowing for deep packet inspection (DPI) and fine-grained control. It also scales\nautomatically to meet fluctuating traffic demands without requiring manual intervention.\nTherefore, AWS Network Firewall is the best fit for the company's requirements because it provides a fully\nmanaged, scalable, and customizable solution for traffic inspection and filtering within the production VPC.\nAuthoritative links:\nAWS Network Firewall: https://aws.amazon.com/network-firewall/\nAWS Network Firewall Documentation: https://docs.aws.amazon.com/network-\nfirewall/latest/developerguide/what-is-aws-network-firewall.html",
    "links": [
      "https://aws.amazon.com/network-firewall/",
      "https://docs.aws.amazon.com/network-"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon RDS for\nPostgreSQL. The company needs a reporting solution that provides data visualization and includes all the data\nsources within the data lake. Only the company's management team should have full access to all the\nvisualizations. The rest of the company should have only limited access.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish",
      "B": "Let's break down why:",
      "C": "Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract, transform,",
      "D": "Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated Query to"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Let's break down why:\nRequirement 1: Data Visualization and Data Lake Integration: The company needs a data visualization tool\nthat can ingest data from both S3 (data lake) and RDS for PostgreSQL. Amazon QuickSight excels at this. It\ncan directly connect to both S3 (using manifests or direct paths) and RDS databases. Options C and D\nprimarily focus on AWS Glue and Athena, which are more suited for data processing and querying, not direct\nvisualization.\nRequirement 2: Different Access Levels (Management vs. Rest of Company): QuickSight has a robust\nsharing mechanism. Dashboards can be shared with individual users or groups (created within QuickSight\nitself), allowing for granular control over who sees what. IAM roles (option A) are primarily used for controlling\naccess to AWS resources, not for fine-grained access to dashboards within QuickSight. S3 bucket policies\n(options C and D) would only control access to the underlying report files, not the dynamic visualizations.\nQuickSight Analysis, Datasets, and Dashboards: QuickSight works by creating an analysis from one or more\ndatasets. The datasets are based on the connected data sources. The analysis is then used to build one or\nmore dashboards. Publishing the dashboards makes them available to others.\nDirect Connection to Data Sources: QuickSight's ability to directly connect to the data sources (S3 and RDS)\neliminates the need for complex ETL processes just for reporting (which would be implied by option C). While\nETL might be part of the broader data lake architecture, it's not the primary function for a reporting solution.\nAthena Federated Query is overkill: Athena Federated Query (option D) allows querying across different data\nsources. Although possible, using QuickSight's direct connectors is a simpler and more suitable approach for\nthis scenario.\nIn summary, option B is the most straightforward solution, using QuickSight's built-in capabilities to connect\nto data sources, create visualizations, and share dashboards with user-level and group-level access control.\nAuthoritative Links:\nAmazon QuickSight: https://aws.amazon.com/quicksight/\nConnecting to Data Sources in QuickSight: https://docs.aws.amazon.com/quicksight/latest/user/data-\nsources.html\nSharing QuickSight Dashboards: https://docs.aws.amazon.com/quicksight/latest/user/sharing-a-\ndashboard.html",
    "links": [
      "https://aws.amazon.com/quicksight/",
      "https://docs.aws.amazon.com/quicksight/latest/user/data-",
      "https://docs.aws.amazon.com/quicksight/latest/user/sharing-a-"
    ]
  },
  {
    "question": "CertyIQ\nA company is implementing a new business application. The application runs on two Amazon EC2 instances and\nuses an Amazon S3 bucket for document storage. A solutions architect needs to ensure that the EC2 instances can\naccess the S3 bucket.\nWhat should the solutions architect do to meet this requirement?",
    "options": {
      "A": "Create an IAM role that grants access to the S3 bucket. Attach the role to the EC2 instances.",
      "B": "Create an IAM policy that grants access to the S3 bucket. Attach the policy to the EC2 instances.",
      "C": "Create an IAM group that grants access to the S3 bucket. Attach the group to the EC2 instances.",
      "D": "Create an IAM user that grants access to the S3 bucket. Attach the user account to the EC2 instances."
    },
    "answer": "A",
    "explanation": "The correct answer is A: Create an IAM role that grants access to the S3 bucket and attach the role to the EC2\ninstances. Here's why:\nIAM Roles are the recommended approach for granting permissions to AWS services like EC2 to access other\nAWS resources such as S3. A role provides temporary security credentials to the EC2 instances, eliminating\nthe need to store long-term credentials directly on the instances. This enhances security because the\ncredentials are automatically rotated and managed by AWS.\nIAM Policies define the permissions granted. While policies are fundamental, they are attached to IAM\nentities. Attaching an IAM policy directly to an EC2 instance is not a standard practice. Instead, the policy is\nattached to an IAM role, which is then associated with the EC2 instance. The instance assumes the role,\ngaining the permissions defined in the policy.\nIAM Groups are collections of IAM users. While groups simplify permission management for users, they are\nnot designed to be associated with EC2 instances for granting access to resources. Groups are a user-centric\nconcept and not appropriate for service-to-service access control.\nIAM Users represent individuals or applications that interact with AWS. Creating an IAM user and embedding\nits credentials on an EC2 instance is a security risk. These credentials would be long-term and static, making\nthem vulnerable if the instance is compromised. Hardcoding or storing credentials on an EC2 instance is a bad\npractice. IAM Roles provide a more secure and manageable solution.\nTherefore, the most secure and best practice approach is to create an IAM role with the necessary S3 access\npermissions and then attach that role to the EC2 instances. This allows the EC2 instances to assume the role\nand obtain temporary credentials to access the S3 bucket.\nFurther Reading:\nIAM Roles for Amazon EC2\nIAM Policies\nSecurity best practices in IAM",
    "links": []
  },
  {
    "question": "CertyIQ\nAn application development team is designing a microservice that will convert large images to smaller,\ncompressed images. When a user uploads an image through the web interface, the microservice should store the\nimage in an Amazon S3 bucket, process and compress the image with an AWS Lambda function, and store the\nimage in its compressed form in a different S3 bucket.\nA solutions architect needs to design a solution that uses durable, stateless components to process the images\nautomatically.\nWhich combination of actions will meet these requirements? (Choose two.)",
    "options": {
      "A": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the S3 bucket to send a",
      "B": "Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as the",
      "C": "Configure the Lambda function to monitor the S3 bucket for new uploads. When an uploaded image is",
      "D": "Launch an Amazon EC2 instance to monitor an Amazon Simple Queue Service (Amazon SQS) queue. When"
    },
    "answer": "A",
    "explanation": "The chosen solution, using SQS and Lambda triggered by SQS, effectively addresses the requirements for\ndurable, stateless image processing.\nOption A is correct because SQS provides a durable queuing mechanism. When an image is uploaded to S3, a\nnotification is sent to the SQS queue. This ensures that even if the Lambda function is temporarily unavailable,\nthe image processing request will persist in the queue until it can be processed. SQS decouples the S3 upload\nevent from the Lambda function invocation, enabling asynchronous processing. This aligns with the need for a\nstateless and reliable processing pipeline. https://aws.amazon.com/sqs/\nOption B is correct because configuring the Lambda function to consume messages from the SQS queue\ndirectly addresses the need for automated processing. Lambda's SQS trigger automatically polls the queue\nand invokes the function when a message arrives. This eliminates the need for custom polling logic. By\ndeleting the message upon successful processing, the queue is kept clean and ensures that each image is\nprocessed only once. https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\nOption C is incorrect because storing filenames in a text file in memory within the Lambda function introduces\nstatefulness. Lambda functions should be stateless, and relying on in-memory storage for tracking processed\nimages is unreliable, especially with concurrent invocations.\nOption D is incorrect because launching an EC2 instance to monitor SQS adds unnecessary complexity and\ncost. Lambda functions can directly integrate with SQS without requiring an intermediary EC2 instance.\nOption E is incorrect because using SNS for further processing is vague and doesn't directly address the\nrequirement of automated image processing. SNS is a notification service, not an event-driven compute\nservice. The application owner would need to manually trigger the image processing, defeating the\nautomation requirement.",
    "links": [
      "https://aws.amazon.com/sqs/",
      "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has a three-tier web application that is deployed on AWS. The web servers are deployed in a public\nsubnet in a VP",
    "options": {
      "C": "Transit Gateway (TGW): TGW is a network transit hub that connects VPCs and on-premises networks.",
      "A": "Network Load Balancer (NLB): NLBs are designed for load balancing TCP, UDP, and TLS traffic to backend",
      "B": "Application Load Balancer (ALB): ALBs operate at the application layer (HTTP/HTTPS) and are primarily",
      "D": "Deploy a Gateway Load Balancer in the inspection VPC. Create a Gateway Load Balancer endpoint to receive"
    },
    "answer": "D",
    "explanation": "The correct answer is D: Deploy a Gateway Load Balancer in the inspection VPC. Create a Gateway Load\nBalancer endpoint to receive the incoming packets and forward the packets to the appliance.\nHere's a detailed justification:\nGateway Load Balancer (GWLB): A GWLB is specifically designed for inline inspection of network traffic. It\nprovides a single entry point for traffic, scales automatically, and distributes traffic to virtual appliances like\nfirewalls. This is exactly what the scenario requires: inspecting traffic before it reaches the web servers.\nGWLB Endpoint: A GWLB endpoint allows you to seamlessly integrate the GWLB (in the inspection VPC) with\nthe web application's VPC. Traffic destined for the web application's public subnet is redirected to the GWLB\nendpoint.\nTraffic Flow: Incoming traffic from the internet flows to the GWLB endpoint in the application's VPC. The\nendpoint forwards the traffic to the GWLB in the inspection VPC. The GWLB distributes the traffic to the\nfirewall appliance for inspection. After inspection, the appliance sends the traffic back through the GWLB,\nback through the endpoint, and finally to the web servers.\nLeast Operational Overhead: Compared to other options, the GWLB solution offers the least operational\noverhead. It's a managed service, meaning AWS handles scaling, availability, and patching. You don't have to\nmanage routing tables as extensively as with Transit Gateway.\nHere's why the other options are less suitable:\nA. Network Load Balancer (NLB): NLBs are designed for load balancing TCP, UDP, and TLS traffic to backend\ntargets. They are not suitable for inspecting packets at the application layer, or generally for integration with\nvirtual firewalls. NLBs do not have native features to forward traffic for inspection and then back to the\noriginal destination.\nB. Application Load Balancer (ALB): ALBs operate at the application layer (HTTP/HTTPS) and are primarily\nused for load balancing web traffic based on content. They are not intended for routing packets to virtual\nappliances for general-purpose packet inspection.\nC. Transit Gateway (TGW): TGW is a network transit hub that connects VPCs and on-premises networks.\nWhile you could use TGW to route traffic through an inspection VPC, it involves more complex route table\nconfigurations and management. It's overkill and adds unnecessary operational complexity compared to the\nGWLB, which is designed specifically for this purpose.\nAuthoritative Links:\nGateway Load Balancer: https://aws.amazon.com/elasticloadbalancing/gateway-load-balancer/\nAWS Network Firewall Integration with GWLB: https://aws.amazon.com/blogs/networking-and-content-\ndelivery/centralized-inspection-architecture-with-aws-network-firewall-gateway-load-balancer-and-route-\nmanager/",
    "links": [
      "https://aws.amazon.com/elasticloadbalancing/gateway-load-balancer/",
      "https://aws.amazon.com/blogs/networking-and-content-"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to improve its ability to clone large amounts of production data into a test environment in the\nsame AWS Region. The data is stored in Amazon EC2 instances on Amazon Elastic Block Store (Amazon EBS)\nvolumes. Modifications to the cloned data must not affect the production environment. The software that accesses\nthis data requires consistently high I/O performance.\nA solutions architect needs to minimize the time that is required to clone the production data into the test\nenvironment.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Take EBS snapshots of the production EBS volumes. Restore the snapshots onto EC2 instance store volumes",
      "B": "Configure the production EBS volumes to use the EBS Multi-Attach feature. Take EBS snapshots of the",
      "C": "Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the new",
      "D": "Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on the"
    },
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the correct answer, and why other options are incorrect:\nWhy Option D is Correct: \"Take EBS snapshots of the production EBS volumes. Turn on the EBS fast\nsnapshot restore feature on the EBS snapshots. Restore the snapshots into new EBS volumes. Attach the\nnew EBS volumes to EC2 instances in the test environment.\"\nThis solution directly addresses the requirement of minimizing the cloning time while maintaining data\nisolation.\n1. EBS Snapshots for Data Duplication: EBS snapshots are the standard and efficient way to create\npoint-in-time copies of EBS volumes. This ensures a consistent and reliable clone of the production\ndata. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html\n2. Fast Snapshot Restore (FSR) for Speed: FSR significantly reduces the time required to restore EBS\nvolumes from snapshots. Normally, restoring from a snapshot involves lazily loading the data blocks\nas they are accessed. FSR pre-initializes the volume in the background, so it's immediately ready for\nhigh I/O performance upon attachment. This satisfies the \"minimize the time that is required to clone\"\nrequirement. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-\nrestore.html\n3. New EBS Volumes for Isolation: By restoring the snapshots into new EBS volumes, the test\nenvironment operates on independent copies of the data. Modifications in the test environment will\nnot affect the production data, meeting the \"Modifications to the cloned data must not affect the\nproduction environment\" requirement.\n4. Attaching to EC2 Instances: Finally, attaching the new EBS volumes to EC2 instances in the test\nenvironment makes the cloned data accessible to the applications.\nWhy Other Options Are Incorrect:\nOption A: Restore to EC2 Instance Store: EC2 instance store volumes are ephemeral, meaning the data is lost\nwhen the instance is stopped or terminated. This makes them unsuitable for a persistent test environment\nwhere data needs to be retained. Further, instance store volumes aren't suitable for large datasets and\ngenerally have poorer durability.\nOption B: EBS Multi-Attach and Attaching Production Volumes: EBS Multi-Attach allows you to attach a\nsingle EBS volume to multiple EC2 instances within the same Availability Zone. However, directly attaching\nproduction volumes to test instances creates a significant risk of data corruption in the production\nenvironment if the test environment makes unintended modifications. This also requires coordinating access\nand locking which is far from ideal. More over, the question states that modifications to cloned data should\nNOT affect production environment. This choice violates it.\nOption C: Creating Volumes and Restoring Without FSR: While this solution does provide data isolation by\ncreating new EBS volumes, it does not address the requirement of minimizing the cloning time. Without FSR,\nrestoring large EBS volumes from snapshots can take a significant amount of time, especially when high I/O\nperformance is immediately required.\nIn summary, option D provides the best balance of data isolation, cloning speed, and performance for the test\nenvironment, leveraging EBS snapshots and FSR for efficient and safe data duplication.",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-"
    ]
  },
  {
    "question": "CertyIQ\nAn ecommerce company wants to launch a one-deal-a-day website on AWS. Each day will feature exactly one\nproduct on sale for a period of 24 hours. The company wants to be able to handle millions of requests each hour\nwith millisecond latency during peak hours.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Use Amazon S3 to host the full website in different S3 buckets. Add Amazon CloudFront distributions. Set",
      "B": "Answer: D",
      "C": "Migrate the full application to run in containers. Host the containers on Amazon Elastic Kubernetes Service",
      "D": "Use an Amazon S3 bucket to host the website's static content. Deploy an Amazon CloudFront distribution."
    },
    "answer": "D",
    "explanation": "Option D is the most suitable solution because it leverages serverless technologies and static content hosting\nfor optimal scalability, performance, and minimal operational overhead. Hosting the static website content\n(HTML, CSS, JavaScript, images) on Amazon S3 and distributing it via Amazon CloudFront provides global\ncontent delivery with low latency and high availability. CloudFront's caching capabilities further reduce load\non the origin S3 bucket.\nUsing Amazon API Gateway and AWS Lambda for backend APIs enables on-demand execution of code\nwithout managing servers. API Gateway handles request routing, authorization, and throttling, while Lambda\nfunctions execute the business logic. This serverless approach scales automatically with traffic, ensuring\nmillisecond latency during peak hours.\nStoring order data in Amazon DynamoDB is ideal because DynamoDB is a fully managed NoSQL database\nservice that provides consistent, single-digit millisecond latency at any scale. DynamoDB's serverless nature\neliminates the need for database administration tasks.\nOptions A, B, and C involve managing infrastructure (EC2 instances, containers, databases), which increases\noperational overhead. Option A's approach of storing order data directly in S3 isn't suitable for transactional\ndata. Option B utilizes ALBs and RDS for MySQL, requiring instance management and database\nadministration. Option C introduces the complexity of container orchestration with EKS. Therefore, Option D\npresents the least operational overhead while meeting the performance requirements.\nHere are some authoritative links for further research:\nAmazon S3: https://aws.amazon.com/s3/\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nAmazon API Gateway: https://aws.amazon.com/api-gateway/\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon DynamoDB: https://aws.amazon.com/dynamodb/",
    "links": [
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/api-gateway/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/dynamodb/"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is using Amazon S3 to design the storage architecture of a new digital media application. The\nmedia files must be resilient to the loss of an Availability Zone. Some files are accessed frequently while other\nfiles are rarely accessed in an unpredictable pattern. The solutions architect must minimize the costs of storing\nand retrieving the media files.\nWhich storage option meets these requirements?",
    "options": {
      "A": "S3 Standard",
      "B": "S3 Intelligent-Tiering.",
      "C": "S3 Standard-Infrequent Access (S3 Standard-IA)",
      "D": "S3 One Zone-Infrequent Access (S3 One Zone-IA)"
    },
    "answer": "B",
    "explanation": "The correct answer is B. S3 Intelligent-Tiering.\nHere's why:\nResilience to Availability Zone Loss: The application requires resilience to the loss of an Availability Zone.\nThis immediately rules out S3 One Zone-IA (Option D) because it stores data in a single Availability Zone. S3\nStandard, S3 Intelligent-Tiering, and S3 Standard-IA all replicate data across multiple Availability Zones,\nproviding the necessary resilience.\nFrequently and Infrequently Accessed Files: The application has both frequently and infrequently accessed\nfiles with an unpredictable access pattern. S3 Standard-IA (Option C) is suitable for data accessed less\nfrequently, but using it for frequently accessed data would be unnecessarily expensive. S3 Standard (Option\nA) would be suitable for all objects but is the most expensive option for objects that are infrequent to access.\nCost Minimization: S3 Intelligent-Tiering automatically moves data between frequent and infrequent access\ntiers based on access patterns, without any operational overhead or retrieval fees. It delivers automatic cost\nsavings when access patterns change. It monitors access patterns and moves objects that have not been\naccessed for 30 consecutive days to the infrequent access tier and then after another 60 days of inactivity to\nthe archive access tier. This ensures the optimal storage cost for both frequently and infrequently accessed\nfiles.\nBenefits of Intelligent-Tiering: S3 Intelligent-Tiering optimizes storage costs by automatically moving data to\nthe most cost-effective access tier without performance impact or operational overhead. This approach is\nideal when access patterns are unpredictable or change over time.\nIn summary, S3 Intelligent-Tiering balances the requirements of Availability Zone resilience, handling both\nfrequently and infrequently accessed files, and minimizing storage costs, making it the most suitable storage\noption.\nRelevant Links:\nAmazon S3 Storage Classes:\nS3 Intelligent-Tiering:",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is storing backup files by using Amazon S3 Standard storage. The files are accessed frequently for 1\nmonth. However, the files are not accessed after 1 month. The company must keep the files indefinitely.\nWhich storage solution will meet these requirements MOST cost-effectively?",
    "options": {
      "A": "It stores data in a single Availability Zone, making it cheaper, but it's inherently less durable than",
      "B": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep Archive after",
      "C": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Standard-Infrequent",
      "D": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 One Zone-Infrequent"
    },
    "answer": "B",
    "explanation": "The question requires the most cost-effective storage solution for backup files frequently accessed for one\nmonth and then indefinitely archived with infrequent access.\nOption B, creating an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep\nArchive after 1 month, is the most cost-effective solution. S3 Glacier Deep Archive offers the lowest storage\ncost among all S3 storage classes, making it ideal for long-term archival. The lifecycle policy automates the\ntransition, eliminating manual intervention. S3 Standard is used for the first month of frequent access. This\napproach balances initial accessibility with long-term cost optimization for rarely accessed data.\nOption A is less suitable. S3 Intelligent-Tiering automatically moves data between frequent, infrequent, and\narchive access tiers based on usage patterns. While convenient, the cost of monitoring and frequent tier\ntransitions might exceed the cost-effectiveness of directly moving to Glacier Deep Archive after the initial\nactive period. The assumption is that the files are never accessed after the initial month, making Intelligent-\nTiering an unnecessary expense.\nOption C involves transitioning to S3 Standard-IA, which is designed for data accessed less frequently but\nstill requires rapid retrieval. Given that the backup files will likely not be accessed after the first month, using\nStandard-IA is more expensive than Glacier Deep Archive because Standard-IA has higher storage costs.\nOption D uses S3 One Zone-IA, which is even less suitable than Standard-IA, although cheaper than Standard-\nIA. It stores data in a single Availability Zone, making it cheaper, but it's inherently less durable than\nStandard-IA, Standard or Glacier. Because these are backup files the durability provided by the storage is of\nutmost importance, and a loss of data due to a single availability zone failure is unacceptable. Moreover, it's\nmore expensive than Glacier Deep Archive.\nTherefore, transitioning to Glacier Deep Archive through a Lifecycle policy offers the best combination of\ncost-effectiveness and long-term archival needs when data will not be accessed for the foreseeable future\nafter the first month.\nRelevant documentation:\nS3 Glacier Deep Archive: Describes the features and cost benefits of S3 Glacier Deep Archive.\nS3 Lifecycle Management: Explains how to automate transitions between storage classes.",
    "links": []
  },
  {
    "question": "CertyIQ\nA company observes an increase in Amazon EC2 costs in its most recent bill. The billing team notices unwanted\nvertical scaling of instance types for a couple of EC2 instances. A solutions architect needs to create a graph\ncomparing the last 2 months of EC2 costs and perform an in-depth analysis to identify the root cause of the\nvertical scaling.\nHow should the solutions architect generate the information with the LEAST operational overhead?",
    "options": {
      "A": "Use AWS Budgets to create a budget report and compare EC2 costs based on instance types.",
      "B": "Use Cost Explorer's granular filtering feature to perform an in-depth analysis of",
      "C": "Use graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs based on instance",
      "D": "Use AWS Cost and Usage Reports to create a report and send it to an Amazon S3 bucket. Use Amazon"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Use Cost Explorer's granular filtering feature to perform an in-depth analysis of\nEC2 costs based on instance types.\nHere's why:\nCost Explorer is designed specifically for cost analysis and provides a user-friendly interface for exploring\nAWS costs. It allows for granular filtering by service (EC2), usage type (instance type), and time range,\nenabling the solutions architect to easily compare EC2 costs for the last two months and identify the\ninstances that have experienced unwanted vertical scaling. Cost Explorer's built-in charting capabilities make\nit straightforward to visualize these cost trends without requiring additional tools or setup. The \"group by\"\nfeature within Cost Explorer lets users easily aggregate costs by instance type. This allows for direct\nidentification of cost increases associated with specific instance types, streamlining the analysis. The goal is\nto analyze past costs, and Cost Explorer is built to efficiently do just that.\nOption A, AWS Budgets, is primarily for setting cost thresholds and receiving alerts when those thresholds\nare exceeded. While it can provide cost information, it's not designed for in-depth historical analysis and\ndoesn't offer the granular filtering capabilities needed to pinpoint specific instance type cost increases as\nreadily as Cost Explorer.\nOption C, the AWS Billing and Cost Management dashboard, provides a high-level overview of costs but lacks\nthe granular filtering and analysis features available in Cost Explorer. It would require more manual effort to\nisolate and analyze the cost increase associated with specific EC2 instance types.\nOption D, AWS Cost and Usage Reports (CUR) with Amazon QuickSight, is powerful but involves significantly\nmore operational overhead. It requires configuring and delivering the CUR to an S3 bucket, then setting up\nQuickSight with S3 as a data source, and finally creating the interactive graph. This process is much more\ncomplex and time-consuming compared to using Cost Explorer's built-in features. While CUR provides\ncomprehensive data, for a focused analysis as described in the question, Cost Explorer is far more efficient.\nTherefore, Cost Explorer provides the least operational overhead and is the most efficient way to achieve the\nrequired in-depth analysis of EC2 costs based on instance types.\nHere's an authoritative link for further research:\nAWS Cost Explorer",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is designing an application. The application uses an AWS Lambda function to receive information\nthrough Amazon API Gateway and to store the information in an Amazon Aurora PostgreSQL database.\nDuring the proof-of-concept stage, the company has to increase the Lambda quotas significantly to handle the\nhigh volumes of data that the company needs to load into the database. A solutions architect must recommend a\nnew design to improve scalability and minimize the configuration effort.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Refactor the Lambda function code to Apache Tomcat code that runs on Amazon EC2 instances. Connect the",
      "B": "Change the platform from Aurora to Amazon DynamoDProvision a DynamoDB Accelerator (DAX) cluster. Use",
      "C": "Set up two Lambda functions. Configure one function to receive the information. Configure the other",
      "D": "Set up two Lambda functions. Configure one function to receive the information. Configure the other"
    },
    "answer": "D",
    "explanation": "The best solution to improve scalability and minimize configuration effort when dealing with high volumes of\ndata between a Lambda function receiving data and an Aurora PostgreSQL database is to use an Amazon SQS\nqueue to decouple the two functions.\nHere's why:\nDecoupling with SQS: SQS acts as a buffer between the data receiving Lambda function and the database\nloading Lambda function. The first Lambda function places messages containing the information into the SQS\nqueue. The second Lambda function retrieves messages from the queue and loads the data into Aurora. This\ndecoupling allows each function to scale independently based on its specific needs.\nScalability: SQS provides virtually unlimited scalability. It can handle large volumes of messages, allowing\nthe system to absorb bursts of data without overwhelming the database. The database loading Lambda\nfunction can then process the messages at a rate the database can handle, preventing overload.\nMinimized Configuration Effort: SQS is a managed service, which reduces the operational burden. Setting up\nthe queue and integrating it with the Lambda functions requires minimal configuration.\nResilience: SQS provides message durability. If the database loading Lambda function fails, the messages\nremain in the queue until they are successfully processed, ensuring data is not lost. This contrasts with SNS,\nwhich is better suited for fan-out scenarios and doesn't guarantee message delivery if no subscribers are\navailable.\nWhy other options are less suitable:\nA: Refactoring to EC2 and Tomcat increases operational complexity. EC2 requires managing servers,\nincluding patching, scaling, and ensuring high availability. This defeats the goal of minimizing configuration\neffort.\nB: Changing to DynamoDB with DAX is a significant architectural change and involves substantial code\nmodification. DAX is for DynamoDB, not Aurora PostgreSQL.\nC: Using SNS doesn't provide the queuing and buffering capabilities of SQS. If the database loading Lambda\nfunction is unavailable, messages sent via SNS might be lost, potentially impacting data consistency. SNS is\nnot suitable for reliable, asynchronous processing.\nIn summary, using SQS for decoupling provides the best combination of scalability, minimized configuration,\nand reliability for this use case.\nRelevant Documentation:\nAmazon SQS: https://aws.amazon.com/sqs/\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon Aurora: https://aws.amazon.com/rds/aurora/",
    "links": [
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/rds/aurora/"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to review its AWS Cloud deployment to ensure that its Amazon S3 buckets do not have\nunauthorized configuration changes.\nWhat should a solutions architect do to accomplish this goal?",
    "options": {
      "A": "Turn on AWS Config with the appropriate rules.",
      "B": "Turn on AWS Trusted Advisor with the appropriate checks.",
      "C": "Turn on Amazon Inspector with the appropriate assessment template.",
      "D": "Turn on Amazon S3 server access logging. Configure Amazon EventBridge (Amazon Cloud Watch Events)."
    },
    "answer": "A",
    "explanation": "The correct answer is A, turning on AWS Config with the appropriate rules. Here's why:\nAWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS\nresources. It continuously monitors and records your AWS resource configurations, allowing you to automate\nthe evaluation of recorded configurations against desired configurations. In this scenario, the goal is to ensure\nS3 bucket configurations remain authorized, which directly aligns with Config's capabilities. You can define\nAWS Config rules that specify the desired state of your S3 buckets (e.g., encryption enabled, public access\nblocked). If a bucket drifts from this desired state, Config will flag it as non-compliant. This makes it perfect\nfor detecting unauthorized configuration changes.\nOption B, AWS Trusted Advisor, primarily provides recommendations on cost optimization, performance,\nsecurity, fault tolerance, and service limits. While it offers security checks, it is more high-level and may not\nprovide the detailed configuration change detection needed for S3 buckets specifically.\nOption C, Amazon Inspector, focuses on security vulnerabilities and deviations from security best practices\nwithin your EC2 instances and container images. It's not designed to monitor and audit configuration changes\nto S3 buckets.\nOption D, turning on Amazon S3 server access logging and configuring Amazon EventBridge (CloudWatch\nEvents), would provide logs of who accessed the bucket. EventBridge could trigger alerts on specific events.\nHowever, it requires significant custom parsing and rule building to determine if the configuration of the\nbucket itself has changed (e.g., a change to bucket policy or encryption). This is a much more complex and\nless direct method than using AWS Config. Config provides managed rules specifically designed for\nconfiguration compliance.\nIn summary, AWS Config is the most suitable service because it's designed to continuously monitor and\nevaluate the configurations of AWS resources, allowing you to proactively identify and remediate\nunauthorized configuration changes in S3 buckets.\nAuthoritative Links:\nAWS Config: https://aws.amazon.com/config/\nAWS Config Rules: https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html",
    "links": [
      "https://aws.amazon.com/config/",
      "https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is launching a new application and will display application metrics on an Amazon CloudWatch\ndashboard. The company's product manager needs to access this dashboard periodically. The product manager\ndoes not have an AWS account. A solutions architect must provide access to the product manager by following the\nprinciple of least privilege.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Share the dashboard from the CloudWatch console. Enter the product manager's email address, and",
      "B": "Create an IAM user specifically for the product manager. Attach the CloudWatchReadOnlyAccess AWS",
      "C": "Create an IAM user for the company's employees. Attach the ViewOnlyAccess AWS managed policy to the",
      "D": "Deploy a bastion server in a public subnet. When the product manager requires access to the dashboard,"
    },
    "answer": "A",
    "explanation": "The best solution is A because it leverages CloudWatch's built-in dashboard sharing feature, providing the\nmost direct and least privileged access for the product manager without requiring an AWS account.\nCloudWatch's sharing feature allows you to create a shareable link to the dashboard that can be accessed\nwithout AWS credentials. This satisfies the requirement of allowing the product manager to view the\ndashboard.\nOption B is less ideal. While it technically grants the product manager access, creating an IAM user solely for\ndashboard viewing is an over-provisioning of access. It also necessitates managing another AWS user and\ndistributing credentials, increasing administrative overhead and security risks. The product manager would\ngain access to the entire AWS console with read-only access to CloudWatch, exceeding the least privilege\nprinciple.\nOption C is also unsuitable. The ViewOnlyAccess AWS managed policy is overly broad and doesn't specifically\ntarget CloudWatch. Sharing credentials among employees introduces significant security risks and hinders\naccountability. Sharing an IAM user's credentials violates AWS best practices and security guidelines.\nOption D is the least efficient and most complex option. Deploying a bastion server just to view a CloudWatch\ndashboard is excessive and unnecessary. It involves managing infrastructure, configuring security groups, and\nsharing RDP credentials, adding significant operational overhead. It's also not the least privileged approach.\nTherefore, option A directly addresses the requirement by using CloudWatch's built-in feature, adhering to\nthe principle of least privilege, and minimizing operational overhead.\nRelevant Documentation:\nSharing CloudWatch dashboards",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is migrating applications to AWS. The applications are deployed in different accounts. The company\nmanages the accounts centrally by using AWS Organizations. The company's security team needs a single sign-on\n(SSO) solution across all the company's accounts. The company must continue managing the users and groups in\nits on-premises self-managed Microsoft Active Directory.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a one-way forest trust or a one-",
      "B": "Here's a detailed justification:",
      "C": "Use AWS Directory Service. Create a two-way trust relationship with the company's self-managed Microsoft",
      "D": "Option C is incorrect because while creating a two-way trust relationship using AWS Directory Service is"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Here's a detailed justification:\nThe requirement is to integrate an existing on-premises Microsoft Active Directory (AD) with AWS SSO to\nprovide centralized single sign-on across multiple AWS accounts managed by AWS Organizations. To achieve\nthis while maintaining user and group management within the on-premises AD, a trust relationship must be\nestablished between the on-premises AD and AWS.\nAWS SSO doesn't directly integrate with self-managed AD. Instead, AWS Directory Service for Microsoft\nActive Directory (AWS Managed Microsoft AD) acts as an intermediary. AWS Managed Microsoft AD is a fully\nmanaged service allowing you to run actual Active Directory in AWS. To link your on-premises AD, you need to\nestablish a trust relationship.\nA two-way forest trust is the appropriate type of trust when you need users in both the on-premises AD forest\nand the AWS Managed Microsoft AD forest to authenticate to resources in the other forest. In this scenario,\non-premises users need to access AWS resources, and potentially, AWS-based users (if you later create\nthem) might need to access on-premises resources. Therefore, a two-way trust ensures seamless\nauthentication in both directions. AWS SSO leverages this trust relationship via AWS Managed Microsoft AD\nto authenticate users from the on-premises AD. The AWS SSO console is used to enable and configure SSO\nacross the AWS Organization.\nOption A is incorrect because a one-way trust is insufficient. A one-way trust only allows authentication in one\ndirection, failing to meet the requirement where AWS SSO needs to authenticate users against the on-\npremises AD.\nOption C is incorrect because while creating a two-way trust relationship using AWS Directory Service is\nnecessary, it doesn't by itself enable SSO. AWS SSO is the service required to centralize and manage single\nsign-on to multiple AWS accounts.\nOption D is incorrect because deploying an on-premises IdP adds unnecessary complexity. AWS SSO is\ndesigned to integrate with existing identity providers, especially Microsoft Active Directory through AWS\nDirectory Service. Using a third-party IdP on-premises duplicates functionality that AWS SSO already\nprovides and increases the management overhead. Moreover, AWS SSO has direct integration capabilities\nwith AWS Managed Microsoft AD, and no further IdP on-premises is needed.\nIn summary, creating a two-way forest trust between the on-premises Active Directory and AWS Directory\nService for Microsoft Active Directory, then enabling and configuring AWS SSO, allows the organization to\ncentralize SSO while maintaining user management in the on-premises environment.\nRelevant Links:\nAWS SSO: https://aws.amazon.com/single-sign-on/\nAWS Directory Service: https://aws.amazon.com/directoryservice/\nHow AWS SSO works with Active Directory: https://docs.aws.amazon.com/singlesignon/latest/userguide/ad-\nconnector.html\nTrust Relationships: https://learn.microsoft.com/en-us/windows-server/identity/ad-ds/plan/how-forest-\ntrusts-work",
    "links": [
      "https://aws.amazon.com/single-sign-on/",
      "https://aws.amazon.com/directoryservice/",
      "https://docs.aws.amazon.com/singlesignon/latest/userguide/ad-",
      "https://learn.microsoft.com/en-us/windows-server/identity/ad-ds/plan/how-forest-"
    ]
  },
  {
    "question": "CertyIQ\nA company provides a Voice over Internet Protocol (VoIP) service that uses UDP connections. The service consists\nof Amazon EC2 instances that run in an Auto Scaling group. The company has deployments across multiple AWS\nRegions.\nThe company needs to route users to the Region with the lowest latency. The company also needs automated\nfailover between Regions.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Here's why:",
      "B": "Deploy an Amazon CloudFront distribution that uses the weighted record as an origin.",
      "C": "Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the",
      "D": "Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group with"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's why:\nUDP Requirement: The VoIP service relies on UDP connections. Application Load Balancers (ALB) only\nsupport HTTP and HTTPS protocols (TCP). Network Load Balancers (NLB) are designed to handle TCP, UDP,\nand TLS traffic. Therefore, NLB is the appropriate choice for this scenario.\nLowest Latency Routing and Automated Failover: AWS Global Accelerator leverages the AWS global\nnetwork to route traffic to the optimal endpoint (closest Region) based on user location and network\nconditions, minimizing latency. It also provides automatic failover; if a Region becomes unavailable, Global\nAccelerator will automatically redirect traffic to a healthy Region.\nIntegration with Auto Scaling Groups: Both NLB and ALB can be integrated with Auto Scaling groups. By\nassociating the target group with the Auto Scaling group, the load balancer automatically registers and\nderegisters instances as they are launched or terminated by the Auto Scaling group.\nRoute 53 Alternatives: While Route 53 latency-based routing can provide similar functionality, Global\nAccelerator offers performance advantages due to its use of the AWS global network and intelligent traffic\nrouting. CloudFront is typically used for caching static content, not for routing UDP-based traffic based on\nlatency.\nWhy other options are incorrect:\nOption B: ALB is unsuitable because it does not support UDP.\nOption C: Route 53 latency records do not provide the same level of optimized routing as Global Accelerator.\nUsing CloudFront as a distribution for this purpose is unnecessary and not the best practice.\nOption D: ALB is unsuitable because it does not support UDP. Route 53 weighted records are used to\ndistribute traffic based on predefined weights, not real-time latency measurements.\nSupporting Links:\nAWS Global Accelerator: https://aws.amazon.com/global-accelerator/\nNetwork Load Balancer: https://aws.amazon.com/elasticloadbalancing/network-load-balancer/\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\nAmazon Route 53: https://aws.amazon.com/route53/",
    "links": [
      "https://aws.amazon.com/global-accelerator/",
      "https://aws.amazon.com/elasticloadbalancing/network-load-balancer/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
      "https://aws.amazon.com/route53/"
    ]
  },
  {
    "question": "CertyIQ\nA development team runs monthly resource-intensive tests on its general purpose Amazon RDS for MySQL DB\ninstance with Performance Insights enabled. The testing lasts for 48 hours once a month and is the only process\nthat uses the database. The team wants to reduce the cost of running the tests without reducing the compute and\nmemory attributes of the DB instance.\nWhich solution meets these requirements MOST cost-effectively?",
    "options": {
      "A": "Stop the DB instance when tests are completed. Restart the DB instance when required.",
      "B": "Use an Auto Scaling policy with the DB instance to automatically scale when tests are completed.",
      "C": "Create a snapshot when tests are completed. Terminate the DB",
      "D": "Modify the DB instance to a low-capacity instance when tests are completed. Modify the DB instance again"
    },
    "answer": "C",
    "explanation": "The most cost-effective solution is C. Create a snapshot when tests are completed. Terminate the DB\ninstance and restore the snapshot when required.\nHere's why:\nCost Optimization: The primary goal is cost reduction. Terminating the DB instance when it's not needed\ncompletely eliminates the compute costs associated with running the instance. AWS charges for RDS\ninstances per hour/day they are running.\nPreserving Data: Snapshots preserve the database state at the end of the tests. This ensures no data loss,\nand the team can easily restore the database to its previous state.\nCompute & Memory Retention: Restoring from a snapshot creates a new DB instance with the same\nspecifications (compute, memory) as the original. This preserves the performance characteristics required\nduring the test.\nAlternative A (Stop/Start): Stopping the instance saves on compute costs, but you still incur costs for\nstorage (database volume) which won't be as significant as the cost of a running instance, but there is a cost.\nStopping and starting instances can take a significant amount of time as well.\nAlternative B (Auto Scaling): Auto Scaling doesn't apply to RDS instance types. Auto Scaling is for\nautomatically scaling the number of EC2 instances in your application, not for vertically scaling an RDS\ninstance.\nAlternative D (Modify Instance): Modifying the instance size involves downtime and does not completely\neliminate costs. While it reduces costs, it doesn't achieve the same level of savings as terminating the\ninstance.\nPerformance Insights: Performance Insights data is associated with the DB instance. Terminating and\nrecreating the instance will lead to losing data that the testing team may want to review later on. The team\ncan export the insights before terminating the instance.\nRestore Time: While restoring from a snapshot does take time, it's a reasonable trade-off for significant cost\nsavings, given the monthly testing frequency.\nTherefore, creating a snapshot, terminating the instance, and restoring when needed provides the optimal\nbalance between cost savings, data preservation, and performance retention.\nSupporting Links:\nAmazon RDS Pricing: https://aws.amazon.com/rds/pricing/\nCreating a DB Snapshot:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html\nRestoring from a DB Snapshot:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_RestoreFromSnapshot.html",
    "links": [
      "https://aws.amazon.com/rds/pricing/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_RestoreFromSnapshot.html"
    ]
  },
  {
    "question": "CertyIQ\nA company that hosts its web application on AWS wants to ensure all Amazon EC2 instances. Amazon RDS DB\ninstances. and Amazon Redshift clusters are configured with tags. The company wants to minimize the effort of\nconfiguring and operating this check.\nWhat should a solutions architect do to accomplish this?",
    "options": {
      "A": "Use AWS Config rules to define and detect resources that are not properly tagged.",
      "B": "Use Cost Explorer to display resources that are not properly tagged. Tag those resources manually.",
      "C": "Write API calls to check all resources for proper tag allocation. Periodically run the code on an EC2 instance.",
      "D": "Write API calls to check all resources for proper tag allocation. Schedule an AWS Lambda function through"
    },
    "answer": "A",
    "explanation": "The correct answer is A because AWS Config provides a managed service to assess, audit, and evaluate the\nconfigurations of your AWS resources. With AWS Config, you can define rules that check if your resources are\ncompliant with desired configurations, including the presence of specific tags. When a resource is found to be\nnon-compliant (missing tags in this case), AWS Config can flag it, allowing for easy identification and\nremediation. This minimizes the operational burden since AWS Config continuously monitors resource\nconfigurations without requiring custom code or manual checks.\nOption B is incorrect because Cost Explorer is primarily used for cost management and visualization and does\nnot provide a built-in mechanism for identifying resources without proper tags. While Cost Explorer can utilize\ntags for cost allocation, it won't proactively alert you to missing tags or enforce their existence. Tagging\nresources manually is a tedious and error-prone approach for an environment where continuous monitoring is\nrequired.\nOptions C and D involve writing custom API calls and managing infrastructure to execute them, either on an\nEC2 instance or through Lambda. While technically feasible, these approaches introduce operational\noverhead associated with code development, deployment, maintenance, and scaling. AWS Config offers a\nmanaged and declarative way to achieve the same goal, thus reducing complexity and operational effort.\nLambda adds unnecessary complexity for this task as AWS Config Rules can be executed and maintained\nwithout code.\nIn summary, AWS Config's pre-built rules, continuous monitoring capabilities, and managed service nature\nmake it the optimal solution for ensuring resources are properly tagged while minimizing effort and\noperational overhead.\nRefer to the following AWS documentation for further understanding:\nAWS Config: https://aws.amazon.com/config/\nAWS Config Rules: https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html",
    "links": [
      "https://aws.amazon.com/config/",
      "https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html"
    ]
  },
  {
    "question": "CertyIQ\nA development team needs to host a website that will be accessed by other teams. The website contents consist of\nHTML, CSS, client-side JavaScript, and images.\nWhich method is the MOST cost-effective for hosting the website?",
    "options": {
      "A": "Containerize the website and host it in AWS Fargate.",
      "B": "Create an Amazon S3 bucket and host the website there.",
      "C": "Deploy a web server on an Amazon EC2 instance to host the website.",
      "D": "Configure an Application Load Balancer with an AWS Lambda target that uses the Express.js framework."
    },
    "answer": "B",
    "explanation": "The most cost-effective solution for hosting a static website composed of HTML, CSS, JavaScript, and images\nis to use Amazon S3. S3 offers low storage costs and efficient content delivery through its integration with\nAmazon CloudFront.\nOption A, containerizing the website and hosting it on AWS Fargate, involves running a container\norchestration service, which is overkill for static content. Fargate is designed for applications that require\ncompute resources, introducing unnecessary complexity and cost.\nOption C, deploying a web server on an Amazon EC2 instance, incurs costs for the EC2 instance itself,\nincluding compute, storage, and potentially bandwidth. Maintaining and managing the EC2 instance also adds\noperational overhead. While EC2 provides flexibility, it's less cost-effective than S3 for static content.\nOption D, configuring an Application Load Balancer with an AWS Lambda target using Express.js, is\nunnecessarily complex and expensive. This setup is suitable for dynamic content generation and serverless\napplications. Using Lambda for serving static files is inefficient and introduces additional latency. An\nApplication Load Balancer is not necessary for static content delivery.\nS3's static website hosting feature allows you to serve the files directly from an S3 bucket, eliminating the\nneed for a web server. Furthermore, you can leverage CloudFront to cache the website content globally,\nreducing latency and further decreasing costs. S3 buckets also provide scalability and high availability by\ndefault. S3's pay-as-you-go pricing model makes it a very cost-effective option for this use case.\nTherefore, the low cost, inherent scalability, and ease of use of Amazon S3 make it the most cost-effective\nsolution for hosting a simple static website.\nRelevant links:\nAmazon S3 Static Website Hosting\nAmazon CloudFront",
    "links": []
  },
  {
    "question": "CertyIQ\nA company runs an online marketplace web application on AWS. The application serves hundreds of thousands of\nusers during peak hours. The company needs a scalable, near-real-time solution to share the details of millions of\nfinancial transactions with several other internal applications. Transactions also need to be processed to remove\nsensitive data before being stored in a document database for low-latency retrieval.\nWhat should a solutions architect recommend to meet these requirements?",
    "options": {
      "A": "Store the transactions data into Amazon DynamoDB. Set up a rule in DynamoDB to remove sensitive data",
      "B": "Other applications can consume the transactions data off the Kinesis data stream.",
      "C": "Stream the transactions data into Amazon Kinesis Data Streams. Use AWS Lambda integration to remove",
      "D": "Store the batched transactions data in Amazon S3 as files. Use AWS Lambda to process every file and"
    },
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the best solution and why the others are less suitable:\nOption C: Stream the transactions data into Amazon Kinesis Data Streams. Use AWS Lambda integration to\nremove sensitive data from every transaction and then store the transactions data in Amazon DynamoDB.\nOther applications can consume the transactions data off the Kinesis data stream.\nThis is the optimal choice because it leverages a combination of AWS services that are specifically designed\nfor real-time data processing, transformation, and sharing.\n1. Real-time Ingestion: Amazon Kinesis Data Streams is designed for ingesting and processing high-\nvolume, real-time data streams. It is perfectly suited for capturing millions of financial transactions.\n2. Data Transformation: The Lambda function provides a serverless and scalable way to remove\nsensitive data from each transaction as it flows through the Kinesis stream. This ensures compliance\nand data security before storage and sharing.\n3. Low-Latency Storage: DynamoDB is a NoSQL database known for its low-latency read and write\ncapabilities, making it ideal for storing the processed transactions and enabling fast retrieval.\n4. Data Sharing: Other applications can subscribe to the Kinesis Data Stream and receive the\ntransformed transactions data in near real-time. This provides a decoupled and scalable mechanism\nfor sharing data with other internal applications.\nWhy other options are less optimal:\nOption A: DynamoDB Streams are primarily intended for auditing or replicating data changes within\nDynamoDB itself, and not optimized for sharing data with a multitude of other applications in near real-time.\nRemoving sensitive data using DynamoDB rules would happen after the transaction is already stored, possibly\nviolating compliance requirements.\nOption B: Kinesis Data Firehose is designed for loading data into data lakes or analytics services, and doesn't\nreadily support direct consumption of data by other real-time applications. Using S3 as a central point can\ncreate latency.\nOption D: Batching and storing in S3, processing with Lambda, and then storing in DynamoDB introduces\nsignificant latency. This approach isn't near real-time. Lambda processing S3 objects is more suited for larger\nfiles and infrequent updates, unlike this requirement.\nIn Conclusion: Option C provides a scalable, near-real-time, secure, and decoupled solution for processing\nfinancial transactions and sharing them with other applications, aligning perfectly with the requirements.\nAuthoritative Links:\nAmazon Kinesis Data Streams: https://aws.amazon.com/kinesis/data-streams/\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon DynamoDB: https://aws.amazon.com/dynamodb/",
    "links": [
      "https://aws.amazon.com/kinesis/data-streams/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/dynamodb/"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts its multi-tier applications on AWS. For compliance, governance, auditing, and security, the\ncompany must track configuration changes on its AWS resources and record a history of API calls made to these\nresources.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.",
      "B": "Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.",
      "C": "Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls.",
      "D": "Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls."
    },
    "answer": "B",
    "explanation": "The correct answer is B: Use AWS Config to track configuration changes and AWS CloudTrail to record API\ncalls. This is because AWS Config and AWS CloudTrail are designed for specific and distinct purposes related\nto compliance, governance, security, and auditing in the AWS cloud.\nAWS Config continuously monitors and records AWS resource configurations, enabling you to automate the\nevaluation of recorded configurations against desired configurations. It provides a detailed view of the\nconfiguration of AWS resources, allowing you to assess, audit, and evaluate the configurations of your\nresources. AWS Config Rules can automatically check whether resources comply with defined standards. This\nis essential for tracking configuration changes. You can explore more about AWS Config at\nhttps://aws.amazon.com/config/.\nAWS CloudTrail, on the other hand, tracks API calls made to AWS resources. It logs account activity, including\nactions taken through the AWS Management Console, AWS SDKs, command-line tools, and other AWS\nservices. CloudTrail provides an event history of your AWS account activity, including who took what action,\nthe resources that were acted upon, and when the event occurred. This is crucial for security analysis,\nresource change tracking, and compliance auditing. Comprehensive information about AWS CloudTrail can be\nfound at https://aws.amazon.com/cloudtrail/.\nOption A is incorrect because it reverses the roles of CloudTrail and Config. CloudTrail is not designed to track\nconfiguration changes directly, and Config is not designed to record API calls. Options C and D are incorrect\nas Amazon CloudWatch is primarily a monitoring service for metrics and logs, rather than a configuration\ntracking or API call recording service. While CloudWatch can be used to monitor logs generated by CloudTrail,\nit isn't a substitute for CloudTrail itself for API call logging. Therefore, AWS Config for configuration changes\nand AWS CloudTrail for API calls provides a robust solution for compliance, governance, auditing, and\nsecurity as required by the company.",
    "links": [
      "https://aws.amazon.com/config/.",
      "https://aws.amazon.com/cloudtrail/."
    ]
  },
  {
    "question": "CertyIQ\nA company is preparing to launch a public-facing web application in the AWS Cloud. The architecture consists of\nAmazon EC2 instances within a VPC behind an Elastic Load Balancer (ELB). A third-party service is used for the\nDNS. The company's solutions architect must recommend a solution to detect and protect against large-scale\nDDoS attacks.\nWhich solution meets these requirements?",
    "options": {
      "A": "Enable Amazon GuardDuty on the account: GuardDuty is a threat detection service that monitors for",
      "B": "D. Enable AWS Shield Advanced and assign the ELB to it: AWS Shield Advanced provides enhanced DDoS",
      "C": "Enable AWS Shield and assign Amazon Route 53 to it: AWS Shield Standard is automatically enabled for",
      "D": "Enable AWS Shield Advanced and assign the ELB to it."
    },
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the correct answer:\nThe problem requires protection against large-scale DDoS attacks for a public-facing web application. Let's\nanalyze each option:\nA. Enable Amazon GuardDuty on the account: GuardDuty is a threat detection service that monitors for\nmalicious activity and unauthorized behavior. While it's good for security, it's not specifically designed for\nDDoS protection. It primarily detects threats after they've potentially impacted the application. It won't\nautomatically mitigate large-scale DDoS attacks in real-time.\nB. Enable Amazon Inspector on the EC2 instances: Inspector is a vulnerability assessment service that\nanalyzes the security posture of EC2 instances. It identifies software vulnerabilities and unintended network\nconfigurations. While it's useful for general security, it doesn't directly address large-scale DDoS attacks at\nthe network layer.\nC. Enable AWS Shield and assign Amazon Route 53 to it: AWS Shield Standard is automatically enabled for\nall AWS customers and provides basic DDoS protection against common, frequently occurring network and\ntransport layer DDoS attacks. While it's a good starting point, it doesn't offer the customized protection and\nadvanced mitigation techniques required for large-scale, sophisticated DDoS attacks. Assigning Route 53\nwould protect the DNS layer, but the question focuses on the web application behind the ELB.\nD. Enable AWS Shield Advanced and assign the ELB to it: AWS Shield Advanced provides enhanced DDoS\nprotection tailored to your specific application. By assigning the ELB to Shield Advanced, you get 24/7 access\nto the AWS DDoS Response Team (DRT), customized protection rules, and cost protection during DDoS\nevents. Shield Advanced can detect and automatically mitigate sophisticated DDoS attacks, ensuring the\navailability of your web application. Since the application is behind an ELB, protecting the ELB directly\nprotects the underlying EC2 instances. This is the only option that provides comprehensive DDoS protection\nfor the described architecture.\nIn summary, AWS Shield Advanced, specifically protecting the ELB, provides the best solution to detect and\nmitigate large-scale DDoS attacks targeted at the public-facing web application.\nAuthoritative Links:\nAWS Shield: https://aws.amazon.com/shield/\nAWS Shield Advanced: https://aws.amazon.com/shield/advanced/\nElastic Load Balancing: https://aws.amazon.com/elasticloadbalancing/\nAmazon GuardDuty: https://aws.amazon.com/guardduty/\nAmazon Inspector: https://aws.amazon.com/inspector/",
    "links": [
      "https://aws.amazon.com/shield/",
      "https://aws.amazon.com/shield/advanced/",
      "https://aws.amazon.com/elasticloadbalancing/",
      "https://aws.amazon.com/guardduty/",
      "https://aws.amazon.com/inspector/"
    ]
  },
  {
    "question": "CertyIQ\nA company is building an application in the AWS Cloud. The application will store data in Amazon S3 buckets in\ntwo AWS Regions. The company must use an AWS Key Management Service (AWS KMS) customer managed key\nto encrypt all data that is stored in the S3 buckets. The data in both S3 buckets must be encrypted and decrypted\nwith the same KMS key. The data and the key must be stored in each of the two Regions.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Create an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with Amazon S3",
      "B": "Create a customer managed multi-Region KMS key. Create an S3 bucket in each Region. Configure",
      "C": "Create a customer managed KMS key and an S3 bucket in each Region. Configure the S3 buckets to use",
      "D": "Create a customer managed KMS key and an S3 bucket in each Region. Configure the S3 buckets to use"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Here's why:\nB. Create a customer managed multi-Region KMS key. Create an S3 bucket in each Region. Configure\nreplication between the S3 buckets. Configure the application to use the KMS key with client-side\nencryption.\nThis solution meets all the requirements with the least operational overhead:\n1. Customer Managed Key: It utilizes a customer managed key, fulfilling the requirement to use a KMS\nkey that the company controls. Critically, it uses a multi-Region KMS key.\n2. Same Key in Both Regions: A multi-Region key ensures that the same logical key is available in both\nRegions for encryption and decryption. This is crucial for seamless data access regardless of which\nRegion the data resides in.\n3. Data and Key Stored in Each Region: Multi-Region keys guarantee the key material exists within\nboth specified Regions, addressing the regional storage requirement.\n4. Encryption with KMS Key: The solution specifies client-side encryption. This means the application\nencrypts the data before sending it to S3 using the multi-Region KMS key. This ensures encryption at\nrest using the KMS key, as the data is already encrypted when stored in S3.\n5. Least Operational Overhead: This approach involves a relatively straightforward setup. The\napplication manages encryption/decryption, which might add some development complexity initially,\nbut it simplifies key management significantly compared to server-side encryption and key\nreplication.\nLet's examine why the other options are less suitable:\nA & C: These options utilize SSE-S3. SSE-S3 uses encryption keys managed by AWS, failing the requirement\nto use a customer managed KMS key.\nD: This solution proposes SSE-KMS with a KMS key. While it uses a customer managed key, it does not use a\nmulti-Region key. Without a multi-Region key, replicating data between Regions encrypted with KMS\nbecomes complex. You'd have to either:\nChange the key used during replication, which adds significant operational overhead.\nRe-encrypt the data after replication, again adding complexity.Furthermore, even if the same key was\nmanually replicated to two different regions, the key ID would be different, and it would be treated as two\ndistinct keys.\nIn summary, option B provides the most efficient and manageable way to meet all the requirements. It\nleverages the benefits of a multi-Region KMS key for seamless data encryption and decryption across both\nAWS Regions with minimal operational burden.\nSupporting Documentation:\nAWS KMS Multi-Region Keys: https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-\noverview.html\nAmazon S3 Encryption: https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html",
    "links": [
      "https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html"
    ]
  },
  {
    "question": "CertyIQ\nA company recently launched a variety of new workloads on Amazon EC2 instances in its AWS account. The\ncompany needs to create a strategy to access and administer the instances remotely and securely. The company\nneeds to implement a repeatable process that works with native AWS services and follows the AWS Well-\nArchitected Framework.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Use the EC2 serial console to directly access the terminal interface of each instance for administration.",
      "B": "Attach the appropriate IAM role to each existing instance and new instance. Use AWS Systems Manager",
      "C": "Create an administrative SSH key pair. Load the public key into each EC2 instance. Deploy a bastion host",
      "D": "Establish an AWS Site-to-Site VPN connection. Instruct administrators to use their local on-premises"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Here's why:\nB. Attach the appropriate IAM role to each existing instance and new instance. Use AWS Systems Manager\nSession Manager to establish a remote SSH session.\nThis solution is the most aligned with the prompt's requirements of security, repeatability, native AWS\nservices, and minimal operational overhead. AWS Systems Manager (SSM) Session Manager allows you to\nsecurely manage EC2 instances without needing to open inbound ports (like SSH 22) or manage SSH keys. By\nusing IAM roles, you can precisely define the permissions granted to SSM, ensuring only authorized users and\ninstances can initiate sessions. This approach minimizes the attack surface and reduces the risk of\nunauthorized access. SSM Session Manager also integrates with AWS CloudTrail for auditing purposes,\nproviding a clear record of all session activities. Furthermore, it leverages the AWS global infrastructure and\neliminates the need to manage bastion hosts or VPNs, thus minimizing operational overhead. The \"repeatable\nprocess\" is achieved by consistently applying the same IAM roles across all instances.\nHere's a breakdown of why the other options are less ideal:\nA. Use the EC2 serial console to directly access the terminal interface of each instance for administration.\nWhile useful for troubleshooting boot issues, the serial console isn't intended for routine administration. It\nlacks the auditability and security features of SSM Session Manager and requires direct AWS Management\nConsole access. It also does not scale well for a large number of instances.\nC. Create an administrative SSH key pair. Load the public key into each EC2 instance. Deploy a bastion host\nin a public subnet to provide a tunnel for administration of each instance. This solution introduces significant\noperational overhead. It requires managing SSH keys, securing the bastion host, and configuring routing\nrules. Maintaining the security and availability of a bastion host adds complexity and ongoing management.\nSSH keys also present a security risk if compromised.\nD. Establish an AWS Site-to-Site VPN connection. Instruct administrators to use their local on-premises\nmachines to connect directly to the instances by using SSH keys across the VPN tunnel. This introduces\nreliance on on-premises infrastructure and the VPN connection. It increases complexity, maintenance\noverhead, and network latency. Additionally, managing SSH keys and distributing them securely to\nadministrators becomes an operational burden and security risk.\nIn summary, Option B leverages native AWS services (IAM and SSM) to provide secure, auditable, and\ncentrally managed access to EC2 instances with minimal operational burden, which is a best practice\naccording to the AWS Well-Architected Framework.\nSupporting Links:\nAWS Systems Manager Session Manager: https://docs.aws.amazon.com/systems-\nmanager/latest/userguide/session-manager.html\nAWS IAM Roles: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\nAWS Well-Architected Framework: https://aws.amazon.com/architecture/well-architected/",
    "links": [
      "https://docs.aws.amazon.com/systems-",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html",
      "https://aws.amazon.com/architecture/well-architected/"
    ]
  },
  {
    "question": "CertyIQ\nA company is hosting a static website on Amazon S3 and is using Amazon Route 53 for DNS. The website is\nexperiencing increased demand from around the world. The company must decrease latency for users who access\nthe website.\nWhich solution meets these requirements MOST cost-effectively?",
    "options": {
      "A": "Replicate the S3 bucket that contains the website to all AWS Regions. Add Route 53 geolocation routing",
      "B": "Provision accelerators in AWS Global Accelerator. Associate the supplied IP addresses with the S3 bucket.",
      "C": "Add an Amazon CloudFront distribution in front of the S3 bucket. Edit the Route 53 entries to point to the",
      "D": "Enable S3 Transfer Acceleration on the bucket. Edit the Route 53 entries to point to the new endpoint."
    },
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the most cost-effective solution for reducing latency for a\nstatic website hosted on S3 using Route 53, while dealing with increased global demand:\nOption C, adding an Amazon CloudFront distribution in front of the S3 bucket and updating Route 53 records,\nleverages the power of Content Delivery Networks (CDNs). CloudFront caches the static website content at\nedge locations around the world. When a user requests content, CloudFront serves it from the nearest edge\nlocation, significantly reducing latency compared to fetching it from the origin S3 bucket, which might be\ngeographically distant. This caching mechanism distributes the load and minimizes the impact of increased\nglobal demand on the origin server. CloudFronts pay-as-you-go pricing model makes it cost-effective as you\nonly pay for the data transferred and requests served.\nOption A is less cost-effective. Replicating the S3 bucket to all AWS Regions is an overkill for a static website.\nMoreover, managing replication across all regions and configuring Route 53 geolocation routing can be\ncomplex and expensive without a significant value add. While geolocation routing would direct users to the\nclosest replica, the replication costs would likely be excessive.\nOption B, using AWS Global Accelerator, is more appropriate for dynamic content or applications that require\nTCP or UDP connections. While it could improve performance, it's designed for accelerating dynamic content\nand is generally more expensive than CloudFront for static content. Global Accelerator also introduces\ncomplexity in terms of provisioning and managing accelerators.\nOption D, enabling S3 Transfer Acceleration, improves the speed of uploads and downloads to S3 using\noptimized network paths. While beneficial for data transfer to S3, it does not directly address the need to\nreduce latency for users accessing the static website content. It speeds up transfers to S3, but users still\nretrieve data directly from the bucket unless a CDN is used. Therefore, it is not suitable.\nIn summary, CloudFront offers the most efficient and cost-effective solution for caching static content and\ndelivering it to users with low latency globally, perfectly aligning with the requirements of the question.\nRelevant links for further research:\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nAmazon S3: https://aws.amazon.com/s3/\nAmazon Route 53: https://aws.amazon.com/route53/\nAWS Global Accelerator: https://aws.amazon.com/global-accelerator/\nS3 Transfer Acceleration: https://aws.amazon.com/s3/transfer-acceleration/",
    "links": [
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/route53/",
      "https://aws.amazon.com/global-accelerator/",
      "https://aws.amazon.com/s3/transfer-acceleration/"
    ]
  },
  {
    "question": "CertyIQ\nA company maintains a searchable repository of items on its website. The data is stored in an Amazon RDS for\nMySQL database table that contains more than 10 million rows. The database has 2 TB of General Purpose SSD\nstorage. There are millions of updates against this data every day through the company's website.\nThe company has noticed that some insert operations are taking 10 seconds or longer. The company has\ndetermined that the database storage performance is the problem.\nWhich solution addresses this performance issue?",
    "options": {
      "A": "Change the storage type to Provisioned IOPS SSD.",
      "B": "Change the DB instance to a memory optimized instance class.",
      "C": "Change the DB instance to a burstable performance instance class.",
      "D": "Enable Multi-AZ RDS read replicas with MySQL native asynchronous replication."
    },
    "answer": "A",
    "explanation": "The question highlights slow insert operations in an Amazon RDS for MySQL database due to database\nstorage performance limitations. General Purpose SSD (gp2/gp3) storage, while suitable for many workloads,\nmight not be sufficient for high-write workloads with millions of daily updates and large tables. Option A,\nchanging the storage type to Provisioned IOPS SSD (io1/io2), directly addresses the performance bottleneck.\nProvisioned IOPS SSD allows you to specify a consistent IOPS (Input/Output Operations Per Second) rate,\nguaranteeing a predictable performance level. This is crucial for applications requiring low latency and high\nthroughput, especially those experiencing latency due to storage I/O.\nOption B, changing the DB instance to a memory-optimized instance class, primarily addresses CPU and\nmemory limitations, not storage performance. While more memory can improve caching, it doesn't solve\nunderlying storage bottlenecks. Option C, changing to a burstable performance instance class, is unsuitable\nbecause burstable instances rely on credit accumulation and can experience performance degradation when\ncredits are exhausted under sustained high workloads like millions of daily updates. Option D, enabling Multi-\nAZ RDS read replicas, improves read scalability and high availability but doesn't address the write\nperformance issues on the primary database instance causing the slow insert operations. Replication adds\noverhead to the primary instance.\nTherefore, switching to Provisioned IOPS SSD is the optimal solution as it provides consistent and guaranteed\nI/O performance, mitigating the latency issues related to storage operations. For further reading:\nAmazon RDS Storage Types:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html\nProvisioned IOPS SSD (io1/io2): https://aws.amazon.com/rds/pricing/ (Refer to the storage section)",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html",
      "https://aws.amazon.com/rds/pricing/"
    ]
  },
  {
    "question": "CertyIQ\nA company has thousands of edge devices that collectively generate 1 TB of status alerts each day. Each alert is\napproximately 2 KB in size. A solutions architect needs to implement a solution to ingest and store the alerts for\nfuture analysis.\nThe company wants a highly available solution. However, the company needs to minimize costs and does not want\nto manage additional infrastructure. Additionally, the company wants to keep 14 days of data available for\nimmediate analysis and archive any data older than 14 days.\nWhat is the MOST operationally efficient solution that meets these requirements?",
    "options": {
      "A": "Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data",
      "B": "Launch Amazon EC2 instances across two Availability Zones and place them behind an Elastic Load Balancer",
      "C": "Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data",
      "D": "Create an Amazon Simple Queue Service (Amazon SQS) standard queue to ingest the alerts, and set the"
    },
    "answer": "A",
    "explanation": "The most operationally efficient solution is A because it leverages fully managed services that require\nminimal infrastructure management and are cost-effective.\nWhy A is the best solution:\nKinesis Data Firehose: It is a fully managed service designed for streaming data ingestion into destinations\nlike S3. It handles scaling, buffering, and data transformation (if needed) automatically, removing the\noperational overhead of managing servers or clusters.\nAmazon S3: It provides highly durable and scalable storage. The pay-as-you-go model and low cost makes it\ncost-effective.\nS3 Lifecycle Management: This feature automates the process of transitioning data between different\nstorage tiers. Configuring a lifecycle policy to move data to Glacier after 14 days automatically handles\narchiving, reducing storage costs for older data without manual intervention.\nWhy other options are less suitable:\nB: Involves manually managing EC2 instances and load balancers, which increases operational overhead.\nScripting the data storage process also requires manual configuration and maintenance.\nC: Amazon OpenSearch Service (successor to Elasticsearch Service) is not the best choice for long-term\narchival storage, as it is more suited for search and analysis of recent data. OpenSearch Service can be\nexpensive.\nD: SQS is not designed for storing large amounts of data long-term. Consumers would need to actively\nprocess the data as it arrives and manage the archival process. This approach introduces complexity and\npotential points of failure and it does not allow the full 1TB of daily alerts to be retained in SQS due to the\nlimits of its data retention policies. Also, SQS's limit message size of 256KB makes it unsuitable for 2KB\nstatus alerts if thousands of edge devices are sending them.\nSupporting Concepts:\nServerless Computing: Kinesis Data Firehose and S3 are serverless services, which minimize the operational\nburden.\nData Lifecycle Management: S3 Lifecycle Policies are designed to automate data tiering based on age or\nother criteria, optimizing costs and storage management.\nCost Optimization: By using S3 Glacier for long-term archival, the solution minimizes storage costs.\nAuthoritative Links:\nAmazon Kinesis Data Firehose: https://aws.amazon.com/kinesis/data-firehose/\nAmazon S3 Lifecycle Management: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-\nconfiguration-concept.html\nAmazon S3 Glacier: https://aws.amazon.com/glacier/",
    "links": [
      "https://aws.amazon.com/kinesis/data-firehose/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-",
      "https://aws.amazon.com/glacier/"
    ]
  },
  {
    "question": "CertyIQ\nA company's application integrates with multiple software-as-a-service (SaaS) sources for data collection. The\ncompany runs Amazon EC2 instances to receive the data and to upload the data to an Amazon S3 bucket for\nanalysis. The same EC2 instance that receives and uploads the data also sends a notification to the user when an\nupload is complete. The company has noticed slow application performance and wants to improve the performance\nas much as possible.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Create an Auto Scaling group so that EC2 instances can scale out. Configure an S3 event notification to send",
      "B": "Create an Amazon AppFlow flow to transfer data between each SaaS source and the S3 bucket. Configure",
      "C": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule for each SaaS source to send output data.",
      "D": "Create a Docker container to use instead of an EC2 instance. Host the containerized application on Amazon"
    },
    "answer": "B",
    "explanation": "The most efficient solution to improve application performance with minimal operational overhead is to use\nAmazon AppFlow for data transfer and S3 event notifications with SNS.\nOption B is the best because:\nAppFlow for Data Transfer: AppFlow directly transfers data from SaaS applications to S3 without the need\nfor EC2 instances. This eliminates the EC2 instance bottleneck and reduces the operational burden of\nmanaging those instances. AppFlow is specifically designed for SaaS data integration and automates much of\nthe process. https://aws.amazon.com/appflow/\nS3 Event Notifications & SNS: S3 event notifications trigger an SNS topic when an upload completes. This is\na serverless and highly scalable approach to notify users without burdening the data transfer process. SNS\nprovides a simple and reliable way to publish messages to subscribers. https://aws.amazon.com/sns/\nWhy other options are not optimal:\nOption A (Auto Scaling): Scaling EC2 instances might improve throughput, but it doesn't address the\nfundamental inefficiency of using EC2 for simple data transfer from SaaS sources. It also increases\noperational complexity.\nOption C (EventBridge): EventBridge can handle events, but using it directly for SaaS data integration isn't its\nprimary purpose. It's better suited for routing events within AWS services, not directly interfacing with\nexternal SaaS applications. AppFlow is purpose-built for SaaS integrations. Furthermore, using EventBridge\nas a target to directly write into S3 is not a best practice.\nOption D (Docker/ECS): Containerizing the application doesn't fundamentally change the data transfer\nbottleneck. EC2 (or ECS) is still involved in receiving and uploading data. Also, CloudWatch Container Insights\nis for monitoring container performance, not for triggering notifications based on S3 uploads.\nIn conclusion, the solution involving AppFlow and S3 event notifications/SNS offers the most direct and\nefficient way to address the slow application performance while minimizing operational overhead. This aligns\nwith the principle of using managed services to reduce manual intervention and improve scalability.",
    "links": [
      "https://aws.amazon.com/appflow/",
      "https://aws.amazon.com/sns/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a highly available image-processing application on Amazon EC2 instances in a single VP",
    "options": {
      "C": "Supporting resources:",
      "A": "Launch the NAT gateway in each Availability Zone.",
      "B": "Replace the NAT gateway with a NAT instance.",
      "D": "Provision an EC2 Dedicated Host to run the EC2 instances."
    },
    "answer": "C",
    "explanation": "The question is focused on minimizing Regional data transfer costs for EC2 instances downloading from and\nuploading to S3 through a NAT gateway.\nOption C: Deploy a gateway VPC endpoint for Amazon S3 is the most cost-effective solution. Gateway\nendpoints provide connectivity to S3 without using the internet gateway or NAT gateway. This avoids data\ntransfer charges associated with routing traffic through the NAT gateway. Data transferred between EC2\ninstances in the VPC and S3 via the gateway endpoint stays within the AWS network, eliminating Regional\ndata transfer charges.\nOption A: Launch the NAT gateway in each Availability Zone does not avoid Regional data transfer charges,\nalthough it improves availability. Data still traverses the NAT gateway, and the associated costs remain. It\nprimarily prevents a single NAT gateway failure from impacting all AZs.\nOption B: Replace the NAT gateway with a NAT instance offers no cost benefit and introduces operational\noverhead. NAT instances are not managed by AWS and require manual configuration, patching, and scaling.\nMoreover, data transfer charges still apply.\nOption D: Provision an EC2 Dedicated Host to run the EC2 instances is irrelevant to the problem. Dedicated\nHosts provide hardware-level isolation but do not affect data transfer charges associated with S3 access.\nTherefore, leveraging a gateway VPC endpoint is the most efficient way to eliminate Regional data transfer\ncharges for S3 access from EC2 instances within a VPC.\nSupporting resources:\nVPC Endpoints\nNAT Gateways\nUnderstanding AWS Data Transfer Costs",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has an on-premises application that generates a large amount of time-sensitive data that is backed up\nto Amazon S3. The application has grown and there are user complaints about internet bandwidth limitations. A\nsolutions architect needs to design a long-term solution that allows for both timely backups to Amazon S3 and\nwith minimal impact on internet connectivity for internal users.\nWhich solution meets these requirements?",
    "options": {
      "A": "AWS VPN and VPC Gateway Endpoint: While VPN provides secure connectivity, it still relies on the",
      "B": "Establish a new AWS Direct Connect connection and direct backup traffic through",
      "C": "AWS Snowball: Snowball is useful for initial large-scale data migrations. Using it daily for backups is",
      "D": "Support Ticket for S3 Limits: S3 service limits are designed for the overall health and availability of the"
    },
    "answer": "B",
    "explanation": "The best solution is B. Establish a new AWS Direct Connect connection and direct backup traffic through\nthis new connection.\nHere's why:\nBandwidth Bottleneck: The core issue is limited internet bandwidth affecting users due to large data backups\nto S3.\nAWS Direct Connect: Direct Connect establishes a dedicated, private network connection between your on-\npremises infrastructure and AWS. This bypasses the public internet, providing consistent and often higher\nbandwidth, lower latency, and more predictable network performance.\nMinimal Impact on Internet: By routing the backup traffic through Direct Connect, it won't compete for\nbandwidth with other internet-bound traffic, alleviating the strain on internal users' connectivity.\nTimely Backups: The dedicated connection ensures backups can complete efficiently and on time, as it's not\nsubject to internet congestion.\nLong-term Solution: Direct Connect is a durable infrastructure solution, ideal for continuous data transfer\nneeds.\nWhy other options are less ideal:\nA. AWS VPN and VPC Gateway Endpoint: While VPN provides secure connectivity, it still relies on the\nexisting internet bandwidth. VPC gateway endpoints allow access to S3 from within a VPC without using\npublic IPs, but the data still travels over the internet to the on-premises network. It does not solve the\nbandwidth issue.\nC. AWS Snowball: Snowball is useful for initial large-scale data migrations. Using it daily for backups is\nimpractical, expensive, and introduces significant delays in data availability.\nD. Support Ticket for S3 Limits: S3 service limits are designed for the overall health and availability of the\nservice, not for solving bandwidth problems. Requesting their removal is unlikely to be successful and isn't a\nproper architectural solution.\nAuthoritative Links:\nAWS Direct Connect: https://aws.amazon.com/directconnect/\nAWS VPN: https://aws.amazon.com/vpn/\nAWS Snowball: https://aws.amazon.com/snowball/",
    "links": [
      "https://aws.amazon.com/directconnect/",
      "https://aws.amazon.com/vpn/",
      "https://aws.amazon.com/snowball/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an Amazon S3 bucket that contains critical data. The company must protect the data from\naccidental deletion.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "options": {
      "A": "Enable versioning on the S3 bucket: S3 Versioning ensures that every version of an object is preserved,",
      "B": "Enable MFA Delete on the S3 bucket: MFA Delete adds an extra layer of security to prevent accidental or",
      "C": "Create a bucket policy on the S3 bucket: While bucket policies control access to the bucket, they don't",
      "D": "Enable default encryption on the S3 bucket: Default encryption protects data at rest, addressing security"
    },
    "answer": "A",
    "explanation": "The correct answer is AB.\nJustification:\nThe primary requirement is to protect the data from accidental deletion. Two features in S3 directly address\nthis: Versioning and MFA Delete.\nA. Enable versioning on the S3 bucket: S3 Versioning ensures that every version of an object is preserved,\neven if it's deleted or overwritten. When an object is deleted, it doesn't truly disappear; instead, a delete\nmarker is created. The previous version remains accessible, allowing for easy recovery from accidental\ndeletions. This is crucial for data protection.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/versioning-workflows.html\nB. Enable MFA Delete on the S3 bucket: MFA Delete adds an extra layer of security to prevent accidental or\nmalicious deletions. When MFA Delete is enabled, deleting a versioned object or permanently deleting the S3\nbucket itself requires multi-factor authentication. This significantly reduces the risk of unauthorized or\nunintentional data loss. It enforces the principle of \"something you know\" (password) and \"something you\nhave\" (MFA device), strengthening the deletion process.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html#MultiFactorAuthenticationDelete\nLet's examine why the other options are incorrect:\nC. Create a bucket policy on the S3 bucket: While bucket policies control access to the bucket, they don't\nprevent accidental deletions. You can restrict who can delete objects, but if someone with the delete\npermission makes a mistake, the policy won't prevent it.\nD. Enable default encryption on the S3 bucket: Default encryption protects data at rest, addressing security\nand compliance concerns related to data confidentiality. However, encryption does not prevent accidental\ndeletion.\nE. Create a lifecycle policy for the objects in the S3 bucket: Lifecycle policies manage the object lifecycle,\nautomatically transitioning them to cheaper storage classes or deleting them after a specified period. If\nmisconfigured, a lifecycle policy could actually cause accidental deletions. It's for automating data\narchival/deletion based on pre-defined rules, not for general protection against accidental deletion.",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/versioning-workflows.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html#MultiFactorAuthenticationDelete"
    ]
  },
  {
    "question": "CertyIQ\nA company has a data ingestion workflow that consists of the following:\n An Amazon Simple Notification Service (Amazon SNS) topic for notifications about new data deliveries\n An AWS Lambda function to process the data and record metadata\nThe company observes that the ingestion workflow fails occasionally because of network connectivity issues.\nWhen such a failure occurs, the Lambda function does not ingest the corresponding data unless the company\nmanually reruns the job.\nWhich combination of actions should a solutions architect take to ensure that the Lambda function ingests all data\nin the future? (Choose two.)",
    "options": {
      "A": "Deploy the Lambda function in multiple Availability Zones. While deploying Lambda in multiple AZs",
      "B": "Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe it to the SNS topic.",
      "C": "Increase the CPU and memory that are allocated to the Lambda function. Increasing CPU and memory",
      "D": "Increase provisioned throughput for the Lambda function. Lambda does not use provisioned throughput,"
    },
    "answer": "B",
    "explanation": "The correct answer is BE. Here's why:\nB. Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe it to the SNS topic.\nThis is a crucial step for building a reliable and resilient data ingestion pipeline. When an SNS topic is\nconnected to an SQS queue via subscription, messages published to the SNS topic are automatically queued\nin the SQS queue. SQS provides a mechanism to buffer the messages, ensuring that they are not lost even if\nthe Lambda function is temporarily unavailable due to network issues or other transient failures. This\nbuffering helps decoupling the data producers (SNS) from the data consumers (Lambda).\nE. Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue.\nBy having the Lambda function consume messages from the SQS queue, the function becomes more fault-\ntolerant. If a processing failure occurs during the initial Lambda execution (due to network issue), the\nmessage remains in the SQS queue. SQS can be configured to automatically retry the message delivery to the\nLambda function after a visibility timeout. This ensures that the Lambda function eventually processes all\nmessages, providing at-least-once delivery semantics. The queue acts as a buffer and retry mechanism,\nguaranteeing data ingestion despite intermittent\nfailures.https://docs.aws.amazon.com/lambda/latest/dg/services-sqs.html\nWhy other options are not suitable:\nA. Deploy the Lambda function in multiple Availability Zones. While deploying Lambda in multiple AZs\nimproves availability, it doesn't address message loss caused by transient network issues during the initial\ninvocation. Lambda inherently runs in multiple AZs managed by AWS.\nC. Increase the CPU and memory that are allocated to the Lambda function. Increasing CPU and memory\nmight help with performance if the Lambda function is resource-constrained, but it won't prevent message\nloss due to network issues.\nD. Increase provisioned throughput for the Lambda function. Lambda does not use provisioned throughput,\nand this action is not applicable. Also, it does not prevent message loss due to network issues.",
    "links": [
      "https://docs.aws.amazon.com/lambda/latest/dg/services-sqs.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has an application that provides marketing services to stores. The services are based on previous\npurchases by store customers. The stores upload transaction data to the company through SFTP, and the data is\nprocessed and analyzed to generate new marketing offers. Some of the files can exceed 200 GB in size.\nRecently, the company discovered that some of the stores have uploaded files that contain personally identifiable\ninformation (PII) that should not have been included. The company wants administrators to be alerted if PII is\nshared again. The company also wants to automate remediation.\nWhat should a solutions architect do to meet these requirements with the LEAST development effort?",
    "options": {
      "A": "Use an Amazon S3 bucket as a secure transfer point. Use Amazon Inspector to scan the objects in the",
      "B": "Use an Amazon S3 bucket as a secure transfer point. Use Amazon Macie to scan the objects in the bucket. If",
      "C": "Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are",
      "D": "Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are"
    },
    "answer": "B",
    "explanation": "The correct answer is B because it leverages purpose-built AWS services for data security and PII detection\nwith minimal development effort. Here's a detailed justification:\nS3 as a Secure Transfer Point: Using S3 provides a scalable and secure storage location for the uploaded\nfiles, replacing the potentially less secure SFTP method. S3 integrates well with other AWS services,\nsimplifying the overall workflow.\nAmazon Macie for PII Detection: Amazon Macie is a fully managed data security and data privacy service that\nuses machine learning and pattern matching to discover sensitive data, including PII. Macie is designed\nspecifically for this purpose and eliminates the need to develop custom scanning algorithms, adhering to the\nprinciple of \"least development effort\". It can automatically detect a wide range of PII types.\nSNS for Notifications: Amazon SNS is a simple and cost-effective way to send notifications to administrators\nwhen PII is detected. This allows for immediate action and investigation.\nWhy other options are less suitable:\nOption A suggests using Amazon Inspector, which primarily focuses on identifying security vulnerabilities and\ndeviations from security best practices within EC2 instances and container images, and doesn't specialize in\nPII detection within S3 objects like Macie. Also S3 Lifecycle policy removing without investigation is not ideal.\nOptions C and D propose implementing custom scanning algorithms in Lambda. Developing and maintaining\ncustom scanning algorithms would require significant development effort and expertise in PII identification,\nwhich contradicts the \"least development effort\" requirement. Option D uses SES which is similar to SNS,\nhowever, SES requires more configuration and isn't as straightforward for simple notification use cases.\nOption D also suggests removing \"meats\" which is not an option from the question.\nIn summary, option B provides a pre-built and managed solution for PII detection and notification, ensuring\nminimal development effort and a robust security posture.\nSupporting Links:\nAmazon Macie: https://aws.amazon.com/macie/\nAmazon S3: https://aws.amazon.com/s3/\nAmazon SNS: https://aws.amazon.com/sns/",
    "links": [
      "https://aws.amazon.com/macie/",
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/sns/"
    ]
  },
  {
    "question": "CertyIQ\nA company needs guaranteed Amazon EC2 capacity in three specific Availability Zones in a specific AWS Region\nfor an upcoming event that will last 1 week.\nWhat should the company do to guarantee the EC2 capacity?",
    "options": {
      "A": "Purchase Reserved Instances that specify the Region needed.",
      "B": "Create an On-Demand Capacity Reservation that specifies the Region needed.",
      "C": "Purchase Reserved Instances that specify the Region and three Availability Zones needed.",
      "D": "Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed."
    },
    "answer": "D",
    "explanation": "The correct answer is D: Create an On-Demand Capacity Reservation that specifies the Region and three\nAvailability Zones needed.\nHere's why:\nOn-Demand Capacity Reservations (ODCR): ODCRs provide a way to reserve EC2 instance capacity in a\nspecific Availability Zone for a specified duration. This guarantees that the required EC2 capacity will be\navailable when needed. Crucially, they allow you to target specific Availability Zones.\nAWS Documentation on Capacity Reservations\nReserved Instances (RI): While RIs offer a billing discount in exchange for a term commitment, they do not\nguarantee capacity. They provide discounted pricing for instances used but do not explicitly reserve\nresources in specific Availability Zones. RIs help lower costs, they do not fulfill the requirement for\nguaranteed capacity. Regional RIs apply to any AZ in the region.\nAWS Documentation on Reserved Instances\nWhy other options are incorrect:\nA & C: Purchasing Reserved Instances: As stated above, RIs do not guarantee EC2 capacity. Purchasing RIs\nonly reduces the cost of running instances; it doesn't reserve physical resources.\nB: Create an On-Demand Capacity Reservation that specifies the Region needed: This is not precise enough.\nThe company requires capacity reservations in three specific Availability Zones. Specifying only the region\nwould not guarantee the resources would be in the desired AZs.\nGiven the short duration (1 week) and the need for a guarantee the On-Demand Capacity Reservation is a\nperfect match.",
    "links": []
  },
  {
    "question": "CertyIQ\nA company's website uses an Amazon EC2 instance store for its catalog of items. The company wants to make sure\nthat the catalog is highly available and that the catalog is stored in a durable location.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Move the catalog to Amazon ElastiCache for Redis.",
      "B": "Deploy a larger EC2 instance with a larger instance store.",
      "C": "Move the catalog from the instance store to Amazon S3 Glacier Deep Archive.",
      "D": "Here's why:"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Here's why:\nThe question highlights two key requirements: high availability and durable storage for the website's catalog.\nInstance store volumes are ephemeral, meaning data is lost when the instance stops, terminates, or fails. This\nmakes them unsuitable for durable storage.\nOption A, moving to ElastiCache for Redis, isn't ideal for durable storage of a full catalog. Redis is an in-\nmemory data store, primarily used for caching to improve performance, not long-term storage. While data\npersistence can be configured, it's not the primary use case and introduces complexity compared to other\noptions.\nOption B, deploying a larger EC2 instance with a larger instance store, doesn't address the fundamental\nproblem of data loss upon instance failure. It merely provides more ephemeral storage, failing to meet the\ndurability requirement.\nOption C, moving to S3 Glacier Deep Archive, provides durable storage but is designed for infrequent access,\nlike archiving. It's unsuitable for a website catalog that requires frequent reads and writes. Retrieval times are\nmeasured in hours, making it impractical for serving web content.\nOption D, moving to Amazon EFS, is the best solution. EFS provides a scalable, highly available, and durable\nfile system that can be mounted by multiple EC2 instances simultaneously. This allows the catalog to be\nstored persistently and accessed by the web servers, ensuring high availability and durability. EFS replicates\ndata across multiple Availability Zones, providing resilience against failures.\nIn summary, EFS addresses both requirements: high availability through shared access and data replication,\nand durability through persistent storage. It's designed for file-based storage that can be easily accessed by\napplications, making it a good fit for a website catalog.\nFurther Reading:\nAmazon EFS: https://aws.amazon.com/efs/\nInstance Store Volumes: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html",
    "links": [
      "https://aws.amazon.com/efs/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html"
    ]
  },
  {
    "question": "CertyIQ\nA company stores call transcript files on a monthly basis. Users access the files randomly within 1 year of the call,\nbut users access the files infrequently after 1 year. The company wants to optimize its solution by giving users the\nability to query and retrieve files that are less than 1-year-old as quickly as possible. A delay in retrieving older\nfiles is acceptable.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "A": "Store individual files with tags in Amazon S3 Glacier Instant Retrieval. Query the tags to retrieve the files",
      "B": "Store individual files in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the files to S3",
      "C": "Store individual files with tags in Amazon S3 Standard storage. Store search metadata for each archive in",
      "D": "Store individual files in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3"
    },
    "answer": "B",
    "explanation": "Option B is the most cost-effective solution because it leverages S3 Intelligent-Tiering to automatically\noptimize storage costs based on access patterns, while also utilizing S3 Glacier for long-term archival.\nHere's a breakdown:\nS3 Intelligent-Tiering: This storage class automatically moves data between frequent, infrequent, and\narchive access tiers based on changing access patterns. This means frequently accessed files (within the first\nyear) reside in the more expensive but faster access tiers, ensuring quick retrieval. Infrequently accessed files\nare moved to cheaper tiers automatically.\nS3 Lifecycle Policies: These policies automate the transition of objects between storage classes. By moving\nfiles older than 1 year to S3 Glacier Flexible Retrieval (formerly S3 Glacier), the company benefits from\nsignificantly lower storage costs for infrequently accessed data.\nAmazon Athena and S3 Glacier Select: Athena enables querying data directly in S3 using standard SQL,\nwhile S3 Glacier Select allows querying data stored in S3 Glacier without needing to restore the entire object.\nThis allows the company to query and retrieve data irrespective of its storage class (S3 Intelligent-Tiering or\nS3 Glacier).\nThe other options are less ideal:\nOption A: S3 Glacier Instant Retrieval is the most expensive S3 Glacier tier. Using it for frequently accessed\nfiles in the first year defeats the purpose of cost optimization. Also, querying based on object tags isn't as\nefficient as using Athena for structured querying.\nOption C: Storing all files in S3 Standard for the first year is more expensive than using S3 Intelligent-Tiering,\nwhich dynamically adjusts storage costs. Storing search metadata in S3 Standard storage alongside the data\nis also redundant, as Athena can be used for that purpose.\nOption D: Storing search metadata in Amazon RDS is more complex and expensive than using Athena for\nquerying data in S3. S3 Glacier Deep Archive offers the lowest storage cost but has the highest retrieval\ntimes, which might be acceptable for older data but is not necessary when Glacier Flexible Retrieval meets\nthe requirement.\nIn summary, Option B provides the best balance of performance and cost-effectiveness by using S3\nIntelligent-Tiering for frequent access, S3 Lifecycle policies for automated archival, and Athena for querying\ndata across different storage classes.\nAuthoritative Links:\nS3 Intelligent-Tiering\nS3 Lifecycle Policies\nAmazon Athena\nS3 Glacier Select\nS3 Storage Classes",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has a production workload that runs on 1,000 Amazon EC2 Linux instances. The workload is powered by\nthird-party software. The company needs to patch the third-party software on all EC2 instances as quickly as\npossible to remediate a critical security vulnerability.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Create an AWS Lambda function: While Lambda is useful for event-driven tasks, it is less suited for",
      "B": "Configure AWS Systems Manager Patch Manager: Patch Manager is designed for automated OS-level",
      "C": "Schedule an AWS Systems Manager maintenance window: Maintenance Windows are valuable for",
      "D": "Use AWS Systems Manager Run Command to run a custom command that applies"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Use AWS Systems Manager Run Command to run a custom command that applies\nthe patch to all EC2 instances.\nHere's a detailed justification:\nThe primary requirement is to apply a third-party software patch to 1,000 EC2 instances as quickly as possible\nto remediate a critical security vulnerability. Speed and wide distribution are key.\nWhy Run Command is Best: AWS Systems Manager (SSM) Run Command allows you to remotely and\nsecurely manage the configuration of your managed instances. It lets you execute shell scripts or commands\non a large number of EC2 instances simultaneously. For rapidly deploying a patch to a known vulnerability,\nRun Command offers the most immediate and direct route. You create a shell script (or similar) containing the\npatching logic and then use Run Command to execute it on all instances. The entire operation can be\nexecuted within minutes.\nWhy other options are less suitable:\nA. Create an AWS Lambda function: While Lambda is useful for event-driven tasks, it is less suited for\ndirectly patching 1,000 EC2 instances. Lambda functions have execution time limits (up to 15 minutes), making\nit challenging to complete patching on a large number of instances reliably. Moreover, you'd still need to\nmanage the EC2 instance interaction, adding unnecessary complexity. The lambda function itself will need to\ncall SSM to run commands on the instances.\nB. Configure AWS Systems Manager Patch Manager: Patch Manager is designed for automated OS-level\npatching based on patch baselines. It's not ideal for deploying third-party software patches quickly, especially\nin an emergency situation. Setting up and configuring patch baselines and ensuring they are applied to 1000\nmachines will take longer than using Run Command directly.\nC. Schedule an AWS Systems Manager maintenance window: Maintenance Windows are valuable for\nscheduled tasks, but not for immediate, emergency patching. They introduce a delay before the patch can be\napplied. The requirement emphasizes quickly as possible, which Maintenance Windows don't fulfill.\nIn summary, Run Command is designed for ad-hoc operations and rapid execution of commands across fleets\nof instances, making it the optimal solution for patching a critical vulnerability as quickly as possible. It\nprovides direct control and immediate deployment capabilities, crucial for addressing security concerns\npromptly.\nHere are authoritative links for further research:\nAWS Systems Manager Run Command: https://docs.aws.amazon.com/systems-\nmanager/latest/userguide/execute-remote-commands.html\nAWS Systems Manager Patch Manager: https://docs.aws.amazon.com/systems-\nmanager/latest/userguide/patch-manager.html\nAWS Systems Manager Maintenance Windows: https://docs.aws.amazon.com/systems-\nmanager/latest/userguide/maintenance-windows.html",
    "links": [
      "https://docs.aws.amazon.com/systems-",
      "https://docs.aws.amazon.com/systems-",
      "https://docs.aws.amazon.com/systems-"
    ]
  },
  {
    "question": "CertyIQ\nA company is developing an application that provides order shipping statistics for retrieval by a REST API. The\ncompany wants to extract the shipping statistics, organize the data into an easy-to-read HTML format, and send\nthe report to several email addresses at the same time every morning.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "options": {
      "A": "Configure the application to send the data to Amazon Kinesis Data Firehose.",
      "B": "Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.",
      "C": "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Glue job",
      "D": "Here's why:"
    },
    "answer": "B",
    "explanation": "The correct answer is BD. Here's why:\nD: Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS\nLambda function to query the application's API for the data. Amazon EventBridge allows for scheduled,\nevent-driven automation. In this scenario, it's ideal for triggering the data extraction and report generation\nprocess every morning. Lambda offers a serverless compute environment where code can be executed\nwithout managing underlying infrastructure. A Lambda function can be coded to query the REST API, retrieve\nthe shipping statistics, and format the data into HTML. This addresses the requirement of extracting data and\norganizing it. https://aws.amazon.com/eventbridge/, https://aws.amazon.com/lambda/\nB: Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.\nAmazon SES is a cost-effective, scalable email service that provides a reliable way to send emails. The\nLambda function, after formatting the data into HTML, can leverage SES to send the report to the specified\nemail addresses. SES handles email delivery and can be configured to handle bounces and complaints. This\nsatisfies the requirement of sending the report to multiple email addresses in an easy-to-read HTML format.\nThe Lambda function would take on the additional task of the HTML formatting. https://aws.amazon.com/ses/\nWhy other options are incorrect:\nA: Configure the application to send the data to Amazon Kinesis Data Firehose. Kinesis Data Firehose is used\nfor streaming data to destinations like S3, Redshift, and Elasticsearch. It's not suitable for querying data from\nan API and generating reports.\nC: Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Glue\njob to query the application's API for the data. AWS Glue is an ETL (Extract, Transform, Load) service\nprimarily designed for processing large datasets. Using Glue to directly query an API and generate a single\nHTML report is overkill and less efficient than using Lambda. Glue is usually used against a data lake, not an\noperational API.\nE: Store the application data in Amazon S3. Create an Amazon Simple Notification Service (Amazon SNS)\ntopic as an S3 event destination to send the report by email. S3 is for object storage and would only make\nsense if the source data (the shipping statistics) were already available within S3, which isn't indicated in the\nproblem description. SNS is for simple notifications and not suited for complex HTML formatting or extracting\ndata from APIs. S3 event notifications are also triggered by data changes, not scheduled events.",
    "links": [
      "https://aws.amazon.com/eventbridge/,",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/ses/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to migrate its on-premises application to AWS. The application produces output files that vary in\nsize from tens of gigabytes to hundreds of terabytes. The application data must be stored in a standard file system\nstructure. The company wants a solution that scales automatically. is highly available, and requires minimum\noperational overhead.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Migrate the application to run as containers on Amazon Elastic Container Service (Amazon ECS). Use Amazon",
      "B": "Migrate the application to run as containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use",
      "C": "Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic File",
      "D": "Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it addresses all the requirements: scalability, high availability, standard file\nsystem structure, and minimal operational overhead.\nScalability: Amazon EFS scales automatically to petabytes without disrupting applications, handling the\nvaried file sizes (tens of gigabytes to hundreds of terabytes). EC2 Auto Scaling ensures the application\ninstances scale based on demand.\nHigh Availability: Using a Multi-AZ Auto Scaling group ensures that the application is highly available\nbecause EC2 instances are spread across multiple Availability Zones. EFS is also designed for high availability\nand durability, replicating data across multiple AZs.\nStandard File System Structure: EFS provides a standard file system interface (NFSv4.1) that allows\napplications to interact with storage in a familiar way.\nMinimum Operational Overhead: EFS is a fully managed service, which eliminates the need for manual\nprovisioning, patching, or backups. EC2 Auto Scaling automates the scaling of instances.\nOption A is incorrect because S3 is an object storage service and does not provide a standard file system\ninterface. Option B is incorrect because EBS is block storage and would not scale elastically for the varying\nsize files, and it has more operational overhead compared to EFS. Option D is incorrect because EBS is block\nstorage and not a shared file system. Mounting and sharing EBS volumes between multiple instances\nintroduces complexities.Here are some authoritative links for further research:\nAmazon EFS: https://aws.amazon.com/efs/\nAmazon EC2 Auto Scaling: https://aws.amazon.com/autoscaling/\nAmazon EC2: https://aws.amazon.com/ec2/",
    "links": [
      "https://aws.amazon.com/efs/",
      "https://aws.amazon.com/autoscaling/",
      "https://aws.amazon.com/ec2/"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to store its accounting records in Amazon S3. The records must be immediately accessible for 1\nyear and then must be archived for an additional 9 years. No one at the company, including administrative users\nand root users, can be able to delete the records during the entire 10-year period. The records must be stored with\nmaximum resiliency.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Store the records in S3 Glacier for the entire 10-year period. Use an access control policy to deny deletion of",
      "B": "Store the records by using S3 Intelligent-Tiering. Use an IAM policy to deny deletion of the records. After 10",
      "C": "Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year.",
      "D": "Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 One Zone-Infrequent Access (S3"
    },
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the correct answer:\nOption C effectively addresses all requirements: immediate accessibility for 1 year, archival for 9 years,\nimmutability (preventing deletion), and maximum resiliency.\n1. Initial Accessibility: S3 Standard offers immediate access to the records for the first year.\n2. Archival Storage: Transitioning the data to S3 Glacier Deep Archive after one year via an S3\nLifecycle policy fulfills the archival requirement and is the most cost-effective storage option for\nlong-term retention with infrequent access. S3 Glacier Deep Archive is designed for data that is\nrarely accessed.\n3. Immutability: S3 Object Lock in compliance mode is crucial. Compliance mode ensures that no one,\nincluding administrative users or the root user, can delete the objects during the specified retention\nperiod (10 years in this case). This definitively meets the immutability requirement.\n4. Resiliency: S3 Standard and S3 Glacier Deep Archive provide the highest levels of data durability\nand availability by storing data across multiple devices and facilities.\nWhy other options are incorrect:\nA: S3 Glacier is not designed for immediate accessibility. It's for infrequent access with retrieval times\nranging from minutes to hours, not the \"immediately accessible\" requirement of the first year. Access control\npolicies and IAM Policies can be bypassed or modified, which means they cannot guarantee immutability.\nB: S3 Intelligent-Tiering automatically moves data between frequent, infrequent, and archive access tiers\nbased on access patterns. While useful for cost optimization, it doesn't guarantee immutability. IAM policies\ncan be bypassed or modified.\nD: S3 One Zone-IA offers lower availability (data is stored in a single Availability Zone), which does not meet\nthe \"maximum resiliency\" requirement. Also, S3 Object Lock in governance mode can be overridden by users\nwith specific IAM permissions, failing to guarantee immutability against administrative users or root.\nAuthoritative Links:\nS3 Storage Classes: https://aws.amazon.com/s3/storage-classes/\nS3 Lifecycle Policies: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-\nconcept.html\nS3 Object Lock: https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html\nObject Lock Compliance vs Governance Mode:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html\nIn summary, Option C provides the optimal balance of accessibility, cost-effectiveness, immutability, and\nresiliency to meet the stated requirements, making it the correct answer.",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs multiple Windows workloads on AWS. The company's employees use Windows file shares that are\nhosted on two Amazon EC2 instances. The file shares synchronize data between themselves and maintain\nduplicate copies. The company wants a highly available and durable storage solution that preserves how users\ncurrently access the files.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Migrate all the data to Amazon S3. Set up IAM authentication for users to access files.",
      "B": "Set up an Amazon S3 File Gateway. Mount the S3 File Gateway on the existing EC2 instances.",
      "C": "Here's why:",
      "D": "Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Here's why:\nThe requirement is to provide a highly available and durable file share solution for Windows workloads while\npreserving the existing user access patterns.\nOption A (Amazon S3 with IAM): While S3 is highly available and durable, it doesn't directly support Windows\nfile shares. Users would need to adapt to a completely different access method, breaking the \"preserve how\nusers currently access the files\" requirement. IAM authentication, although secure, doesn't replicate the file\nshare experience.\nOption B (Amazon S3 File Gateway): S3 File Gateway provides a way to access S3 objects as files, but it\ndoesn't replicate a traditional Windows file share. It adds complexity to the existing EC2 setup and doesn't\ninherently provide high availability for the gateway itself. Users would still potentially experience access\ndisruptions if the gateway instance fails.\nOption C (Amazon FSx for Windows File Server with Multi-AZ): Amazon FSx for Windows File Server is a\nfully managed service specifically designed for providing native Windows file server capabilities in the cloud.\nIt's built on Windows Server, so it directly supports SMB file shares and integrates seamlessly with existing\nActive Directory environments for authentication. A Multi-AZ configuration provides high availability by\nautomatically replicating data across multiple Availability Zones. This ensures that if one AZ fails, the file\nshare remains accessible to users. Migrating the data to FSx for Windows File Server preserves the existing\nuser access patterns as they continue to use familiar Windows file share protocols.\nOption D (Amazon EFS with Multi-AZ): Amazon EFS is a network file system designed for Linux-based\nworkloads. While it offers high availability and durability, it's not a native Windows file server solution and\ndoesn't directly support SMB file shares. Integrating EFS with Windows workloads would require additional\nconfiguration and wouldn't provide the same seamless experience as FSx for Windows File Server.\nIn conclusion, Option C provides the best solution as it directly addresses all the requirements: high\navailability, durability, preservation of existing user access methods, and native support for Windows file\nshares through Amazon FSx for Windows File Server with a Multi-AZ configuration.\nAuthoritative Links for Further Research:\nAmazon FSx for Windows File Server: https://aws.amazon.com/fsx/windows/\nHigh Availability (HA) and Multi-AZ: https://docs.aws.amazon.com/fsx/latest/WindowsGuide/high-\navailability.html",
    "links": [
      "https://aws.amazon.com/fsx/windows/",
      "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/high-"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is developing a VPC architecture that includes multiple subnets. The architecture will host\napplications that use Amazon EC2 instances and Amazon RDS DB instances. The architecture consists of six\nsubnets in two Availability Zones. Each Availability Zone includes a public subnet, a private subnet, and a\ndedicated subnet for databases. Only EC2 instances that run in the private subnets can have access to the RDS\ndatabases.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Create a new route table that excludes the route to the public subnets' CIDR blocks. Associate the route",
      "B": "Create a security group that denies inbound traffic from the security group that is assigned to instances in",
      "C": "The EC2 instances and",
      "D": "Create a new peering connection between the public subnets and the private subnets. Create a different"
    },
    "answer": "C",
    "explanation": "The correct solution (C) leverages security groups to control network access to the RDS instances. Security\ngroups act as virtual firewalls at the instance level, controlling both inbound and outbound traffic. To meet\nthe requirement that only EC2 instances in the private subnets can access the RDS databases, we create a\nsecurity group that specifically allows inbound traffic only from the security group assigned to those EC2\ninstances in the private subnets.\nHere's why the other options are not ideal:\nA: Creating a route table that excludes routes to public subnets' CIDR blocks and associating it with the\ndatabase subnets would primarily affect routing and not precisely control which instances can access the\ndatabase. It might block general traffic from public subnets to database subnets, but it wouldn't isolate\naccess to only EC2 instances associated with a specific security group in the private subnets. It also makes\nthe configuration less flexible and scalable.\nB: Denying inbound traffic from the public subnet's security group at the database instance is a step in the\nright direction, but it doesn't explicitly allow traffic from the private subnets. This approach might\ninadvertently block legitimate traffic originating from other sources within the private subnets that were\nintended to connect to the database. The best practice is to use a principle of least privilege by allowing only\nthe necessary traffic.\nD: Peering connections are for connecting VPCs, not subnets within the same VPC. The EC2 instances and\nRDS instances are already within the same VPC, so subnet-level peering is unnecessary and not a feasible\nsolution. Peering creates network connectivity between entire VPCs, adding unnecessary complexity and cost\nfor this specific requirement. Subnets within a VPC inherently communicate via the VPC's internal routing.\nTherefore, option C is the most precise, secure, and efficient way to ensure that only EC2 instances in the\nprivate subnets can access the RDS instances, adhering to the principle of least privilege and utilizing the\ninherent security features provided by security groups.\nSupporting Links:\nAmazon VPC Security Groups: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-groups.html\nControlling Traffic to Resources Using Security Groups:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithSecurityGroups.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-groups.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithSecurityGroups.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has registered its domain name with Amazon Route 53. The company uses Amazon API Gateway in the\nca-central-1 Region as a public interface for its backend microservice APIs. Third-party services consume the APIs\nsecurely. The company wants to design its API Gateway URL with the company's domain name and corresponding\ncertificate so that the third-party services can use HTTPS.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Create stage variables in API Gateway with Name=\"Endpoint-URL\" and Value=\"Company Domain Name\" to",
      "B": "Create Route 53 DNS records with the company's domain name. Point the alias record to the Regional API",
      "C": "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain",
      "D": "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it aligns with the best practices for using custom domain names with API\nGateway and ensuring secure HTTPS communication. Here's why:\nRegional API Gateway Endpoint: Creating a Regional API Gateway endpoint is essential. It allows associating\na custom domain name and certificate within a specific AWS Region (ca-central-1 in this case), providing lower\nlatency and better control compared to edge-optimized endpoints in this scenario.\nCustom Domain Association: Associating the API Gateway endpoint with the company's domain name is a key\nstep in providing a user-friendly and branded URL for the APIs. This is the fundamental step toward using the\ncompany's domain instead of the default API Gateway URL.\nACM Certificate Import and Region Specificity: Importing the SSL/TLS certificate into AWS Certificate\nManager (ACM) in the same Region as the API Gateway (ca-central-1) is crucial. ACM certificates are region-\nspecific, and the API Gateway needs to be able to access the certificate to establish secure HTTPS\nconnections. Answer D fails on this very important concept.\nCertificate Attachment: Attaching the certificate to the API Gateway endpoint ensures that the API Gateway\nuses the certificate for SSL/TLS termination, enabling HTTPS for clients accessing the APIs.\nRoute 53 Configuration: Configuring Route 53 to route traffic to the API Gateway endpoint is the final step.\nRoute 53 maps the company's domain name to the API Gateway endpoint, ensuring that requests to the\ncustom domain name are directed to the API Gateway.Option A is incorrect as stage variables are generally\nfor API Gateway configuration values, not for custom domain mapping. Option B is incorrect because, while\npointing a Route 53 record to an API Gateway endpoint is correct, the ACM certificate must be in the same\nRegion as the API Gateway. Option D incorrectly states that ACM certificates need to be in us-east-1. They\nneed to be in the same region as the API Gateway. It also incorrectly states that the certificate needs to be\nattached to the API Gateway APIs (plural). The certificate is attached to the API Gateway endpoint.\nSupporting Links:\nCustom Domain Names for API Gateway:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-custom-domain-name.html\nAWS Certificate Manager (ACM): https://aws.amazon.com/certificate-manager/\nAmazon Route 53: https://aws.amazon.com/route53/",
    "links": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-custom-domain-name.html",
      "https://aws.amazon.com/certificate-manager/",
      "https://aws.amazon.com/route53/"
    ]
  },
  {
    "question": "CertyIQ\nA company is running a popular social media website. The website gives users the ability to upload images to share\nwith other users. The company wants to make sure that the images do not contain inappropriate content. The\ncompany needs a solution that minimizes development effort.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Use Amazon Comprehend to detect inappropriate content. Use human review for low-confidence predictions.",
      "B": "Use Amazon Rekognition to detect inappropriate content. Use human review for low-confidence predictions.",
      "C": "Use Amazon SageMaker to detect inappropriate content. Use ground truth to label low-confidence",
      "D": "Use AWS Fargate to deploy a custom machine learning model to detect inappropriate content. Use ground"
    },
    "answer": "B",
    "explanation": "The correct answer is B because it leverages Amazon Rekognition's pre-trained models specifically designed\nfor image analysis, including the detection of explicit or suggestive content. This significantly reduces\ndevelopment effort compared to building a custom model.\nAmazon Rekognition's Content Moderation feature can identify various types of inappropriate content, such\nas nudity, violence, and hate symbols. The service provides a confidence score for each detection. For\ndetections with low confidence scores, a human review workflow can be implemented to ensure accuracy and\nprevent false positives or negatives. This hybrid approach balances automation with human oversight.\nOption A is incorrect because Amazon Comprehend is a natural language processing (NLP) service, best\nsuited for analyzing text, not images. Option C is incorrect because Amazon SageMaker is a machine learning\nplatform for building, training, and deploying custom models. While it's possible to build a custom image\nmoderation model with SageMaker, it requires significantly more development effort than using Rekognition's\npre-built capabilities. Ground Truth is used for data labeling in SageMaker and isn't the optimal choice when\npre-trained models with confidence scores and built in human review workflows are available. Option D is\nincorrect because deploying a custom machine learning model on AWS Fargate also requires significant\ndevelopment and operational overhead compared to utilizing a managed service like Rekognition. Fargate\nprovides serverless compute for containers, not a machine learning image analysis service. Building custom\nmodels and infrastructure is unnecessary when a suitable managed service already exists. Rekognition\nprovides a more straightforward and efficient solution for the stated requirements.Authoritative links:\nAmazon Rekognition Content Moderation: https://aws.amazon.com/rekognition/content-moderation/",
    "links": [
      "https://aws.amazon.com/rekognition/content-moderation/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to run its critical applications in containers to meet requirements for scalability and availability.\nThe company prefers to focus on maintenance of the critical applications. The company does not want to be\nresponsible for provisioning and managing the underlying infrastructure that runs the containerized workload.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Use Amazon EC2 instances, and install Docker on the instances.",
      "B": "Use Amazon Elastic Container Service (Amazon ECS) on Amazon EC2 worker nodes.",
      "C": "Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate.",
      "D": "Use Amazon EC2 instances from an Amazon Elastic Container Service (Amazon ECS)-optimized Amazon"
    },
    "answer": "C",
    "explanation": "The best solution is to use Amazon ECS on AWS Fargate because it abstracts away the need to manage the\nunderlying infrastructure. Fargate is a serverless compute engine for containers that lets you run containers\nwithout managing servers or clusters. This aligns perfectly with the company's preference to focus on\napplication maintenance and avoid infrastructure management.\nOption A requires manual installation and configuration of Docker on EC2 instances, placing the burden of\nserver maintenance and scaling on the company. Option B, while using ECS, still involves managing EC2\nworker nodes. This means patching, scaling, and ensuring the availability of the underlying EC2 instances.\nOption D, using an ECS-optimized AMI on EC2, simplifies some aspects of EC2 management, but the company\nis still responsible for the EC2 instances themselves.\nFargate eliminates these responsibilities by handling the provisioning, scaling, and patching of the\ninfrastructure. The company simply defines the container image, CPU, and memory requirements, and Fargate\nhandles the rest. This reduces operational overhead and allows the company to focus solely on their critical\napplications. Furthermore, Fargate integrates well with other AWS services and offers cost optimization\nbenefits.https://aws.amazon.com/fargate/https://aws.amazon.com/ecs/",
    "links": [
      "https://aws.amazon.com/fargate/https://aws.amazon.com/ecs/"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts more than 300 global websites and applications. The company requires a platform to analyze\nmore than 30 TB of clickstream data each day.\nWhat should a solutions architect do to transmit and process the clickstream data?",
    "options": {
      "A": "Design an AWS Data Pipeline to archive the data to an Amazon S3 bucket and run an Amazon EMR cluster",
      "B": "Create an Auto Scaling group of Amazon EC2 instances to process the data and send it to an Amazon S3",
      "C": "Cache the data to Amazon CloudFront. Store the data in an Amazon S3 bucket. When an object is added to",
      "D": "Collect the data from Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to transmit the data"
    },
    "answer": "D",
    "explanation": "The most effective solution for handling 30 TB of daily clickstream data from 300+ global websites and\napplications for analysis involves a scalable and robust data ingestion and processing pipeline using AWS's\nspecialized services for big data. Option D is the best approach.\nHere's a breakdown:\nData Ingestion (Amazon Kinesis Data Streams): Kinesis Data Streams is designed for real-time ingestion of\nhigh-volume data streams. It can handle the constant flow of clickstream data from numerous sources\nglobally. It allows the data to be buffered and processed in manageable chunks.\nData Delivery (Amazon Kinesis Data Firehose): Kinesis Data Firehose is ideal for reliably loading data into\ndata lakes and data warehouses. It can automatically scale to match the incoming data volume and deliver\ndata to S3. It offers data transformation and batching capabilities.\nData Lake (Amazon S3): S3 provides a scalable and cost-effective storage solution for the large volume of\nclickstream data. It serves as a central repository for raw and processed data, forming the foundation of the\ndata lake.\nData Warehousing (Amazon Redshift): Redshift is a fast, fully managed, petabyte-scale data warehouse\nservice in the cloud. It is optimized for analytical queries and can efficiently analyze the large dataset stored\nin S3.\nOption A is less desirable because while EMR can process large datasets, using Data Pipeline solely for\narchiving and triggering EMR jobs introduces unnecessary complexity. Data Pipeline is typically used for\norchestration, and Kinesis Data Firehose is more streamlined for data delivery to S3. Option B involves\nmanaging EC2 instances, which adds operational overhead compared to using managed services like Kinesis\nand Redshift. While caching data with CloudFront, as suggested in Option C, can improve website\nperformance, it doesn't directly address the core problem of analyzing 30 TB of clickstream data daily. A\nLambda function triggered by S3 objects may struggle with the sheer volume of data.\nIn summary, using Kinesis Data Streams and Firehose ensures real-time data ingestion and delivery to a data\nlake (S3), where it can be efficiently analyzed by Redshift, offering a scalable, managed, and cost-effective\nsolution.\nRelevant links:\nAmazon Kinesis Data Streams: https://aws.amazon.com/kinesis/data-streams/\nAmazon Kinesis Data Firehose: https://aws.amazon.com/kinesis/data-firehose/\nAmazon S3: https://aws.amazon.com/s3/\nAmazon Redshift: https://aws.amazon.com/redshift/",
    "links": [
      "https://aws.amazon.com/kinesis/data-streams/",
      "https://aws.amazon.com/kinesis/data-firehose/",
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/redshift/"
    ]
  },
  {
    "question": "CertyIQ\nA company has a website hosted on AWS. The website is behind an Application Load Balancer (ALB) that is\nconfigured to handle HTTP and HTTPS separately. The company wants to forward all requests to the website so\nthat the requests will use HTTPS.\nWhat should a solutions architect do to meet this requirement?",
    "options": {
      "A": "Update the ALB's network ACL to accept only HTTPS traffic.",
      "B": "Create a rule that replaces the HTTP in the URL with HTTPS.",
      "C": "Create a listener rule on the ALB to redirect HTTP traffic to HTTPS.",
      "D": "Replace the ALB with a Network Load Balancer configured to use Server Name Indication (SNI)."
    },
    "answer": "C",
    "explanation": "The correct solution is to create a listener rule on the Application Load Balancer (ALB) to redirect HTTP traffic\nto HTTPS (Option C). Here's why:\nAn ALB listens for incoming traffic on specified ports and protocols. To redirect HTTP traffic to HTTPS, you\nconfigure a listener on port 80 (HTTP) and add a rule to redirect all incoming requests to port 443 (HTTPS).\nThis is a standard practice for ensuring secure communication.\nOption A (Updating the ALB's network ACL) is incorrect because network ACLs operate at the subnet level\nand control inbound and outbound traffic based on IP addresses and ports. While you can restrict HTTP traffic\nat the network ACL level, this would block HTTP traffic entirely rather than redirecting it to HTTPS. Network\nACLs are not designed for URL redirection.\nOption B (Creating a rule to replace HTTP in the URL with HTTPS) is incorrect because ALBs don't directly\nmanipulate URLs in that way. ALBs can perform content-based routing, but not URL rewriting in the context\ndescribed. The primary function of the ALB in this scenario is to redirect traffic based on listener rules.\nOption D (Replacing the ALB with a Network Load Balancer) is incorrect because Network Load Balancers\n(NLBs) operate at Layer 4 (TCP/UDP), while ALBs operate at Layer 7 (Application Layer). NLBs do not have the\ncapability to inspect HTTP headers or redirect traffic based on the HTTP protocol. Server Name Indication\n(SNI) on NLBs handles TLS certificates for multiple domains but does not redirect HTTP to HTTPS. NLBs are\ntypically used for high-performance, low-latency applications where protocol-level inspection is not required.\nAn ALB is better suited for HTTP/HTTPS traffic management and URL redirection.\nTherefore, the most efficient and appropriate method is to use an ALB listener rule to redirect HTTP traffic to\nHTTPS, ensuring all communication with the website is secure.\nFurther Research:\nAWS Documentation on ALB Listeners:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html\nAWS Documentation on ALB Rules:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-rules.html\nAWS Documentation on Network ACLs: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-\nacls.html\nAWS Documentation on Network Load Balancers:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html",
    "links": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-rules.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is developing a two-tier web application on AWS. The company's developers have deployed the\napplication on an Amazon EC2 instance that connects directly to a backend Amazon RDS database. The company\nmust not hardcode database credentials in the application. The company must also implement a solution to\nautomatically rotate the database credentials on a regular basis.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Store the database credentials in the instance metadata. Use Amazon EventBridge (Amazon CloudWatch",
      "B": "Store the database credentials in a configuration file in an encrypted Amazon S3 bucket. Use Amazon",
      "C": "Store the database credentials as a secret in AWS Secrets Manager. Turn on automatic rotation for the",
      "D": "Store the database credentials as encrypted parameters in AWS Systems Manager Parameter Store. Turn on"
    },
    "answer": "C",
    "explanation": "Option C is the most efficient and secure solution because it leverages AWS Secrets Manager for credential\nmanagement and rotation.\nHere's why:\n1. Secrets Manager: AWS Secrets Manager is specifically designed for securely storing and managing\nsensitive information like database credentials. It eliminates the need to hardcode credentials in\napplications.\n2. Automatic Rotation: Secrets Manager provides automatic secret rotation, significantly reducing the\noperational overhead associated with manually managing credentials. The service handles the\ncomplexities of updating the database and the stored secret.\n3. IAM Integration: IAM roles assigned to EC2 instances can be granted permissions to access specific\nsecrets stored in Secrets Manager. This allows the application running on the EC2 instance to\nretrieve the database credentials securely.\n4. Least Operational Overhead: Solutions A, B, and D require custom Lambda functions and scheduling\nmechanisms to manage credential rotation. Option C automates this process, which results in the\nlowest operational overhead because Secrets Manager handles the complexity of rotating the\ncredentials.\n5. Security Best Practices: Using Secrets Manager aligns with security best practices by centralizing\nand securing sensitive information and using encryption.\nWhy other options are not ideal:\nOption A: Storing credentials in instance metadata is not recommended for sensitive information like\ndatabase credentials. Also, instance metadata is not designed for automated rotation.\nOption B: Storing credentials in S3, even encrypted, is a less secure and operationally heavier approach than\nusing Secrets Manager. Implementing the rotation logic and managing S3 versions introduces unnecessary\ncomplexity.\nOption D: Parameter Store can store secrets, but automatic rotation of encrypted parameters requires\ncustom configuration and Lambda functions. Secrets Manager is a more streamlined and secure solution for\nthis specific use case.\nHere are some authoritative links for further research:\nAWS Secrets Manager Documentation\nRotating AWS Secrets Manager secrets automatically\nAWS Identity and Access Management (IAM)",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is deploying a new public web application to AWS. The application will run behind an Application Load\nBalancer (ALB). The application needs to be encrypted at the edge with an SSL/TLS certificate that is issued by an\nexternal certificate authority (CA). The certificate must be rotated each year before the certificate expires.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "ACM's managed renewal feature only works for ACM-issued",
      "B": "Because the certificate is externally issued, ACM cannot",
      "C": "Use AWS Certificate Manager (ACM) Private Certificate Authority to issue an SSL/TLS certificate from the",
      "D": "Use AWS Certificate Manager (ACM) to import an SSL/TLS certificate. Apply the certificate to the ALB. Use"
    },
    "answer": "D",
    "explanation": "The correct answer is D because it addresses the specific requirements of using a certificate issued by an\nexternal CA and the need to manually rotate it. Let's break down why the other options are incorrect and why\nD is the best fit:\nA, B, and C are incorrect: These options involve using ACM to issue a certificate. The question explicitly states\nthat the certificate is issued by an external CA. ACM's managed renewal feature only works for ACM-issued\ncertificates. Therefore, relying on ACM to issue and automatically rotate the certificate is not possible when\nusing an externally issued certificate. Option B contains the erroneous step of importing key material from a\ncertificate (a certificate is the key material). Option C involves a private certificate authority, an unnecessary\nlevel of complexity when an existing public CA is being used.\nD is correct: This option acknowledges the need to import the externally issued certificate into ACM. ACM\nserves as a central repository for managing certificates, regardless of where they are issued. Once imported,\nthe certificate can be associated with the ALB. Because the certificate is externally issued, ACM cannot\nautomatically renew it. Therefore, the solution leverages Amazon EventBridge (formerly CloudWatch Events)\nto monitor the certificate's expiration date. When the certificate is nearing expiry, EventBridge triggers a\nnotification (e.g., via SNS to an operations team), prompting a manual rotation process. This process involves\nobtaining a new certificate from the external CA, importing it into ACM, and updating the ALB to use the new\ncertificate.\nIn summary, option D correctly acknowledges the externally issued certificate, utilizes ACM for management,\nand implements a notification system to ensure manual rotation occurs before the certificate expires, thus\nmeeting all requirements.\nRelevant links:\nAWS Certificate Manager (ACM): https://aws.amazon.com/certificate-manager/\nImporting Certificates into ACM: https://docs.aws.amazon.com/acm/latest/userguide/import-certificate.html\nAmazon EventBridge: https://aws.amazon.com/eventbridge/\nApplication Load Balancer (ALB): https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
    "links": [
      "https://aws.amazon.com/certificate-manager/",
      "https://docs.aws.amazon.com/acm/latest/userguide/import-certificate.html",
      "https://aws.amazon.com/eventbridge/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs its infrastructure on AWS and has a registered base of 700,000 users for its document\nmanagement application. The company intends to create a product that converts large .pdf files to .jpg image files.\nThe .pdf files average 5 MB in size. The company needs to store the original files and the converted files. A\nsolutions architect must design a scalable solution to accommodate demand that will grow rapidly over time.\nWhich solution meets these requirements MOST cost-effectively?",
    "options": {
      "A": "Save the .pdf files to Amazon S3. Configure an S3 PUT event to invoke an AWS Lambda function to convert",
      "B": "C. Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon",
      "D": "Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon"
    },
    "answer": "A",
    "explanation": "Here's why option A is the most cost-effective solution for the PDF to JPG conversion problem, along with\nsupporting justifications and links:\nOption A leverages the strengths of Amazon S3 and AWS Lambda for a serverless and scalable solution. S3\nprovides highly durable and cost-effective object storage for both the original PDFs and the converted JPGs.\nIts event notification system allows triggering the Lambda function directly upon PDF upload, automating the\nconversion process. Lambda offers pay-per-execution pricing, meaning you only pay for the compute time\nused during the conversion, making it exceptionally cost-efficient for variable workloads. This serverless\napproach eliminates the need to manage EC2 instances, reducing operational overhead and associated costs.\nOption B is incorrect because DynamoDB is primarily designed for fast key-value or document data storage\nand retrieval, not for storing large binary files like PDFs and JPGs. Storing these files in DynamoDB would be\nsignificantly more expensive than using S3, and the read/write capacity units required would add to cost\nunnecessarily. Also, using DynamoDB streams for processing is less efficient than S3 event notifications in\nthis scenario.\nOptions C and D involve using Elastic Beanstalk with EC2 instances and EBS/EFS for storage. While these\noptions could work, they are less cost-effective because they require maintaining EC2 instances even during\nperiods of low demand. EBS, being block storage, is primarily for persistent storage attached to EC2\ninstances, while EFS is better suited for shared file systems. Neither are as economical for storing large\nnumbers of files as S3. The added operational complexity of managing EC2 instances, Auto Scaling, and\nstorage volumes further increases the total cost.\nIn summary, the serverless architecture provided by S3 and Lambda offers the best combination of scalability,\nautomation, and cost-effectiveness for this specific PDF-to-JPG conversion task.\nSupporting Links:\nAmazon S3: https://aws.amazon.com/s3/\nAWS Lambda: https://aws.amazon.com/lambda/\nS3 Event Notifications: https://docs.aws.amazon.com/AmazonS3/latest/userguide/event-notifications-\noverview.html\nAWS Lambda Pricing: https://aws.amazon.com/lambda/pricing/",
    "links": [
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/lambda/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/event-notifications-",
      "https://aws.amazon.com/lambda/pricing/"
    ]
  },
  {
    "question": "CertyIQ\nA company has more than 5 TB of file data on Windows file servers that run on premises. Users and applications\ninteract with the data each day.\nThe company is moving its Windows workloads to AWS. As the company continues this process, the company\nrequires access to AWS and on-premises file storage with minimum latency. The company needs a solution that\nminimizes operational overhead and requires no significant changes to the existing file access patterns. The\ncompany uses an AWS Site-to-Site VPN connection for connectivity to AWS.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Deploy and configure Amazon FSx for Windows File Server on AWS. Move the on-premises file data to FSx",
      "B": "Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to the S3 File",
      "C": "Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to Amazon",
      "D": "Deploy and configure Amazon FSx for Windows File Server on AWS. Deploy and configure an Amazon FSx"
    },
    "answer": "D",
    "explanation": "The correct answer is D because it directly addresses all requirements with a balanced approach to\nperformance, operational overhead, and minimal disruption. Here's a breakdown:\nMinimum Latency: Deploying FSx File Gateway on-premises allows the on-premises workloads to access the\nfile data stored in FSx for Windows File Server in AWS with minimal latency. The FSx File Gateway acts as a\nlocal cache for frequently accessed files, ensuring quick access for on-premises users and applications.\nAccess to AWS and On-Premises: FSx for Windows File Server on AWS stores the data in the cloud, providing\naccess to cloud workloads. The FSx File Gateway provides access to the same data for on-premises\nworkloads.\nMinimized Operational Overhead: FSx File Gateway is a fully managed service, reducing the operational\nburden of managing storage infrastructure on-premises. The management of the data is centralized on AWS.\nNo Significant Changes to File Access Patterns: Both FSx for Windows File Server and FSx File Gateway\nsupport the standard SMB protocol, allowing existing Windows applications to access files without significant\ncode modifications.\nWhy other options are incorrect:\nA is incorrect because moving all the data to AWS and accessing it over Site-to-Site VPN would likely\nincrease the latency for on-premises workloads.\nB and C are incorrect because S3 File Gateway replicates data to Amazon S3, which is object storage and\ndoesn't natively support SMB protocol. Thus, the workloads would need to be reconfigured to use S3 APIs,\nwhich would be a significant change to existing file access patterns, and S3 File Gateway on-premises still\nmight increase the latency for on-premises workloads compared to a dedicated solution like FSx File\nGateway.\nHere are some authoritative links for further research:\nAmazon FSx for Windows File Server: https://aws.amazon.com/fsx/windows/\nAmazon FSx File Gateway: https://aws.amazon.com/fsx/file-gateway/\nAmazon S3 File Gateway: https://aws.amazon.com/storagegateway/file/",
    "links": [
      "https://aws.amazon.com/fsx/windows/",
      "https://aws.amazon.com/fsx/file-gateway/",
      "https://aws.amazon.com/storagegateway/file/"
    ]
  },
  {
    "question": "CertyIQ\nA hospital recently deployed a RESTful API with Amazon API Gateway and AWS Lambda. The hospital uses API\nGateway and Lambda to upload reports that are in PDF format and JPEG format. The hospital needs to modify the\nLambda code to identify protected health information (PHI) in the reports.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Use existing Python libraries to extract the text from the reports and to identify the PHI from the extracted",
      "B": "Use Amazon Textract to extract the text from the reports. Use Amazon SageMaker to identify the PHI from",
      "C": "Here's a detailed justification:",
      "D": "Use Amazon Rekognition to extract the text from the reports. Use Amazon Comprehend Medical to identify"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Here's a detailed justification:\nThe core requirement is identifying PHI (Protected Health Information) within PDF and JPEG reports uploaded\nvia API Gateway and Lambda. The solution must minimize operational overhead.\nOption A (Python Libraries): While feasible, this approach requires significant custom coding to handle PDF\nand JPEG parsing (potentially complex) and implementing a PHI identification algorithm. This increases\ndevelopment effort, testing requirements, and ongoing maintenance, leading to higher operational overhead.\nOption B (Textract and SageMaker): Textract is excellent for text extraction. However, using SageMaker for\nPHI identification introduces unnecessary complexity. SageMaker is designed for building, training, and\ndeploying custom machine learning models. For a specialized task like PHI detection (which is already\naddressed by a dedicated service), SageMaker is overkill and introduces significant operational overhead\nrelated to model management, infrastructure, and training data.\nOption C (Textract and Comprehend Medical): Textract efficiently extracts text from both PDF and JPEG\nformats. Comprehend Medical is a HIPAA-eligible natural language processing (NLP) service specifically\ndesigned for analyzing medical text and identifying PHI. This combination minimizes custom code and\nleverages AWS-managed services, resulting in the least operational overhead. It offers built-in PHI detection\ncapabilities.\nOption D (Rekognition and Comprehend Medical): Rekognition is primarily designed for image analysis,\nincluding object detection and facial recognition. While it can extract text through OCR (Optical Character\nRecognition), Textract is optimized for document processing and will yield better results with PDF and\ncomplex document layouts. Also, Rekognition might not be as accurate as Textract in extracting text from\nthese specific report formats. Comprehend Medical is suitable for the PHI extraction task, but Rekognition is\nnot the ideal choice for text extraction in this scenario, creating additional overhead.\nTherefore, using Amazon Textract to extract text and Amazon Comprehend Medical to identify PHI offers the\nbest balance of functionality, accuracy, and minimal operational burden by utilizing purpose-built, managed\nAWS services tailored to these specific tasks. This solution aligns with the \"less code is better\" principle and\nsimplifies the deployment and maintenance processes.\nAuthoritative Links:\nAmazon Textract: https://aws.amazon.com/textract/\nAmazon Comprehend Medical: https://aws.amazon.com/comprehend/medical/\nAmazon SageMaker: https://aws.amazon.com/sagemaker/\nAmazon Rekognition: https://aws.amazon.com/rekognition/",
    "links": [
      "https://aws.amazon.com/textract/",
      "https://aws.amazon.com/comprehend/medical/",
      "https://aws.amazon.com/sagemaker/",
      "https://aws.amazon.com/rekognition/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an application that generates a large number of files, each approximately 5 MB in size. The files\nare stored in Amazon S3. Company policy requires the files to be stored for 4 years before they can be deleted.\nImmediate accessibility is always required as the files contain critical business data that is not easy to reproduce.\nThe files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first\n30 days.\nWhich storage solution is MOST cost-effective?",
    "options": {
      "A": "This",
      "B": "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 One Zone-Infrequent Access (S3",
      "C": "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3",
      "D": "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it offers the most cost-effective solution while adhering to the stated\nrequirements of immediate accessibility and long-term retention.\nHere's a detailed breakdown:\nS3 Standard: Initially, files are stored in S3 Standard for the first 30 days. This ensures fast and frequent\naccess when the files are actively used. S3 Standard provides high availability (99.99%) and durability\n(99.999999999%) for frequently accessed data.\nS3 Standard-IA: After 30 days, the lifecycle policy automatically transitions the files to S3 Standard-IA. This\nstorage class is designed for data that is infrequently accessed but requires rapid retrieval when needed. It\noffers lower storage costs compared to S3 Standard while maintaining similar performance characteristics.\nSince the question specifies immediate accessibility is always required, S3 Standard-IA is a suitable choice\nfor infrequently accessed data.\n4-Year Retention & Deletion: The lifecycle policy is configured to permanently delete the files after 4 years,\nfulfilling the company's retention policy.\nNow, let's examine why the other options are less optimal:\nOption A (S3 Glacier): Moving files directly to S3 Glacier after 30 days is not ideal. S3 Glacier is designed for\narchival storage and retrieval can take several hours. The requirement of \"immediate accessibility\" makes\nGlacier an unsuitable option for data that may need to be accessed at any moment.\nOption B (S3 One Zone-IA): While S3 One Zone-IA offers lower storage costs than S3 Standard-IA, it stores\ndata in a single Availability Zone. This exposes the data to a higher risk of data loss in the event of an\nAvailability Zone failure. S3 One Zone-IA is appropriate when data is easily reproducible. However, the\nquestion states that data is not easy to reproduce.\nOption D (S3 Standard-IA followed by S3 Glacier): Moving to Glacier after 4 years is unnecessary, since\ndeletion is required at that point. It adds complexity and cost without providing any benefit. Moreover, files\ncan be stored in S3 Standard-IA for 4 years and then be deleted.\nIn summary, option C provides the best balance between cost, accessibility, and durability, meeting all of the\ncompany's requirements in the most cost-effective way. Using S3 Standard for immediate needs, transitioning\nto S3 Standard-IA for infrequent access, and deleting after 4 years aligns with the best practices for S3\nlifecycle management.\nAuthoritative Links:\nAmazon S3 Storage Classes: https://aws.amazon.com/s3/storage-classes/\nS3 Lifecycle Management: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-\nconfiguration-concept.html",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts an application on multiple Amazon EC2 instances. The application processes messages from an\nAmazon SQS queue, writes to an Amazon RDS table, and deletes the message from the queue. Occasional\nduplicate records are found in the RDS table. The SQS queue does not contain any duplicate messages.\nWhat should a solutions architect do to ensure messages are being processed once only?",
    "options": {
      "A": "Use the CreateQueue API call to create a new queue.",
      "B": "Use the AddPermission API call to add appropriate permissions.",
      "C": "Use the ReceiveMessage API call to set an appropriate wait time.",
      "D": "Use the ChangeMessageVisibility API call to increase the"
    },
    "answer": "D",
    "explanation": "The most appropriate solution to address the issue of duplicate records in the RDS table, despite the absence\nof duplicate messages in the SQS queue, is D. Use the ChangeMessageVisibility API call to increase the\nvisibility timeout.\nHere's a detailed justification:\nThe problem indicates that the same message is being processed multiple times, leading to duplicate entries\nin the RDS database. Since the queue itself isn't the source of duplicates, the issue likely lies in how the EC2\ninstances are handling message processing and acknowledging completion to SQS.\nSQS employs a visibility timeout mechanism. When an EC2 instance retrieves a message from the queue, the\nmessage becomes invisible to other consumers for a specified duration (the visibility timeout). This prevents\nother instances from processing the same message simultaneously. The receiving instance is then expected\nto delete the message from the queue upon successful processing.\nIf an EC2 instance fails to process a message within the visibility timeout (e.g., due to a crash, network issue,\nor slow processing), the message becomes visible again in the queue. This allows another instance (or even\nthe original instance after recovery) to pick up the message and re-process it, hence leading to duplicates.\nIncreasing the visibility timeout using the ChangeMessageVisibility API call provides more time for the EC2\ninstance to process the message and delete it from the queue before it becomes visible again. This reduces\nthe likelihood of another instance grabbing the same message and creating a duplicate record in the RDS\ntable.\nOptions A, B, and C are not relevant to addressing the identified problem.\nA (CreateQueue): Creating a new queue doesn't solve the underlying issue of messages being processed\nmultiple times.\nB (AddPermission): Adding permissions is about access control, not about preventing duplicate processing.\nC (ReceiveMessage Wait Time): While adjusting wait time can improve polling efficiency, it doesn't prevent\nduplicate processing if an instance fails to process a message within the visibility timeout.\nBy increasing the visibility timeout to a value that comfortably exceeds the maximum expected processing\ntime for a message, the solution architect can minimize the risk of duplicate processing and ensure messages\nare processed only once. In cases where processing time can vary greatly, a combination of increased visibility\ntimeout and implementing idempotent message processing logic in the application can provide a robust\nsolution against duplicates. Idempotency ensures that even if a message is processed multiple times, the\nresult is the same as processing it once, preventing data inconsistencies.Consider using Dead Letter Queues\nto manage message that exceeds retry count.\nRelevant Documentation:\nAmazon SQS Visibility Timeout:\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-\ntimeout.html\nAmazon SQS ChangeMessageVisibility:\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ChangeMessageVisibility.html",
    "links": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ChangeMessageVisibility.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is designing a new hybrid architecture to extend a company's on-premises infrastructure to\nAWS. The company requires a highly available connection with consistent low latency to an AWS Region. The\ncompany needs to minimize costs and is willing to accept slower traffic if the primary connection fails.\nWhat should the solutions architect do to meet these requirements?",
    "options": {
      "A": "Provision an AWS Direct Connect connection to a Region. Provision a VPN connection as a backup if the",
      "B": "Provision a VPN tunnel connection to a Region for private connectivity. Provision a second VPN tunnel for",
      "C": "Provision an AWS Direct Connect connection to a Region. Provision a second Direct Connect connection to",
      "D": "Provision an AWS Direct Connect connection to a Region. Use the Direct Connect failover attribute from the"
    },
    "answer": "A",
    "explanation": "The correct answer is A because it provides a cost-effective solution for high availability and consistent low\nlatency. Direct Connect (DX) offers a dedicated network connection from the on-premises environment to\nAWS, fulfilling the requirement for high availability and consistent low latency under normal circumstances.\nThis dedicated connection bypasses the public internet, resulting in more predictable and lower latency than\nVPN connections.\nThe requirement to minimize costs while accepting slower traffic during failover is met by using a VPN\nconnection as a backup. VPN connections are established over the internet and are generally less expensive\nthan a redundant Direct Connect connection. When the primary Direct Connect link fails, traffic can be routed\nthrough the VPN connection, ensuring business continuity, although at a reduced speed and potentially higher\nlatency.\nOption B is incorrect because VPN connections, while cost-effective, do not offer the consistently low latency\nrequired during normal operation. VPN connections are susceptible to internet traffic fluctuations.\nOption C is incorrect as it proposes a redundant Direct Connect connection. While this would offer higher\navailability, it significantly increases costs. The company has already stated a willingness to accept slower\ntraffic if the primary connection fails, making the cost of a second Direct Connect circuit unnecessary.\nOption D is incorrect because the Direct Connect failover attribute within the AWS CLI doesn't automatically\ncreate a backup connection. The Direct Connect failover attribute allows you to influence which path traffic\ntakes but does not instantiate backup connections. It influences the traffic routing, given the primary\nconnection is already established. The failover attribute helps prioritize connections and adjust routing\npreferences but will not create connections from scratch.\nIn conclusion, a primary Direct Connect connection coupled with a VPN as a fallback mechanism delivers the\nbest balance between high performance and cost efficiency, aligning perfectly with the company's\nrequirements.\nFurther Research:\nAWS Direct Connect: https://aws.amazon.com/directconnect/\nAWS VPN: https://aws.amazon.com/vpn/",
    "links": [
      "https://aws.amazon.com/directconnect/",
      "https://aws.amazon.com/vpn/"
    ]
  },
  {
    "question": "CertyIQ\nA company is running a business-critical web application on Amazon EC2 instances behind an Application Load\nBalancer. The EC2 instances are in an Auto Scaling group. The application uses an Amazon Aurora PostgreSQL\ndatabase that is deployed in a single Availability Zone. The company wants the application to be highly available\nwith minimum downtime and minimum loss of data.\nWhich solution will meet these requirements with the LEAST operational effort?",
    "options": {
      "A": "Place the EC2 instances in different AWS Regions. Use Amazon Route 53 health checks to redirect traffic.",
      "B": "Configure the Auto Scaling group to use multiple Availability Zones. Configure the database as Multi-AZ.",
      "C": "Configure the Auto Scaling group to use one Availability Zone. Generate hourly snapshots of the database.",
      "D": "Configure the Auto Scaling group to use multiple AWS Regions. Write the data from the application to"
    },
    "answer": "B",
    "explanation": "Option B is the most appropriate solution because it achieves high availability for both the application and the\ndatabase with minimal operational overhead.\nHere's a detailed breakdown:\nAuto Scaling Group with Multiple Availability Zones (AZs): Spreading EC2 instances across multiple AZs\nensures that if one AZ fails, the application remains available in other AZs. This provides redundancy and fault\ntolerance at the application tier.\nMulti-AZ Aurora PostgreSQL: Configuring the database as Multi-AZ creates a synchronous standby replica in\na different AZ. If the primary database instance fails, Aurora automatically fails over to the standby,\nminimizing downtime and data loss. This addresses the high availability requirement for the database.\nAmazon RDS Proxy: RDS Proxy is an optional service that further enhances availability and scalability. It\nmanages database connections, reducing the load on the database and protecting it from connection storms.\nWhile not strictly required for basic Multi-AZ failover, it improves connection management, especially under\nheavy load and failover situations.\nOther options are less efficient or introduce unnecessary complexity:\nOption A (Cross-Region Replication): Using cross-region replication for the database adds complexity and\npotential latency. It is also overkill since the application is running in a single region. Also, failover between\nregions generally introduces more downtime than a Multi-AZ failover.\nOption C (Hourly Snapshots): Relying on snapshots for recovery introduces a significant recovery time\nobjective (RTO) and recovery point objective (RPO). Data loss is possible up to the last snapshot. This is not\nsuitable for a business-critical application requiring minimal downtime and data loss.\nOption D (Multi-Region ASG and S3/Lambda integration): This introduces substantial architectural\ncomplexity for data persistence compared to using Aurora Multi-AZ. The S3/Lambda pathway for database\nwrites introduces latency and increased operational overhead.\nTherefore, configuring the ASG across multiple AZs, using Aurora Multi-AZ, and leveraging RDS Proxy\nprovide the best balance between high availability, minimal downtime/data loss, and operational simplicity,\nmaking it the most suitable option.\nSupporting Links:\nAmazon Aurora High Availability: https://aws.amazon.com/rds/aurora/features/\nAmazon RDS Proxy: https://aws.amazon.com/rds/proxy/\nAuto Scaling Groups: https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html",
    "links": [
      "https://aws.amazon.com/rds/aurora/features/",
      "https://aws.amazon.com/rds/proxy/",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html"
    ]
  },
  {
    "question": "CertyIQ\nA company's HTTP application is behind a Network Load Balancer (NLB). The NLB's target group is configured to\nuse an Amazon EC2 Auto Scaling group with multiple EC2 instances that run the web service.\nThe company notices that the NLB is not detecting HTTP errors for the application. These errors require a manual\nrestart of the EC2 instances that run the web service. The company needs to improve the application's availability\nwithout writing custom scripts or code.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Enable HTTP health checks on the NLB, supplying the URL of the company's application.",
      "B": "Add a cron job to the EC2 instances to check the local application's logs once each minute. If HTTP errors are",
      "C": "Replace the NLB with an Application Load Balancer. Enable HTTP health checks by supplying the URL of the",
      "D": "Create an Amazon Cloud Watch alarm that monitors the UnhealthyHostCount metric for the NLB. Configure"
    },
    "answer": "C",
    "explanation": "The correct answer is C, which involves replacing the Network Load Balancer (NLB) with an Application Load\nBalancer (ALB) and configuring HTTP health checks and an Auto Scaling action. Here's why:\nNLB Limitations: NLBs operate at Layer 4 (TCP/UDP), meaning they are unaware of the application-layer\nprotocol (HTTP) and cannot interpret HTTP status codes. They can only check if a TCP connection can be\nestablished on the configured port, not if the application is functioning correctly.\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\nALB's HTTP Awareness: ALBs, on the other hand, operate at Layer 7 (HTTP/HTTPS), enabling them to\nperform health checks based on HTTP status codes (e.g., 200 OK, 400 Bad Request, 500 Internal Server\nError). This allows the ALB to determine if the application is truly healthy and route traffic accordingly.\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\nHTTP Health Checks: By configuring HTTP health checks on the ALB with a specific URL, the load balancer\nwill periodically send requests to that URL and verify the HTTP status code returned. If the status code\nindicates an error, the ALB will mark the instance as unhealthy.\nAuto Scaling Integration: The Auto Scaling group can be configured with lifecycle hooks or be configured\ndirectly to respond to ALB health check failures. When the ALB marks an instance as unhealthy, Auto Scaling\ncan be configured to automatically replace the unhealthy instance with a new one, thus improving application\navailability without manual intervention. https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-\nprocess.html\nWhy other options are wrong:\nA: NLBs cannot do HTTP health checks.\nB: Requires custom scripting, and the company wants to avoid it. Also, it is less reliable than ALB health\nchecks.\nD: While CloudWatch can monitor the NLB, it cannot detect HTTP-level errors due to NLB's Layer 4 operation.\nMonitoring UnhealthyHostCount may only indicate that an EC2 instance is unreachable at the TCP level, not\nthat the application is failing to respond to HTTP requests.\nIn summary, switching to an ALB enables HTTP health checks, which allows for the automated detection and\nreplacement of unhealthy instances, fulfilling the company's requirements for improved application\navailability without custom scripting or code.",
    "links": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a shopping application that uses Amazon DynamoDB to store customer information. In case of\ndata corruption, a solutions architect needs to design a solution that meets a recovery point objective (RPO) of 15\nminutes and a recovery time objective (RTO) of 1 hour.\nWhat should the solutions architect recommend to meet these requirements?",
    "options": {
      "A": "DynamoDB Global Tables: While Global Tables provide replication for high availability and disaster",
      "B": "D. Schedule Amazon Elastic Block Store (Amazon EBS) snapshots for the DynamoDB table every 15 minutes.",
      "C": "Export to S3 Glacier: Exporting data to S3 Glacier is a suitable strategy for long-term archiving and",
      "D": "EBS Snapshots for DynamoDB: DynamoDB does not reside on EBS volumes. It is a NoSQL database service"
    },
    "answer": "B",
    "explanation": "The correct answer is B because it aligns with the stated RPO and RTO requirements most effectively and\nefficiently.\nHere's why:\nDynamoDB Point-in-Time Recovery (PITR): DynamoDB PITR provides automatic backups of your table data.\nYou can restore the table to any point in time within the past 35 days. This perfectly addresses the need for\nrecovering data to a specific point within the last 15 minutes (RPO). The restoration process also falls within\nthe 1-hour RTO.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html\nWhy other options are incorrect:\nA. DynamoDB Global Tables: While Global Tables provide replication for high availability and disaster\nrecovery, their primary purpose is not granular point-in-time recovery for data corruption scenarios within a\nsingle table. Switching regions might help avoid regional failures, but if the corruption is replicated, it won't\nsolve the problem. They do not meet the specific RPO requirement of 15 minutes for data corruption within a\ntable.\nC. Export to S3 Glacier: Exporting data to S3 Glacier is a suitable strategy for long-term archiving and\ncompliance, but it is too slow for a 1-hour RTO. Glacier is designed for infrequently accessed data, and\nretrieving data from Glacier can take several hours. This does not satisfy the 1-hour RTO requirement.\nD. EBS Snapshots for DynamoDB: DynamoDB does not reside on EBS volumes. It is a NoSQL database service\nthat internally handles storage. Taking EBS snapshots won't capture the data or the configuration of your\nDynamoDB table, and it's not the correct way to back up DynamoDB data.\nIn summary, DynamoDB PITR is the most appropriate solution as it offers the required granularity for recovery,\nmeeting both the 15-minute RPO and the 1-hour RTO. Other options do not directly address the specific\nrequirements of recovering from data corruption with the given recovery objectives.",
    "links": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a photo processing application that needs to frequently upload and download pictures from\nAmazon S3 buckets that are located in the same AWS Region. A solutions architect has noticed an increased cost\nin data transfer fees and needs to implement a solution to reduce these costs.\nHow can the solutions architect meet this requirement?",
    "options": {
      "A": "Deploy Amazon API Gateway into a public subnet and adjust the route table to route S3 calls through it.",
      "B": "Deploy a NAT gateway into a public subnet and attach an endpoint policy that allows access to the S3",
      "C": "Deploy the application into a public subnet and allow it to route through an internet gateway to access the",
      "D": "Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy"
    },
    "answer": "D",
    "explanation": "The correct solution is D. Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy\nthat allows access to the S3 buckets.\nHere's a detailed justification:\nThe core problem is the data transfer cost between the application and S3 within the same AWS Region. Data\ntransfer to S3 is generally free. However, data transfer from S3 incurs costs, and these costs increase\nsignificantly when the data traverses the public internet.\nOptions A, B, and C all involve routing traffic via the public internet, defeating the purpose of cost reduction.\nDeploying API Gateway (A), NAT Gateway (B), or using an Internet Gateway directly (C) means the application\nis accessing S3 over the internet, incurring outbound data transfer fees. While API Gateway offers other\nbenefits, in this scenario, it adds unnecessary complexity and cost. Similarly, NAT Gateways are for enabling\ninstances in private subnets to access the internet, not to optimize S3 access.\nOption D leverages a VPC endpoint, which establishes a direct, private connection between your VPC and S3,\nwithout traversing the public internet. A VPC endpoint specifically for S3 creates a gateway within your VPC\nthat allows your application to access S3 buckets as if they were within the same network. This eliminates\npublic internet data transfer costs, as traffic stays within the AWS network.\nFurthermore, the endpoint policy attached to the S3 VPC gateway endpoint controls access to the S3\nbuckets. This policy specifies which S3 buckets and actions are allowed for resources within the VPC,\nenhancing security. It restricts access based on IAM roles, users, and conditions. Without the policy, the\nendpoint won't function.\nTherefore, deploying an S3 VPC gateway endpoint and attaching an endpoint policy is the most efficient and\ncost-effective way to enable the photo processing application to access S3 buckets without incurring public\ninternet data transfer costs. It keeps the data within the AWS internal network, reducing costs and improving\nsecurity.\nHere are authoritative links for further research:\nAWS VPC Endpoints: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\nGateway VPC Endpoints for S3: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html\nS3 Pricing: https://aws.amazon.com/s3/pricing/ (Focus on the \"Data Transfer\" section)",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html",
      "https://aws.amazon.com/s3/pricing/"
    ]
  },
  {
    "question": "CertyIQ\nA company recently launched Linux-based application instances on Amazon EC2 in a private subnet and launched\na Linux-based bastion host on an Amazon EC2 instance in a public subnet of a VP",
    "options": {
      "C": "Replace the current security group of the bastion host with one that only allows inbound access from the",
      "A": "Replace the current security group of the bastion host with one that only allows inbound access from the",
      "B": "Replace the current security group of the bastion host with one that only allows inbound access from the",
      "D": "Here's why:"
    },
    "answer": "C",
    "explanation": "The correct answer is CD. Here's why:\nC: The bastion host acts as a secure gateway to the application instances. To allow access from the on-\npremises network, the security group associated with the bastion host must permit inbound traffic from the\ncompany's external IP range. This restricts access to only traffic originating from the known and trusted\ncompany network, enhancing security. This is because all traffic from on-premise will come through the\ncompany's internet connection, which will have a specific, and usually static, external IP range.\nD: The application instances in the private subnet should not be directly accessible from the internet. Instead,\naccess is granted via the bastion host. Therefore, the application instances' security group should only allow\ninbound SSH traffic from the private IP address of the bastion host. This ensures that only traffic originating\nfrom the bastion host can connect to the application instances, restricting the attack surface. This\nimplements a layered security approach.\nWhy the other options are incorrect:\nA: Restricting inbound access to the bastion host to only application instances would prevent the on-premises\nnetwork from accessing it.\nB: Allowing the internal IP range of the company would not work, because traffic from the company will be\nrouted through the internet, and therefore come from the external IP range of the company.\nE: Using the public IP address of the bastion host in the application instance's security group is less secure\nbecause public IP addresses can sometimes change, and more importantly, defeats the purpose of using the\nbastion host as a secure intermediary, as the application servers are exposed to outside IPs.\nSupporting Concepts and Links:\nBastion Host: A server whose purpose is to provide access to a private network from an external network,\nsuch as the internet. https://aws.amazon.com/quickstart/architecture/linux-bastion/\nSecurity Groups: Act as a virtual firewall for your EC2 instances to control inbound and outbound traffic.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html\nPrinciple of Least Privilege: Granting only the minimum necessary permissions. In this case, restricting\naccess to the bastion host and application instances based on IP addresses aligns with this principle.",
    "links": [
      "https://aws.amazon.com/quickstart/architecture/linux-bastion/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is designing a two-tier web application. The application consists of a public-facing web tier\nhosted on Amazon EC2 in public subnets. The database tier consists of Microsoft SQL Server running on Amazon\nEC2 in a private subnet. Security is a high priority for the company.\nHow should security groups be configured in this situation? (Choose two.)",
    "options": {
      "A": "Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0.",
      "B": "Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0.",
      "C": "Configure the security group for the database tier to allow inbound traffic on port 1433 from the security",
      "D": "Configure the security group for the database tier to allow outbound traffic on ports 443 and 1433 to the"
    },
    "answer": "A",
    "explanation": "Here's a detailed justification for why options A and C are the correct choices for configuring security groups\nin the given two-tier web application scenario:\nOption A: Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0.\nThis configuration allows HTTPS traffic (port 443) from any source (0.0.0.0/0) to reach the web servers. Since\nthe web tier is public-facing, it needs to accept inbound traffic from users on the internet. HTTPS is the\nstandard secure protocol for web communication, encrypting data in transit. Restricting inbound access to\nport 443 (and potentially port 80 for HTTP redirects to HTTPS) minimizes the attack surface. A more\nrestrictive approach could involve limiting access to specific IP ranges of known users or employing a Web\nApplication Firewall (WAF) for more granular control.\nOption C: Configure the security group for the database tier to allow inbound traffic on port 1433 from the\nsecurity group for the web tier.\nThis configuration allows traffic on port 1433 (the default port for Microsoft SQL Server) from the web tier's\nsecurity group to reach the database tier. This is crucial for the web servers to connect to the database server.\nInstead of specifying IP addresses, using the web tier's security group ensures that only instances within that\ngroup can connect to the database. This dynamic association simplifies management, as new web servers\nadded to the web tier automatically gain database access without manually updating IP addresses in the\ndatabase security group. The database tier should only allow traffic from the web tier and no other sources.\nWhy other options are incorrect:\nOption B: The web tier needs to make outbound calls, but likely not on port 443 to 0.0.0.0/0. The web tier\nmight need outbound access to other AWS services or external APIs, but this would be governed by different\nrules and likely restricted to specific services or IP ranges rather than the entire internet on a particular port\nlike 443.\nOption D and E: The database tier only needs to accept inbound connections from the web tier on port 1433. It\ndoes not need to initiate outbound traffic to the web tier. Option E is also incorrect in that it allows inbound\ntraffic on port 443, which is unneeded and increases the attack surface.\nKey Cloud Computing Concepts Reinforced:\nSecurity Groups: Acting as virtual firewalls controlling inbound and outbound traffic at the instance level.\nLeast Privilege Principle: Granting only the necessary permissions to resources, minimizing potential damage\nfrom security breaches.\nTwo-Tier Architecture: A common application architecture separating the presentation tier (web tier) from\nthe data tier (database tier).\nNetwork Segmentation: Isolating resources (like the database tier in a private subnet) to reduce the blast\nradius of security incidents.\nAuthoritative Links for Further Research:\nAWS Security Groups: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\nSecurity Best Practices in AWS: https://aws.amazon.com/security/",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html",
      "https://aws.amazon.com/security/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to move a multi-tiered application from on premises to the AWS Cloud to improve the\napplication's performance. The application consists of application tiers that communicate with each other by way\nof RESTful services. Transactions are dropped when one tier becomes overloaded. A solutions architect must\ndesign a solution that resolves these issues and modernizes the application.\nWhich solution meets these requirements and is the MOST operationally efficient?",
    "options": {
      "A": "Use Amazon API Gateway and direct transactions to the AWS Lambda functions as the application layer. Use",
      "B": "Use Amazon CloudWatch metrics to analyze the application performance history to determine the servers'",
      "C": "Use Amazon Simple Notification Service (Amazon SNS) to handle the messaging between application servers",
      "D": "Use Amazon Simple Queue Service (Amazon SQS) to handle the messaging between application servers"
    },
    "answer": "A",
    "explanation": "Here's a breakdown of why option A is the best choice and why the others fall short, along with supporting\nconcepts and links.\nOption A (API Gateway + Lambda + SQS) offers the most operationally efficient and modern solution for the\ncompany's requirements. Let's dissect it:\nAmazon API Gateway: Acts as a front door for the application. It allows the company to manage API access,\nauthorization, throttling, and versioning in a centralized and scalable manner. It abstracts the underlying\ncomplexities of the backend services. https://aws.amazon.com/api-gateway/\nAWS Lambda: Enables the company to modernize its application by leveraging serverless computing. Lambda\nfunctions can handle individual requests or transactions without the need to manage servers. This\nsignificantly reduces operational overhead and allows for automatic scaling based on demand. Lambda can\nalso integrate directly with API Gateway. https://aws.amazon.com/lambda/\nAmazon SQS: Introduces asynchronous communication between the application tiers. When one tier is\noverloaded, SQS acts as a buffer, preventing transaction drops. The application can decouple its components\nso that they can fail or go down at any time. SQS provides durability and fault tolerance.\nhttps://aws.amazon.com/sqs/\nWhy other options are less suitable:\nOption B: Increasing EC2 instance sizes (vertical scaling) might alleviate the problem in the short term, but it's\nnot a modern or efficient solution. It does not address the root cause of the issue and does not allow you to\ndecouple components. It involves unnecessary resource allocation even during periods of low demand.\nOption C: While SNS can handle messaging, it's primarily designed for push notifications and fanout\nscenarios. It doesn't provide the same level of queuing and buffering capabilities as SQS, making it less\nsuitable for preventing transaction drops during overload. Also, running application servers on EC2 within an\nAuto Scaling group is not as operationally efficient as Lambda, which eliminates server management.\nOption D: Using SQS is beneficial, but continuing to run application servers on EC2 instances with Auto\nScaling groups isn't as operationally efficient as using Lambda. Lambda reduces management overhead by\nabstracting the underlying infrastructure. EC2 instances can add management overhead in the long term.",
    "links": [
      "https://aws.amazon.com/api-gateway/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/sqs/"
    ]
  },
  {
    "question": "CertyIQ\nA company receives 10 TB of instrumentation data each day from several machines located at a single factory. The\ndata consists of JSON files stored on a storage area network (SAN) in an on-premises data center located within\nthe factory. The company wants to send this data to Amazon S3 where it can be accessed by several additional\nsystems that provide critical near-real-time analytics. A secure transfer is important because the data is\nconsidered sensitive.\nWhich solution offers the MOST reliable data transfer?",
    "options": {
      "A": "AWS DataSync over public internet",
      "B": "AWS DataSync over AWS Direct Connect. Here's a detailed justification:",
      "C": "AWS Database Migration Service (AWS DMS) over public internet",
      "D": "AWS Database Migration Service (AWS DMS) over AWS Direct Connect"
    },
    "answer": "B",
    "explanation": "The correct answer is B. AWS DataSync over AWS Direct Connect. Here's a detailed justification:\nData Volume and Real-Time Analytics: The company generates a significant 10 TB of data daily and requires\nnear-real-time analytics. This necessitates a robust and efficient transfer mechanism.\nReliability and Security: Secure transfer is paramount due to the sensitive nature of the data.\nAWS DataSync: AWS DataSync is a purpose-built service designed for moving large amounts of data\nbetween on-premises storage and AWS services. It handles data encryption in transit and at rest, ensuring\ndata security. DataSync is optimized for network utilization, maximizing transfer speeds and minimizing\ntransfer times. https://aws.amazon.com/datasync/\nAWS Direct Connect: AWS Direct Connect establishes a dedicated network connection from the company's\non-premises data center to AWS. This connection bypasses the public internet, providing a more reliable,\nsecure, and consistent network experience. Direct Connect significantly reduces network latency and\nincreases bandwidth compared to internet-based transfers. https://aws.amazon.com/directconnect/\nWhy A is less ideal: While AWS DataSync is appropriate, transferring over the public internet (option A)\nintroduces vulnerabilities to security breaches and network congestion, making the transfer less reliable and\nsecure than using a dedicated connection.\nWhy C and D are incorrect: AWS Database Migration Service (DMS) is primarily for migrating databases.\nWhile it could technically transfer JSON files stored as BLOBs, it's not designed or optimized for this purpose.\nDataSync is much more efficient and appropriate for large file transfers. Additionally, DMS adds unnecessary\ncomplexity and overhead for a simple file transfer scenario.\nCombined benefits of DataSync and Direct Connect: By combining AWS DataSync with AWS Direct Connect,\nthe company gets a secure, reliable, and high-throughput data transfer pipeline. Direct Connect minimizes\nlatency and ensures consistent network performance, while DataSync optimizes the transfer process. The\nencryption during transfer provided by DataSync in conjunction with Direct Connect reduces exposure of the\ndata.\nConsistency: Consistent, reliable data transfer is crucial for supporting near-real-time analytics because\ninterruptions or delays in the data transfer can result in outdated or incomplete analyses.\nCost considerations: While Direct Connect involves additional costs, the improved reliability, security, and\nthroughput may justify the investment, especially considering the critical nature of the data and analytics.\nTherefore, using AWS DataSync over AWS Direct Connect is the most reliable data transfer solution for\ntransferring the instrumentation data to Amazon S3 because it offers enhanced security, dedicated\nbandwidth, and optimized transfer mechanisms.",
    "links": [
      "https://aws.amazon.com/datasync/",
      "https://aws.amazon.com/directconnect/"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to configure a real-time data ingestion architecture for its application. The company needs an\nAPI, a process that transforms data as the data is streamed, and a storage solution for the data.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Deploy an Amazon EC2 instance to host an API that sends data to an Amazon Kinesis data stream. Create an",
      "B": "Deploy an Amazon EC2 instance to host an API that sends data to AWS Glue. Stop source/destination",
      "C": "Configure an Amazon API Gateway API to send data to an Amazon Kinesis data stream. Create an Amazon",
      "D": "Configure an Amazon API Gateway API to send data to AWS Glue. Use AWS Lambda functions to transform"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it offers a serverless, scalable, and managed solution for real-time data\ningestion with minimal operational overhead.\nHere's why:\nAPI Gateway: Amazon API Gateway allows you to create and manage APIs without managing any servers. It\nhandles tasks like traffic management, authorization, and monitoring, significantly reducing operational\nburden compared to hosting an API on an EC2 instance. (Reference: https://aws.amazon.com/api-gateway/)\nKinesis Data Streams: Kinesis Data Streams is a fully managed, scalable, and durable real-time data\nstreaming service. It's designed to ingest high-velocity data streams. Using it directly as the target of the API\nensures immediate ingestion.\nKinesis Data Firehose: Kinesis Data Firehose is designed for loading data into data lakes and data\nwarehouses. Using it with Kinesis Data Streams as a source is a common pattern. Firehose automatically\nscales, buffers, and compresses data. (Reference: https://aws.amazon.com/kinesis/data-firehose/)\nLambda Functions: Lambda allows you to execute code without provisioning or managing servers. Using\nLambda functions within the Kinesis Data Firehose delivery stream enables real-time data transformations\nwithout the operational overhead of managing EC2 instances. This is done during data loading to S3 by\nFirehose (Reference: https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html)\nS3: Amazon S3 provides highly durable and scalable object storage, ideal for storing the transformed data.\nWhy other options are less suitable:\nOption A: Hosting the API on an EC2 instance increases operational overhead due to server management,\npatching, scaling, and monitoring.\nOption B: AWS Glue is primarily designed for ETL (Extract, Transform, Load) jobs, which are typically batch-\noriented, and is not the optimal solution for real-time data ingestion. AWS Glue is also overkill for the simple\ntransformation needed in this case. The \"stop source/destination checking\" is also a security concern and not\nneeded with the API Gateway/Kinesis solution.\nOption D: Similar to B, using AWS Glue for data transformation is generally better suited for batch-oriented\nETL pipelines, and API Gateway doesn't natively integrate with it for real-time streaming.\nTherefore, option C is the most efficient and cost-effective choice for real-time data ingestion with the least\noperational overhead by leveraging serverless services.",
    "links": [
      "https://aws.amazon.com/api-gateway/)",
      "https://aws.amazon.com/kinesis/data-firehose/)",
      "https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html)"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to keep user transaction data in an Amazon DynamoDB table. The company must retain the data\nfor 7 years.\nWhat is the MOST operationally efficient solution that meets these requirements?",
    "options": {
      "A": "DynamoDB Point-in-Time Recovery (PITR): While PITR allows restoring the table to any point in time within",
      "B": "This simplifies backup management and reduces",
      "C": "On-Demand Backups with S3 Lifecycle: Creating on-demand backups manually is not operationally",
      "D": "EventBridge, Lambda, and S3 Lifecycle: While this approach can meet the retention requirement, it"
    },
    "answer": "B",
    "explanation": "The correct answer is B: Use AWS Backup to create backup schedules and retention policies for the table.\nThis is the most operationally efficient solution for retaining DynamoDB data for 7 years for several reasons:\n1. Centralized Backup Management: AWS Backup provides a single pane of glass to manage backups\nacross various AWS services, including DynamoDB. This simplifies backup management and reduces\nthe operational overhead associated with managing backups using separate service-specific tools.\nhttps://aws.amazon.com/backup/\n2. Automated Scheduling and Retention: AWS Backup allows defining backup schedules and retention\npolicies that automatically handle the creation and deletion of backups based on the defined\nparameters. This eliminates the need for manual intervention and ensures compliance with the 7-year\nretention requirement.\n3. Compliance and Auditing: AWS Backup offers features for compliance reporting and auditing,\nallowing you to demonstrate adherence to data retention policies. https://docs.aws.amazon.com/aws-\nbackup/latest/devguide/whatis.html\n4. Cost Optimization: AWS Backup optimizes backup storage by using incremental backups and data\ncompression, which helps reduce storage costs.\nNow, let's look at why other options are less efficient:\nA. DynamoDB Point-in-Time Recovery (PITR): While PITR allows restoring the table to any point in time within\nthe past 35 days, it doesn't satisfy the 7-year retention requirement.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html\nC. On-Demand Backups with S3 Lifecycle: Creating on-demand backups manually is not operationally\nefficient because it requires manual intervention and scheduling. Also, managing backups in S3 requires\nconfiguring S3 Lifecycle rules, adding operational complexity. The responsibility of remembering to perform\nthese tasks remains with the user.\nD. EventBridge, Lambda, and S3 Lifecycle: While this approach can meet the retention requirement, it\ninvolves more complex configurations than using AWS Backup. Building and maintaining a custom Lambda\nfunction introduces operational overhead for development, testing, and maintenance. The management of the\nS3 lifecycle, though automated, adds to the solution's complexity compared to AWS Backup.",
    "links": [
      "https://aws.amazon.com/backup/",
      "https://docs.aws.amazon.com/aws-",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is planning to use an Amazon DynamoDB table for data storage. The company is concerned about cost\noptimization. The table will not be used on most mornings. In the evenings, the read and write traffic will often be\nunpredictable. When traffic spikes occur, they will happen very quickly.\nWhat should a solutions architect recommend?",
    "options": {
      "A": "Create a DynamoDB table in on-demand capacity mode.",
      "B": "Create a DynamoDB table with a global secondary index.",
      "C": "Create a DynamoDB table with provisioned capacity and auto scaling.",
      "D": "Create a DynamoDB table in provisioned capacity mode, and configure it as a global table."
    },
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A (Create a DynamoDB table in on-demand capacity mode) is the\nmost appropriate solution for the scenario described:\nThe scenario highlights two key cost optimization concerns: the table is largely unused during mornings, and\ntraffic is unpredictable with rapid spikes during evenings. On-demand capacity mode is ideally suited for\nthese situations. In on-demand capacity mode, DynamoDB automatically scales capacity in response to actual\nworkload needs. This means you only pay for the reads and writes your application performs, without needing\nto provision capacity upfront. This aligns perfectly with the morning downtime where no costs are incurred.\nBecause traffic spikes happen rapidly, provisioned capacity with autoscaling (option C) might not react\nquickly enough. Autoscaling takes some time to adjust capacity, potentially leading to throttling during the\ninitial moments of a spike. On-demand mode, on the other hand, scales instantly.\nA global secondary index (option B) is useful for querying data using attributes other than the primary key but\ndoesn't address the cost optimization and unpredictable traffic concerns. A global table (option D) is for multi-\nregion replication for disaster recovery and low-latency access in different geographical areas, which is also\nirrelevant to the given problem. It would simply increase costs without providing a benefit.\nTherefore, on-demand capacity mode provides the best balance of cost efficiency during periods of low\nactivity and responsiveness to unpredictable, rapid traffic spikes by eliminating the need for upfront capacity\nplanning and enabling automatic and instant scaling.\nHere are some resources for further reading:\nDynamoDB On-Demand Capacity:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.OnDemand\nDynamoDB Auto Scaling:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html",
    "links": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.OnDemand",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html"
    ]
  },
  {
    "question": "CertyIQ\nA company recently signed a contract with an AWS Managed Service Provider (MSP) Partner for help with an\napplication migration initiative. A solutions architect needs ta share an Amazon Machine Image (AMI) from an\nexisting AWS account with the MSP Partner's AWS account. The AMI is backed by Amazon Elastic Block Store\n(Amazon EBS) and uses an AWS Key Management Service (AWS KMS) customer managed key to encrypt EBS\nvolume snapshots.\nWhat is the MOST secure way for the solutions architect to share the AMI with the MSP Partner's AWS account?",
    "options": {
      "A": "Make the encrypted AMI and snapshots publicly available. Modify the key policy to allow the MSP Partner's",
      "B": "Here's why:",
      "C": "Modify the launchPermission property of the AMI. Share the AMI with the MSP Partner's AWS account only.",
      "D": "Export the AMI from the source account to an Amazon S3 bucket in the MSP Partner's AWS account, Encrypt"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Here's why:\nA is incorrect: Making the AMI and snapshots publicly available is highly insecure. It exposes the company's\ndata to anyone, violating confidentiality and compliance requirements.\nB is correct: Modifying the launchPermission property of the AMI allows you to selectively share the AMI with\nthe MSP Partner's AWS account. This approach limits access to only the intended recipient. Crucially,\nbecause the EBS volumes are encrypted with a KMS key, the MSP Partner's account needs permission to use\nthat key. Modifying the key policy to grant the MSP Partner's AWS account the necessary permissions\nenables them to launch instances from the AMI. This balances security with functionality, allowing the MSP\nPartner to use the shared AMI while maintaining control over access to the underlying data.\nC is incorrect: While sharing the AMI with the MSP's account is correct, modifying the key policy to trust a\nnew KMS key owned by the MSP is less straightforward and potentially less secure. This would necessitate\nre-encrypting the EBS volumes using the new key, adding unnecessary complexity and potential for data loss\nor corruption. Also, it requires the original account to relinquish some control over the encryption process. It is\ngenerally best practice to maintain control of KMS keys used to encrypt your data whenever possible.\nD is incorrect: Exporting the AMI and storing it in an S3 bucket, even if encrypted with the MSP's KMS key,\nintroduces unnecessary complexity and potential vulnerabilities. It involves creating a copy of the AMI, which\ncan increase storage costs and introduce inconsistencies. Furthermore, exporting and importing AMIs can be\ntime-consuming and may require downtime. Sharing the AMI directly via launchPermission is a more efficient\nand secure approach.\nIn summary, option B provides the most secure and efficient way to share the encrypted AMI with the MSP\nPartner by granting the MSP Partner access to the existing KMS key used to encrypt the EBS volumes,\nwithout exposing the AMI publicly or needing to create copies.\nSupporting links:\nSharing AMIs: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-intro.html\nKMS Key Policies: https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html\nEncryption at Rest with KMS: https://docs.aws.amazon.com/kms/latest/developerguide/services-\nsupported.html",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-intro.html",
      "https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html",
      "https://docs.aws.amazon.com/kms/latest/developerguide/services-"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is designing the cloud architecture for a new application being deployed on AWS. The\nprocess should run in parallel while adding and removing application nodes as needed based on the number of jobs\nto be processed. The processor application is stateless. The solutions architect must ensure that the application is\nloosely coupled and the job items are durably stored.\nWhich design should the solutions architect use?",
    "options": {
      "A": "Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon Machine Image",
      "B": "Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine Image",
      "C": "Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine Image",
      "D": "Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon Machine Image"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it utilizes Amazon SQS for durable and loosely coupled job storage and an\nAuto Scaling group configured to scale based on the queue's depth, aligning with the requirements. Here's a\nbreakdown:\nAmazon SQS for Durable Storage and Decoupling: SQS (Simple Queue Service) acts as a message queue,\nproviding durable storage for the jobs. This ensures that jobs are not lost even if application instances fail and\npromotes loose coupling between the job producer and the processor. https://aws.amazon.com/sqs/\nAMI and Launch Template: The Amazon Machine Image (AMI) provides a template for launching EC2\ninstances, and the launch template specifies the configuration for instances within the Auto Scaling group,\nincluding the AMI, instance type, and security groups. Using Launch Templates over Launch Configurations is\ngenerally recommended due to their added functionality.\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-templates-vs-launch-configurations.html\nAuto Scaling Group: The Auto Scaling group allows for the dynamic provisioning and termination of EC2\ninstances based on defined policies. This enables the application to scale up or down depending on the job\nload. https://aws.amazon.com/autoscaling/\nScaling Policy Based on SQS Queue Depth: By setting the scaling policy to add or remove nodes based on\nthe number of items in the SQS queue, the Auto Scaling group automatically adjusts the number of\nprocessing instances to match the workload. This ensures efficient resource utilization and timely processing\nof jobs.\nOptions A and D incorrectly use Amazon SNS. SNS (Simple Notification Service) is designed for publish-\nsubscribe messaging, suitable for broadcasting messages to multiple subscribers but not for queueing jobs\nthat need to be reliably processed one by one. SNS does not provide built-in durability in the same way as\nSQS. https://aws.amazon.com/sns/\nOption B uses network usage for scaling. Scaling based on network usage is not directly tied to the number of\njobs waiting to be processed, therefore not meeting the requirement efficiently.",
    "links": [
      "https://aws.amazon.com/sqs/",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-templates-vs-launch-configurations.html",
      "https://aws.amazon.com/autoscaling/",
      "https://aws.amazon.com/sns/"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts its web applications in the AWS Cloud. The company configures Elastic Load Balancers to use\ncertificates that are imported into AWS Certificate Manager (ACM). The company's security team must be notified\n30 days before the expiration of each certificate.\nWhat should a solutions architect recommend to meet this requirement?",
    "options": {
      "A": "Add a rule in ACM to publish a custom message to an Amazon Simple Notification Service (Amazon SNS)",
      "B": "Create an AWS Config rule that checks for certificates that will expire within 30 days. Configure Amazon",
      "C": "Use AWS Trusted Advisor to check for certificates that will expire within 30 days. Create an Amazon",
      "D": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to detect any certificates that will expire"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the best solution:\nOption B: Create an AWS Config rule that checks for certificates that will expire within 30 days. Configure\nAmazon EventBridge (Amazon CloudWatch Events) to invoke a custom alert by way of Amazon Simple\nNotification Service (Amazon SNS) when AWS Config reports a noncompliant resource.\nThis approach leverages the strengths of three AWS services for proactive certificate expiration\nmanagement:\n1. AWS Config: AWS Config continuously monitors and records the configuration of your AWS\nresources, including ACM certificates. You can define Config rules to evaluate whether your\nresources comply with desired configurations. In this case, you'd create a Config rule that checks if\nany ACM certificates are expiring within the next 30 days. If a certificate is found to be expiring soon,\nthe rule marks the certificate as \"noncompliant.\"\n2. Amazon EventBridge (Amazon CloudWatch Events): EventBridge enables you to react to changes in\nyour AWS environment. We can configure an EventBridge rule to listen for events from AWS Config\nrelated to \"noncompliant\" resource changes. Specifically, we're looking for events indicating that a\ncertificate has been flagged as noncompliant because it's nearing expiration.\n3. Amazon Simple Notification Service (Amazon SNS): SNS is a messaging service that facilitates\nsending notifications. EventBridge, upon detecting a noncompliant certificate via Config, triggers the\nSNS topic, which in turn sends a notification to the security team.\nWhy this is superior to the other options:\nOption A (ACM Rule): ACM itself doesn't directly offer native rules to trigger notifications based on\ncertificate expiry dates. While ACM manages renewals, creating daily custom messages and managing logic\nis inefficient and not within ACM's intended functionality.\nOption C (Trusted Advisor): While Trusted Advisor provides best practice checks, including certificate\nexpiration, it's not designed for real-time, automated notifications. Relying solely on Trusted Advisor requires\nmanual checks or CloudWatch Alarms based on limited metrics, which is less proactive and harder to\ncustomize compared to AWS Config. Additionally, Trusted Advisor might have limitations on the frequency of\nchecks.\nOption D (EventBridge and Lambda): Although feasible, this option is more complex. EventBridge would need\na custom pattern to identify certificates expiring within 30 days. A Lambda function would then be needed to\nparse the event data and send the SNS notification. This adds overhead in terms of development,\nmaintenance, and potential for Lambda function errors. The native Config rule simplifies the process.\nBenefits of Option B:\nAutomation: Fully automated monitoring and notification process.\nProactive: Provides notification 30 days before expiration, allowing ample time for renewal or replacement.\nScalable: Easily scales to handle a large number of certificates.\nCentralized Configuration Management: Leverages the strengths of AWS Config for infrastructure-as-code\nand compliance tracking.\nMinimal Custom Code: Relies on managed services, reducing the need for custom code and associated\nmaintenance.\nSupporting Links:\nAWS Config: https://aws.amazon.com/config/\nAWS EventBridge: https://aws.amazon.com/eventbridge/\nAmazon SNS: https://aws.amazon.com/sns/\nAWS Certificate Manager (ACM): https://aws.amazon.com/certificate-manager/\nIn summary, Option B provides the most efficient, scalable, and manageable solution by integrating AWS\nConfig for compliance checks, EventBridge for event-driven automation, and SNS for reliable notifications. It\nminimizes custom code while providing proactive and automated certificate expiration monitoring.",
    "links": [
      "https://aws.amazon.com/config/",
      "https://aws.amazon.com/eventbridge/",
      "https://aws.amazon.com/sns/",
      "https://aws.amazon.com/certificate-manager/"
    ]
  },
  {
    "question": "CertyIQ\nA company's dynamic website is hosted using on-premises servers in the United States. The company is launching\nits product in Europe, and it wants to optimize site loading times for new European users. The site's backend must\nremain in the United States. The product is being launched in a few days, and an immediate solution is needed.\nWhat should the solutions architect recommend?",
    "options": {
      "A": "Launch an Amazon EC2 instance in us-east-1 and migrate the site to it.",
      "B": "Move the website to Amazon S3. Use Cross-Region Replication between Regions.",
      "C": "Use Amazon CloudFront with a custom origin pointing to the on-premises servers.",
      "D": "Use an Amazon Route 53 geoproximity routing policy pointing to on-premises servers."
    },
    "answer": "C",
    "explanation": "The correct answer is C: Use Amazon CloudFront with a custom origin pointing to the on-premises servers.\nHere's why:\nExplanation:\nCloudFront is a Content Delivery Network (CDN) service. It caches static and dynamic content at edge\nlocations around the world. By using CloudFront with a custom origin (pointing to the on-premises servers in\nthe United States), the company can serve the website's content from locations closer to European users,\nsignificantly reducing latency and improving loading times. This immediate solution addresses the primary\nneed of optimizing site loading times for European users without requiring significant infrastructure changes\nor backend migration.\nOption A is incorrect because migrating the entire site to a new EC2 instance in us-east-1 will not improve\nperformance for European users. It is still geographically distant from them.\nOption B is incorrect because moving the entire website to S3 and using Cross-Region Replication does not\naddress the dynamic nature of the website. S3 is best suited for static content. Also, replicating the entire\nwebsite to a new region is an unnecessary and time-consuming solution if the backend must remain in the US.\nOption D is incorrect because Route 53 geoproximity routing can route traffic based on geographical location,\nit does not cache content. Therefore, while it can direct European users to the US servers, the latency issues\nassociated with distance remain.\nWhy CloudFront is the best immediate solution:\nCDN Caching: CloudFront caches content at edge locations in Europe, minimizing the distance data has to\ntravel to reach users.\nCustom Origin: The website backend remaining in the US allows the application logic to remain where it is,\nenabling quick implementation without significant changes to the current architecture.\nFast Implementation: CloudFront is designed to be easy to set up and integrate with existing infrastructure,\nallowing for a rapid rollout.\nCost-Effective: Compared to replicating the entire website to a new region, using CloudFront is more cost-\neffective. It only caches necessary content.\nDynamic content acceleration: CloudFront can cache dynamic content.\nSupporting Links:\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nUsing Custom Origins with CloudFront:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-custom-origin.html",
    "links": [
      "https://aws.amazon.com/cloudfront/",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-custom-origin.html"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to reduce the cost of its existing three-tier web architecture. The web, application, and database\nservers are running on Amazon EC2 instances for the development, test, and production environments. The EC2\ninstances average 30% CPU utilization during peak hours and 10% CPU utilization during non-peak hours.\nThe production EC2 instances run 24 hours a day. The development and test EC2 instances run for at least 8 hours\neach day. The company plans to implement automation to stop the development and test EC2 instances when they\nare not in use.\nWhich EC2 instance purchasing solution will meet the company's requirements MOST cost-effectively?",
    "options": {
      "A": "Use Spot Instances for the production EC2 instances. Use Reserved Instances for the development and test",
      "B": "Use Reserved Instances for the production EC2 instances. Use On-Demand Instances for the development",
      "C": "Use Spot blocks for the production EC2 instances. Use Reserved Instances for the development and test EC2",
      "D": "Use On-Demand Instances for the production EC2 instances. Use Spot blocks for the development and test"
    },
    "answer": "B",
    "explanation": "The optimal solution balances cost savings and operational needs. Option B, using Reserved Instances for\nproduction and On-Demand instances for development/test, achieves this balance most effectively.\nProduction instances run 24/7. Reserved Instances provide significant cost savings (up to 75%) compared to\nOn-Demand pricing for long-term, consistent usage. Committing to Reserved Instances for production\nensures predictable pricing and lower overall expenses since production workload is constant.\nhttps://aws.amazon.com/ec2/pricing/reserved-instances/\nDevelopment and test instances are only needed for at least 8 hours a day. While automation will shut them\ndown when not in use, Spot Instances (Option A & D) are unsuitable for development and test environments.\nSpot Instances can be interrupted with little notice, disrupting crucial testing or development tasks. Spot\nBlocks (Option C & D) offer more reliability but typically involve higher prices than On-Demand.\nSince the instances are only run for 8 hours, On-Demand Instances offer the most cost-effective option for\nDevelopment and Test environments. They provide flexibility without any long-term commitments or risk of\ninterruption. The minimal usage duration does not justify the upfront investment and commitment required for\nReserved Instances. https://aws.amazon.com/ec2/pricing/on-demand/\nTherefore, Reserved Instances for the stable, 24/7 production environment and On-Demand Instances for the\nshorter, flexible development and test environments gives the best cost benefit.",
    "links": [
      "https://aws.amazon.com/ec2/pricing/reserved-instances/",
      "https://aws.amazon.com/ec2/pricing/on-demand/"
    ]
  },
  {
    "question": "CertyIQ\nA company has a production web application in which users upload documents through a web interface or a mobile\napp. According to a new regulatory requirement. new documents cannot be modified or deleted after they are\nstored.\nWhat should a solutions architect do to meet this requirement?",
    "options": {
      "A": "Store the uploaded documents in an Amazon S3 bucket with S3 Versioning and S3 Object Lock enabled.",
      "B": "Store the uploaded documents in an Amazon S3 bucket. Configure an S3 Lifecycle policy to archive the",
      "C": "Store the uploaded documents in an Amazon S3 bucket with S3 Versioning enabled. Configure an ACL to",
      "D": "Store the uploaded documents on an Amazon Elastic File System (Amazon EFS) volume. Access the data by"
    },
    "answer": "A",
    "explanation": "The correct answer is A: Store the uploaded documents in an Amazon S3 bucket with S3 Versioning and S3\nObject Lock enabled. This solution directly addresses the regulatory requirement that documents cannot be\nmodified or deleted after storage. Let's break down why this works and why other options are less suitable.\nJustification for Option A:\nS3 Versioning: Enabling S3 Versioning ensures that every change to an object in the bucket creates a new\nversion. This means that if a user attempts to modify a document, the original version remains intact, fulfilling\nthe immutability requirement.\nS3 Object Lock: S3 Object Lock prevents objects from being deleted or overwritten for a fixed amount of time\nor indefinitely. There are two modes:\nGovernance mode: Allows certain users with specific permissions to bypass the lock.\nCompliance mode: Prevents anyone, including the root user, from deleting or overwriting the locked object.\nThis is crucial for meeting strict regulatory requirements.\nCombination is Key: Using S3 Versioning alone only ensures previous versions are kept; it doesn't prevent\ndeletion. S3 Object Lock, combined with versioning, provides the complete immutability guarantee.\nWhy Other Options Are Incorrect:\nOption B: S3 with Lifecycle Policy: While S3 Lifecycle policies are useful for archiving data to cheaper\nstorage tiers, they don't prevent modification or deletion during the active period. Archiving simply moves the\ndata; it doesn't lock it.\nOption C: S3 Versioning and ACL: While S3 Versioning ensures that previous versions are retained, it doesn't,\nby itself, prevent deletion of objects. Additionally, ACLs can be complex to manage and can be easily\nmisconfigured, making them less reliable for regulatory compliance. They also don't provide deletion\nprevention like Object Lock.\nOption D: Amazon EFS in Read-Only Mode: EFS is a network file system, and while mounting it in read-only\nmode would prevent modifications through the mount point, it doesn't guarantee immutability. An\nadministrator with access to the underlying EFS infrastructure could still modify or delete the files. EFS also\nlacks built-in versioning and object locking features designed for immutability like S3.\nIn summary: S3 Versioning and S3 Object Lock, particularly in Compliance mode, offer a robust and easily\nauditable solution to guarantee that documents cannot be modified or deleted, fulfilling the stringent\nregulatory requirement. S3's features are specifically designed for object storage and data durability with\ncompliance in mind.\nAuthoritative Links:\nS3 Versioning: https://docs.aws.amazon.com/AmazonS3/latest/userguide/versioning-workflows.html\nS3 Object Lock: https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/versioning-workflows.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has several web servers that need to frequently access a common Amazon RDS MySQL Multi-AZ DB\ninstance. The company wants a secure method for the web servers to connect to the database while meeting a\nsecurity requirement to rotate user credentials frequently.\nWhich solution meets these requirements?",
    "options": {
      "A": "Store the database user credentials in AWS Secrets Manager. Grant the necessary IAM permissions to allow",
      "B": "Store the database user credentials in AWS Systems Manager OpsCenter. Grant the necessary IAM",
      "C": "Store the database user credentials in a secure Amazon S3 bucket. Grant the necessary IAM permissions to",
      "D": "Store the database user credentials in files encrypted with AWS Key Management Service (AWS KMS) on"
    },
    "answer": "A",
    "explanation": "The best solution is A: Store the database user credentials in AWS Secrets Manager and grant web servers\nIAM permissions to access it.\nHere's why:\nSecure Credential Storage: AWS Secrets Manager is specifically designed for managing secrets like\ndatabase credentials. It encrypts secrets at rest and in transit, providing a high level of security.\nAutomatic Rotation: Secrets Manager allows automatic rotation of database credentials, fulfilling the\nrequirement for frequent credential rotation. This significantly reduces the risk associated with long-lived\ncredentials.\nCentralized Management: It offers a central location to manage and control access to the database\ncredentials. Changes can be made easily and applied across all web servers.\nIAM Integration: Secrets Manager integrates seamlessly with IAM, allowing fine-grained control over which\nweb servers can access specific secrets. This adheres to the principle of least privilege.\nAuditing: AWS CloudTrail logs all access to secrets, providing a comprehensive audit trail for compliance and\nsecurity monitoring.\nOptions B, C, and D are less suitable:\nB (Systems Manager OpsCenter): OpsCenter is primarily designed for managing operational issues, not for\nstoring sensitive credentials.\nC (S3 Bucket): While you can encrypt files in S3, it lacks the dedicated credential management features of\nSecrets Manager, like automatic rotation and built-in IAM integration specific to secrets.\nD (KMS Encrypted Files): Storing encrypted files on the web servers themselves increases the risk of\nexposure if the servers are compromised. It also makes credential rotation and centralized management more\ndifficult.\nSecrets Manager provides a robust and secure solution that addresses the specific requirements outlined in\nthe scenario, focusing on secure storage, automatic rotation, centralized management, and fine-grained\naccess control via IAM.\nAuthoritative Links:\nAWS Secrets Manager: https://aws.amazon.com/secrets-manager/\nIAM Best Practices: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html",
    "links": [
      "https://aws.amazon.com/secrets-manager/",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts an application on AWS Lambda functions that are invoked by an Amazon API Gateway API. The\nLambda functions save customer data to an Amazon Aurora MySQL database. Whenever the company upgrades\nthe database, the Lambda functions fail to establish database connections until the upgrade is complete. The\nresult is that customer data is not recorded for some of the event.\nA solutions architect needs to design a solution that stores customer data that is created during database\nupgrades.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Provision an Amazon RDS proxy to sit between the Lambda functions and the database. Configure the",
      "B": "Increase the run time of the Lambda functions to the maximum. Create a retry mechanism in the code that",
      "C": "Persist the customer data to Lambda local storage. Configure new Lambda functions to scan the local",
      "D": "Store the customer data in an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Create a new"
    },
    "answer": "D",
    "explanation": "The correct answer is D: Store the customer data in an Amazon Simple Queue Service (Amazon SQS) FIFO\nqueue. Create a new Lambda function that polls the queue and stores the customer data in the database.\nHere's why:\nReliability and Durability: SQS provides a highly reliable and durable queuing mechanism. When the Aurora\ndatabase is unavailable during upgrades, the Lambda functions can still successfully write the customer data\nto the SQS FIFO queue without data loss.\nDecoupling: SQS decouples the data ingestion (Lambda functions) from the data processing/storage (new\nLambda function). This decoupling ensures that the application continues to accept customer data even when\nthe database is temporarily unavailable. This adheres to the well-architected framework principle of loosely\ncoupling services to improve resilience.\nFIFO (First-In, First-Out): The FIFO queue guarantees that the customer data is processed in the order it was\nreceived. This is important for maintaining data integrity and ensuring that events are processed in the correct\nsequence.\nAsynchronous Processing: The new Lambda function polling the queue processes the data asynchronously,\nminimizing the impact on the performance of the initial Lambda functions invoked by API Gateway. This\nasynchronous model also provides fault tolerance. If the processing Lambda fails, the messages remain in the\nqueue until they are successfully processed.\nScalability: SQS is a fully managed service that automatically scales to handle varying workloads, ensuring\nthat the queue can accommodate the volume of customer data generated during database upgrades.\nLet's examine why the other options are less suitable:\nA: Amazon RDS Proxy: While RDS Proxy can help with connection management and reduce database load, it\ndoesn't inherently store data when the database is completely unavailable during upgrades. The proxy is only\nas effective as the underlying database's availability.\nB: Increased Lambda Runtime and Retry Mechanism: Increasing Lambda runtime and adding retries can help\nwith transient errors, but it won't solve the problem of prolonged database unavailability during upgrades.\nAlso, Lambda functions are designed to be short-lived; increasing the runtime significantly can lead to\nperformance and cost inefficiencies.\nC: Lambda Local Storage: Lambda local storage is ephemeral and limited in size. It's not designed for durable\nstorage of data during prolonged outages. The data stored on the Lambda's local storage is lost when the\nLambda execution environment is terminated. This is not a reliable approach for handling database upgrades.\nAuthoritative Links:\nAmazon SQS: https://aws.amazon.com/sqs/\nAmazon Lambda: https://aws.amazon.com/lambda/\nWell-Architected Framework: https://wa.aws.amazon.com/wellarchitected/2020-07-02T19-33-\n28/index.en.html",
    "links": [
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/lambda/",
      "https://wa.aws.amazon.com/wellarchitected/2020-07-02T19-33-"
    ]
  },
  {
    "question": "CertyIQ\nA survey company has gathered data for several years from areas in the United States. The company hosts the\ndata in an Amazon S3 bucket that is 3 TB in size and growing. The company has started to share the data with a\nEuropean marketing firm that has S3 buckets. The company wants to ensure that its data transfer costs remain as\nlow as possible.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Configure the Requester Pays feature on the company's S3 bucket.",
      "B": "S3 Cross-Region Replication: This replicates the entire 3 TB dataset to the marketing firm's bucket. The",
      "C": "Cross-Account Access: While this allows the marketing firm to access the data, the survey company would",
      "D": "S3 Intelligent-Tiering with Sync: Intelligent-Tiering optimizes storage costs based on access patterns"
    },
    "answer": "A",
    "explanation": "The correct answer is A: Configure the Requester Pays feature on the company's S3 bucket.\nHere's why: The primary goal is to minimize the survey company's data transfer costs when sharing data with\nthe European marketing firm.\nRequester Pays: When the company enables Requester Pays on its S3 bucket, the marketing firm (the\nrequester) is responsible for all data transfer and request costs associated with accessing the data in the\nbucket. This directly shifts the cost burden from the survey company to the marketing firm, thus minimizing\nthe survey company's costs.\nWhy other options are less optimal:\nB. S3 Cross-Region Replication: This replicates the entire 3 TB dataset to the marketing firm's bucket. The\nsurvey company would incur the costs of replication (storage, data transfer out of the source region, and\nrequests). The replication is a continuous process, resulting in on-going costs. This does not minimize the\nsurvey company's transfer costs.\nC. Cross-Account Access: While this allows the marketing firm to access the data, the survey company would\nstill bear the data transfer costs when the marketing firm downloads or processes the data. Granting cross-\naccount access only provides permission; it doesn't change who pays for data transfer.\nD. S3 Intelligent-Tiering with Sync: Intelligent-Tiering optimizes storage costs based on access patterns\nwithin the survey company's S3 bucket. It does not affect the data transfer costs when the marketing firm\naccesses the data from their location after the sync. Also, syncing the bucket would involve the survey\ncompany paying for the initial data transfer and incurring ongoing synchronization costs.\nIn summary, Requester Pays is the only option that directly and completely shifts the responsibility for data\ntransfer costs to the marketing firm, fulfilling the requirement of minimizing the survey company's costs.\nRelevant Link:\nUsing Requester Pays buckets - Amazon Simple Storage Service",
    "links": []
  },
  {
    "question": "CertyIQ\nA company uses Amazon S3 to store its confidential audit documents. The S3 bucket uses bucket policies to\nrestrict access to audit team IAM user credentials according to the principle of least privilege. Company managers\nare worried about accidental deletion of documents in the S3 bucket and want a more secure solution.\nWhat should a solutions architect do to secure the audit documents?",
    "options": {
      "A": "Enable the versioning and MFA Delete features on the S3 bucket.",
      "B": "Enable multi-factor authentication (MFA) on the IAM user credentials for each audit team IAM user account.",
      "C": "Add an S3 Lifecycle policy to the audit team's IAM user accounts to deny the s3:DeleteObject action during",
      "D": "Use AWS Key Management Service (AWS KMS) to encrypt the S3 bucket and restrict audit team IAM user"
    },
    "answer": "A",
    "explanation": "The best solution to prevent accidental deletion of confidential audit documents in an S3 bucket while\nmaintaining the principle of least privilege and addressing management's concerns is to enable versioning\nand MFA Delete on the S3 bucket.\nVersioning, when enabled on an S3 bucket, preserves every version of an object. This means that if a file is\naccidentally deleted or overwritten, the previous versions are still available for recovery. This mitigates data\nloss from accidental deletion. [https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html]\nMFA Delete adds an extra layer of security. When enabled, deleting a version of an object or permanently\ndeleting the entire bucket requires multi-factor authentication. This prevents unauthorized or accidental\ndeletion, even if an IAM user's credentials are compromised. This makes accidental deletion significantly more\ndifficult. [https://docs.aws.amazon.com/AmazonS3/latest/userguide/VersioningwithMFA.html]\nOption B, enabling MFA on IAM user credentials, increases security but doesn't prevent accidental deletion\nwithin the IAM user's granted permissions. A user with s3:DeleteObject permission could still accidentally\ndelete files.\nOption C, adding an S3 Lifecycle policy to deny deletion, is overly complex and impractical. Lifecycle policies\nare designed for automating transitions or deletions based on object age, not temporary restriction of deletion\nactions based on audit dates. Furthermore, this adds complexity to IAM user management.\nOption D, using KMS encryption and restricting access to the key, addresses encryption and access control\nbut doesn't directly prevent accidental deletion if a user with delete permissions has access to the KMS key.\nWhile encryption is essential, it doesn't solve the specific problem of accidental deletion. Encryption secures\nthe data but doesn't prevent authorized users from deleting it.\nTherefore, enabling both versioning and MFA Delete provides a comprehensive solution for preventing\naccidental deletion while minimizing administrative overhead and adhering to security best practices.",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html]",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/VersioningwithMFA.html]"
    ]
  },
  {
    "question": "CertyIQ\nA company is using a SQL database to store movie data that is publicly accessible. The database runs on an\nAmazon RDS Single-AZ DB instance. A script runs queries at random intervals each day to record the number of\nnew movies that have been added to the database. The script must report a final total during business hours.\nThe company's development team notices that the database performance is inadequate for development tasks\nwhen the script is running. A solutions architect must recommend a solution to resolve this issue.\nWhich solution will meet this requirement with the LEAST operational overhead?",
    "options": {
      "A": "Modify the DB instance to be a Multi-AZ deployment.",
      "B": "Create a read replica of the database. Configure the script to query only the read replica.",
      "C": "Instruct the development team to manually export the entries in the database at the end of each day.",
      "D": "Use Amazon ElastiCache to cache the common queries that the script runs against the database."
    },
    "answer": "B",
    "explanation": "The best solution is to create a read replica of the database and configure the script to query only the read\nreplica. This is because read replicas are designed specifically to offload read traffic from the primary\ndatabase instance. By directing the script's queries to the read replica, the load on the primary RDS instance\nis significantly reduced, resolving the performance issues experienced by the development team during script\nexecution.\nOption A, modifying the DB instance to be a Multi-AZ deployment, improves availability and durability but\ndoesn't directly address the performance issues caused by read queries. A Multi-AZ deployment is primarily\nfor failover purposes in case of an infrastructure failure.\nOption C, manually exporting the entries daily, is not a scalable or automated solution. It involves significant\nmanual effort and introduces potential for human error. It also doesn't provide real-time or near-real-time\ndata for the script.\nOption D, using Amazon ElastiCache, can improve query performance for frequently accessed data, but it\nrequires modifying the application to utilize the cache. Caching the common queries might alleviate some\npressure, but doesn't solve the underlying issue of the script's queries impacting the primary database's\nperformance for all development tasks. It also adds complexity to the architecture without fully addressing\nthe problem. Moreover, determining which queries are \"common\" and configuring the cache appropriately\nintroduces additional overhead. The script's queries run at random intervals and involve finding new movies,\nso it's unlikely that caching will be as effective as offloading those queries entirely to a read replica.\nRead replicas offer a straightforward and efficient way to isolate read traffic without requiring significant\napplication changes or manual intervention. This approach provides the least operational overhead while\neffectively addressing the performance bottleneck. This solution specifically addresses the problem by\noffloading the read queries from the primary instance, thus it directly improves the performance for other\nworkloads, like development.\nRelevant Documentation:\nAmazon RDS Read Replicas:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.ReadReplicas.html\nAmazon RDS Multi-AZ Deployments:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.ReadReplicas.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has applications that run on Amazon EC2 instances in a VP",
    "options": {
      "C": "Create an S3 bucket in the same AWS Region as the EC2 instances.",
      "A": "Configure an S3 gateway endpoint.",
      "B": "Create an S3 bucket in a private subnet.",
      "D": "Configure a NAT gateway in the same subnet as the EC2 instances."
    },
    "answer": "A",
    "explanation": "The correct answer is A, configuring an S3 gateway endpoint. Here's why:\nThe requirement is to allow EC2 instances to access S3 without traversing the internet, adhering to strict\nsecurity regulations. An S3 gateway endpoint provides private connectivity between your VPC and S3,\navoiding the public internet. It essentially creates a route within the AWS network that directly connects your\nVPC to S3. No internet gateway or NAT gateway is involved.\nOption B, creating an S3 bucket in a private subnet, is incorrect. S3 buckets exist globally within a region, not\nwithin VPC subnets. The location of the bucket doesn't affect how EC2 instances access it. The access\nmethod is the key.\nOption C, creating an S3 bucket in the same region, while a best practice for performance and cost, doesn't\naddress the core security requirement of avoiding internet traffic. EC2 instances would still need a way to\nreach S3, and without an endpoint, they would default to using public endpoints.\nOption D, configuring a NAT gateway, is also incorrect. A NAT gateway allows instances in a private subnet to\ninitiate outbound internet traffic, but it doesn't provide a private connection to S3. It would enable internet\naccess, which is against the requirements. The traffic would still traverse the public internet.\nS3 gateway endpoints are specifically designed for securely connecting to S3 from within a VPC without\nusing public IPs. They enhance security by preventing data from leaving the AWS network and are more cost-\neffective than routing traffic through a NAT gateway for S3 access. Gateway endpoints support a route table\nentry and allow you to specify S3 buckets to allow access to, providing granular control.\nFor further reading, refer to the AWS documentation on VPC endpoints for S3:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is storing sensitive user information in an Amazon S3 bucket. The company wants to provide secure\naccess to this bucket from the application tier running on Amazon EC2 instances inside a VP",
    "options": {
      "C": "Create a bucket policy that limits access to only the application tier running in the VPC:",
      "A": "Configure a VPC gateway endpoint for Amazon S3 within the VPC:",
      "B": "Create a bucket policy to make the objects in the S3 bucket public: This is a major security vulnerability",
      "D": "Create an IAM user with an S3 access policy and copy the IAM credentials to the EC2 instance: While"
    },
    "answer": "A",
    "explanation": "The correct answer is AC. Here's why:\nA. Configure a VPC gateway endpoint for Amazon S3 within the VPC:\nA VPC gateway endpoint for S3 is a crucial element for secure access. It enables EC2 instances within a VPC\nto access S3 without traversing the internet. Traffic between the EC2 instances and S3 remains within the\nAWS network, enhancing security and reducing latency. It also eliminates the need for internet gateways,\nNAT gateways/instances, or public IPs on the EC2 instances. This aligns with best practices for securing VPC\nresources. [https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html]\nC. Create a bucket policy that limits access to only the application tier running in the VPC:\nA bucket policy is essential to restrict access to the S3 bucket. By creating a policy that allows access only\nfrom the specified VPC or a specific range of private IP addresses associated with the EC2 instances in the\napplication tier, you ensure that only authorized resources can access the sensitive data. This principle of\nleast privilege is a cornerstone of security. This prevents other unauthorized AWS resources or external\nentities from accessing the S3 bucket. You can even further restrict access by referencing specific IAM roles\nassociated with the EC2 instances. [https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-\nbucket-policies.html]\nWhy the other options are incorrect:\nB. Create a bucket policy to make the objects in the S3 bucket public: This is a major security vulnerability\nand would expose the sensitive user information to anyone on the internet. It's directly contradictory to the\nrequirement of secure access.\nD. Create an IAM user with an S3 access policy and copy the IAM credentials to the EC2 instance: While\nIAM roles are necessary for EC2 instances to assume permissions, copying IAM credentials directly onto EC2\ninstances is a bad practice. It presents a security risk; if the instance is compromised, the credentials are also\ncompromised. It is better to utilize IAM roles. Furthermore, using VPC endpoints avoids the need for this and\nallows resource-based access.\nE. Create a NAT instance and have the EC2 instances use the NAT instance to access the S3 bucket: Using\na NAT instance would allow access to S3 via the internet. While the NAT instance would need to have the\nright S3 permissions via an attached role or a role that the NAT instance user assumed, this approach isn't the\nmost secure nor the most efficient. VPC endpoints provide a more secure and direct connection to S3. You\nalso incur charges for NAT gateway data processing, which you avoid using a gateway endpoint.",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html]",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an on-premises application that is powered by a MySQL database. The company is migrating the\napplication to AWS to increase the application's elasticity and availability.\nThe current architecture shows heavy read activity on the database during times of normal operation. Every 4\nhours, the company's development team pulls a full export of the production database to populate a database in\nthe staging environment. During this period, users experience unacceptable application latency. The development\nteam is unable to use the staging environment until the procedure completes.\nA solutions architect must recommend replacement architecture that alleviates the application latency issue. The\nreplacement architecture also must give the development team the ability to continue using the staging\nenvironment without delay.\nWhich solution meets these requirements?",
    "options": {
      "A": "Using mysqldump: While mysqldump creates backups, restoring the staging database from a mysqldump",
      "B": "Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Use database cloning to create the",
      "C": "RDS MySQL with Multi-AZ and Read Replicas: While RDS MySQL Multi-AZ improves availability and read",
      "D": "RDS MySQL with Multi-AZ and Read Replicas, plus mysqldump: This option suffers from the same issue"
    },
    "answer": "B",
    "explanation": "The correct solution is B: Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production and use\ndatabase cloning to create the staging database on-demand. Here's why:\nAurora MySQL Multi-AZ with Replicas: Aurora provides higher availability and performance compared to\nstandard RDS MySQL due to its architecture and storage engine. Multi-AZ ensures automatic failover in case\nof primary instance failure, minimizing downtime. Aurora Replicas offload read traffic from the primary\ninstance, improving application performance during normal operation.\nhttps://aws.amazon.com/rds/aurora/\nDatabase Cloning: Aurora offers a fast and efficient cloning feature. Cloning creates a point-in-time,\nwriteable copy of the database volume without incurring significant downtime or impacting production\nperformance. This addresses the development team's need for a readily available staging environment\nwithout the 4-hour latency issue caused by a full database export. Cloning is near-instantaneous, so staging\ndatabase creation would not delay the development team.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Aurora.Managing.Clone.html\nLet's analyze why the other options are less suitable:\nA. Using mysqldump: While mysqldump creates backups, restoring the staging database from a mysqldump\nbackup still takes a significant amount of time (similar to the current 4-hour export) and causes application\nlatency, defeating the purpose.\nC. RDS MySQL with Multi-AZ and Read Replicas: While RDS MySQL Multi-AZ improves availability and read\nreplicas offload read traffic, using the standby instance for the staging database is not recommended. The\nstandby instance is critical for failover and should not be directly accessed for other purposes. Furthermore,\nthis option doesn't address the need for a quick and efficient staging database creation method, as it implies\nusing the already existing standby.\nD. RDS MySQL with Multi-AZ and Read Replicas, plus mysqldump: This option suffers from the same issue\nas option A - mysqldump takes too long, causing latency issues and delaying the development team.",
    "links": [
      "https://aws.amazon.com/rds/aurora/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Aurora.Managing.Clone.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is designing an application where users upload small files into Amazon S3. After a user uploads a file,\nthe file requires one-time simple processing to transform the data and save the data in JSON format for later\nanalysis.\nEach file must be processed as quickly as possible after it is uploaded. Demand will vary. On some days, users will\nupload a high number of files. On other days, users will upload a few files or no files.\nWhich solution meets these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Configure Amazon EMR to read text files from Amazon S3. Run processing scripts to transform the data.",
      "B": "In summary, Option C's event-driven architecture with S3, SQS, Lambda, and DynamoDB provides the best"
    },
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the best solution, along with supporting concepts and links:\nOption C is the most efficient and scalable solution with the least operational overhead because it leverages\nserverless technologies for event-driven processing. When a file is uploaded to S3, an event notification is\ntriggered and placed into an SQS queue. SQS acts as a buffer, decoupling the S3 upload process from the\nprocessing component and handling varying demand. The Lambda function is triggered by messages in the\nSQS queue. Lambda's serverless nature means it automatically scales based on the number of messages in\nthe queue, handling both high and low traffic volumes efficiently without requiring any manual management\nof servers. DynamoDB is a NoSQL database perfectly suited for storing JSON data and scaling rapidly based\non read and write activity. This solution minimizes operational overhead because there's no need to manage\nEC2 instances, EMR clusters, or Kinesis streams.\nOption A is less ideal because Amazon EMR is designed for large-scale data processing and analytics, which\nis overkill for simple file transformations. Managing an EMR cluster also introduces significant operational\noverhead. Storing the JSON in Aurora might be suitable, but DynamoDB is a better fit for this use case due to\nits flexible schema.\nOption B involves using EC2 instances to poll the SQS queue and process the data. While it works, it adds\noperational overhead because you're responsible for managing the EC2 instances, scaling them up or down,\nand ensuring high availability. This is much less efficient and cost-effective compared to Lambda.\nOption D using EventBridge and Kinesis Data Streams adds unnecessary complexity and overhead. Kinesis\nData Streams is designed for high-throughput, real-time data streaming, which isn't needed for one-time file\nprocessing. EventBridge is also not needed as S3 can directly publish to SQS. Using Lambda for the\nprocessing is good, but Aurora might not be the optimal choice for JSON storage compared to DynamoDB.\nIn summary, Option C's event-driven architecture with S3, SQS, Lambda, and DynamoDB provides the best\nbalance of scalability, efficiency, and minimal operational overhead for this particular use case.\nRelevant links:\nAmazon S3 Event Notifications:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html\nAmazon SQS: https://aws.amazon.com/sqs/\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon DynamoDB: https://aws.amazon.com/dynamodb/",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html",
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/dynamodb/"
    ]
  },
  {
    "question": "CertyIQ\nAn application allows users at a company's headquarters to access product data. The product data is stored in an\nAmazon RDS MySQL DB instance. The operations team has isolated an application performance slowdown and\nwants to separate read traffic from write traffic. A solutions architect needs to optimize the application's\nperformance quickly.\nWhat should the solutions architect recommend?",
    "options": {
      "A": "Change the existing database to a Multi-AZ deployment. Serve the read requests from the primary",
      "B": "Change the existing database to a Multi-AZ deployment. Serve the read requests from the secondary",
      "C": "Create read replicas for the database. Configure the read replicas with half of the compute and storage",
      "D": "Create read replicas for the database. Configure the read replicas with the same compute and storage"
    },
    "answer": "D",
    "explanation": "The best solution for separating read and write traffic in this scenario is to create read replicas.\nOption D is correct because read replicas are specifically designed to handle read-heavy workloads,\noffloading the primary database instance. This helps improve the application's performance by reducing the\nload on the primary RDS instance, allowing it to focus on write operations. Configuring the read replicas with\nthe same compute and storage resources ensures they can handle the read workload efficiently without\nbecoming a bottleneck. If the read replicas have insufficient resources, they may not be able to handle the\nread load adequately and could become overloaded, negating the benefits of separating read and write\ntraffic.\nOption A and B are incorrect because while Multi-AZ deployments provide high availability and failover\ncapabilities, they don't inherently separate read and write traffic. In a Multi-AZ setup, the secondary instance\nis a standby and doesn't serve read requests unless a failover occurs. Multi-AZ is primarily for disaster\nrecovery, not performance optimization for read-heavy workloads.\nOption C is incorrect because configuring the read replicas with half the compute and storage of the source\ndatabase may lead to performance issues, especially if the read workload is substantial. Under-provisioning\nthe read replicas can lead to latency and bottlenecks, defeating the purpose of separating read traffic. Read\nreplicas should ideally have similar resources to handle the expected read load effectively.\nTherefore, creating read replicas with the same compute and storage resources as the source database is the\nmost effective way to optimize the application's performance by separating read and write traffic.\nFurther Research:\nAmazon RDS Read Replicas:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/AuroraMySQL.Replication.ReadReplicas.html\nAmazon RDS Multi-AZ Deployments:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/AuroraMySQL.Replication.ReadReplicas.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html"
    ]
  },
  {
    "question": "CertyIQ\nAn Amazon EC2 administrator created the following policy associated with an IAM group containing several users:\nWhat is the effect of this policy?",
    "options": {},
    "answer": "C",
    "explanation": "C is correct.0.0/24 , the following five IP addresses are reserved:0.0: Network address.0.1: Reserved by AWS\nfor the VPC router.0.2: Reserved by AWS. The IP address of the DNS server is the base of the VPC network\nrange plus two. ...0.3: Reserved by AWS for future use.0.255: Network broadcast address.",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has a large Microsoft SharePoint deployment running on-premises that requires Microsoft Windows\nshared file storage. The company wants to migrate this workload to the AWS Cloud and is considering various\nstorage options. The storage solution must be highly available and integrated with Active Directory for access\ncontrol.\nWhich solution will satisfy these requirements?",
    "options": {
      "A": "Configure Amazon EFS storage and set the Active Directory domain for authentication.",
      "B": "Create an SMB file share on an AWS Storage Gateway file gateway in two Availability Zones.",
      "C": "Create an Amazon S3 bucket and configure Microsoft Windows Server to mount it as a volume.",
      "D": "Create an Amazon FSx for Windows File Server file system on AWS and set the Active Directory domain for"
    },
    "answer": "D",
    "explanation": "The best solution is to use Amazon FSx for Windows File Server, due to its features specifically designed to\naddress the described needs.\nOption D is correct because Amazon FSx for Windows File Server provides fully managed native Microsoft\nWindows file servers backed by SSD storage. It is designed to integrate seamlessly with Active Directory for\nauthentication and authorization, fulfilling the requirement for access control. Its support for the SMB\nprotocol allows it to be accessed as a standard Windows file share. FSx for Windows File Server can also be\ndeployed in a Multi-AZ configuration, offering high availability.\nOption A is incorrect because Amazon EFS (Elastic File System) is primarily designed for Linux workloads and\nwhile it can integrate with Active Directory via IAM and NFS, it is not a native Windows file server and lacks\ndirect SMB protocol support, making it a less suitable fit for a SharePoint migration requiring Windows shared\nfile storage.\nOption B is incorrect because while AWS Storage Gateway's file gateway can present SMB file shares and\nintegrate with Active Directory, it's a hybrid solution that caches data locally. It uses S3 as the primary\nstorage location, which is not ideal for low-latency, high-performance SharePoint workloads. Placing a file\ngateway in two Availability Zones does not natively provide high availability for the SMB share itself without\nadditional configuration and complexity, adding operational overhead.\nOption C is incorrect because while Amazon S3 can store files, it is object storage and not a file system\nsuitable for directly mounting as a Windows volume in the way required by SharePoint. While there are tools\nto achieve this, it isn't a direct solution and would be more complex and prone to performance issues than\nusing a native Windows file server solution. The required integration and latency requirements would not be\nmet easily using S3 for the application in question.\nIn summary, Amazon FSx for Windows File Server is the optimal choice due to its native support for the SMB\nprotocol, Active Directory integration, high availability through Multi-AZ deployment, and its ability to\nfunction as a fully managed Windows file server, seamlessly integrating with SharePoint.\nSupporting Links:\nAmazon FSx for Windows File Server: https://aws.amazon.com/fsx/windows/\nAWS Storage Gateway: https://aws.amazon.com/storagegateway/\nAmazon EFS: https://aws.amazon.com/efs/",
    "links": [
      "https://aws.amazon.com/fsx/windows/",
      "https://aws.amazon.com/storagegateway/",
      "https://aws.amazon.com/efs/"
    ]
  },
  {
    "question": "CertyIQ\nAn image-processing company has a web application that users use to upload images. The application uploads the\nimages into an Amazon S3 bucket. The company has set up S3 event notifications to publish the object creation\nevents to an Amazon Simple Queue Service (Amazon SQS) standard queue. The SQS queue serves as the event\nsource for an AWS Lambda function that processes the images and sends the results to users through email.\nUsers report that they are receiving multiple email messages for every uploaded image. A solutions architect\ndetermines that SQS messages are invoking the Lambda function more than once, resulting in multiple email\nmessages.\nWhat should the solutions architect do to resolve this issue with the LEAST operational overhead?",
    "options": {
      "A": "Set up long polling in the SQS queue by increasing the ReceiveMessage wait time to 30 seconds.",
      "B": "Change the SQS standard queue to an SQS FIFO queue. Use the message deduplication ID to discard",
      "C": "Increase the visibility timeout in the SQS queue to a value that is greater than the total of the function",
      "D": "Relevant Documentation:"
    },
    "answer": "C",
    "explanation": "The issue is duplicate processing of SQS messages by the Lambda function. This indicates the Lambda\nfunction isn't completing and acknowledging receipt of the message within the SQS visibility timeout, leading\nSQS to re-queue the message for another attempt.\nOption A, setting up long polling, helps with efficient message retrieval but does not prevent duplicate\nprocessing if the Lambda function fails or takes too long to process a message within the visibility timeout.\nLong polling primarily reduces the cost associated with empty responses from SQS.\nOption B, switching to an SQS FIFO queue with message deduplication, would prevent duplicates. However, it\nintroduces significant operational overhead. It requires code changes, potentially impacting the existing\napplication's architecture and message ordering assumptions. FIFO queues also have lower throughput limits\nthan standard queues, potentially affecting the application's performance. It's also unnecessary because the\nprocessing order doesn't appear to be a critical requirement based on the problem description.\nOption C, increasing the visibility timeout, directly addresses the root cause. The visibility timeout is the\nduration a message remains invisible to other consumers after being retrieved from the queue. By increasing\nthe visibility timeout to be greater than the combined Lambda function timeout and batch window (the\nmaximum time Lambda waits before processing messages in a batch), it ensures that SQS doesn't re-queue\nthe message while the Lambda function is still processing it. This significantly reduces the chances of\nduplicate processing with minimal code or infrastructure changes.\nOption D, modifying the Lambda function to immediately delete messages, is risky. If the Lambda function\nfails after deleting the message but before completing the processing, the message will be lost entirely. This\nguarantees data loss, which is undesirable. It also adds complexity to the Lambda function and might not\nsolve the underlying timeout issue.\nTherefore, Option C is the best solution because it addresses the root cause with the least operational\noverhead, ensuring that the Lambda function has sufficient time to process the message and preventing\npremature re-queuing by SQS. It minimizes the changes required to the existing system and avoids the\npotential data loss associated with Option D.\nRelevant Documentation:\nSQS Visibility Timeout:\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-\ntimeout.html\nAWS Lambda with SQS: https://docs.aws.amazon.com/lambda/latest/dg/services-sqs.html",
    "links": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-",
      "https://docs.aws.amazon.com/lambda/latest/dg/services-sqs.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is implementing a shared storage solution for a gaming application that is hosted in an on-premises\ndata center. The company needs the ability to use Lustre clients to access data. The solution must be fully\nmanaged.\nWhich solution meets these requirements?",
    "options": {
      "A": "Create an AWS Storage Gateway file gateway. Create a file share that uses the required client protocol.",
      "B": "Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance.",
      "C": "Create an Amazon Elastic File System (Amazon EFS) file system, and configure it to support Lustre. Attach",
      "D": "Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the"
    },
    "answer": "D",
    "explanation": "The question requires a fully managed shared storage solution accessible via Lustre clients for an on-\npremises application. Let's analyze why option D is the correct choice.\nOption D, creating an Amazon FSx for Lustre file system, directly addresses the requirements. FSx for Lustre\nis a fully managed, high-performance file system optimized for compute-intensive workloads, including those\nneeding the Lustre file system. Lustre clients can connect directly to the FSx for Lustre file system, fulfilling\nthe access requirement. The fully managed nature of FSx for Lustre eliminates the need for manual setup,\nconfiguration, and maintenance, which aligns with the question's stipulation. Attaching the file system to the\norigin server and connecting application servers to the file system creates the shared storage environment\nthe application needs.\nOption A, using AWS Storage Gateway file gateway, is incorrect. While Storage Gateway provides file sharing\ncapabilities, it primarily caters to connecting on-premises environments to AWS storage services like S3. It\ndoes not inherently support the Lustre file system protocol. It's more suited for file-based backups and\narchival, not high-performance shared storage.\nOption B, setting up an Amazon EC2 Windows instance with a Windows file share, involves significant manual\neffort. It requires installing, configuring, and maintaining the Windows file share role, negating the \"fully\nmanaged\" requirement. Furthermore, Windows file shares aren't designed for the high-performance compute\nworkloads typically associated with Lustre.\nOption C, creating an Amazon Elastic File System (EFS) file system, and configuring it to support Lustre, is not\na supported configuration. Amazon EFS is a fully managed NFS file system. EFS does not natively support the\nLustre protocol. Trying to \"configure\" it to support Lustre is not a feasible solution within the AWS ecosystem.\nTherefore, Amazon FSx for Lustre is the only fully managed AWS service that natively supports the Lustre file\nsystem protocol, making it the best solution for this scenario.\nRelevant links:\nAmazon FSx for Lustre: https://aws.amazon.com/fsx/lustre/\nAWS Storage Gateway: https://aws.amazon.com/storagegateway/\nAmazon Elastic File System (EFS): https://aws.amazon.com/efs/",
    "links": [
      "https://aws.amazon.com/fsx/lustre/",
      "https://aws.amazon.com/storagegateway/",
      "https://aws.amazon.com/efs/"
    ]
  },
  {
    "question": "CertyIQ\nA company's containerized application runs on an Amazon EC2 instance. The application needs to download\nsecurity certificates before it can communicate with other business applications. The company wants a highly\nsecure solution to encrypt and decrypt the certificates in near real time. The solution also needs to store data in\nhighly available storage after the data is encrypted.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Create AWS Secrets Manager secrets for encrypted certificates. Manually update the certificates as",
      "B": "Create an AWS Lambda function that uses the Python cryptography library to receive and perform",
      "C": "Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the",
      "D": "Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it offers a secure, readily available, and operationally efficient solution for\nmanaging and encrypting certificates.\nHere's why:\nAWS KMS for Encryption: AWS Key Management Service (KMS) is designed for managing encryption keys\nsecurely. Using a KMS customer-managed key allows the company to control the key lifecycle and access\npermissions. This ensures that only authorized entities (the EC2 instance via its role) can use the key for\nencryption and decryption operations. https://aws.amazon.com/kms/\nEC2 Role for Access Control: Granting the EC2 instance access to the KMS key via an IAM role is a best\npractice. It allows the EC2 instance to use the KMS key without embedding credentials within the instance\nitself.\nAmazon S3 for Highly Available Storage: Amazon S3 provides highly available and durable storage. Storing\nthe encrypted data in S3 ensures that it is protected against data loss and is accessible when needed.\nhttps://aws.amazon.com/s3/\nOption A is less desirable because it involves manual updates of certificates in Secrets Manager, which\nintroduces operational overhead and potential for human error. While Secrets Manager is good for storing\nsecrets, KMS is better for encryption.\nOption B is unnecessarily complex. Using a Lambda function with the Python cryptography library introduces\nadditional overhead (managing the Lambda function, dependencies, execution environment) and is not the\nmost efficient solution for simple encryption/decryption tasks.\nOption D is not as good as C because Amazon EBS volumes are primarily designed for block storage for EC2\ninstances. While EBS can be encrypted, it's not ideal for storing data independently that needs to be highly\navailable. S3 provides greater availability, scalability, and durability than EBS in this scenario. Furthermore,\nEBS is attached to a specific availability zone, whereas S3 is region-wide.",
    "links": [
      "https://aws.amazon.com/kms/",
      "https://aws.amazon.com/s3/"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is designing a VPC with public and private subnets. The VPC and subnets use IPv4 CIDR\nblocks. There is one public subnet and one private subnet in each of three Availability Zones (AZs) for high\navailability. An internet gateway is used to provide internet access for the public subnets. The private subnets\nrequire access to the internet to allow Amazon EC2 instances to download software updates.\nWhat should the solutions architect do to enable Internet access for the private subnets?",
    "options": {
      "A": "Create three NAT gateways, one for each public subnet in each AZ. Create a private route table for each AZ",
      "B": "Create three NAT instances, one for each private subnet in each AZ. Create a private route table for each AZ",
      "C": "In summary, NAT Gateways provide a scalable, highly available, and managed solution for outbound internet",
      "D": "Create an egress-only internet gateway on one of the public subnets. Update the route table for the private"
    },
    "answer": "A",
    "explanation": "The correct solution is to use NAT Gateways for internet access from private subnets. Here's why:\nOption A is correct because it leverages NAT Gateways which are the AWS recommended way to provide\noutbound internet access to instances in private subnets while preventing inbound traffic from the internet.\nDeploying one NAT Gateway per Availability Zone (AZ) ensures high availability and prevents a single point of\nfailure. Each private subnet's route table then points to the NAT Gateway within its respective AZ for all\ntraffic destined outside the VPC (non-VPC CIDR block traffic, commonly represented as 0.0.0.0/0). This\narchitecture allows instances in private subnets to initiate outbound connections (e.g., downloading software\nupdates) without exposing them directly to the internet.\nOption B is incorrect because NAT Instances, while a functional alternative, require more administrative\noverhead and manual scaling. They also represent a single point of failure within each AZ unless configured\nfor high availability (which increases complexity). AWS recommends NAT Gateways over NAT Instances.\nOption C is incorrect. You cannot associate an Internet Gateway with a private subnet. Internet Gateways are\ndesigned for public subnets to allow bidirectional communication with the internet. Connecting a private\nsubnet directly to the internet defeats the purpose of having a private subnet and introduces significant\nsecurity risks.\nOption D is incorrect. Egress-only internet gateways are designed for IPv6 traffic only. The question\nspecifically states that the VPC and subnets use IPv4 CIDR blocks, rendering egress-only internet gateways\nunusable in this scenario. They also only permit outbound traffic initiated from inside the VPC.\nIn summary, NAT Gateways provide a scalable, highly available, and managed solution for outbound internet\naccess from private subnets in IPv4 environments, aligning with AWS best practices. The key is to create a\nNAT Gateway in each public subnet of each AZ, and then route all outbound (non-VPC) traffic from the\ncorresponding private subnet to that NAT Gateway.\nRelevant AWS documentation:\nNAT Gateways: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\nNAT Instances: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-instances.html\nInternet Gateways: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html\nEgress-Only Internet Gateways: https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-\ngateway.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-instances.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to migrate an on-premises data center to AWS. The data center hosts an SFTP server that stores\nits data on an NFS-based file system. The server holds 200 GB of data that needs to be transferred. The server\nmust be hosted on an Amazon EC2 instance that uses an Amazon Elastic File System (Amazon EFS) file system.\nWhich combination of steps should a solutions architect take to automate this task? (Choose two.)",
    "options": {
      "A": "Launch the EC2 instance into the same Availability Zone as the EFS file system: While launching the EC2",
      "B": "Install an AWS DataSync agent in the on-premises data center: AWS DataSync is specifically designed",
      "C": "Create a secondary Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instance for the data:",
      "D": "Manually use an operating system copy command to push the data to the EC2 instance: Manually"
    },
    "answer": "B",
    "explanation": "The correct answer is BE because it outlines the most efficient and secure way to migrate the data from the\non-premises SFTP server to the Amazon EFS file system mounted on an EC2 instance.\nJustification:\nB. Install an AWS DataSync agent in the on-premises data center: AWS DataSync is specifically designed\nfor automated and accelerated data transfer between on-premises storage and AWS storage services.\nInstalling a DataSync agent on-premises is crucial for accessing and moving data from the SFTP server.\nDataSync optimizes the transfer process with features like incremental transfers, encryption, and data\nverification. The agent acts as a bridge, allowing DataSync to securely access the on-premises NFS-based file\nsystem. More information can be found on the AWS DataSync documentation:\nhttps://aws.amazon.com/datasync/\nE. Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server: After\ninstalling the agent, the next step is to configure DataSync with the correct source (the on-premises SFTP\nserver) and destination (the Amazon EFS file system). This involves creating a location for the SFTP server,\nproviding the necessary credentials for DataSync to access it. This configuration also includes specifying the\nEFS file system as the destination location within AWS. DataSync can then be configured to schedule or\ninitiate the data transfer.\nWhy other options are incorrect:\nA. Launch the EC2 instance into the same Availability Zone as the EFS file system: While launching the EC2\ninstance in the same Availability Zone as the EFS file system is important for performance once the migration\nis complete, it doesn't directly contribute to the automation of the data migration process. It's a prerequisite\nfor EFS usage but not the solution for the problem.\nC. Create a secondary Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instance for the data:\nCreating an EBS volume is unnecessary and inefficient for the purpose outlined. The goal is to store the data\non EFS, not EBS. Using EBS as an intermediary step introduces complexity and cost without providing any\nadded benefit.\nD. Manually use an operating system copy command to push the data to the EC2 instance: Manually\ncopying data is time-consuming, error-prone, and doesn't leverage the automation capabilities of AWS\nservices. It is not a suitable solution for migrating 200 GB of data and does not scale effectively. It also\ndoesn't provide the data verification or incremental transfer capabilities of DataSync.",
    "links": [
      "https://aws.amazon.com/datasync/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an AWS Glue extract, transform, and load (ETL) job that runs every day at the same time. The job\nprocesses XML data that is in an Amazon S3 bucket. New data is added to the S3 bucket every day. A solutions\narchitect notices that AWS Glue is processing all the data during each run.\nWhat should the solutions architect do to prevent AWS Glue from reprocessing old data?",
    "options": {
      "A": "Edit the job to use job bookmarks.",
      "B": "Edit the job to delete data after the data is processed.",
      "C": "Edit the job by setting the NumberOfWorkers field to 1.",
      "D": "Use a FindMatches machine learning (ML) transform."
    },
    "answer": "A",
    "explanation": "The correct answer is A, using job bookmarks in AWS Glue. Here's why:\nAWS Glue job bookmarks are specifically designed to track the state of ETL jobs, particularly for incremental\ndata processing. They allow Glue to remember which data has already been processed, enabling it to only\nprocess new data during subsequent runs. Without job bookmarks, Glue will scan the entire input dataset\neach time the job runs, leading to reprocessing of old data and increased costs.\nJob bookmarks work by storing information about the last processed files or records, such as the last modified\ntimestamp or a unique identifier. When the job runs again, it uses this information to determine the starting\npoint for data processing. This approach significantly reduces processing time and resource consumption, as\nit avoids redundant operations on data that has already been transformed and loaded.\nOption B, deleting data after processing, is a destructive approach and can lead to data loss if there are any\nissues during the ETL process. It also eliminates the possibility of reprocessing data for other purposes or\ncorrecting errors. Option C, setting NumberOfWorkers to 1, simply reduces the parallelism of the job, slowing\ndown the overall processing time but not preventing reprocessing of old data. Option D, using FindMatches, is\nan ML transform for deduplication and record linkage and is not relevant to preventing the reprocessing of old\ndata during ETL runs.\nJob bookmarks are the best solution because they maintain data integrity, enable incremental processing, and\nreduce costs by avoiding redundant operations, all while being specifically designed for this use case within\nthe AWS Glue ecosystem. They are a standard best practice for efficient ETL workflows with continuously\nupdated data sources.\nFor more information, refer to the official AWS documentation:\nUsing Job Bookmarks to Track Processed Data: https://docs.aws.amazon.com/glue/latest/dg/monitor-\ncontinuations.html",
    "links": [
      "https://docs.aws.amazon.com/glue/latest/dg/monitor-"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect must design a highly available infrastructure for a website. The website is powered by\nWindows web servers that run on Amazon EC2 instances. The solutions architect must implement a solution that\ncan mitigate a large-scale DDoS attack that originates from thousands of IP addresses. Downtime is not\nacceptable for the website.\nWhich actions should the solutions architect take to protect the website from such an attack? (Choose two.)",
    "options": {
      "A": "Use AWS Shield Advanced to stop the DDoS attack: AWS Shield Advanced provides enhanced DDoS",
      "B": "Configure Amazon GuardDuty to automatically block the attackers: GuardDuty is a threat detection",
      "C": "Configure the website to use Amazon CloudFront for both static and dynamic content: CloudFront, a CDN",
      "D": "Use an AWS Lambda function to automatically add attacker IP addresses to VPC network ACLs: While"
    },
    "answer": "A",
    "explanation": "Here's a breakdown of why options A and C are the correct choices and why the others aren't, focusing on\nDDoS mitigation and high availability in the context of Windows-based web servers on EC2:\nA. Use AWS Shield Advanced to stop the DDoS attack: AWS Shield Advanced provides enhanced DDoS\nprotection beyond the standard protection included with all AWS customers. It offers 24/7 access to the AWS\nDDoS Response Team (DRT) and advanced detection and mitigation techniques. This is crucial for handling\nlarge-scale DDoS attacks originating from thousands of IP addresses. Shield Advanced is specifically\ndesigned to protect against sophisticated attacks targeting web applications running on AWS resources.\nhttps://aws.amazon.com/shield/\nC. Configure the website to use Amazon CloudFront for both static and dynamic content: CloudFront, a CDN\n(Content Delivery Network), distributes your website content across a globally distributed network of edge\nlocations. This helps to absorb a DDoS attack by serving content from multiple locations, thereby reducing the\nload on the origin servers (EC2 instances). It also caches static content, further reducing the load on the\norigin. Configuring it for both static and dynamic content allows CloudFront to protect the origin even if\nattackers are requesting dynamic pages. CloudFront also integrates with AWS Shield for more robust\nprotection. https://aws.amazon.com/cloudfront/\nLet's examine why the other options are not the best choices:\nB. Configure Amazon GuardDuty to automatically block the attackers: GuardDuty is a threat detection\nservice that monitors your AWS environment for malicious activity. While GuardDuty can detect potential\nDDoS activity, it doesn't automatically block attackers. It primarily focuses on identifying threats and\ngenerating security findings; it isn't a primary DDoS mitigation tool. The response to GuardDuty findings\nrequires additional configuration (such as custom scripts/automation to block IPs).\nD. Use an AWS Lambda function to automatically add attacker IP addresses to VPC network ACLs: While\ntechnically possible, this approach is not scalable or effective against large-scale DDoS attacks. Network\nACLs have limitations on the number of rules they can hold, and managing a list of thousands of attacker IPs\nwould be cumbersome and potentially impact performance. Additionally, Lambda invocation per IP discovered\nmay add considerable cost and latency.\nE. Use EC2 Spot Instances in an Auto Scaling group with a target tracking scaling policy that is set to 80%\nCPU utilization: Spot Instances and Auto Scaling help with cost optimization and scaling based on resource\nutilization. While useful for general scaling, they don't directly address DDoS mitigation. Scaling up in\nresponse to a DDoS attack might help keep the service online for legitimate users for some time, but it won't\nstop the attack itself and could be quickly overwhelmed in a large-scale attack. Moreover, Spot Instances can\nbe terminated, potentially exacerbating the issue during an attack.",
    "links": [
      "https://aws.amazon.com/shield/",
      "https://aws.amazon.com/cloudfront/"
    ]
  },
  {
    "question": "CertyIQ\nA company is preparing to deploy a new serverless workload. A solutions architect must use the principle of least\nprivilege to configure permissions that will be used to run an AWS Lambda function. An Amazon EventBridge\n(Amazon CloudWatch Events) rule will invoke the function.\nWhich solution meets these requirements?",
    "options": {
      "A": "Add an execution role to the function with lambda:InvokeFunction as the action and * as the principal.",
      "B": "Add an execution role to the function with lambda:InvokeFunction as the action and Service:",
      "C": "Add a resource-based policy to the function with lambda:* as the action and Service: events.amazonaws.com",
      "D": "Add a resource-based policy to the function with lambda:InvokeFunction as the"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Add a resource-based policy to the function with lambda:InvokeFunction as the\naction and Service: events.amazonaws.com as the principal.\nHere's a detailed justification:\nThe question emphasizes the principle of least privilege, which dictates granting only the necessary\npermissions. An EventBridge rule needs permission to invoke the Lambda function. Therefore, the policy\nshould only grant the lambda:InvokeFunction action. Options A and B incorrectly suggest using an execution\nrole. Execution roles are for the Lambda function to access other AWS services, not for services to invoke the\nLambda function.\nThe necessary permissions for EventBridge to trigger a Lambda function are defined in a resource-based\npolicy attached to the Lambda function itself. This is different from an IAM role that's assumed by the Lambda\nfunction. Resource-based policies grant permissions to principals (in this case, EventBridge) to perform\nactions on the resource (the Lambda function). Option C is too broad as it uses lambda:*, which grants all\nLambda permissions, violating the principle of least privilege.\nOption D specifies lambda:InvokeFunction, which is the exact permission EventBridge needs. The Service:\nevents.amazonaws.com principal specifies that only the EventBridge service is authorized to invoke the\nfunction. This adheres to the principle of least privilege. Specifying the service principal is important to limit\naccess to invocations and prevent unauthorized actors from triggering the Lambda function. This is the most\nsecure and precise approach.\nHere are some authoritative links for further research:\nAWS Lambda Permissions: https://docs.aws.amazon.com/lambda/latest/dg/security-iam.html\nUsing Resource-Based Policies for Lambda: https://docs.aws.amazon.com/lambda/latest/dg/access-control-\nresource-based.html\nEventBridge Permissions: https://docs.aws.amazon.com/eventbridge/latest/userguide/auth-and-access-\ncontrol-eventbridge.html",
    "links": [
      "https://docs.aws.amazon.com/lambda/latest/dg/security-iam.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/access-control-",
      "https://docs.aws.amazon.com/eventbridge/latest/userguide/auth-and-access-"
    ]
  },
  {
    "question": "CertyIQ\nA company is preparing to store confidential data in Amazon S3. For compliance reasons, the data must be\nencrypted at rest. Encryption key usage must be logged for auditing purposes. Keys must be rotated every year.\nWhich solution meets these requirements and is the MOST operationally efficient?",
    "options": {
      "A": "Server-side encryption with customer-provided keys (SSE-C)",
      "B": "Server-side encryption with Amazon S3 managed keys (SSE-S3)",
      "C": "Server-side encryption with AWS KMS keys (SSE-KMS) with manual rotation",
      "D": "Server-side encryption with AWS KMS keys (SSE-KMS) with automatic rotation"
    },
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the most operationally efficient solution for storing\nconfidential data in Amazon S3 with encryption, key usage logging, and annual key rotation:\nThe requirements are data encryption at rest, logged key usage for auditing, and annual key rotation. All\noptions except SSE-S3 provide encryption. However, SSE-S3 (option B) doesn't offer detailed logging of key\nusage for auditing. SSE-C (option A) involves managing and providing the encryption keys yourself, which\nsignificantly increases operational overhead and responsibility for security. This option would also not offer\nCloudTrail logging of the key usage.\nSSE-KMS using AWS Key Management Service (KMS) (options C and D) meets the requirements by allowing\nfor encrypted data storage, generating logs of key usage using AWS CloudTrail, and enabling key rotation.\nCloudTrail logs the API calls made to KMS, providing an auditable record of key usage.\nOption D, SSE-KMS with automatic rotation, is the most operationally efficient. AWS KMS can automatically\nrotate the encryption keys every year. This automation reduces the operational burden of manually rotating\nkeys (option C), minimizing the risk of human error and ensuring compliance. With automatic rotation, the key\nis rotated without the application needing to be re-encrypted.\nTherefore, SSE-KMS with automatic key rotation (option D) provides a balance between security, compliance,\nand operational efficiency, making it the most suitable solution.\nFurther research:\nAWS KMS: https://aws.amazon.com/kms/\nS3 Encryption: https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html\nAWS CloudTrail: https://aws.amazon.com/cloudtrail/",
    "links": [
      "https://aws.amazon.com/kms/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html",
      "https://aws.amazon.com/cloudtrail/"
    ]
  },
  {
    "question": "CertyIQ\nA bicycle sharing company is developing a multi-tier architecture to track the location of its bicycles during peak\noperating hours. The company wants to use these data points in its existing analytics platform. A solutions\narchitect must determine the most viable multi-tier option to support this architecture. The data points must be\naccessible from the REST API.\nWhich action meets these requirements for storing and retrieving location data?",
    "options": {
      "A": "Use Amazon Athena with Amazon S3.",
      "B": "Use Amazon API Gateway with AWS Lambda.",
      "C": "Use Amazon QuickSight with Amazon Redshift.",
      "D": "Use Amazon API Gateway with Amazon Kinesis Data Analytics."
    },
    "answer": "B",
    "explanation": "The correct answer is B: Use Amazon API Gateway with AWS Lambda. Here's why:\nThe problem requires a multi-tier architecture that provides REST API access to location data for bicycle\ntracking, which will be used in an existing analytics platform. API Gateway is specifically designed for\ncreating, publishing, maintaining, monitoring, and securing REST APIs. It acts as a front door for applications\nto access data, logic, or functionality from backend services.\nLambda provides a serverless compute environment. It can be triggered by API Gateway requests to process\nincoming location data and store it in a database (not explicitly mentioned in the provided options, but implied\nas necessary for persistence). Lambda can also retrieve this data and return it as a response to API Gateway\nrequests. This enables reading, writing, and processing the data needed for the analytical platform.\nOption A, using Athena with S3, is suited for querying data stored in S3 using SQL. While Athena provides a\ncost-effective way to analyze large datasets, it doesn't directly support a REST API endpoint. It is more\ntailored to ad-hoc querying rather than a transactional system providing real-time data via APIs.\nOption C, using QuickSight with Redshift, represents an analytics dashboarding solution. QuickSight is\nprimarily for visualization and analysis of data, not for serving REST API requests. Redshift is a data\nwarehouse that can store large volumes of data but requires an API or interface layer to expose that data.\nOption D, API Gateway with Kinesis Data Analytics, is not suitable for the scenario described. Kinesis Data\nAnalytics is designed for real-time data processing and transformation, not for storing and retrieving data.\nWhile Kinesis can process the location data stream, it doesnt naturally serve as a data store accessible via\nREST APIs. The output of Kinesis would still typically need to be persisted somewhere (like S3 or a database)\nfor long-term storage and retrieval. Lambda is a simpler and more flexible approach for handling the API and\npotential data storage interaction.\nIn summary, the combination of API Gateway for REST API access and Lambda for data processing and\nstorage interaction provides the most viable and flexible solution for the bicycle sharing company's\nrequirements.\nAuthoritative Links:\nAmazon API Gateway: https://aws.amazon.com/api-gateway/\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon Athena: https://aws.amazon.com/athena/\nAmazon QuickSight: https://aws.amazon.com/quicksight/\nAmazon Redshift: https://aws.amazon.com/redshift/\nAmazon Kinesis Data Analytics: https://aws.amazon.com/kinesis/data-analytics/",
    "links": [
      "https://aws.amazon.com/api-gateway/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/athena/",
      "https://aws.amazon.com/quicksight/",
      "https://aws.amazon.com/redshift/",
      "https://aws.amazon.com/kinesis/data-analytics/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an automobile sales website that stores its listings in a database on Amazon RDS. When an\nautomobile is sold, the listing needs to be removed from the website and the data must be sent to multiple target\nsystems.\nWhich design should a solutions architect recommend?",
    "options": {
      "A": "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the",
      "B": "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the",
      "C": "Subscribe to an RDS event notification and send an Amazon Simple Queue Service (Amazon SQS) queue",
      "D": "Subscribe to an RDS event notification and send an Amazon Simple Notification Service (Amazon SNS) topic"
    },
    "answer": "D",
    "explanation": "The correct answer is D, which suggests using RDS event notifications, an SNS topic fanned out to multiple\nSQS queues, and Lambda functions. Let's break down why.\nRDS event notifications provide a near real-time mechanism for reacting to database changes. When an\nautomobile is sold, the RDS database change will trigger this notification.\nAmazon SNS (Simple Notification Service) acts as a pub/sub service. Using SNS, you can publish one\nmessage, and it will be distributed to multiple subscribers. In this case, the SNS topic will receive the RDS\nevent notification.\nFan-out involves distributing a single event to multiple independent subscribers. Instead of having each target\nsystem directly subscribe to RDS event notifications, SNS allows you to \"fan out\" the notification to different\nSQS queues. This decoupling ensures that each target system can consume the message at its own pace and\nhandle failures independently. Each SQS queue can be customized to handle the specific needs of each target\nsystem.\nAmazon SQS (Simple Queue Service) provides a reliable queuing mechanism. Each target system will have its\nown dedicated SQS queue. The SQS queue acts as a buffer, ensuring that the target systems don't miss any\nupdates even if they are temporarily unavailable.\nAWS Lambda functions are used to consume messages from the SQS queues and update the target systems.\nThe Lambda functions provide the necessary transformation and integration logic to interact with each target\nsystem's API or data format.\nOption A is less desirable because it creates a direct dependency between the database update and a single\nSQS queue. It doesn't efficiently scale to multiple target systems and might lead to bottlenecks if the queue\nconsumer is slow. Option B, while using a FIFO queue, doesn't address the need to distribute the information\nto multiple target systems effectively. Option C attempts to use SNS topics after the SQS queue, which is\nlogically backward. SNS is more efficient when distributing messages to multiple queues, then having them\nfiltered. The structure in Option C would be highly inefficient. Using SQS to fan out to SNS would make SQS a\nbottle neck and remove its purpose of buffering.\nIn summary, option D provides a scalable, loosely coupled, and reliable solution that leverages the strengths\nof RDS event notifications, SNS for fan-out, SQS for buffering, and Lambda for data transformation and\nintegration.References:\nAWS SNS: https://aws.amazon.com/sns/\nAWS SQS: https://aws.amazon.com/sqs/\nAWS Lambda: https://aws.amazon.com/lambda/\nRDS Event Notifications: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html",
    "links": [
      "https://aws.amazon.com/sns/",
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/lambda/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to store data in Amazon S3 and must prevent the data from being changed. The company wants\nnew objects that are uploaded to Amazon S3 to remain unchangeable for a nonspecific amount of time until the\ncompany decides to modify the objects. Only specific users in the company's AWS account can have the ability 10\ndelete the objects.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Create an S3 Glacier vault. Apply a write-once, read-many (WORM) vault lock policy to the objects.",
      "B": "Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Set a retention period of 100 years. Use",
      "C": "Create an S3 bucket. Use AWS CloudTrail to track any S3 API events that modify the objects. Upon",
      "D": "Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Add a legal hold to the objects. Add the"
    },
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the correct solution and why the other options are not\nsuitable:\nWhy Option D is Correct:\nOption D leverages Amazon S3 Object Lock with legal hold functionality, addressing the company's\nrequirements directly. S3 Object Lock prevents objects from being deleted or overwritten for a specified\nperiod or indefinitely. By enabling versioning, previous versions of objects are preserved, providing an extra\nlayer of protection. A legal hold provides the indefinite immutability needed until the company explicitly\ndecides to modify objects. Granting the s3:PutObjectLegalHold permission to specific IAM users gives them the\nnecessary authorization to remove the legal hold, allowing object deletion when the company decides. This\ncontrolled access ensures that only authorized personnel can make changes to the immutability status.\nWhy Other Options Are Incorrect:\nOption A: S3 Glacier vault with WORM vault lock policy: S3 Glacier is primarily for archival storage and has\nretrieval times that are unsuitable for general data storage. While it offers write-once, read-many (WORM)\ncapabilities, it does not offer a mechanism for specific users to remove this immutability when necessary\nwithout destroying the vault. This is important because the company wants to delete the objects eventually,\neven if that isn't for a long time.\nOption B: S3 Object Lock with Retention Period (Governance Mode): While this option correctly uses S3\nObject Lock, setting a fixed retention period (even 100 years) does not align with the requirement for an\nindefinite period of immutability until the company decides to modify the objects. Governance mode can be\noverridden by users with specific permissions and may not provide a strong enough guarantee of immutability\nfor the company's compliance needs.\nOption C: S3 Bucket with CloudTrail and Backups: This approach is reactive rather than preventative. It relies\non detecting modifications and restoring from backups, which is inefficient, complex, and may lead to data\nloss in the time between the modification and the restoration. This option does not meet the primary\nrequirement of preventing data from being changed in the first place and lacks immutability features.\nIn summary:\nOption D is the only solution that fulfills all the requirements: preventing data from being changed indefinitely\nuntil the company decides to modify the objects, and allowing specific users to authorize the removal of\nimmutability.\nAuthoritative Links:\nAmazon S3 Object Lock: https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html\nS3 Versioning: https://docs.aws.amazon.com/AmazonS3/latest/userguide/versioning-overview.html\nIAM Permissions for S3 Object Lock: https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-\niam-id-based-policy-examples.html#s3-object-lock-permissions",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/versioning-overview.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-"
    ]
  },
  {
    "question": "CertyIQ\nA social media company allows users to upload images to its website. The website runs on Amazon EC2 instances.\nDuring upload requests, the website resizes the images to a standard size and stores the resized images in Amazon\nS3. Users are experiencing slow upload requests to the website.\nThe company needs to reduce coupling within the application and improve website performance. A solutions\narchitect must design the most operationally efficient process for image uploads.\nWhich combination of actions should the solutions architect take to meet these requirements? (Choose two.)",
    "options": {
      "A": "Configure the application to upload images to S3 Glacier.",
      "B": "Configure the web server to upload the original images to Amazon S3. This shifts the responsibility of",
      "C": "Configure the application to upload images directly from each user's browser to Amazon S3 through the use",
      "D": "Configure S3 Event Notifications to invoke an AWS Lambda function when an image is uploaded. Use the"
    },
    "answer": "B",
    "explanation": "The correct solution involves minimizing the load on the EC2 instances and decoupling the image resizing\nprocess from the web application.\nB. Configure the web server to upload the original images to Amazon S3. This shifts the responsibility of\nstoring the images from the EC2 instances to S3, freeing up the EC2 instances to handle web requests more\nefficiently. Direct uploads to S3 reduce the load on the web servers, as they no longer need to manage the\nstorage aspect. This is a key step in reducing coupling.\nD. Configure S3 Event Notifications to invoke an AWS Lambda function when an image is uploaded. Use the\nfunction to resize the image. This triggers a serverless Lambda function whenever a new image is uploaded\nto S3. The Lambda function then handles the image resizing asynchronously. This fully decouples the resizing\nprocess from the web application. S3 Event Notifications provide a highly scalable and reliable mechanism to\ntrigger the Lambda function, ensuring that images are resized in a timely manner without impacting the\nwebsite's performance. Lambda's on-demand nature only consumes resources during the resizing, making it\noperationally efficient.\nWhy other options are incorrect:\nA: S3 Glacier is designed for long-term archival storage and retrieval, not for actively used images requiring\nresizing.\nC: While using presigned URLs can offload image uploads from the web server, it doesn't address the image\nresizing requirement.\nE: EventBridge on a schedule would not be triggered by the uploads themselves, introducing latency and\ninefficiencies in image resizing. S3 event notifications provide instant triggers when images are uploaded.\nAuthoritative Links:\nAmazon S3: https://aws.amazon.com/s3/\nAWS Lambda: https://aws.amazon.com/lambda/\nS3 Event Notifications: https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-event-\nnotifications.html",
    "links": [
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/lambda/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-event-"
    ]
  },
  {
    "question": "CertyIQ\nA company recently migrated a message processing system to AWS. The system receives messages into an\nActiveMQ queue running on an Amazon EC2 instance. Messages are processed by a consumer application running\non Amazon EC2. The consumer application processes the messages and writes results to a MySQL database\nrunning on Amazon EC2. The company wants this application to be highly available with low operational\ncomplexity.\nWhich architecture offers the HIGHEST availability?",
    "options": {
      "A": "Add a second ActiveMQ server to another Availability Zone. Add an additional consumer EC2 instance in",
      "B": "Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an additional",
      "C": "Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an additional",
      "D": "Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an Auto Scaling"
    },
    "answer": "D",
    "explanation": "Option D provides the highest availability and lowest operational complexity due to several key AWS services.\n1. Amazon MQ with active/standby brokers (Multi-AZ): This eliminates the need to manage ActiveMQ\nservers on EC2 instances. Amazon MQ is a managed message broker service, and the active/standby\nconfiguration across two Availability Zones (AZs) ensures automatic failover in case of an AZ outage.\nThis is a significant improvement in availability compared to managing ActiveMQ on EC2, where you'd\nhave to manually handle failover and replication. https://aws.amazon.com/mq/\n2. Auto Scaling group for consumer EC2 instances across two AZs: This provides automatic scaling\nand high availability for the consumer application. Auto Scaling automatically adjusts the number of\nEC2 instances based on demand, ensuring that the application can handle traffic spikes. Distributing\ninstances across multiple AZs ensures that the application remains available even if one AZ fails.\nhttps://aws.amazon.com/autoscaling/\n3. Amazon RDS for MySQL with Multi-AZ enabled: This provides automatic failover and replication for\nthe MySQL database. RDS Multi-AZ creates a standby replica of the database in another AZ,\nautomatically failing over to the standby instance in case of a failure in the primary AZ. This\nsignificantly reduces downtime and simplifies database management.\nhttps://aws.amazon.com/rds/mysql/\nBy using these managed services and features, option D reduces operational overhead by eliminating the\nneed to manage the message broker, scaling consumer instances, or handle database replication and failover\nmanually.\nOption A is less desirable as maintaining ActiveMQ servers on EC2 increases operational complexity. Option B\nlacks auto-scaling for the consumer instances, resulting in potential availability issues if traffic increases\nsignificantly. Option C similarly lacks auto-scaling for the consumer instances. Consequently, option D\neffectively handles message brokering, scaling, and data redundancy via RDS Multi-AZ across AZs. This is the\nbest solution for high availability and simplified operations.",
    "links": [
      "https://aws.amazon.com/mq/",
      "https://aws.amazon.com/autoscaling/",
      "https://aws.amazon.com/rds/mysql/"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts a containerized web application on a fleet of on-premises servers that process incoming\nrequests. The number of requests is growing quickly. The on-premises servers cannot handle the increased\nnumber of requests. The company wants to move the application to AWS with minimum code changes and\nminimum development effort.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Use AWS Fargate on Amazon Elastic Container Service (Amazon ECS) to run the containerized web",
      "B": "Use two Amazon EC2 instances to host the containerized web application. Use an Application Load Balancer",
      "C": "Use AWS Lambda with a new code that uses one of the supported languages. Create multiple Lambda",
      "D": "Use a high performance computing (HPC) solution such as AWS ParallelCluster to establish an HPC cluster"
    },
    "answer": "A",
    "explanation": "The best solution is A because it directly addresses the requirements of minimal code changes, minimal\ndevelopment effort, and least operational overhead for migrating a containerized application to AWS.\nHere's why:\nAWS Fargate and ECS: Fargate allows you to run containers without managing the underlying EC2 instances.\nECS orchestrates the containers. This significantly reduces operational overhead because you don't need to\nprovision, manage, and scale EC2 instances yourself.\nContainerization Compatibility: The application is already containerized, making it a good fit for ECS and\nFargate. This eliminates the need for significant code changes or re-architecting the application. The existing\ncontainers can be deployed almost as-is.\nApplication Load Balancer (ALB): ALB efficiently distributes incoming traffic across the container instances\nmanaged by ECS/Fargate. It provides features like health checks, traffic routing based on content, and SSL\ntermination.\nService Auto Scaling: ECS Service Auto Scaling automatically adjusts the number of running container\ninstances based on the incoming request load. This ensures optimal performance and cost efficiency without\nmanual intervention.\nOption B (EC2 instances) increases operational overhead compared to Fargate, as you must manage the EC2\ninstances, including patching, scaling, and capacity planning.\nOption C (Lambda and API Gateway) requires significant code changes because you need to re-write the\napplication logic to fit the Lambda function model. This involves substantially more development effort. It\nmight also not be a direct port of the web application's existing logic.\nOption D (AWS ParallelCluster) is designed for High-Performance Computing (HPC) workloads that typically\ninvolve tightly coupled parallel jobs. While it can scale, it is overkill and unnecessarily complex for a web\napplication and introduces higher operational overhead than Fargate/ECS. It's suitable for scientific\nsimulations, not general web traffic.\nAuthoritative Links:\nAWS Fargate: https://aws.amazon.com/fargate/\nAmazon ECS: https://aws.amazon.com/ecs/\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\nService Auto Scaling: https://aws.amazon.com/autoscaling/",
    "links": [
      "https://aws.amazon.com/fargate/",
      "https://aws.amazon.com/ecs/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
      "https://aws.amazon.com/autoscaling/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses 50 TB of data for reporting. The company wants to move this data from on premises to AWS. A\ncustom application in the companys data center runs a weekly data transformation job. The company plans to\npause the application until the data transfer is complete and needs to begin the transfer process as soon as\npossible.\nThe data center does not have any available network bandwidth for additional workloads. A solutions architect\nmust transfer the data and must configure the transformation job to continue to run in the AWS Cloud.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Use AWS DataSync to move the data. Create a custom transformation job by using AWS Glue.",
      "B": "Order an AWS Snowcone device to move the data. Deploy the transformation application to the device.",
      "C": "Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. Create a custom",
      "D": "Order an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute. Copy the data"
    },
    "answer": "C",
    "explanation": "Here's a breakdown of why option C is the best solution and why the others are less suitable, focusing on\nminimizing operational overhead and meeting the requirements:\nJustification for Option C (Correct):\nOption C leverages AWS Snowball Edge Storage Optimized for data transfer and AWS Glue for\ntransformation, which offers the least operational overhead:\n1. Snowball Edge for Data Transfer: Snowball Edge is ideal for transferring large datasets (50 TB)\nwhen network bandwidth is limited. This aligns perfectly with the scenario's constraint of no available\nnetwork bandwidth. It avoids network congestion and faster compared to DataSync or direct network\ntransfer.\n2. Storage Optimized Choice: The \"Storage Optimized\" Snowball Edge is selected specifically for its\nlarge storage capacity, catering to the 50 TB data volume.\n3. AWS Glue for Transformation: AWS Glue is a fully managed ETL (Extract, Transform, Load) service.\nBy creating a custom transformation job in Glue, the company can replicate its existing\ntransformation logic in the cloud without the overhead of managing EC2 instances. AWS Glue\nhandles the infrastructure, scaling, and monitoring automatically.\n4. Minimizing Operational Overhead: This solution minimizes operational overhead because Snowball\nEdge simplifies the data transfer process, and AWS Glue handles the transformation job's\ninfrastructure management. The company doesn't need to manage EC2 instances for either the data\ntransfer or the transformation process.\nWhy other options are less optimal:\nOption A (AWS DataSync + Glue): DataSync is a network-based data transfer service. The question states the\ndata center has no available network bandwidth for additional workloads, making DataSync an unsuitable\nchoice.\nOption B (Snowcone + Application on Device): AWS Snowcone is designed for edge computing in rugged\nenvironments. It has a smaller storage capacity than Snowball Edge, and deploying the transformation\napplication directly on the device would add complexity and operational overhead because of limited\nprocessing capability.\nOption D (Snowball Edge with EC2 + New EC2 Instance): While this solution could technically work, running\nthe transformation application on a separate EC2 instance in the AWS Cloud after the data is transferred from\nthe Snowball Edge introduces more operational overhead. You would need to manage, patch, and scale this\nEC2 instance. Using AWS Glue is a more managed and efficient transformation approach. Plus, using EC2 on\nsnowball for small transformation jobs add complexity for management purpose of the infrastructure.\nSupporting Concepts & Links:\nAWS Snowball Edge: https://aws.amazon.com/snowball/\nAWS Glue: https://aws.amazon.com/glue/\nData Transfer Options: https://aws.amazon.com/products/storage/data-transfer/\nIn conclusion, Option C provides the most efficient and cost-effective approach by leveraging Snowball Edge\nfor data transfer and AWS Glue for the data transformation job, minimizing operational overhead and meeting\nall stated requirements.",
    "links": [
      "https://aws.amazon.com/snowball/",
      "https://aws.amazon.com/glue/",
      "https://aws.amazon.com/products/storage/data-transfer/"
    ]
  },
  {
    "question": "CertyIQ\nA company has created an image analysis application in which users can upload photos and add photo frames to\ntheir images. The users upload images and metadata to indicate which photo frames they want to add to their\nimages. The application uses a single Amazon EC2 instance and Amazon DynamoDB to store the metadata.\nThe application is becoming more popular, and the number of users is increasing. The company expects the\nnumber of concurrent users to vary significantly depending on the time of day and day of week. The company must\nensure that the application can scale to meet the needs of the growing user base.\nWhich solution meats these requirements?",
    "options": {
      "A": "Use AWS Lambda to process the photos. Store the photos and metadata in DynamoDB.",
      "B": "Use Amazon Kinesis Data Firehose to process the photos and to store the photos and metadata.",
      "C": "Use AWS Lambda to process the photos. Store the photos in Amazon S3. Retain DynamoDB to store the",
      "D": "Increase the number of EC2 instances to three. Use Provisioned IOPS SSD (io2) Amazon Elastic Block Store"
    },
    "answer": "C",
    "explanation": "The correct answer is C: Use AWS Lambda to process the photos. Store the photos in Amazon S3. Retain\nDynamoDB to store the metadata.\nHere's a detailed justification:\nThe primary requirement is to scale the image analysis application to handle a growing user base with varying\nconcurrency. The initial architecture using a single EC2 instance and DynamoDB becomes a bottleneck as the\nnumber of users increases.\nOption A is suboptimal. While using Lambda for processing provides scalability, storing photos in DynamoDB\nis generally not recommended. DynamoDB is optimized for storing metadata and small documents, not large\nbinary files like images, which can become expensive and inefficient.\nOption B is inappropriate. Kinesis Data Firehose is designed for streaming data ingestion and loading into data\nstores/analytics tools. It's not intended for general image processing tasks like adding photo frames. Storing\nimages in Firehose is not a viable solution.\nOption D does not fully address the scalability issue. Simply increasing the number of EC2 instances to three\nstill introduces scalability limitations and requires managing the EC2 instances. It also relies on EBS for\nstoring photos which isn't ideal in terms of cost and scalability for image storage. Provisioned IOPS EBS\nvolumes also adds unnecessary complexity and cost if high IOPS are not consistently needed.\nOption C provides the best solution because it leverages the strengths of different AWS services:\nAWS Lambda: Lambda allows serverless execution of the image processing logic (adding photo frames). It\nautomatically scales up or down based on demand, ensuring the application can handle varying levels of\nconcurrency without manual intervention. This achieves the scalability requirement.\nAWS Lambda Documentation: Provides overview and features about Lambda service.\nAmazon S3: S3 is object storage for storing large amounts of data such as images. S3 offers durability,\nscalability, and cost-effectiveness for storing the photos.\nAmazon S3 Documentation: Provides overview and features about S3 service.\nDynamoDB: DynamoDB is a NoSQL database is best for storing metadata, it efficiently handles the growing\namount of user metadata (photo frame preferences).\nAmazon DynamoDB Documentation: Provides overview and features about DynamoDB service.\nBy separating the image processing task to Lambda, storing photos in S3, and retaining metadata in\nDynamoDB, the application gains scalability, cost-effectiveness, and improved performance. The solution\naligns perfectly with the demands of a growing user base and fluctuating concurrency, making it the ideal\nchoice.",
    "links": []
  },
  {
    "question": "CertyIQ\nA medical records company is hosting an application on Amazon EC2 instances. The application processes\ncustomer data files that are stored on Amazon S3. The EC2 instances are hosted in public subnets. The EC2\ninstances access Amazon S3 over the internet, but they do not require any other network access.\nA new requirement mandates that the network traffic for file transfers take a private route and not be sent over\nthe internet.\nWhich change to the network architecture should a solutions architect recommend to meet this requirement?",
    "options": {
      "A": "Create a NAT gateway. Configure the route table for the public subnets to send traffic to Amazon S3 through",
      "B": "Configure the security group for the EC2 instances to restrict outbound traffic so that only traffic to the S3",
      "C": "Move the EC2 instances to private subnets. Create a VPC endpoint for Amazon S3, and link the endpoint to",
      "D": "Remove the internet gateway from the VPC. Set up an AWS Direct Connect connection, and route traffic to"
    },
    "answer": "C",
    "explanation": "Option C is the most suitable solution because it allows for private connectivity to S3 without traversing the\ninternet, aligning perfectly with the requirement. Moving the EC2 instances to private subnets enhances\nsecurity by isolating them from direct internet exposure. Creating a VPC endpoint for S3 establishes a private\nconnection between the VPC and S3, ensuring all traffic remains within the AWS network. Associating the\nVPC endpoint with the private subnet's route table directs S3-bound traffic through the endpoint. This setup\nachieves the desired private route for file transfers, fulfilling the mandate of avoiding internet transmission.\nVPC endpoints are cost-effective and highly scalable.\nOption A is incorrect because a NAT gateway is primarily used for allowing instances in private subnets to\ninitiate outbound internet connections, not for directing traffic privately to S3.Option B is incorrect because\nsecurity groups control traffic based on IP addresses and ports but don't provide a private route for S3\naccess; the traffic would still traverse the internet.Option D is incorrect because AWS Direct Connect\nestablishes a dedicated network connection between on-premises infrastructure and AWS, which is\nunnecessary and overly complex for the described requirement of internal S3 access. It also involves\nsignificant costs and setup overhead compared to a VPC endpoint.\nTherefore, option C provides the most secure, efficient, and cost-effective solution for establishing a private\nconnection to S3 for EC2 instances.\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-\ns3.htmlhttps://aws.amazon.com/vpc/pricing/",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-",
      "https://aws.amazon.com/vpc/pricing/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses a popular content management system (CMS) for its corporate website. However, the required\npatching and maintenance are burdensome. The company is redesigning its website and wants anew solution. The\nwebsite will be updated four times a year and does not need to have any dynamic content available. The solution\nmust provide high scalability and enhanced security.\nWhich combination of changes will meet these requirements with the LEAST operational overhead? (Choose two.)",
    "options": {
      "A": "Configure Amazon CloudFront in front of the website to use HTTPS functionality.",
      "B": "Deploy an AWS WAF web ACL in front of the website to provide HTTPS functionality: AWS WAF is a web",
      "C": "Create and deploy an AWS Lambda function to manage and serve the website content: Lambda is ideal",
      "D": "Create the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static"
    },
    "answer": "A",
    "explanation": "The best solution, with the least operational overhead, is a combination of using Amazon S3 for static website\nhosting and Amazon CloudFront for content delivery with HTTPS.\nD. Create the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static\nwebsite hosting enabled.\nStatic Website Hosting: S3 provides static website hosting. This eliminates the need for servers (EC2\ninstances) and operating system maintenance. S3 handles the underlying infrastructure management,\nensuring high availability and scalability automatically.\nLow Operational Overhead: S3 requires minimal operational effort. Uploading static files to an S3 bucket is\nstraightforward, and S3 handles scaling and availability.\nCost-Effective: S3 storage is relatively inexpensive, and you only pay for the storage you use and the data\nyou transfer.\nA. Configure Amazon CloudFront in front of the website to use HTTPS functionality.\nContent Delivery Network (CDN): CloudFront is a CDN that caches website content at edge locations\nglobally. This provides faster content delivery to users, regardless of their location.\nHTTPS Functionality: CloudFront allows you to use HTTPS for secure communication with users, improving\nwebsite security. You can use AWS Certificate Manager (ACM) to easily provision and manage SSL/TLS\ncertificates for CloudFront.\nEnhanced Security: CloudFront protects against DDoS attacks and other threats at the edge.\nIntegration with S3: CloudFront integrates seamlessly with S3, making it easy to distribute content stored in\nS3.\nWhy other options are not ideal:\nB. Deploy an AWS WAF web ACL in front of the website to provide HTTPS functionality: AWS WAF is a web\napplication firewall that helps protect your web applications from common web exploits. While WAF is\nbeneficial for security, it doesn't directly provide HTTPS functionality. HTTPS is usually configured at the CDN\nlevel, so A is the more applicable choice.\nC. Create and deploy an AWS Lambda function to manage and serve the website content: Lambda is ideal\nfor dynamic content and event-driven computing. This is a more complex solution and would have higher\noperational overhead than using S3 for static website hosting, which aligns better with the prompt\nrequirement to keep it simple.\nE. Create the new website. Deploy the website by using an Auto Scaling group of Amazon EC2 instances\nbehind an Application Load Balancer: EC2 instances require operating system maintenance, patching, and\nscaling considerations, making them a higher-overhead solution than S3 for a static website.\nAuthoritative Links:\nAmazon S3 Static Website Hosting:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html\nAmazon CloudFront: https://aws.amazon.com/cloudfront/",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html",
      "https://aws.amazon.com/cloudfront/"
    ]
  },
  {
    "question": "CertyIQ\nA company stores its application logs in an Amazon CloudWatch Logs log group. A new policy requires the\ncompany to store all application logs in Amazon OpenSearch Service (Amazon Elasticsearch Service) in near-real\ntime.\nWhich solution will meet this requirement with the LEAST operational overhead?",
    "options": {
      "A": "Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service (Amazon",
      "B": "Create an AWS Lambda function. Use the log group to invoke the function to write the logs to Amazon",
      "C": "Create an Amazon Kinesis Data Firehose delivery stream. Configure the log group as the delivery streams",
      "D": "Install and configure Amazon Kinesis Agent on each application server to deliver the logs to Amazon Kinesis"
    },
    "answer": "A",
    "explanation": "The most efficient way to stream CloudWatch Logs to Amazon OpenSearch Service (Amazon Elasticsearch\nService) with the least operational overhead is to use a CloudWatch Logs subscription.\nOption A is correct because CloudWatch Logs subscriptions offer a direct and managed integration with\nAmazon OpenSearch Service. A subscription allows you to define a filter pattern to select specific log events\nand automatically stream them to your OpenSearch domain. This eliminates the need for custom coding or\ninfrastructure management. The CloudWatch Logs service handles the streaming and scaling, which\nminimizes operational overhead.\nOption B requires creating and managing a Lambda function, which adds complexity and operational overhead\nfor maintenance, monitoring, and potential scaling issues.\nOption C, using Kinesis Data Firehose, is also a viable option, but CloudWatch Logs subscriptions provide a\nmore direct path for sending logs to Amazon OpenSearch Service. Kinesis Data Firehose is generally better\nsuited for broader data ingestion needs, but introduces an extra layer of configuration and management\ncompared to direct subscriptions.\nOption D involves installing and configuring Kinesis Agent on each application server and managing Kinesis\nData Streams, leading to the highest operational overhead. This method introduces complexity with agent\nmanagement, stream configuration, and potential data loss if agents are not properly configured. This option\nis unnecessary when the other options provide simpler, more managed solutions.\nTherefore, Option A provides the simplest and most efficient solution due to the direct integration between\nCloudWatch Logs and Amazon OpenSearch Service using subscriptions. It avoids the operational overhead of\nmanaging custom functions, agents, or additional streaming services.\nReference links:\nCloudWatch Logs Subscriptions\nSending CloudWatch Logs to Amazon OpenSearch Service",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is building a web-based application running on Amazon EC2 instances in multiple Availability Zones.\nThe web application will provide access to a repository of text documents totaling about 900 TB in size. The\ncompany anticipates that the web application will experience periods of high demand. A solutions architect must\nensure that the storage component for the text documents can scale to meet the demand of the application at all\ntimes. The company is concerned about the overall cost of the solution.\nWhich storage solution meets these requirements MOST cost-effectively?",
    "options": {
      "A": "Amazon Elastic Block Store (Amazon EBS): EBS volumes are block storage and are attached to individual",
      "B": "Amazon Elastic File System (Amazon EFS): EFS provides shared file storage for EC2 instances. While EFS",
      "C": "Amazon OpenSearch Service (Amazon Elasticsearch Service): OpenSearch is a search and analytics",
      "D": "Amazon S3. Here's why:"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Amazon S3. Here's why:\nScalability and Durability: Amazon S3 (Simple Storage Service) is designed for virtually unlimited scalability\nand high durability. It can easily handle petabytes of data and is built to withstand the loss of data through\nredundant storage across multiple devices and facilities. This aligns directly with the requirement for scaling\nto accommodate 900 TB and meeting potentially high demand. https://aws.amazon.com/s3/\nCost-Effectiveness: S3 offers various storage classes (Standard, Intelligent-Tiering, Standard-IA, One Zone-\nIA, Glacier, Glacier Deep Archive) that allow optimization for different access patterns and cost\nconsiderations. For data accessed frequently, S3 Standard might be appropriate, whereas infrequently\naccessed data can be stored in cheaper tiers like S3 Standard-IA or Glacier, reducing overall storage costs.\nhttps://aws.amazon.com/s3/storage-classes/\nWeb Application Integration: S3 integrates seamlessly with web applications. EC2 instances can easily\nretrieve and serve text documents directly from S3 via HTTP/HTTPS. The S3 API simplifies the process of\naccessing and managing objects.\nOther options and why they are less ideal:\nA. Amazon Elastic Block Store (Amazon EBS): EBS volumes are block storage and are attached to individual\nEC2 instances. Storing 900 TB on EBS would be prohibitively expensive and complex to manage, requiring a\nlarge number of attached volumes across instances, and scaling would be less efficient. EBS is better suited\nfor operating systems, application software, and frequently accessed data closely tied to a specific instance.\nB. Amazon Elastic File System (Amazon EFS): EFS provides shared file storage for EC2 instances. While EFS\ncan scale, storing 900 TB would still be relatively more expensive than S3, and the performance\ncharacteristics, while suitable for some shared file workloads, are typically not optimized for serving static\nweb content like text documents as effectively as S3. EFS's cost structure makes it less favorable for large\ndatasets than S3, especially when considering infrequent access scenarios.\nhttps://aws.amazon.com/efs/pricing/\nC. Amazon OpenSearch Service (Amazon Elasticsearch Service): OpenSearch is a search and analytics\nengine. While it could store and index the text documents, it's not optimized for simply storing and serving\nlarge files. Moreover, indexing 900 TB of data in OpenSearch would be much more costly and complex than\nstoring the files in S3. OpenSearch is for full-text search and analysis, not general-purpose large-scale\nstorage.\nIn summary, S3 provides the best combination of scalability, cost-effectiveness, and integration capabilities\nfor storing and serving a large repository of text documents for a web application experiencing varying\ndemand.",
    "links": [
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/s3/storage-classes/",
      "https://aws.amazon.com/efs/pricing/"
    ]
  },
  {
    "question": "CertyIQ\nA global company is using Amazon API Gateway to design REST APIs for its loyalty club users in the us-east-1\nRegion and the ap-southeast-2 Region. A solutions architect must design a solution to protect these API Gateway\nmanaged REST APIs across multiple accounts from SQL injection and cross-site scripting attacks.\nWhich solution will meet these requirements with the LEAST amount of administrative effort?",
    "options": {
      "A": "Set up AWS WAF in both Regions. Associate Regional web ACLs with an API stage.",
      "B": "Here's a detailed justification:",
      "C": "Set up AWS Shield in bath Regions. Associate Regional web ACLs with an API stage.",
      "D": "Set up AWS Shield in one of the Regions. Associate Regional web ACLs with an API stage."
    },
    "answer": "B",
    "explanation": "The correct answer is B. Here's a detailed justification:\nThe problem requires a solution to protect API Gateway REST APIs across multiple AWS accounts and regions\n(us-east-1 and ap-southeast-2) from common web exploits like SQL injection and cross-site scripting (XSS)\nwith minimal administrative overhead.\nOption B, setting up AWS Firewall Manager and centrally configuring AWS WAF rules, is the most efficient\napproach. AWS Firewall Manager is designed to centrally manage and deploy AWS WAF rules across multiple\naccounts and regions. This provides a single pane of glass for managing security policies. With Firewall\nManager, you can define a WAF policy once and apply it consistently across all your APIs, significantly\nreducing administrative effort. You create an AWS WAF rule group within Firewall Manager and then deploy\nthat rule group across the accounts and regions in question. The WAF rules can be tailored to specifically\nblock SQL injection and XSS attacks using managed rule groups or custom rules based on regular\nexpressions or other criteria.\nOption A, setting up AWS WAF in both regions and associating Regional web ACLs with an API stage, while\nfunctionally correct, requires more manual configuration. You would need to create and maintain the same\nWAF rules in each region separately. This increases administrative overhead and the potential for\ninconsistencies. It doesn't offer the central management that Firewall Manager does.\nOptions C and D involve AWS Shield. AWS Shield provides protection against Distributed Denial of Service\n(DDoS) attacks, which is not the primary concern in this scenario. While Shield Advanced offers some WAF\nintegration, it's not the core focus for SQL injection and XSS protection. Furthermore, AWS Shield's pricing\nmodel is different and potentially more expensive than Firewall Manager for this use case. Shield also does\nnot provide the centralized management across multiple accounts that Firewall Manager provides.\nTherefore, AWS Firewall Manager with centrally managed AWS WAF rules offers the best balance of security\neffectiveness and administrative efficiency for protecting API Gateway REST APIs across multiple accounts\nand regions from SQL injection and XSS attacks.\nAuthoritative links:\nAWS Firewall Manager: https://aws.amazon.com/firewall-manager/\nAWS WAF: https://aws.amazon.com/waf/",
    "links": [
      "https://aws.amazon.com/firewall-manager/",
      "https://aws.amazon.com/waf/"
    ]
  },
  {
    "question": "CertyIQ\nA company has implemented a self-managed DNS solution on three Amazon EC2 instances behind a Network Load\nBalancer (NLB) in the us-west-2 Region. Most of the company's users are located in the United States and Europe.\nThe company wants to improve the performance and availability of the solution. The company launches and\nconfigures three EC2 instances in the eu-west-1 Region and adds the EC2 instances as targets for a new NL",
    "options": {
      "B": "Create a standard accelerator in AWS Global Accelerator. Create endpoint groups in",
      "A": "Route 53 Geolocation with CloudFront: While Route 53 Geolocation routing directs traffic based on the",
      "C": "Elastic IPs and Route 53 Geolocation: Attaching Elastic IPs to individual EC2 instances and using Route 53",
      "D": "Application Load Balancers (ALBs) with Route 53 Latency Routing and CloudFront: While ALBs offer"
    },
    "answer": "B",
    "explanation": "The best solution is B. Create a standard accelerator in AWS Global Accelerator. Create endpoint groups in\nus-west-2 and eu-west-1. Add the two NLBs as endpoints for the endpoint groups.\nHere's why:\nGlobal Performance & Availability: The primary requirement is to improve performance and availability for\nusers in the US and Europe. AWS Global Accelerator is specifically designed for this. It leverages the AWS\nglobal network to direct user traffic to the closest healthy endpoint, improving latency and reliability.\nNLB Compatibility: Global Accelerator is directly compatible with Network Load Balancers (NLBs). It can\ntreat the NLBs in us-west-2 and eu-west-1 as endpoints.\nSimple Configuration: The proposed solution involves creating a Global Accelerator, defining endpoint\ngroups for each region, and associating the respective NLBs with those groups. This is a relatively\nstraightforward configuration.\nFault Tolerance: Global Accelerator continually monitors the health of endpoints. If an NLB or the EC2\ninstances behind it become unhealthy, Global Accelerator will automatically redirect traffic to the healthy\nendpoint in the other region, ensuring high availability.\nWhy other options are not optimal:\nA. Route 53 Geolocation with CloudFront: While Route 53 Geolocation routing directs traffic based on the\nuser's location, it doesn't inherently provide the performance benefits of Global Accelerator's anycast\naddressing and AWS global network optimization. CloudFront caches static content but won't significantly\nimprove the performance of dynamic DNS queries served by EC2 instances. The user will still reach the\nregional NLBs that they have in place, which is only a regional solution.\nC. Elastic IPs and Route 53 Geolocation: Attaching Elastic IPs to individual EC2 instances and using Route 53\ngeolocation would work but presents several challenges. It involves managing individual instances directly,\nincreasing operational complexity. It would be difficult to implement health checks and ensure automated\nfailover. It also won't benefit from the anycast network that Global Accelerator provides.\nD. Application Load Balancers (ALBs) with Route 53 Latency Routing and CloudFront: While ALBs offer\nmore features than NLBs, the core problem remains the lack of global traffic management and network\noptimization. Route 53 Latency-based routing is useful, but it isn't as performant as Global Accelerator as it\nstill has to traverse over the public internet to the designated region. CloudFront will only help for cached\ncontent. In addition, the company is already using NLBs, switching to ALBs would require more significant\narchitecture changes.\nIn summary: Global Accelerator with NLBs in different regions provides the best balance of performance,\navailability, and ease of implementation for routing traffic to DNS servers for users in the US and Europe.\nAuthoritative Links:\nAWS Global Accelerator: https://aws.amazon.com/global-accelerator/\nAWS Route 53: https://aws.amazon.com/route53/\nAWS Network Load Balancer: https://aws.amazon.com/elasticloadbalancing/network-load-balancer/",
    "links": [
      "https://aws.amazon.com/global-accelerator/",
      "https://aws.amazon.com/route53/",
      "https://aws.amazon.com/elasticloadbalancing/network-load-balancer/"
    ]
  },
  {
    "question": "CertyIQ\nA company is running an online transaction processing (OLTP) workload on AWS. This workload uses an\nunencrypted Amazon RDS DB instance in a Multi-AZ deployment. Daily database snapshots are taken from this\ninstance.\nWhat should a solutions architect do to ensure the database and snapshots are always encrypted moving forward?",
    "options": {
      "A": "Here's why:",
      "B": "Create a new encrypted Amazon Elastic Block Store (Amazon EBS) volume and copy the snapshots to it.",
      "C": "Copy the snapshots and enable encryption using AWS Key Management Service (AWS KMS) Restore",
      "D": "Copy the snapshots to an Amazon S3 bucket that is encrypted using server-side encryption with AWS Key"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's why:\nEncryption at Rest for RDS: Amazon RDS supports encryption at rest, but it must be enabled during instance\ncreation or by restoring from an encrypted snapshot. You cannot directly encrypt an existing unencrypted\nRDS instance in place.\nSnapshot Encryption: The process involves creating an encrypted copy of the most recent DB snapshot. This\nencrypted snapshot serves as the source for creating a new, encrypted RDS instance.\nRestoring from Encrypted Snapshot: By restoring the encrypted snapshot, a new RDS instance is launched\nwith encryption enabled.\nReplacing the Existing Instance: To fully transition to an encrypted setup, you need to replace the original,\nunencrypted instance with the newly created encrypted one. This might involve updating application\nconnection strings to point to the new instance and decommissioning the old one.\nWhy other options are incorrect:\nB: Encrypting an EBS volume and copying snapshots there doesn't encrypt the RDS instance itself. RDS\nsnapshots are stored separately and managed by AWS. It doesn't mention how you will create a new\nencrypted RDS instance.\nC: While you can copy snapshots and encrypt them, restoring an encrypted snapshot to an existing\nunencrypted DB instance isn't possible. RDS requires you to create a new encrypted instance from an\nencrypted snapshot.\nD: Copying snapshots to an encrypted S3 bucket doesn't encrypt the RDS instance or the snapshots in a way\nthat RDS can use directly for creating an encrypted instance. S3 encryption is separate from RDS encryption\nat rest.\nIn summary, Option A provides the correct and complete process to move an unencrypted RDS instance to an\nencrypted setup: encrypt a copy of the snapshot, restore from the encrypted snapshot to create a new\nencrypted instance, and replace the existing instance with the new one.\nSupporting Documentation:\nAmazon RDS Encryption\nEncrypting Amazon RDS Resources",
    "links": []
  },
  {
    "question": "CertyIQ\nA company wants to build a scalable key management infrastructure to support developers who need to encrypt\ndata in their applications.\nWhat should a solutions architect do to reduce the operational burden?",
    "options": {
      "A": "Use multi-factor authentication (MFA) to protect the encryption keys.",
      "B": "Use AWS Key Management Service (AWS KMS) to protect the encryption keys.",
      "C": "Use AWS Certificate Manager (ACM) to create, store, and assign the encryption keys.",
      "D": "Use an IAM policy to limit the scope of users who have access permissions to protect the encryption keys."
    },
    "answer": "B",
    "explanation": "The correct answer is B: Use AWS Key Management Service (AWS KMS) to protect the encryption keys.\nHere's a detailed justification:\nAWS KMS is a managed service specifically designed to simplify the creation, storage, and control of\nencryption keys used to encrypt data. Using KMS reduces the operational burden by offloading key\nmanagement responsibilities from the company to AWS. KMS handles tasks like key generation, rotation,\nstorage, and deletion in a secure and compliant manner. This frees developers from having to build and\nmaintain their own key management infrastructure, which can be complex and time-consuming.\nOption A, using MFA, enhances security by requiring multiple authentication factors, but it doesn't address\nthe core operational burden of managing keys. MFA protects against unauthorized access, but it doesn't\nsimplify key generation, storage, or rotation.\nOption C, using AWS Certificate Manager (ACM), is primarily for managing SSL/TLS certificates used for\nsecuring network traffic. ACM is not designed for general-purpose key management for application data\nencryption. While ACM can create and store private keys for certificates, it's not the appropriate tool for\nmanaging keys used within application code to encrypt arbitrary data.\nOption D, using an IAM policy to limit access, is a crucial security practice. However, similar to MFA, it doesn't\nalleviate the burden of creating, storing, rotating, and managing the encryption keys themselves. IAM helps\ncontrol who can use the keys, but it doesn't manage the lifecycle of the keys.\nTherefore, KMS offers the most direct solution to reducing the operational burden associated with key\nmanagement. It provides a centralized, managed service for handling encryption keys, allowing developers to\nfocus on their application code rather than the complexities of key management. KMS integrates well with\nother AWS services and supports various encryption algorithms.\nFor further research, you can refer to the following AWS documentation:\nAWS Key Management Service (KMS): https://aws.amazon.com/kms/\nAWS Certificate Manager (ACM): https://aws.amazon.com/certificate-manager/\nIAM Policies: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html",
    "links": [
      "https://aws.amazon.com/kms/",
      "https://aws.amazon.com/certificate-manager/",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has a dynamic web application hosted on two Amazon EC2 instances. The company has its own SSL\ncertificate, which is on each instance to perform SSL termination.\nThere has been an increase in traffic recently, and the operations team determined that SSL encryption and\ndecryption is causing the compute capacity of the web servers to reach their maximum limit.\nWhat should a solutions architect do to increase the application's performance?",
    "options": {
      "A": "Create a new SSL certificate using AWS Certificate Manager (ACM). Install the ACM certificate on each",
      "B": "It also adds complexity to the architecture.",
      "C": "Create another EC2 instance as a proxy server. Migrate the SSL certificate to the new instance and configure",
      "D": "Import the SSL certificate into AWS Certificate Manager (ACM). Create an Application Load Balancer with an"
    },
    "answer": "D",
    "explanation": "The correct answer is D because it addresses the performance bottleneck caused by SSL\nencryption/decryption on the EC2 instances by offloading this task to a dedicated service: the Application\nLoad Balancer (ALB).\nHere's a detailed justification:\nSSL Offloading: SSL encryption and decryption are CPU-intensive tasks. By moving this responsibility to the\nALB, the EC2 instances are freed from this burden, allowing them to focus on serving application logic. This\nsignificantly improves their performance and reduces CPU utilization.\nApplication Load Balancer (ALB): An ALB is designed for layer 7 (application layer) load balancing. It can\nhandle HTTPS traffic, performing SSL termination and then forwarding the decrypted traffic to the EC2\ninstances. This ensures secure communication while optimizing resource utilization.\nAWS Certificate Manager (ACM): ACM simplifies the process of obtaining, managing, and deploying SSL/TLS\ncertificates for use with AWS services. Importing the existing SSL certificate into ACM allows the ALB to\neasily access and use it for SSL termination.\nScalability and High Availability: ALBs are highly scalable and provide built-in high availability. They can\nautomatically distribute traffic across multiple EC2 instances, ensuring that the application remains available\neven if one instance fails.\nCost-Effectiveness: While there is a cost associated with using an ALB, the performance gains and reduced\nresource consumption on the EC2 instances can lead to overall cost savings.\nWhy other options are incorrect:\nA: Creating a new SSL certificate using ACM and installing it on each instance does not solve the underlying\nproblem of SSL processing burdening the EC2 instances. It merely replaces the existing certificate.\nB: Storing the SSL certificate in S3 and referencing it from the EC2 instances would not offload the SSL\ndecryption processing. The EC2 instances would still need to perform the decryption themselves. This adds\ncomplexity without solving the core issue.\nC: Creating a proxy server (another EC2 instance) shifts the SSL decryption burden to the proxy. While it\nmight alleviate some load on the original web servers, it's not as scalable or efficient as using a managed\nservice like ALB. It also adds complexity to the architecture.\nAuthoritative Links:\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\nAWS Certificate Manager: https://aws.amazon.com/certificate-manager/\nSSL Offloading: https://aws.amazon.com/blogs/security/how-to-deploy-https-ssl-tls-with-aws-certificate-\nmanager-and-elastic-load-balancer/",
    "links": [
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
      "https://aws.amazon.com/certificate-manager/",
      "https://aws.amazon.com/blogs/security/how-to-deploy-https-ssl-tls-with-aws-certificate-"
    ]
  },
  {
    "question": "CertyIQ\nA company has a highly dynamic batch processing job that uses many Amazon EC2 instances to complete it. The\njob is stateless in nature, can be started and stopped at any given time with no negative impact, and typically takes\nupwards of 60 minutes total to complete. The company has asked a solutions architect to design a scalable and\ncost-effective solution that meets the requirements of the job.\nWhat should the solutions architect recommend?",
    "options": {
      "A": "Implement EC2 Spot Instances.",
      "B": "Purchase EC2 Reserved Instances.",
      "C": "Implement EC2 On-Demand Instances.",
      "D": "Implement the processing on AWS Lambda."
    },
    "answer": "A",
    "explanation": "The correct answer is A, Implement EC2 Spot Instances. Here's why:\nSpot Instances are unused EC2 capacity available in the AWS cloud at steep discounts compared to On-\nDemand prices. They are ideal for fault-tolerant, stateless, and flexible workloads, perfectly fitting the\ncompany's batch processing job requirements. Because the job is stateless and can be interrupted without\nnegative impact, the possibility of Spot Instance interruptions is mitigated. This allows the company to\nleverage significant cost savings, often up to 90%, compared to On-Demand Instances.\nReserved Instances, while providing cost savings, require a commitment to instance usage for a specific\nperiod (1 or 3 years). Given the dynamic nature of the batch processing job, purchasing Reserved Instances\nmay result in wasted capacity and reduced cost-effectiveness if the job doesn't consistently require that\ncapacity.\nOn-Demand Instances offer flexibility but are the most expensive option. For a job that can tolerate\ninterruptions and aims for cost optimization, On-Demand Instances are not the best choice.\nAWS Lambda is not suitable because it has execution time limits. Typical Lambda execution is capped at 15\nminutes. The batch processing job takes upwards of 60 minutes to complete.\nTherefore, Spot Instances provide the most cost-effective and scalable solution for this specific scenario.\nThey allow the company to leverage the flexibility of EC2 while minimizing costs, given the job's fault\ntolerance and flexibility.\nFurther Research:\nAWS EC2 Spot Instances: https://aws.amazon.com/ec2/spot/\nAWS EC2 Pricing: https://aws.amazon.com/ec2/pricing/",
    "links": [
      "https://aws.amazon.com/ec2/spot/",
      "https://aws.amazon.com/ec2/pricing/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs its two-tier ecommerce website on AWS. The web tier consists of a load balancer that sends\ntraffic to Amazon EC2 instances. The database tier uses an Amazon RDS DB instance. The EC2 instances and the\nRDS DB instance should not be exposed to the public internet. The EC2 instances require internet access to\ncomplete payment processing of orders through a third-party web service. The application must be highly\navailable.\nWhich combination of configuration options will meet these requirements? (Choose two.)",
    "options": {
      "A": "Use an Auto Scaling group to launch the EC2 instances in private subnets. Deploy an RDS Multi-AZ DB",
      "B": "Configure a VPC with two private subnets and two NAT gateways across two Availability Zones. Deploy an",
      "C": "Use an Auto Scaling group to launch the EC2 instances in public subnets across two Availability Zones.",
      "D": "Configure a VPC with two public subnets, two private subnets, and two NAT gateways across two Availability"
    },
    "answer": "A",
    "explanation": "Let's break down why options A and E are the correct choices for achieving high availability, secure\ncommunication with a third-party service, and keeping the EC2 instances and RDS DB instance private.\nOption A: Use an Auto Scaling group to launch the EC2 instances in private subnets. Deploy an RDS Multi-\nAZ DB instance in private subnets.\nThis addresses a core requirement: keeping both the EC2 instances and the RDS database out of the public\ninternet. Private subnets do not have direct internet access. Using an Auto Scaling group across multiple\nAvailability Zones ensures high availability of the web tier, which is essential for the ecommerce website. A\nMulti-AZ RDS DB instance does the same for the database tier, providing failover in case of an outage.\nOption E: Configure a VPC with two public subnets, two private subnets, and two NAT gateways across two\nAvailability Zones. Deploy an Application Load Balancer in the public subnets.\nThis configuration provides the necessary network infrastructure for the application. The Application Load\nBalancer (ALB) sits in public subnets, allowing it to receive traffic from the internet and distribute it to the EC2\ninstances in the private subnets. The two private subnets, each in a different Availability Zone, provide a\nhighly available environment. NAT Gateways, each in a different Availability Zone, allow the EC2 instances in\nthe private subnets to initiate outbound traffic to the third-party payment processor while remaining\ninaccessible from the internet. The redundancy of two NAT Gateways enhances availability.\nWhy other options are incorrect:\nOption B: Placing the ALB in private subnets prevents it from receiving traffic from the public internet,\ndefeating its purpose.\nOption C: Launching EC2 instances in public subnets exposes them to the internet directly, which violates the\nsecurity requirements.\nOption D: Having only one public and one private subnet doesn't inherently guarantee availability across\nAvailability Zones. It also lacks redundancy in NAT Gateway.\nIn essence, Options A and E establish a secure and highly available architecture:\nSecurity: Private subnets for EC2 instances and RDS, and NAT gateways control outbound traffic.\nAvailability: Multi-AZ RDS and Auto Scaling group across AZs ensures high availability.\nFunctionality: ALB in public subnets handles inbound traffic and distributes it to web tier. NAT gateways\nallow private EC2s to connect to payment processing on the internet.\nSupporting Documentation:\nAmazon VPC: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Introduction.html\nAmazon RDS Multi-AZ: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\nAuto Scaling: https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-\nscaling.html\nNAT Gateway: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\nApplication Load Balancer:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Introduction.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect needs to implement a solution to reduce a company's storage costs. All the company's data is\nin the Amazon S3 Standard storage class. The company must keep all data for at least 25 years. Data from the\nmost recent 2 years must be highly available and immediately retrievable.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive immediately.",
      "B": "Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 2 years.",
      "C": "Use S3 Intelligent-Tiering. Activate the archiving option to ensure that data is archived in S3 Glacier Deep",
      "D": "Set up an S3 Lifecycle policy to transition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA)"
    },
    "answer": "B",
    "explanation": "The correct solution is to transition objects to S3 Glacier Deep Archive after 2 years using an S3 Lifecycle\npolicy. Here's why:\nRequirement 1: Cost Reduction: S3 Glacier Deep Archive offers the lowest storage cost among all S3 storage\nclasses, making it ideal for long-term archival data. https://aws.amazon.com/s3/storage-classes/glacier/\nRequirement 2: Data Retention: The company needs to retain data for 25 years, which S3 Glacier Deep\nArchive fully supports.\nRequirement 3: Highly Available & Immediately Retrievable for 2 Years: The most recent 2 years of data\nneed to be highly available and immediately retrievable. Keeping the data in S3 Standard for the first 2 years\nsatisfies this requirement.\nS3 Lifecycle Policies: These policies automate the transition of objects between different S3 storage classes\nbased on defined rules. This is crucial for managing data retention and storage costs efficiently.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-concept.html\nOption A is incorrect because it immediately transitions all data to S3 Glacier Deep Archive, violating the\nrequirement for the most recent 2 years of data to be highly available and immediately retrievable.\nOption C, using S3 Intelligent-Tiering with the archiving option, would likely be more expensive than directly\ntransitioning to S3 Glacier Deep Archive after 2 years, since Intelligent-Tiering monitors access patterns.\nWhile it archives infrequently accessed data, it's designed for data with varying access patterns, not purely\narchival needs with a defined accessibility window.\nOption D is incorrect because S3 One Zone-IA is not designed for long-term archival storage with minimal\naccess. It also trades off availability by storing data in a single Availability Zone. It's cheaper than S3\nStandard, but not as cost-effective as S3 Glacier Deep Archive for long-term retention. The transition to S3\nGlacier Deep Archive after 2 years is correct, but the initial transition to S3 One Zone-IA is unnecessary and\nintroduces availability risks.\nTherefore, transitioning to S3 Glacier Deep Archive only after the data has been stored in S3 Standard for the\nfirst 2 years balances cost optimization with the accessibility requirements. This approach ensures that the\nmost recent data is readily available while older data is stored in the most cost-effective manner for long-\nterm archival.",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/glacier/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-concept.html"
    ]
  },
  {
    "question": "CertyIQ\nA media company is evaluating the possibility of moving its systems to the AWS Cloud. The company needs at\nleast 10 TB of storage with the maximum possible I/O performance for video processing, 300 TB of very durable\nstorage for storing media content, and 900 TB of storage to meet requirements for archival media that is not in use\nanymore.\nWhich set of services should a solutions architect recommend to meet these requirements?",
    "options": {
      "A": "Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for",
      "B": "Amazon EBS for maximum performance, Amazon EFS for durable data storage, and Amazon S3 Glacier for",
      "C": "Amazon EC2 instance store for maximum performance, Amazon EFS for durable data storage, and Amazon",
      "D": "Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3"
    },
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the correct answer and why the other options are less\nsuitable for the media company's storage requirements:\nOption D: Amazon EC2 instance store, Amazon S3, and Amazon S3 Glacier\nAmazon EC2 Instance Store (for Maximum Performance): Instance stores provide the highest I/O\nperformance directly attached to the EC2 instance. This makes them ideal for video processing, which\nrequires fast read/write speeds for manipulating large video files. The ephemeral nature of instance store is\nacceptable as data can be readily retrieved from durable storage.\nReference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\nAmazon S3 (for Durable Data Storage): S3 is designed for 99.999999999% durability, making it suitable for\nstoring critical media content long-term. S3 offers a balance of cost-effectiveness and accessibility for\nfrequently accessed media. With 300TB of storage, it is appropriate since the media content is often used.\nReference: https://aws.amazon.com/s3/storage-classes/\nAmazon S3 Glacier (for Archival Storage): Glacier is a low-cost storage option specifically designed for\ninfrequently accessed data. It is perfect for archival media that is no longer in active use. With 900TB of\nstorage, it is appropriate since the data is rarely used.\nReference: https://aws.amazon.com/glacier/\nWhy other options are incorrect:\nOption A (Amazon EBS, Amazon S3, and Amazon S3 Glacier): While EBS provides persistent block storage\nfor EC2, it doesn't offer the same level of I/O performance as instance store for intensive workloads like video\nprocessing. EBS is also more expensive per GB than instance store. It is also inappropriate to use Amazon EBS\nas an external storage location, since the EBS volume is attached to an EC2 instance.\nOption B (Amazon EBS, Amazon EFS, and Amazon S3 Glacier): EFS is a scalable file storage service for use\nwith EC2, but it is not optimized for maximum I/O performance. Video processing would benefit more from the\nhigh throughput of instance store. EFS is also more expensive than S3 for durable storage. It is also\ninappropriate to use Amazon EBS as an external storage location, since the EBS volume is attached to an EC2\ninstance.\nOption C (Amazon EC2 Instance Store, Amazon EFS, and Amazon S3): While instance store is correct for\nvideo processing, EFS is less durable and more expensive than S3 for general durable media storage. S3\nGlacier is a better fit for the archival storage requirements rather than using S3 for both durable and archival,\nsince Glacier is designed and priced for archival storage.\nIn summary: Option D provides the most cost-effective and performance-optimized solution by leveraging\ninstance store for performance, S3 for durability, and Glacier for archival, fully meeting all storage needs\noutlined in the problem.",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html",
      "https://aws.amazon.com/s3/storage-classes/",
      "https://aws.amazon.com/glacier/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to run applications in containers in the AWS Cloud. These applications are stateless and can\ntolerate disruptions within the underlying infrastructure. The company needs a solution that minimizes cost and\noperational overhead.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Use Spot Instances in an Amazon EC2 Auto Scaling group to run the application containers.",
      "B": "Use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS)",
      "C": "Use On-Demand Instances in an Amazon EC2 Auto Scaling group to run the application containers.",
      "D": "Use On-Demand Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group."
    },
    "answer": "B",
    "explanation": "The correct answer is B. Use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS)\nmanaged node group.\nHere's a detailed justification:\nThe problem statement emphasizes minimizing cost and operational overhead while running stateless\ncontainerized applications that can tolerate disruptions. Spot Instances offer significant cost savings\ncompared to On-Demand Instances because they utilize spare EC2 capacity. However, Spot Instances can be\nterminated with short notice (2-minute warning), making them unsuitable for stateful or disruption-sensitive\nworkloads. Because the application is stateless and tolerant to disruptions, using Spot Instances aligns with\nthe cost optimization requirement.\nUsing Amazon EKS managed node groups simplifies the management of the underlying EC2 instances that\nrun the containers. EKS handles patching, updating, and scaling the nodes within the group. By using\nmanaged node groups, the company reduces operational overhead associated with managing EC2 instances\ndirectly. Further, EKS offers inherent benefits for managing container deployments, scaling, and\norchestration, all which contribute to reduced operational overhead.\nCombining Spot Instances with EKS managed node groups creates a cost-effective and operationally efficient\nsolution. EKS orchestrates the containers, handling the redeployment of containers if a Spot Instance is\nterminated, because of the applications stateless nature. Options C and D use On-Demand Instances, which\nare more expensive than Spot Instances, and would violate the cost minimization requirement. Option A, while\nusing Spot Instances, lacks the orchestration and management features of EKS, leading to higher operational\noverhead. Running containers directly on EC2 instances without an orchestrator like Kubernetes introduces\ncomplexity in deployment, scaling, and management.\nTherefore, using Spot Instances in an Amazon EKS managed node group is the best approach to meet the\nrequirements of minimizing cost and operational overhead while running disruption-tolerant, stateless\ncontainerized applications.\nSupporting Documentation:\nAmazon EKS Managed Node Groups: https://docs.aws.amazon.com/eks/latest/userguide/managed-node-\ngroups.html\nAmazon EC2 Spot Instances: https://aws.amazon.com/ec2/spot/\nKubernetes Concepts: https://kubernetes.io/docs/concepts/",
    "links": [
      "https://docs.aws.amazon.com/eks/latest/userguide/managed-node-",
      "https://aws.amazon.com/ec2/spot/",
      "https://kubernetes.io/docs/concepts/"
    ]
  },
  {
    "question": "CertyIQ\nA company is running a multi-tier web application on premises. The web application is containerized and runs on a\nnumber of Linux hosts connected to a PostgreSQL database that contains user records. The operational overhead\nof maintaining the infrastructure and capacity planning is limiting the company's growth. A solutions architect\nmust improve the application's infrastructure.\nWhich combination of actions should the solutions architect take to accomplish this? (Choose two.)",
    "options": {
      "A": "Migrate the PostgreSQL database to Amazon Aurora:",
      "B": "Migrate the web application to be hosted on Amazon EC2 instances: While moving to EC2 is a step",
      "C": "Set up an Amazon CloudFront distribution for the web application content: CloudFront primarily focuses",
      "D": "Set up Amazon ElastiCache between the web application and the PostgreSQL database: ElastiCache is a"
    },
    "answer": "A",
    "explanation": "The correct answer is AE. Here's why:\nA. Migrate the PostgreSQL database to Amazon Aurora:\nManaged Database Service: Migrating the on-premises PostgreSQL database to Amazon Aurora PostgreSQL\noffers a fully managed database service. This significantly reduces the operational overhead associated with\ntasks such as patching, backups, and database scaling. Aurora also offers superior performance compared to\nstandard PostgreSQL deployments.\nScalability and Availability: Aurora provides automatic scaling capabilities, allowing the database to handle\nfluctuating workloads without manual intervention. It also offers high availability features, ensuring minimal\ndowntime.\nReduced Operational Burden: The company is currently burdened by the operational overhead of maintaining\nthe database infrastructure. Aurora removes this burden, freeing up resources for other tasks.\nE. Migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container Service\n(Amazon ECS):\nServerless Containerization: Fargate is a serverless compute engine for ECS that allows you to run\ncontainers without managing underlying EC2 instances. This eliminates the operational overhead of\nprovisioning, patching, and scaling EC2 instances.\nSimplified Deployment and Management: ECS simplifies the deployment, management, and scaling of\ncontainerized applications. Fargate integration further simplifies the process by removing the need to manage\nthe underlying infrastructure.\nImproved Scalability and Availability: ECS with Fargate offers improved scalability and availability for the\nweb application. ECS can automatically scale the number of containers based on demand, while Fargate\nensures that the containers are always running on healthy infrastructure.\nWhy the other options are less suitable:\nB. Migrate the web application to be hosted on Amazon EC2 instances: While moving to EC2 is a step\ntowards the cloud, it still requires managing the underlying operating systems, patching, and scaling, which\ndoesn't significantly alleviate the operational overhead.\nC. Set up an Amazon CloudFront distribution for the web application content: CloudFront primarily focuses\non content delivery and caching, improving application performance for geographically distributed users.\nWhile beneficial, it doesn't address the core problem of reducing operational overhead related to the web\napplication and database infrastructure.\nD. Set up Amazon ElastiCache between the web application and the PostgreSQL database: ElastiCache is a\ncaching service that can improve application performance by caching frequently accessed data. However, it\ndoesn't directly address the operational overhead of managing the underlying infrastructure and database.\nAuthoritative Links:\nAmazon Aurora: https://aws.amazon.com/rds/aurora/\nAWS Fargate: https://aws.amazon.com/fargate/\nAmazon Elastic Container Service (ECS): https://aws.amazon.com/ecs/",
    "links": [
      "https://aws.amazon.com/rds/aurora/",
      "https://aws.amazon.com/fargate/",
      "https://aws.amazon.com/ecs/"
    ]
  },
  {
    "question": "CertyIQ\nAn application runs on Amazon EC2 instances across multiple Availability Zonas. The instances run in an Amazon\nEC2 Auto Scaling group behind an Application Load Balancer. The application performs best when the CPU\nutilization of the EC2 instances is at or near 40%.\nWhat should a solutions architect do to maintain the desired performance across all instances in the group?",
    "options": {
      "A": "Use a simple scaling policy to dynamically scale the Auto Scaling group.",
      "B": "Use a target tracking policy to dynamically scale the Auto Scaling group.",
      "C": "Use an AWS Lambda function ta update the desired Auto Scaling group capacity.",
      "D": "Use scheduled scaling actions to scale up and scale down the Auto Scaling group."
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the best solution and why the other options are less suitable\nfor maintaining desired performance based on CPU utilization in an Auto Scaling group:\nThe core requirement is to keep the EC2 instance CPU utilization around 40%. To achieve this, the Auto\nScaling group must dynamically adjust the number of instances based on the actual CPU load. A target\ntracking scaling policy is specifically designed for this scenario. It allows you to set a target value for a\nmetric (in this case, CPU utilization) and the Auto Scaling group automatically adjusts the number of instances\nto maintain that target. It continuously monitors the CloudWatch metric (CPUUtilization) and adds or removes\ninstances as needed to stay close to the 40% target.\nOption A, simple scaling policy, is less ideal. While it allows scaling based on metrics, it operates with fixed\nadjustments. You would need to define how many instances to add or remove based on breaches of upper and\nlower thresholds. It doesn't continuously strive to maintain a specific target like 40%. Configuring the exact\nadjustments required for a complex workload would be more cumbersome than using target tracking.\nOption C, using an AWS Lambda function, introduces unnecessary complexity. While Lambda can update the\ndesired capacity, you would need to write custom code to monitor CloudWatch metrics, calculate the required\ncapacity adjustments, and then update the Auto Scaling group. Target tracking provides a managed solution\nthat handles all of this automatically, reducing operational overhead. Relying on Lambda would also mean\nmanaging code, potential errors in your logic, and the IAM permissions required for the Lambda function.\nOption D, scheduled scaling actions, is not suitable for dynamic CPU-based performance maintenance.\nScheduled scaling only adjusts the capacity based on a pre-defined schedule. It doesn't react to real-time\nchanges in CPU utilization. If the workload fluctuates unpredictably, scheduled scaling would be ineffective\nand could lead to either over-provisioning (wasting resources) or under-provisioning (impacting performance).\nIn summary, target tracking scaling is the most efficient and appropriate solution because it directly\naddresses the requirement of maintaining a specific CPU utilization target for optimal application\nperformance. It is a managed service solution, so is preferred over building a solution using Lambda.\nSupporting Documentation:\nAWS Auto Scaling Target Tracking Scaling Policies:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\nAWS Auto Scaling Simple Scaling Policies: https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-\nscaling-simple.html\nAWS Auto Scaling Scheduled Scaling:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html",
    "links": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is developing a file-sharing application that will use an Amazon S3 bucket for storage. The company\nwants to serve all the files through an Amazon CloudFront distribution. The company does not want the files to be\naccessible through direct navigation to the S3 URL.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Write individual policies for each S3 bucket to grant read permission for only CloudFront access.",
      "B": "Create an IAM user. Grant the user read permission to objects in the S3 bucket. Assign the user to",
      "C": "Write an S3 bucket policy that assigns the CloudFront distribution ID as the Principal and assigns the target",
      "D": "Also, the Distribution ID changes, thus the OAI is the stable element to"
    },
    "answer": "D",
    "explanation": "The correct answer is D: Create an origin access identity (OAI). Assign the OAI to the CloudFront\ndistribution. Configure the S3 bucket permissions so that only the OAI has read permission.\nHere's a detailed justification:\nThe primary goal is to restrict direct access to the S3 bucket while allowing CloudFront to serve the files.\nOrigin Access Identity (OAI) is specifically designed for this purpose. An OAI is a virtual user identity that\nCloudFront uses to fetch private content from your S3 bucket.\nWhy OAI is the right approach: OAI provides a secure way to allow CloudFront to access the S3 bucket\nwithout making the content publicly accessible via the S3 URL. It enforces access control at the bucket level.\nSteps involved:\n1. Create an OAI: This creates a unique CloudFront user that identifies your distribution.\n2. Assign the OAI to CloudFront: When configuring the CloudFront distribution, you specify the OAI.\nThis tells CloudFront to use this identity when requesting content from the S3 origin.\n3. Configure S3 Bucket Permissions: Modify the S3 bucket policy to grant the OAI read access to the\nobjects. Importantly, remove any public read access. This ensures that only the CloudFront\ndistribution (acting through the OAI) can retrieve objects from the bucket.\nWhy other options are incorrect:\nA: Writing individual policies for each S3 bucket (if there were many) is not scalable. It is also not necessary\nbecause OAI already achieves the desired outcome by controlling access at the bucket level.\nB: Using an IAM user is generally discouraged for service-to-service authentication in this scenario. OAI is\nspecifically designed for CloudFront-S3 access control and provides a more streamlined and secure solution.\nAlso, it's not clear how you would \"assign\" the IAM user to CloudFront, making the process conceptually\nflawed.\nC: While you do use an S3 bucket policy, only assigning the CloudFront distribution ID as the Principal without\nthe context of OAI doesn't prevent direct S3 access. The S3 bucket policy needs to grant the OAI permission,\nnot just the CloudFront distribution ID. Also, the Distribution ID changes, thus the OAI is the stable element to\nuse.\nIn essence, OAI enables a secure, dedicated channel for CloudFront to fetch content from S3 without\nexposing the bucket's contents to the public internet via direct S3 URL access.\nAuthoritative Links:\nAWS Documentation on Restricting Access to Amazon S3 Content by Using an Origin Access Identity:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-\nto-s3.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-"
    ]
  },
  {
    "question": "CertyIQ\nA companys website provides users with downloadable historical performance reports. The website needs a\nsolution that will scale to meet the companys website demands globally. The solution should be cost-effective,\nlimit the provisioning of infrastructure resources, and provide the fastest possible response time.\nWhich combination should a solutions architect recommend to meet these requirements?",
    "options": {
      "A": "Amazon CloudFront and Amazon S3",
      "B": "AWS Lambda and Amazon DynamoDB: While Lambda and DynamoDB are scalable services, they're better",
      "C": "Application Load Balancer with Amazon EC2 Auto Scaling: This setup involves provisioning and managing",
      "D": "Amazon Route 53 with internal Application Load Balancers: This is a possible configuration for load"
    },
    "answer": "A",
    "explanation": "The correct answer is A: Amazon CloudFront and Amazon S3. Here's why:\nScalability and Global Reach: Amazon CloudFront is a content delivery network (CDN) that replicates your\ncontent across edge locations globally. This ensures low latency and fast download speeds for users\nregardless of their geographical location. https://aws.amazon.com/cloudfront/\nCost-Effectiveness and Limited Provisioning: S3 offers a highly scalable and cost-effective storage solution\nfor the historical performance reports. You only pay for the storage you use. CloudFront further optimizes cost\nby caching content at edge locations, reducing the load on your origin (S3). Using S3 & CloudFront eliminates\nthe need to manage servers or other compute infrastructure, reducing operational overhead.\nFastest Response Time: CloudFront caches the reports close to the users, minimizing the distance the data\nneeds to travel. This directly translates to lower latency and faster download speeds compared to serving\ncontent directly from an origin server.\nNow, let's examine why the other options are less suitable:\nB. AWS Lambda and Amazon DynamoDB: While Lambda and DynamoDB are scalable services, they're better\nsuited for dynamic data and application logic. Serving static files (historical reports) using Lambda would be\nless cost-effective and more complex than using S3 and CloudFront. DynamoDB is not optimized for storing\nand serving static files.\nC. Application Load Balancer with Amazon EC2 Auto Scaling: This setup involves provisioning and managing\nEC2 instances, which is less cost-effective and requires more operational overhead than S3 and CloudFront. It\ndoesn't inherently provide global distribution like CloudFront. While Auto Scaling provides scalability, it\ndoesn't match the built-in global distribution and caching capabilities of CloudFront.\nD. Amazon Route 53 with internal Application Load Balancers: This is a possible configuration for load\nbalancing. However, this would distribute traffic between regional load balancers. It doesn't take into account\nthat the files are static and suitable for caching closer to the users as CloudFront does. Furthermore, it\nrequires provisioning infrastructure, failing to limit provisioning of infrastructure resources.\nIn summary, the combination of Amazon CloudFront and Amazon S3 is the most efficient and cost-effective\nsolution for delivering downloadable historical performance reports globally with minimal infrastructure\nprovisioning and optimal response times.",
    "links": [
      "https://aws.amazon.com/cloudfront/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an Oracle database on premises. As part of the companys migration to AWS, the company wants\nto upgrade the database to the most recent available version. The company also wants to set up disaster recovery\n(DR) for the database. The company needs to minimize the operational overhead for normal operations and DR\nsetup. The company also needs to maintain access to the database's underlying operating system.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Migrate the Oracle database to an Amazon EC2 instance. Set up database replication to a different AWS",
      "B": "Migrate the Oracle database to Amazon RDS for Oracle. Activate Cross-Region automated backups to",
      "C": "Migrate the Oracle database to Amazon RDS Custom for Oracle. Create a read replica for the database in",
      "D": "Migrate the Oracle database to Amazon RDS for Oracle. Create a standby database in another Availability"
    },
    "answer": "C",
    "explanation": "The best solution is C, migrating the Oracle database to Amazon RDS Custom for Oracle and creating a read\nreplica in another AWS Region. Here's why:\nRDS Custom for Oracle: This service provides managed database capabilities while allowing access to the\nunderlying operating system. This meets the requirement of needing OS access, which standard RDS does not\nprovide.\nDatabase Upgrade: RDS Custom allows for database upgrades while maintaining control over the process.\nDisaster Recovery: Creating a read replica in another AWS Region directly addresses the disaster recovery\nrequirement. The read replica can be promoted to a standalone, writeable instance in the event of a primary\nregion failure, minimizing downtime.\nOperational Overhead: RDS Custom automates many operational tasks, reducing the operational overhead\ncompared to managing Oracle on EC2. Read replicas further contribute to reduced overhead by allowing for\nDR testing without impacting the primary instance.\nCost-Effectiveness: While not explicitly mentioned in the requirements, RDS Custom generally provides a\ngood balance between control and cost-effectiveness compared to fully self-managed EC2 instances.\nOption A (EC2) requires significant operational overhead, defeating the purpose of minimizing it. Option B\n(RDS for Oracle with Cross-Region Automated Backups) doesn't provide access to the underlying OS. Option\nD (RDS for Oracle with Standby Database in another AZ) addresses high availability, not disaster recovery\nacross regions.\nReferences:\nAmazon RDS Custom for Oracle\nAmazon RDS Read Replicas",
    "links": []
  },
  {
    "question": "CertyIQ\nA company wants to move its application to a serverless solution. The serverless solution needs to analyze existing\nand new data by using SL. The company stores the data in an Amazon S3 bucket. The data requires encryption and\nmust be replicated to a different AWS Region.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Here's why:",
      "B": "Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR) to",
      "C": "Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted",
      "D": "Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's why:\nRequirements Alignment: The question mandates a serverless solution for data analysis using SQL, data\nencryption at rest, and cross-region replication for disaster recovery/availability.\nS3 Cross-Region Replication (CRR): CRR directly addresses the requirement to replicate data to another\nAWS Region. This feature provides automatic, asynchronous copying of objects across S3 buckets in different\nAWS Regions.\nServer-Side Encryption with AWS KMS Multi-Region Keys (SSE-KMS): SSE-KMS satisfies the encryption at\nrest requirement. Using KMS Multi-Region keys is crucial because if your primary region KMS becomes\nunavailable, your replicated bucket still has access to decrypt the data. With SSE-S3, you can't use Multi-\nRegion keys.\nAmazon Athena: Athena is a serverless query service that allows you to analyze data stored in S3 using\nstandard SQL. This fulfills the SQL-based analysis requirement without the need to manage any\ninfrastructure.\nWhy A is better than C: SSE-KMS Multi-Region keys provide more control and flexibility for encryption key\nmanagement than SSE-S3 (Amazon S3 managed keys). If you have an issue in your primary region, it would\ntake longer to re-encrypt the objects in your secondary region if you were using SSE-S3.\nWhy A is better than B and D: Amazon RDS is a database service requiring instance management, making the\nsolution not serverless. Athena is the right tool for SQL queries on data in S3 in a serverless way.\nIn summary: Option A gives the required level of encryption in a serverless, secure, and easy-to-manage way.\nAuthoritative Links:\nS3 Cross-Region Replication (CRR):\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html\nSSE-KMS: https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html\nAthena: https://aws.amazon.com/athena/\nSSE-S3: https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html\nKMS Multi-Region Keys: https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-\noverview.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html",
      "https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html",
      "https://aws.amazon.com/athena/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html",
      "https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-"
    ]
  },
  {
    "question": "CertyIQ\nA company runs workloads on AWS. The company needs to connect to a service from an external provider. The\nservice is hosted in the provider's VP",
    "options": {
      "C": "NAT Gateway uses public IP addresses and is not a secure, private option.",
      "A": "Create a VPC peering connection between the company's VPC and the provider's VPC. Update the route",
      "B": "Ask the provider to create a virtual private gateway in its VPC. Use AWS PrivateLink to connect to the target",
      "D": "Ask the provider to create a VPC endpoint for the target service. Use AWS PrivateLink to connect to the"
    },
    "answer": "D",
    "explanation": "The correct answer is D, asking the provider to create a VPC endpoint for the target service and using AWS\nPrivateLink to connect. Here's why:\nPrivate Connectivity: PrivateLink ensures traffic doesn't traverse the public internet, fulfilling the\nrequirement for private connectivity. It establishes a direct connection between the VPCs without exposing\nthe traffic to the public internet.\nRestricted Access: A VPC endpoint for the target service limits access to only that specific service, adhering\nto the restriction requirement. This reduces the attack surface and improves security.\nCompany-Initiated Connection: PrivateLink allows the company to initiate the connection from its VPC to the\nprovider's service through the endpoint.\nVPC Peering (Option A - Incorrect): VPC peering establishes a connection between two entire VPCs. It doesn't\nrestrict access to a single service within the provider's VPC, violating the \"restricted to the target service\"\nrequirement. Also, VPC peering requires overlapping CIDR block consideration.\nVirtual Private Gateway (Option B - Incorrect): A Virtual Private Gateway (VGW) is primarily used for VPN\nconnections or Direct Connect. It is less suitable for a private, service-specific connection like the one\ndescribed. While AWS PrivateLink could theoretically leverage Direct Connect, it's overkill for the scenario.\nNAT Gateway (Option C - Incorrect): A NAT gateway enables instances in a private subnet to connect to the\ninternet or other AWS services, but it doesn't provide private connectivity to a specific service in another VPC.\nNAT Gateway uses public IP addresses and is not a secure, private option.\nAWS PrivateLink Benefits: PrivateLink enhances security by eliminating the need for public IPs or internet\ngateways, simplifying network management and reducing the risk of data exposure.\nIn summary, option D provides the most secure and compliant solution by establishing a private, service-\nspecific connection initiated from the company's VPC using PrivateLink and a VPC endpoint.\nFurther Research:\nAWS PrivateLink: https://aws.amazon.com/privatelink/\nVPC Endpoints: https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html",
    "links": [
      "https://aws.amazon.com/privatelink/",
      "https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is migrating its on-premises PostgreSQL database to Amazon Aurora PostgreSQL. The on-premises\ndatabase must remain online and accessible during the migration. The Aurora database must remain synchronized\nwith the on-premises database.\nWhich combination of actions must a solutions architect take to meet these requirements? (Choose two.)",
    "options": {
      "A": "Create an ongoing replication task: To keep the Aurora PostgreSQL database synchronized with the on-",
      "B": "Create a database backup of the on-premises database: A database backup is useful for the initial data",
      "C": "Create an AWS Database Migration Service (AWS DMS) replication server: DMS is the AWS service",
      "D": "Convert the database schema by using the AWS Schema Conversion Tool (AWS SCT): While AWS SCT is"
    },
    "answer": "A",
    "explanation": "The correct answer is AC. Here's why:\nA. Create an ongoing replication task: To keep the Aurora PostgreSQL database synchronized with the on-\npremises PostgreSQL database while the on-premises database remains online, you need a continuous\nreplication solution. AWS Database Migration Service (DMS) achieves this through replication tasks. These\ntasks capture changes from the source database and apply them to the target database in near real-time,\nensuring data consistency. This ongoing replication is crucial for a zero-downtime migration.\nC. Create an AWS Database Migration Service (AWS DMS) replication server: DMS is the AWS service\nspecifically designed for database migrations. A replication server is the core component of DMS, responsible\nfor reading data from the source database (on-premises PostgreSQL), transforming the data if needed, and\nloading the data into the target database (Aurora PostgreSQL). Without a DMS replication server, you cannot\nperform the ongoing replication task needed for synchronization.\nAWS Database Migration Service Documentation\nAWS DMS Replication Task\nHere's why the other options are not suitable:\nB. Create a database backup of the on-premises database: A database backup is useful for the initial data\nload into Aurora PostgreSQL, but it doesn't address the ongoing synchronization requirement. A backup\nrepresents a point-in-time snapshot, and any changes after the backup would not be reflected in the Aurora\ndatabase.\nD. Convert the database schema by using the AWS Schema Conversion Tool (AWS SCT): While AWS SCT is\noften used in database migrations to convert the database schema from one engine to another (e.g., Oracle to\nPostgreSQL), it is not always necessary for PostgreSQL-to-PostgreSQL migrations unless there are\ncompatibility issues or optimizations needed. Importantly, schema conversion doesn't handle ongoing data\nsynchronization.\nE. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to monitor the database\nsynchronization: EventBridge rules can monitor events and trigger actions. While you could potentially use\nEventBridge to monitor DMS metrics related to replication status, it doesn't actually perform the replication.\nFurthermore, CloudWatch metrics can be used more directly to monitor DMS tasks rather than involving\nEventBridge. Therefore, EventBridge isn't a primary requirement for the initial synchronization process itself.",
    "links": []
  },
  {
    "question": "CertyIQ\nA company uses AWS Organizations to create dedicated AWS accounts for each business unit to manage each\nbusiness unit's account independently upon request. The root email recipient missed a notification that was sent to\nthe root user email address of one account. The company wants to ensure that all future notifications are not\nmissed. Future notifications must be limited to account administrators.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Configure the companys email server to forward notification email messages that are sent to the AWS",
      "B": "Here's why:",
      "C": "Configure all AWS account root user email messages to be sent to one administrator who is responsible for",
      "D": "Configure all existing AWS accounts and all newly created accounts to use the same root user email"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Here's why:\nOption B addresses the core requirement of ensuring that notifications sent to the root user email address are\nnot missed and are limited to account administrators. Configuring the root user email address as a distribution\nlist ensures that multiple administrators receive the notifications, reducing the risk of a single point of failure\n(like one administrator missing the email).\nUsing distribution lists linked to the root user email address is a manageable approach that aligns with AWS\nbest practices. Importantly, it prevents reliance on a single individual. AWS suggests using distribution lists,\nand not individual email addresses, for critical administrative roles, like the root user.\nThe utilization of AWS account alternate contacts in the AWS Organizations console provides an additional\nlayer of notification delivery. Alternate contacts can be set for billing, technical, and security purposes,\nallowing for different administrators to receive different types of notifications relevant to their roles. By\nconfiguring alternate contacts, notifications intended for these specific areas are less likely to be overlooked\nand are routed to the appropriate personnel.\nHere's why the other options are less suitable:\nA: Forwarding all emails from all root users to all users in the organization is overly broad and creates\nunnecessary noise, violating the requirement to limit notifications to administrators.\nC: Relying on a single administrator to forward alerts introduces a single point of failure and increases the\nlikelihood of delays or missed notifications.\nD: Using the same root user email address for all accounts creates a security risk and management\ncomplexity, as a compromise of that email address would affect all accounts.\nSupporting Documentation:\nAWS Organizations Best Practices\nAWS Account Management",
    "links": []
  },
  {
    "question": "CertyIQ\nA company runs its ecommerce application on AWS. Every new order is published as a massage in a RabbitMQ\nqueue that runs on an Amazon EC2 instance in a single Availability Zone. These messages are processed by a\ndifferent application that runs on a separate EC2 instance. This application stores the details in a PostgreSQL\ndatabase on another EC2 instance. All the EC2 instances are in the same Availability Zone.\nThe company needs to redesign its architecture to provide the highest availability with the least operational\noverhead.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Create a",
      "B": "Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Create a",
      "C": "Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Create another",
      "D": "Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Create another"
    },
    "answer": "B",
    "explanation": "The goal is to achieve high availability with minimal operational overhead for an e-commerce application\ncurrently running on EC2 instances within a single Availability Zone. Option B provides the best approach.\nMigrating the RabbitMQ queue to a redundant pair on Amazon MQ (active/standby) eliminates the single point\nof failure associated with running RabbitMQ on a single EC2 instance. Amazon MQ manages the underlying\ninfrastructure, reducing operational overhead.\nCreating a Multi-AZ Auto Scaling group for the application EC2 instances ensures that the application\nremains available even if one Availability Zone fails. The Auto Scaling group will automatically launch new\ninstances in a healthy Availability Zone.\nMigrating the PostgreSQL database to a Multi-AZ deployment of Amazon RDS for PostgreSQL is crucial for\ndatabase availability. RDS manages the replication and failover process, significantly reducing operational\noverhead compared to managing PostgreSQL on EC2. Multi-AZ RDS provides synchronous replication to a\nstandby instance in a different AZ, ensuring data durability and rapid failover.\nOptions A, C, and D have drawbacks. Running the database or RabbitMQ on EC2 instances within Auto Scaling\ngroups, even across multiple AZs, requires managing replication, failover, and patching, increasing\noperational overhead. Amazon MQ and RDS offer managed services with built-in high availability, simplifying\nmanagement.\nTherefore, Option B provides the optimal solution by leveraging managed services (Amazon MQ and RDS) to\nachieve high availability with the least operational burden.\nReferences:\nAmazon MQ: https://aws.amazon.com/mq/\nAmazon RDS Multi-AZ: https://aws.amazon.com/rds/features/multi-az/\nAuto Scaling: https://aws.amazon.com/autoscaling/",
    "links": [
      "https://aws.amazon.com/mq/",
      "https://aws.amazon.com/rds/features/multi-az/",
      "https://aws.amazon.com/autoscaling/"
    ]
  },
  {
    "question": "CertyIQ\nA reporting team receives files each day in an Amazon S3 bucket. The reporting team manually reviews and copies\nthe files from this initial S3 bucket to an analysis S3 bucket each day at the same time to use with Amazon\nQuickSight. Additional teams are starting to send more files in larger sizes to the initial S3 bucket.\nThe reporting team wants to move the files automatically analysis S3 bucket as the files enter the initial S3\nbucket. The reporting team also wants to use AWS Lambda functions to run pattern-matching code on the copied\ndata. In addition, the reporting team wants to send the data files to a pipeline in Amazon SageMaker Pipelines.\nWhat should a solutions architect do to meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Create a Lambda function to copy the files to the analysis S3 bucket. Create an S3 event notification for the",
      "B": "Create a Lambda function to copy the files to the analysis S3 bucket. Configure the analysis S3 bucket to",
      "C": "Configure S3 replication between the S3 buckets. Create an S3 event notification for the analysis S3 bucket.",
      "D": "Here's a detailed justification:"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Here's a detailed justification:\nS3 Replication: S3 Replication is the most efficient way to automatically copy objects between S3 buckets.\nIt's a managed service, reducing operational overhead compared to writing and managing a Lambda function\nfor copying. It replicates data as soon as it's written to the source bucket.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html\nEventBridge for Fan-Out: Amazon EventBridge (formerly CloudWatch Events) provides a scalable and\ndecoupled way to route events to multiple targets. This addresses the requirement to trigger both a Lambda\nfunction for pattern matching and the SageMaker pipeline. S3 can send event notifications to EventBridge\nupon object creation. https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-what-is.html\nObjectCreated Event: Configuring an ObjectCreated rule in EventBridge triggers the rule when a new object is\ncreated in the analysis S3 bucket.\nTargets: By configuring the Lambda function and SageMaker Pipelines as targets for the EventBridge rule,\nboth are automatically invoked when a new object arrives in the analysis bucket.\nWhy other options are not optimal:\nA & B: While a Lambda function could copy files, it adds operational overhead. S3 Replication handles the\ncopying automatically. Additionally, option B is incorrect because the question asks to run Lambda functions\nand SageMaker pipelines after the files are copied to the analysis bucket.\nC: S3 event notifications sent directly to multiple destinations can be less scalable and harder to manage than\nusing EventBridge. EventBridge provides more robust event routing and filtering capabilities.\nIn summary, option D combines the efficiency of S3 Replication with the event-driven architecture of\nEventBridge for a scalable, low-overhead solution to meet all the requirements.",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html",
      "https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-what-is.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect needs to help a company optimize the cost of running an application on AWS. The application\nwill use Amazon EC2 instances, AWS Fargate, and AWS Lambda for compute within the architecture.\nThe EC2 instances will run the data ingestion layer of the application. EC2 usage will be sporadic and\nunpredictable. Workloads that run on EC2 instances can be interrupted at any time. The application front end will\nrun on Fargate, and Lambda will serve the API layer. The front-end utilization and API layer utilization will be\npredictable over the course of the next year.\nWhich combination of purchasing options will provide the MOST cost-effective solution for hosting this\napplication? (Choose two.)",
    "options": {
      "A": "Use Spot Instances for the data ingestion layer: The data ingestion layer's EC2 usage is sporadic and",
      "B": "Use On-Demand Instances for the data ingestion layer: On-Demand Instances are the most expensive",
      "C": "Purchase a 1-year Compute Savings Plan for the front end and API layer: The application front end runs on",
      "D": "Purchase 1-year All Upfront Reserved instances for the data ingestion layer: Given the unpredictable"
    },
    "answer": "A",
    "explanation": "The correct answer is A and C.\nHere's why:\nA. Use Spot Instances for the data ingestion layer: The data ingestion layer's EC2 usage is sporadic and\nunpredictable, and workloads can be interrupted. Spot Instances are ideal for these scenarios. Spot Instances\nleverage spare EC2 capacity and are offered at significantly reduced prices compared to On-Demand\nInstances. The application's tolerance for interruption aligns perfectly with the characteristics of Spot\nInstances, making them a cost-effective choice for this layer.https://aws.amazon.com/ec2/spot/\nC. Purchase a 1-year Compute Savings Plan for the front end and API layer: The application front end runs on\nFargate, and the API layer runs on Lambda. Both have predictable utilization over the next year. Savings Plans\noffer significant discounts in exchange for a commitment to a consistent amount of compute usage\n(measured in dollars per hour). A Compute Savings Plan provides the most flexibility, since it applies to EC2,\nFargate, and Lambda, whereas the EC2 Instance Savings Plan applies only to\nEC2.https://aws.amazon.com/savingsplans/compute-savings-plans/\nHere's why the other options are not as suitable:\nB. Use On-Demand Instances for the data ingestion layer: On-Demand Instances are the most expensive\noption and should be avoided when workloads can tolerate interruptions, as in the data ingestion layer.\nD. Purchase 1-year All Upfront Reserved instances for the data ingestion layer: Given the unpredictable\nnature of EC2 usage in the data ingestion layer, committing to Reserved Instances is risky and may lead to\npaying for unused capacity. Also Reserved Instances offer the least amount of flexibility\nE. Purchase a 1-year EC2 instance Savings Plan for the front end and API layer: The API layer is on Lambda,\nso EC2 Instance Savings Plan can't be used. It is a better option to choose compute savings plan as it can\napply to all different kinds of compute services and provides more flexibility.",
    "links": [
      "https://aws.amazon.com/ec2/spot/",
      "https://aws.amazon.com/savingsplans/compute-savings-plans/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a web-based portal that provides users with global breaking news, local alerts, and weather\nupdates. The portal delivers each user a personalized view by using mixture of static and dynamic content. Content\nis served over HTTPS through an API server running on an Amazon EC2 instance behind an Application Load\nBalancer (ALB). The company wants the portal to provide this content to its users across the world as quickly as\npossible.\nHow should a solutions architect design the application to ensure the LEAST amount of latency for all users?",
    "options": {
      "A": "Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve all static and dynamic",
      "B": "This approach also adds operational complexity and cost for maintaining duplicate infrastructure across",
      "C": "Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve the static content."
    },
    "answer": "A",
    "explanation": "The optimal solution for minimizing latency for global users accessing a personalized news portal involves\nleveraging Amazon CloudFront to cache and deliver both static and dynamic content.\nOption A, deploying the application stack in a single AWS Region and using CloudFront with the ALB as an\norigin, is the most effective approach. CloudFront is a content delivery network (CDN) that caches content at\nedge locations globally. When a user requests content, CloudFront serves it from the nearest edge location,\nsignificantly reducing latency. Specifying the ALB as the origin allows CloudFront to retrieve content that\nisn't already cached, including dynamically generated personalized content.\nOption B, deploying in multiple regions and using Route 53 latency routing, is less efficient. While Route 53\nlatency routing directs users to the closest region, it doesn't cache content at edge locations. This means\nusers still experience latency when accessing dynamic content, as requests must travel to the regional ALB.\nThis approach also adds operational complexity and cost for maintaining duplicate infrastructure across\nmultiple regions.\nOption C, using CloudFront only for static content, doesn't address the latency issue for dynamic content.\nSince the portal provides personalized views, the dynamic content plays a crucial role in user experience, and\nserving it directly from the ALB negates the benefits of CloudFront for a significant portion of the content.\nOption D, using Route 53 geolocation routing, while beneficial, doesn't directly address latency as effectively\nas CloudFront. Geolocation routing directs users based on their geographic location, but it doesn't cache\ncontent closer to users like CloudFront does. It's also less flexible than latency routing in some scenarios\nwhere network conditions vary.\nCloudFront's ability to cache dynamic content, coupled with its global network of edge locations, makes it the\nmost effective solution for minimizing latency and delivering a fast, responsive user experience for a web\nportal with personalized content. The single region approach combined with CloudFront simplifies the\narchitecture and reduces operational overhead.\nAuthoritative Links:\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nAmazon Route 53: https://aws.amazon.com/route53/",
    "links": [
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/route53/"
    ]
  },
  {
    "question": "CertyIQ\nA gaming company is designing a highly available architecture. The application runs on a modified Linux kernel\nand supports only UDP-based traffic. The company needs the front-end tier to provide the best possible user\nexperience. That tier must have low latency, route traffic to the nearest edge location, and provide static IP\naddresses for entry into the application endpoints.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Configure Amazon Route 53 to forward requests to an Application Load Balancer. Use AWS Lambda for the",
      "B": "Configure Amazon CloudFront to forward requests to a Network Load Balancer. Use AWS Lambda for the",
      "C": "Here's why:",
      "D": "Configure Amazon API Gateway to forward requests to an Application Load Balancer. Use Amazon EC2"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Here's why:\nAWS Global Accelerator is designed to route traffic to the optimal AWS endpoint based on user location,\nproviding low latency and improved user experience, which directly addresses the requirement for routing\ntraffic to the nearest edge location. Crucially, it also provides static IP addresses for applications.\nhttps://aws.amazon.com/global-accelerator/\nNetwork Load Balancer (NLB) is required because the application supports only UDP-based traffic.\nApplication Load Balancers (ALB) only support HTTP/HTTPS traffic and are therefore not a viable option. NLB\nis designed for high throughput and low latency, ideal for UDP applications.\nhttps://aws.amazon.com/elasticloadbalancing/network-load-balancer/\nAmazon EC2 instances with EC2 Auto Scaling group is the best compute option. AWS Lambda does not offer\nfull control over the underlying Linux kernel as required by the scenario. EC2 instances allows full control\nover OS, is necessary because the application runs on a modified Linux kernel. The Auto Scaling group\nensures high availability by automatically adjusting the number of EC2 instances based on demand.\nhttps://aws.amazon.com/ec2/autoscaling/\nOption A is incorrect because Application Load Balancer does not support UDP traffic. Option B is wrong\nbecause CloudFront is primarily designed for caching static content. While it can accelerate dynamic content\ndelivery, it is not optimized for UDP-based traffic and requires a distribution for forwarding traffic. Option D is\nincorrect because API Gateway is designed for API management and request routing, not high-throughput,\nlow-latency UDP traffic and ALB does not support UDP traffic.",
    "links": [
      "https://aws.amazon.com/global-accelerator/",
      "https://aws.amazon.com/elasticloadbalancing/network-load-balancer/",
      "https://aws.amazon.com/ec2/autoscaling/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to migrate its existing on-premises monolithic application to AWS. The company wants to keep\nas much of the front-end code and the backend code as possible. However, the company wants to break the\napplication into smaller applications. A different team will manage each application. The company needs a highly\nscalable solution that minimizes operational overhead.\nWhich solution will meet these requirements?",
    "options": {
      "A": "AWS Lambda: Lambda is suitable for event-driven, stateless functions, but not ideal for hosting entire",
      "B": "AWS Amplify: Amplify is more focused on front-end development and mobile app backends. While it can",
      "C": "Amazon EC2 instances with Auto Scaling Group: While this option provides scalability, it involves",
      "D": "Host the application on Amazon Elastic Container Service (Amazon ECS). Set up an"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Host the application on Amazon Elastic Container Service (Amazon ECS). Set up an\nApplication Load Balancer with Amazon ECS as the target.\nHere's a detailed justification:\nThe problem statement emphasizes breaking down a monolithic application into smaller, independently\nmanageable applications while minimizing operational overhead and ensuring high scalability.\nAmazon ECS is a fully managed container orchestration service. Containerization allows you to package each\nsmaller application (resulting from breaking down the monolith) into a separate container. These containers\ncan then be deployed and managed independently by different teams, fulfilling the requirement of\ndecentralized management. https://aws.amazon.com/ecs/\nApplication Load Balancer (ALB) provides intelligent routing of traffic to different containerized applications\nrunning on ECS. This aligns with the requirement of exposing the split-up applications via a load-balanced\nendpoint. ALB offers advanced features like content-based routing, enabling fine-grained control over how\ntraffic is distributed. https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\nECS, being a managed service, significantly reduces the operational overhead compared to managing EC2\ninstances directly (as in option C). You don't have to worry about patching, scaling, or maintaining the\nunderlying infrastructure. ECS also handles the complexities of container orchestration, enabling you to focus\non application development and deployment.\nECS integrates well with other AWS services, such as CloudWatch for monitoring and AWS Auto Scaling for\nautomatically adjusting the number of containers based on traffic. This ensures high availability and\nscalability.\nWhy other options are less suitable:\nA. AWS Lambda: Lambda is suitable for event-driven, stateless functions, but not ideal for hosting entire\napplications (even smaller, broken-down ones) that require more persistent resources or have more complex\narchitectures. It might be part of a larger solution, but not the core hosting platform as described in this\noption.\nB. AWS Amplify: Amplify is more focused on front-end development and mobile app backends. While it can\nconnect to back-end services, it's not the primary service to host the backend application components derived\nfrom the monolithic breakdown. It doesn't directly address the requirement of breaking down and managing\nthe backend.\nC. Amazon EC2 instances with Auto Scaling Group: While this option provides scalability, it involves\nsignificantly more operational overhead compared to ECS. You would need to manage the EC2 instances,\nincluding patching, security updates, and scaling the infrastructure. ECS abstracts away much of this\ncomplexity.\nIn conclusion, ECS with ALB is the most suitable solution because it allows for the decomposition of the\nmonolithic application into independently deployable and manageable containerized applications, while also\nminimizing operational overhead through a managed service and providing scalability through ALB and ECS\nfeatures.",
    "links": [
      "https://aws.amazon.com/ecs/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/"
    ]
  },
  {
    "question": "CertyIQ\nA company recently started using Amazon Aurora as the data store for its global ecommerce application. When\nlarge reports are run, developers report that the ecommerce application is performing poorly. After reviewing\nmetrics in Amazon CloudWatch, a solutions architect finds that the ReadIOPS and CPUUtilizalion metrics are\nspiking when monthly reports run.\nWhat is the MOST cost-effective solution?",
    "options": {
      "A": "Migrate the monthly reporting to Amazon Redshift.",
      "B": "Migrate the monthly reporting to an Aurora Replica.",
      "C": "Migrate the Aurora database to a larger instance class.",
      "D": "Increase the Provisioned IOPS on the Aurora instance."
    },
    "answer": "B",
    "explanation": "The correct answer is B: Migrate the monthly reporting to an Aurora Replica. Here's why:\nThe problem is that running large monthly reports on the primary Aurora instance causes performance\ndegradation for the ecommerce application due to increased ReadIOPS and CPU utilization. This means the\nreporting queries are impacting the operational workload.\nOption A, migrating the monthly reporting to Amazon Redshift, would certainly solve the performance issue\nby isolating the reporting workload. However, it is not the most cost-effective solution. Redshift requires data\nwarehousing infrastructure and ETL (Extract, Transform, Load) processes to move data from Aurora to\nRedshift. This incurs extra cost in terms of infrastructure, software, and development effort.\nOption B, migrating the monthly reporting to an Aurora Replica, provides a read-only copy of the data that can\nbe used for reporting without impacting the primary database. Aurora Replicas are designed for read scaling\nand offloading read workloads from the primary instance. This minimizes the impact on the ecommerce\napplication's performance. The cost of an Aurora Replica is generally lower than migrating to a separate data\nwarehouse like Redshift.\nOption C, migrating the Aurora database to a larger instance class, would increase the capacity of the primary\ndatabase. While this might improve performance for both the application and the reporting queries, it\naddresses the symptom rather than the root cause. This is a more expensive solution than using an Aurora\nReplica because it involves upgrading the primary database instance, which is used for all operations, not just\nreports.\nOption D, increasing the Provisioned IOPS on the Aurora instance, might help with the ReadIOPS spikes, but it\nwon't alleviate the CPU utilization. Also, Provisioned IOPS incurs additional costs. This also tackles the\nsymptom rather than isolating the problem, making it less effective and potentially more expensive than using\nan Aurora Replica.\nTherefore, using an Aurora Replica is the most cost-effective solution as it leverages Aurora's built-in read\nscaling capabilities to isolate the reporting workload from the primary database, addressing the performance\nissue directly without requiring expensive infrastructure changes or complex ETL processes.\nFor further research, consult the AWS documentation on Amazon Aurora Read Replicas:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.ReadReplicas.html and\nAurora features: https://aws.amazon.com/rds/aurora/features/",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.ReadReplicas.html",
      "https://aws.amazon.com/rds/aurora/features/"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts a website analytics application on a single Amazon EC2 On-Demand Instance. The analytics\nsoftware is written in PHP and uses a MySQL database. The analytics software, the web server that provides PHP,\nand the database server are all hosted on the EC2 instance. The application is showing signs of performance\ndegradation during busy times and is presenting 5xx errors. The company needs to make the application scale\nseamlessly.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "A": "Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use",
      "B": "Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use",
      "C": "Migrate the database to an Amazon Aurora MySQL DB instance. Create an AWS Lambda function to stop the",
      "D": "Authoritative Links:"
    },
    "answer": "D",
    "explanation": "The correct answer is D because it provides a scalable and cost-effective solution to address the application's\nperformance issues. Let's break down why:\nDatabase Migration to Aurora MySQL: Migrating the MySQL database to Amazon Aurora MySQL (a managed,\nMySQL-compatible relational database engine) addresses a major performance bottleneck. Aurora offers\nimproved performance and scalability compared to running MySQL on a single EC2 instance. This offloads\ndatabase operations from the EC2 instance, freeing up resources for the web application.\nAMI and Launch Template: Creating an Amazon Machine Image (AMI) of the web application ensures a\nconsistent and repeatable deployment. A launch template is then created based on the AMI, defining the\nconfiguration for new EC2 instances. This makes sure that the application deployment is consistent across all\ninstances.\nAuto Scaling Group (ASG): An Auto Scaling group automatically adjusts the number of EC2 instances based\non demand. It ensures the application scales seamlessly during peak loads and reduces costs during off-peak\nhours by terminating unnecessary instances.\nSpot Fleet Integration: Using Spot Instances in the ASG provides significant cost savings. Spot Instances are\nunused EC2 capacity offered at discounted prices. While Spot Instances can be interrupted, the ASG\nautomatically replaces terminated Spot Instances with new ones, maintaining the desired capacity.\nApplication Load Balancer (ALB): The Application Load Balancer distributes incoming traffic across the EC2\ninstances in the ASG. This ensures high availability and even load distribution, preventing any single instance\nfrom becoming overwhelmed.\nOption A is partially correct but doesn't offer cost optimization through spot instances or auto-scaling\ncapabilities. Option B has similar limitations with added complexity and cost by using weighted routing in\nRoute53, which isn't as dynamically responsive as ALB and ASG. Option C focuses on scaling up (vertical\nscaling) a single EC2 instance, instead of providing a scalable architecture based on multiple instances\n(horizontal scaling). Option C, while invoking a lambda function based on CPU utilization, lacks the ability to\nrespond to traffic automatically and cost-effectively like the Auto Scaling Group within option D.\nAuthoritative Links:\nAmazon Aurora: https://aws.amazon.com/rds/aurora/\nAmazon EC2 Auto Scaling: https://aws.amazon.com/autoscaling/\nAmazon EC2 Spot Instances: https://aws.amazon.com/ec2/spot/\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
    "links": [
      "https://aws.amazon.com/rds/aurora/",
      "https://aws.amazon.com/autoscaling/",
      "https://aws.amazon.com/ec2/spot/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a stateless web application in production on a group of Amazon EC2 On-Demand Instances behind\nan Application Load Balancer. The application experiences heavy usage during an 8-hour period each business\nday. Application usage is moderate and steady overnight. Application usage is low during weekends.\nThe company wants to minimize its EC2 costs without affecting the availability of the application.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Use Spot Instances for the entire workload.",
      "B": "Use Reserved Instances for the baseline level of usage. Use Spot instances for any additional capacity that",
      "C": "In summary, Reserved Instances secure a cost-effective baseline capacity, while Spot Instances provide an",
      "D": "Use Dedicated Instances for the baseline level of usage. Use On-Demand Instances for any additional"
    },
    "answer": "B",
    "explanation": "The optimal solution leverages the predictable usage patterns to minimize costs while maintaining availability.\nOption B (Reserved Instances for baseline and Spot Instances for peak) is the most cost-effective and robust\napproach.\nHere's why:\nReserved Instances (RIs): RIs offer significant discounts (up to 75%) compared to On-Demand Instances in\nexchange for a commitment to a specific instance type and region for a period (usually 1 or 3 years). The\nmoderate and steady overnight usage, as well as low weekend usage, defines a \"baseline\" capacity that is\npredictably required. RIs are ideally suited for this predictable baseline, providing guaranteed capacity at a\nreduced cost.\nSpot Instances: Spot Instances offer unused EC2 capacity at steep discounts (up to 90% off On-Demand\nprices). However, Spot Instances can be terminated with a two-minute warning if the Spot price exceeds the\nbid price. The heavy usage during the 8-hour period represents a peak demand above the baseline. Spot\nInstances are perfect for this additional, non-critical capacity. The stateless nature of the web application\nmakes it resilient to Spot Instance interruptions. The Application Load Balancer can simply redirect traffic to\nthe remaining instances (Reserved or Spot) if a Spot Instance is terminated.\nWhy other options are less suitable:\nOption A (Spot Instances only): Relying solely on Spot Instances is risky for production workloads requiring\nhigh availability. During periods of high demand, Spot prices can spike or capacity can become limited, leading\nto terminations and potential service disruptions.\nOption C (On-Demand + Spot): Using On-Demand Instances for the baseline is more expensive than using\nReserved Instances for a predictable usage pattern.\nOption D (Dedicated + On-Demand): Dedicated Instances are expensive and primarily used for compliance or\nlicensing reasons. They are not cost-effective for general web application hosting and are not necessary for\nthis scenario. The same cost consideration exists for On-Demand in the \"spike\" described in option C.\nIn summary, Reserved Instances secure a cost-effective baseline capacity, while Spot Instances provide an\naffordable means to handle peak demand, all while maintaining high availability thanks to the Application\nLoad Balancer and stateless architecture.\nAuthoritative Links:\nAmazon EC2 Reserved Instances: https://aws.amazon.com/ec2/reserved-instances/\nAmazon EC2 Spot Instances: https://aws.amazon.com/ec2/spot/\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
    "links": [
      "https://aws.amazon.com/ec2/reserved-instances/",
      "https://aws.amazon.com/ec2/spot/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to retain application log files for a critical application for 10 years. The application team\nregularly accesses logs from the past month for troubleshooting, but logs older than 1 month are rarely accessed.\nThe application generates more than 10 TB of logs per month.\nWhich storage option meets these requirements MOST cost-effectively?",
    "options": {
      "A": "Store the logs in Amazon S3. Use AWS Backup to move logs more than 1 month old to S3 Glacier Deep",
      "B": "Here's a detailed justification:",
      "C": "Store the logs in Amazon CloudWatch Logs. Use AWS Backup to move logs more than 1 month old to S3",
      "D": "Store the logs in Amazon CloudWatch Logs. Use Amazon S3 Lifecycle policies to move logs more than 1"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Here's a detailed justification:\nThe requirement is to store application logs for 10 years cost-effectively, with frequent access to the past\nmonth's logs and infrequent access to older logs. This scenario is a perfect fit for tiered storage using Amazon\nS3 and S3 Glacier Deep Archive.\nOption B is most cost-effective for the following reasons:\n1. Amazon S3 for Recent Logs: Storing logs in Amazon S3 initially allows for quick and easy access to\nthe most recent (1-month old) logs, which are frequently needed for troubleshooting. S3 offers high\navailability and durability, making it suitable for critical application logs.\n2. S3 Lifecycle Policies for Archiving: S3 Lifecycle policies automate the transition of older logs to S3\nGlacier Deep Archive. This is crucial for cost optimization because Glacier Deep Archive is the\ncheapest storage class, designed for long-term data archiving where retrieval times of several hours\nare acceptable.\n3. Cost Efficiency: By moving older, infrequently accessed logs to Glacier Deep Archive, the overall\nstorage cost is significantly reduced compared to keeping all logs in standard S3 storage.\n4. Automation: S3 Lifecycle policies automatically manage the data transition based on defined rules\n(age of the logs), eliminating manual intervention and potential errors.\nOptions A, C, and D are less optimal:\nOption A (AWS Backup with S3): AWS Backup is designed for data protection and disaster recovery, not for\ncost-effective long-term archival of log data. Using AWS Backup to move logs would be significantly more\nexpensive than S3 Lifecycle policies because Backup is charged based on protected storage and data\ntransfer, as well as requiring restore operation costs when needed.\nOption C (CloudWatch Logs with AWS Backup): CloudWatch Logs is suitable for real-time monitoring and\nanalysis of log data. While CloudWatch Logs can archive data to S3, it's generally more expensive for long-\nterm storage than directly storing and managing logs in S3. Additionally, using AWS Backup from\nCloudWatch Logs to Glacier Deep Archive incurs unnecessary costs associated with using Backup services\nwhere an S3 Lifecycle policy is sufficient.\nOption D (CloudWatch Logs with S3 Lifecycle): Similar to option C, storing logs directly in CloudWatch Logs\nbefore moving them to S3 introduces an unnecessary cost layer. It's more efficient and cheaper to store the\nlogs directly into S3 to begin with.\nIn summary, Option B leverages the cost-effective tiered storage approach provided by Amazon S3 and S3\nGlacier Deep Archive, utilizing S3 Lifecycle policies for automated data management and archiving.\nSupporting Documentation:\nAmazon S3 Storage Classes: https://aws.amazon.com/s3/storage-classes/\nS3 Lifecycle Policies: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-\nexamples.html\nS3 Glacier Deep Archive: https://aws.amazon.com/glacier/deep-archive/",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-",
      "https://aws.amazon.com/glacier/deep-archive/"
    ]
  },
  {
    "question": "CertyIQ\nA company has a data ingestion workflow that includes the following components:\nAn Amazon Simple Notification Service (Amazon SNS) topic that receives notifications about new data deliveries\nAn AWS Lambda function that processes and stores the data\nThe ingestion workflow occasionally fails because of network connectivity issues. When failure occurs, the\ncorresponding data is not ingested unless the company manually reruns the job.\nWhat should a solutions architect do to ensure that all notifications are eventually processed?",
    "options": {
      "A": "Configure the Lambda function for deployment across multiple Availability Zones.",
      "B": "Modify the Lambda function's configuration to increase the CPU and memory allocations for the function.",
      "C": "Configure the SNS topics retry strategy to increase both the number of retries and the wait time between",
      "D": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as the on-failure destination. Modify the"
    },
    "answer": "D",
    "explanation": "The correct answer is D because it implements a robust solution for handling intermittent failures in the data\ningestion workflow using a dead-letter queue (DLQ) mechanism.\nHere's a breakdown of why:\nProblem: The current architecture relies on direct invocation of the Lambda function by SNS. Network issues\ncause invocations to fail, leading to data loss unless manually retried. SNS doesn't natively retry failed\ndeliveries to Lambda indefinitely, leading to message loss.\nSolution with SQS DLQ: Configuring an SQS queue as a DLQ for failed SNS notifications ensures that\nmessages that cannot be delivered to the Lambda function due to temporary network issues are preserved.\nThis provides a reliable mechanism for capturing those failed notifications. The Lambda function can then be\nmodified to consume messages from this SQS queue. This separates the initial ingestion from the processing\nof failed messages, providing a safety net.\nWhy other options are incorrect:\nA: Configuring the Lambda function for deployment across multiple Availability Zones: Lambda is already\ndesigned to run across multiple AZs for high availability. While helpful for general resilience, this doesn't\naddress the specific issue of failed SNS deliveries due to transient network problems. The underlying issue is\nthat SNS gives up on delivery when it cannot successfully reach the function, regardless of the AZ.\nB: Modify the Lambda function's configuration to increase the CPU and memory allocations for the\nfunction: Increasing resources might help with function performance if it was timing out, but it doesn't\naddress the core problem of failed SNS deliveries due to network connectivity issues.\nC: Configure the SNS topics retry strategy to increase both the number of retries and the wait time\nbetween retries: While this sounds plausible, SNS retry policies have limitations. SNS does have retry policies\n(delivery policies), however SNS only retries for a specific number of attempts before discarding the message.\nThis still leads to message loss if the network issue persists beyond those retries. SQS as a DLQ is a better\napproach.\nBenefits of SQS DLQ:\nDurability: SQS provides durable message storage.\nReliability: Messages are guaranteed to be delivered at least once (and possibly more than once).\nDecoupling: Decouples the initial data ingestion from the processing of failed messages, allowing for\nindependent scaling and management.\nObservability: Provides visibility into failed messages, allowing for root cause analysis and debugging.\nSQS as a DLQ: Amazon SQS dead-letter queues are queues that source queues (or subscriptions) can target\nfor messages that cannot be processed successfully. A common use is for failure cases in message\nconsumption using services such as SNS.\nRelevant Documentation:\nUsing AWS Lambda with Amazon SQS\nAmazon SQS Dead-Letter Queues\nSNS Message Delivery Retries\nTherefore, configuring an SQS queue as the on-failure destination provides the most robust and reliable\nsolution for ensuring that all notifications are eventually processed, even in the presence of intermittent\nnetwork issues.",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has a service that produces event data. The company wants to use AWS to process the event data as it\nis received. The data is written in a specific order that must be maintained throughout processing. The company\nwants to implement a solution that minimizes operational overhead.\nHow should a solutions architect accomplish this?",
    "options": {
      "A": "Here's why:",
      "B": "Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing",
      "C": "Create an Amazon Simple Queue Service (Amazon SQS) standard queue to hold messages. Set up an AWS",
      "D": "Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's why:\nFIFO (First-In, First-Out) Queues: The core requirement is maintaining the order of events. Amazon SQS FIFO\nqueues guarantee that messages are processed in the exact order they were sent. This aligns directly with\nthe \"specific order that must be maintained\" requirement.\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\nAWS Lambda for Processing: AWS Lambda provides a serverless compute environment, which means the\ncompany doesn't have to manage servers. This fulfills the requirement of minimizing operational overhead.\nLambda can be easily triggered by SQS messages. https://aws.amazon.com/lambda/\nSQS and Lambda Integration: The tight integration between SQS and Lambda allows for automatic scaling\nand processing of messages as they arrive in the queue. Lambda functions can be configured to poll the SQS\nqueue and process messages concurrently, while maintaining the FIFO order within each message group if\nconfigured appropriately. This ensures that all messages are eventually processed.\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\nWhy other options are incorrect:\nB: SNS and Lambda: SNS is a pub/sub messaging service, which doesn't inherently guarantee message\nordering. It's designed for distributing messages to multiple subscribers concurrently, which contradicts the\nordering requirement.\nC: SQS Standard Queue and Lambda: Standard SQS queues offer best-effort ordering, but do not guarantee\nthat messages will be delivered in the exact order they were sent. This does not satisfy the ordering\nrequirement.\nD: SNS and SQS: While SNS can fan out messages to multiple SQS queues, SNS itself does not guarantee\nordering. Even if each SQS queue subscriber processed in order, the SNS delivery to different queues would\nstill break the global order. Furthermore, it requires more configuration than the simpler SQS FIFO and\nLambda option.\nTherefore, using SQS FIFO queues to hold the messages and using Lambda to process them is the most\nappropriate solution for processing event data while maintaining order and minimizing operational overhead.",
    "links": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html",
      "https://aws.amazon.com/lambda/",
      "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is migrating an application from on-premises servers to Amazon EC2 instances. As part of the\nmigration design requirements, a solutions architect must implement infrastructure metric alarms. The company\ndoes not need to take action if CPU utilization increases to more than 50% for a short burst of time. However, if the\nCPU utilization increases to more than 50% and read IOPS on the disk are high at the same time, the company\nneeds to act as soon as possible. The solutions architect also must reduce false alarms.\nWhat should the solutions architect do to meet these requirements?",
    "options": {
      "A": "Create Amazon CloudWatch composite alarms where possible.",
      "B": "Create Amazon CloudWatch dashboards to visualize the metrics and react to issues quickly: While",
      "C": "Create Amazon CloudWatch Synthetics canaries to monitor the application and raise an alarm:",
      "D": "Create single Amazon CloudWatch metric alarms with multiple metric thresholds where possible: Single"
    },
    "answer": "A",
    "explanation": "The correct answer is A: Create Amazon CloudWatch composite alarms where possible.\nHere's a detailed justification:\nThe scenario requires triggering an alarm only when both CPU utilization is high and read IOPS are high, and\nto avoid false alarms triggered by short bursts of CPU utilization.\nComposite alarms in CloudWatch are designed to solve this exact problem. They allow you to combine\nmultiple existing metric alarms (e.g., one for CPU utilization exceeding 50%, another for high read IOPS) into a\nsingle alarm. The composite alarm's state changes to ALARM only when all specified member alarms are in\nthe ALARM state. This directly addresses the requirement that the company needs to act only when both\nconditions (high CPU and high read IOPS) are true.\nOption A is correct because the composite alarm reduces false alarms. The company does not want to take\naction if CPU utilization increases to more than 50% for a short burst of time. This could be true, when using\none alarm metric only (CPU). Using a composite alarm that triggers only when CPU utilization is more than\n50% AND the read IOPS is high can reduce false positives.\nOptions B, C, and D are incorrect for the following reasons:\nB. Create Amazon CloudWatch dashboards to visualize the metrics and react to issues quickly: While\ndashboards are valuable for visualization and monitoring, they don't automate the alarming process or reduce\nfalse positives based on combined metric conditions. A human would have to constantly monitor the\ndashboard and manually react, which isn't scalable or reliable for timely responses.\nC. Create Amazon CloudWatch Synthetics canaries to monitor the application and raise an alarm:\nSynthetics canaries are used to monitor the availability and performance of web applications and APIs by\nsimulating user traffic. This doesn't directly address the need to monitor server-level metrics like CPU\nutilization and read IOPS and combine their conditions for alarming. This can lead to many false positives.\nD. Create single Amazon CloudWatch metric alarms with multiple metric thresholds where possible: Single\nmetric alarms can only have thresholds for one metric. While you can set a threshold for CPU utilization, you\ncannot incorporate read IOPS into the same single alarm's threshold. This is because a single alarm is\ndesigned to monitor one specific metric and trigger based on its behavior relative to pre-defined limits.\nIn summary, composite alarms provide the ability to define complex alarm conditions based on the states of\nmultiple metric alarms, making them ideal for scenarios requiring correlation of different metrics for effective\nalarming and reduced false positives.\nSupporting documentation:\nUsing composite alarms - Amazon CloudWatch:\nCloudWatch Alarms:",
    "links": []
  },
  {
    "question": "CertyIQ\nA company wants to migrate its on-premises data center to AWS. According to the company's compliance\nrequirements, the company can use only the ap-northeast-3 Region. Company administrators are not permitted to\nconnect VPCs to the internet.\nWhich solutions will meet these requirements? (Choose two.)",
    "options": {
      "A": "Use AWS Control Tower to implement data residency guardrails to deny internet access and deny access",
      "B": "Use rules in AWS WAF to prevent internet access. Deny access to all AWS Regions except ap-northeast-",
      "C": "Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining",
      "D": "Create an outbound rule for the network ACL in each VPC to deny all traffic from 0.0.0.0/0. Create an"
    },
    "answer": "A",
    "explanation": "The correct answer is AC. Here's why:\nA. Use AWS Control Tower to implement data residency guardrails to deny internet access and deny access\nto all AWS Regions except ap-northeast-3.\nAWS Control Tower is designed to manage and govern multi-account AWS environments. One of its core\nfunctionalities is to enforce policies and compliance across an organization. Control Tower's guardrails\nprovide preventative or detective controls. Data residency guardrails, specifically, can enforce that data\nresides within a specified region. Guardrails can also prevent resources from being created in non-compliant\nregions. Moreover, Control Tower can enforce guardrails that prevent VPCs from having direct internet\naccess, usually by denying the creation of Internet Gateways or NAT Gateways. This directly addresses both\nthe regional compliance and the no-internet-access requirements. Control Tower provides centralized\ngovernance.\nC. Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining\ninternet access. Deny access to all AWS Regions except ap-northeast-3.\nAWS Organizations allows you to centrally manage and govern multiple AWS accounts. Service Control\nPolicies (SCPs) are a key feature of Organizations. SCPs act as permission boundaries, controlling the\nmaximum permissions available to accounts within an organization. You can create an SCP that explicitly\ndenies the creation of Internet Gateways (IGWs) and NAT Gateways, which are the primary methods for a VPC\nto access the internet. You can also create an SCP that denies the creation of any resource in regions other\nthan ap-northeast-3. This prevents users from creating resources in unapproved regions, thereby ensuring\ncompliance. SCPs apply to all users and roles within the affected accounts (excluding the management\naccount root user) and can't be overridden by IAM policies within those accounts. This provides a strong,\ncentrally enforced compliance mechanism.\nWhy the other options are incorrect:\nB. Use rules in AWS WAF to prevent internet access. Deny access to all AWS Regions except ap-northeast-\n3 in the AWS account settings. AWS WAF protects web applications, not VPC infrastructure. There's no\n\"deny access to all AWS Regions\" setting in the account settings.\nD. Create an outbound rule for the network ACL in each VPC to deny all traffic from 0.0.0.0/0. Create an\nIAM policy for each user to prevent the use of any AWS Region other than ap-northeast-3. Network ACLs\nare a good security layer but can be complex to manage at scale across many VPCs. IAM policies per user,\nwhile effective, are difficult to manage compared to centralized control via SCPs, which provides centralized\ngovernance.\nE. Use AWS Config to activate managed rules to detect and alert for internet gateways and to detect and\nalert for new resources deployed outside of ap-northeast-3. AWS Config detects and alerts on non-\ncompliant configurations. It does not prevent the creation of resources or internet gateways, so it's detective,\nnot preventative. The requirement is to prevent internet access and resource creation in unapproved regions.\nSupporting Links:\nAWS Control Tower: https://aws.amazon.com/controltower/\nAWS Organizations: https://aws.amazon.com/organizations/\nService Control Policies (SCPs):\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html",
    "links": [
      "https://aws.amazon.com/controltower/",
      "https://aws.amazon.com/organizations/",
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
    ]
  },
  {
    "question": "CertyIQ\nA company uses a three-tier web application to provide training to new employees. The application is accessed for\nonly 12 hours every day. The company is using an Amazon RDS for MySQL DB instance to store information and\nwants to minimize costs.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Configure an IAM policy for AWS Systems Manager Session Manager. Create an IAM role for the policy.",
      "B": "Create an Amazon ElastiCache for Redis cache cluster that gives users the ability to access the data from",
      "C": "Launch an Amazon EC2 instance. Create an IAM role that grants access to Amazon RDS. Attach the role to",
      "D": "Create AWS Lambda functions to start and stop the DB instance. Create Amazon EventBridge (Amazon"
    },
    "answer": "D",
    "explanation": "The most cost-effective solution for stopping and starting an RDS instance based on a schedule is using AWS\nLambda and Amazon EventBridge (formerly CloudWatch Events). Lambda allows you to execute code without\nprovisioning or managing servers. EventBridge triggers Lambda functions based on a defined schedule.\nOption D provides a simple, serverless way to automate the on/off cycle of the RDS instance.\nOption A, while mentioning IAM, focuses on using Systems Manager Session Manager, which isn't directly\nrelated to scheduling the instance's on/off state. Systems Manager is typically used for managing EC2\ninstances, not directly controlling RDS start/stop functions.\nOption B involves implementing ElastiCache. ElastiCache is for caching data to improve application\nperformance, not for substituting a database while it's stopped. While technically feasible to store some data\nin ElastiCache, it adds unnecessary complexity and cost for the given requirement of cost optimization for an\nintermittently used RDS instance. The primary data store is the RDS database itself. Switching to and\ninvalidating caches is more complex than simply stopping and starting the RDS instance.\nOption C involves creating an EC2 instance and using cron jobs. While feasible, this approach requires\nmanaging an EC2 instance, increasing operational overhead and cost compared to a serverless approach. The\nEC2 instance would need to be running constantly to trigger the start/stop scripts, negating some of the cost\nsavings intended.\nLambda and EventBridge (Option D) eliminate the need to manage a dedicated server or complex caching\nstrategies. Lambda function execution time is charged by the millisecond, making it extremely cost-effective\nfor this use case. EventBridge provides a reliable and scalable scheduling mechanism. The Lambda functions\nwill only run when invoked by EventBridge, thus consuming very few resources overall and minimizing costs\nduring idle periods.\nIn summary, using Lambda and EventBridge provides a serverless, event-driven, and cost-optimized approach\nfor automatically starting and stopping an RDS instance based on a predefined schedule, directly addressing\nthe business requirement of minimizing costs while only needing access for 12 hours per day.\nSupporting Links:\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon EventBridge: https://aws.amazon.com/eventbridge/\nStarting and Stopping an RDS Instance:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StartInstance.html",
    "links": [
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/eventbridge/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StartInstance.html"
    ]
  },
  {
    "question": "CertyIQ\nA company sells ringtones created from clips of popular songs. The files containing the ringtones are stored in\nAmazon S3 Standard and are at least 128 KB in size. The company has millions of files, but downloads are\ninfrequent for ringtones older than 90 days. The company needs to save money on storage while keeping the most\naccessed files readily available for its users.\nWhich action should the company take to meet these requirements MOST cost-effectively?",
    "options": {
      "A": "Configure S3 Standard-Infrequent Access (S3 Standard-IA) storage for the initial storage tier of the objects.",
      "B": "Move the files to S3 Intelligent-Tiering and configure it to move objects to a less expensive storage tier after",
      "C": "Configure S3 inventory to manage objects and move them to S3 Standard-Infrequent Access (S3 Standard-",
      "D": "Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3\nStandard-Infrequent Access (S3 Standard-IA) after 90 days.\nHere's why:\nThe company needs to reduce storage costs for infrequently accessed ringtones (older than 90 days) while\nensuring frequently accessed files remain readily available. S3 Lifecycle policies are designed precisely for\nthis purpose. They automate the movement of objects between different storage classes based on predefined\nrules, like age.\nOption D directly addresses the requirements: An S3 Lifecycle policy can be configured to transition objects\nfrom S3 Standard (the initial storage class) to S3 Standard-IA after 90 days. This is a cost-effective way to\nstore infrequently accessed data while maintaining quick retrieval times when needed.\nLet's analyze why the other options are less optimal:\nOption A (S3 Standard-IA initially): While S3 Standard-IA is cheaper than S3 Standard, it incurs retrieval\ncosts. Initially storing all files in Standard-IA would unnecessarily add retrieval costs for files that are\nfrequently accessed in the first 90 days. This would be less cost-effective than using Standard initially.\nOption B (S3 Intelligent-Tiering): S3 Intelligent-Tiering automatically moves data between tiers based on\naccess patterns. While it sounds suitable, it includes a monthly monitoring and automation fee per object, and\nthe question specifies millions of files. For objects only accessed infrequently after 90 days, this monitoring\nfee can quickly surpass the cost savings of moving to infrequent access tiers, especially when a simpler\nlifecycle rule is sufficient. While efficient, intelligent tiering is more costly for data that is almost guaranteed\nto be accessed infrequently after a certain period.\nOption C (S3 Inventory and manual move): S3 Inventory provides a list of objects and their metadata. While\nuseful for auditing, it doesn't automate the transition. The question requires automation to reduce operational\noverhead. Using S3 Inventory to trigger manual moves would involve custom scripting or manual intervention,\nwhich is less efficient and more complex than an S3 Lifecycle policy.\nIn summary, S3 Lifecycle policies are the most straightforward and cost-effective method for automatically\nmoving objects between storage classes based on their age, directly addressing the company's needs without\nunnecessary complexity or additional fees.\nAuthoritative Links:\nS3 Lifecycle Policies: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-\nconsiderations.html\nS3 Storage Classes: https://aws.amazon.com/s3/storage-classes/\nS3 Intelligent-Tiering: https://aws.amazon.com/s3/intelligent-tiering/",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-",
      "https://aws.amazon.com/s3/storage-classes/",
      "https://aws.amazon.com/s3/intelligent-tiering/"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to save the results from a medical trial to an Amazon S3 repository. The repository must allow a\nfew scientists to add new files and must restrict all other users to read-only access. No users can have the ability\nto modify or delete any files in the repository. The company must keep every file in the repository for a minimum of\n1 year after its creation date.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Use S3 Object Lock in governance mode with a legal hold of 1 year.",
      "B": "Use S3 Object Lock in compliance mode with a retention period of 365 days.",
      "C": "Use an IAM role to restrict all users from deleting or changing objects in the S3 bucket. Use an S3 bucket",
      "D": "Configure the S3 bucket to invoke an AWS Lambda function every time an object is added. Configure the"
    },
    "answer": "B",
    "explanation": "The correct answer is B: Use S3 Object Lock in compliance mode with a retention period of 365 days. Here's\nwhy:\nRequirement for Immutability: The core requirement is that no user can modify or delete any files in the S3\nrepository. This necessitates immutability.\nS3 Object Lock: S3 Object Lock is designed to prevent objects from being deleted or overwritten for a fixed\namount of time or indefinitely. This addresses the immutability requirement directly. There are two modes:\ncompliance and governance.\nCompliance Mode: In compliance mode, even the root user cannot override the retention settings. Once a\nretention period is set, it's strictly enforced. This aligns perfectly with the strict requirement that no users can\ndelete or modify files.\nGovernance Mode: Governance mode allows users with specific IAM permissions to bypass the retention\nsettings. This is not suitable since the problem specifies that no users should be able to bypass the retention\npolicy.\nRetention Period of 365 Days: The requirement of keeping every file for a minimum of one year (365 days) is\nsatisfied by setting the retention period accordingly.\nWhy other options are incorrect:\nA: S3 Object Lock in governance mode with a legal hold of 1 year. While governance mode provides a level of\nimmutability, authorized users can remove legal holds. This contradicts the requirement that no users can\nmodify or delete files.\nC: IAM role to restrict all users from deleting or changing objects in the S3 bucket. Use an S3 bucket policy\nto only allow the IAM role. IAM and bucket policies can restrict access, but they don't guarantee\nimmutability. A user with sufficient permissions could still modify or delete objects. This is a preventative\nmeasure, not a guaranteed solution.\nD: Configure the S3 bucket to invoke an AWS Lambda function every time an object is added. Configure the\nfunction to track the hash of the saved object so that modified objects can be marked accordingly. This\napproach is complex, inefficient, and doesn't prevent modification or deletion. It only detects changes after\nthey occur, which violates the problem's core immutability requirement. S3 Object Lock provides a native and\nmuch simpler solution.\nIn summary, S3 Object Lock in compliance mode directly enforces immutability and the specified retention\nperiod, making it the most appropriate and efficient solution.\nRelevant Links:\nS3 Object Lock",
    "links": []
  },
  {
    "question": "CertyIQ\nA large media company hosts a web application on AWS. The company wants to start caching confidential media\nfiles so that users around the world will have reliable access to the files. The content is stored in Amazon S3\nbuckets. The company must deliver the content quickly, regardless of where the requests originate\ngeographically.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Use AWS DataSync to connect the S3 buckets to the web application.",
      "B": "Deploy AWS Global Accelerator to connect the S3 buckets to the web application.",
      "C": "Deploy Amazon CloudFront to connect the S3 buckets to CloudFront edge servers.",
      "D": "Use Amazon Simple Queue Service (Amazon SQS) to connect the S3 buckets to the web application."
    },
    "answer": "C",
    "explanation": "The correct answer is C: Deploy Amazon CloudFront to connect the S3 buckets to CloudFront edge servers.\nHere's a detailed justification:\nThe primary requirement is to cache confidential media files stored in S3 for fast and reliable delivery to users\nglobally. Amazon CloudFront is a content delivery network (CDN) specifically designed for this purpose.\nCloudFront caches content at edge locations distributed around the world. When a user requests a file,\nCloudFront serves it from the nearest edge location, minimizing latency and improving performance. Since the\ncontent is stored in S3, CloudFront can be configured as the front-end delivery mechanism.\nOption A, using AWS DataSync, is incorrect because DataSync is used for transferring large amounts of data\nbetween on-premises storage and AWS storage services. It is not designed for real-time content delivery or\ncaching.\nOption B, deploying AWS Global Accelerator, is not ideal for this scenario. Global Accelerator improves the\nperformance of TCP and UDP traffic by routing user traffic to the optimal AWS endpoint. While it can improve\nperformance, it doesn't provide content caching like CloudFront. Further, CloudFront offers fine-grained\ncontrol over content caching policies and integrates directly with S3 for content delivery.\nOption D, using Amazon SQS, is incorrect because SQS is a message queue service used for decoupling\ncomponents of distributed applications. It's not related to content delivery or caching. SQS does not cache\ncontent or improve latency for end users downloading media files.\nCloudFront offers several benefits for this use case:\nGlobal Reach: Edge locations are strategically located worldwide for low latency delivery.\nCaching: Content is cached at edge locations, reducing load on S3 and speeding up delivery.\nSecurity: Can be integrated with AWS WAF and provides features for securing content, including signed\nURLs.\nIntegration with S3: Seamless integration with S3 for origin storage.\nCustomizable caching behavior: CloudFront allows you to define how long content is cached and can\ninvalidate content based on your business needs.\nFor further reading:\nAmazon CloudFront Documentation\nAmazon S3 Documentation",
    "links": []
  },
  {
    "question": "CertyIQ\nA company produces batch data that comes from different databases. The company also produces live stream\ndata from network sensors and application APIs. The company needs to consolidate all the data into one place for\nbusiness analytics. The company needs to process the incoming data and then stage the data in different Amazon\nS3 buckets. Teams will later run one-time queries and import the data into a business intelligence tool to show key\nperformance indicators (KPIs).\nWhich combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)",
    "options": {
      "A": "Use Amazon Athena for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.",
      "B": "Use Amazon Kinesis Data Analytics for one-time queries. Use Amazon QuickSight to create dashboards for",
      "C": "Create custom AWS Lambda functions to move the individual records from the databases to an Amazon",
      "D": "Use an AWS Glue extract, transform, and load (ETL) job to convert the data into JSON format. Load the data"
    },
    "answer": "A",
    "explanation": "The best combination for this scenario is A and E.\nHere's why:\nA. Use Amazon Athena for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.\nAthena is a serverless, interactive query service that directly analyzes data in S3 using standard SQL. This\neliminates the need for managing infrastructure and simplifies one-time queries.\nhttps://aws.amazon.com/athena/\nQuickSight is a fully managed, serverless BI service that allows easy dashboard creation and KPI visualization.\nIt integrates seamlessly with Athena (and other AWS data sources). https://aws.amazon.com/quicksight/\nCombined, these services provide a straightforward, low-overhead solution for ad-hoc analysis and KPI\nreporting from the data consolidated in S3.\nE. Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake. Use\nAWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache Parquet\nformat.\nLake Formation simplifies the setup of a data lake, automatically discovering and cataloging data. Blueprints\nhelp automate the creation of data lakes from common data sources. https://aws.amazon.com/lake-formation/\nAWS Glue is a fully managed ETL service that automates the process of data discovery, transformation, and\nloading. Its crawlers can infer schema and load the data in different formats into S3.\nhttps://aws.amazon.com/glue/\nParquet is a columnar storage format, optimizing data retrieval for analytics and is ideal for Athena queries.\nBy using Lake Formation and Glue, the company minimizes operational overhead by automating the data\ningestion, transformation and cataloging processes.\nWhy other options are less ideal:\nB: Kinesis Data Analytics is designed for real-time streaming data analysis. It isn't the optimal choice for one-\ntime queries against batch data or staged data.\nC: Creating custom Lambda functions to move individual records from databases to Redshift is complex, high-\noverhead, and doesn't scale well. It increases operational burden and costs.\nD: OpenSearch Service is typically used for search and log analytics, not for general-purpose data\nwarehousing or BI workloads. Converting the data to JSON and loading it into OpenSearch is less efficient\nthan using Parquet and Athena. The need to manage multiple OpenSearch Clusters adds to overhead.",
    "links": [
      "https://aws.amazon.com/athena/",
      "https://aws.amazon.com/quicksight/",
      "https://aws.amazon.com/lake-formation/",
      "https://aws.amazon.com/glue/"
    ]
  },
  {
    "question": "CertyIQ\nA company stores data in an Amazon Aurora PostgreSQL DB cluster. The company must store all the data for 5\nyears and must delete all the data after 5 years. The company also must indefinitely keep audit logs of actions that\nare performed within the database. Currently, the company has automated backups configured for Aurora.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "options": {
      "A": "Take a manual snapshot of the DB cluster: While you could take a manual snapshot, this doesn't automate",
      "B": "Create a lifecycle policy for the automated backups: You cannot directly apply a lifecycle policy (like",
      "C": "Configure automated backup retention for 5 years: While this would meet the 5-year retention",
      "D": "Configure an Amazon CloudWatch Logs export for the DB cluster: This is crucial for meeting the"
    },
    "answer": "D",
    "explanation": "The correct answer is DE. Let's break down why:\nD. Configure an Amazon CloudWatch Logs export for the DB cluster: This is crucial for meeting the\nrequirement of indefinitely keeping audit logs. Aurora PostgreSQL generates audit logs, which can be\nstreamed to CloudWatch Logs. From there, you can retain them indefinitely, satisfying the audit log\nrequirement.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.Concepts.PostgreSQL.html\nand https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/postgresql_cloudwatch_logs.html\nE. Use AWS Backup to take the backups and to keep the backups for 5 years: AWS Backup provides a\ncentralized service to manage and automate backups across AWS services, including Aurora. You can define\nbackup plans that specify the backup frequency and retention period. Setting the retention period to 5 years\nensures the data is retained as required and automatically deleted after that period.\nhttps://aws.amazon.com/backup/\nNow, let's address why the other options are incorrect:\nA. Take a manual snapshot of the DB cluster: While you could take a manual snapshot, this doesn't automate\nthe 5-year retention and deletion requirement. It would require manual intervention to delete the snapshot\nafter 5 years. Also, it's a one-time event rather than a managed backup strategy.\nB. Create a lifecycle policy for the automated backups: You cannot directly apply a lifecycle policy (like\nthose used with S3) to RDS automated backups. Automated backups are managed by RDS itself and have a\nretention period configured within RDS.\nC. Configure automated backup retention for 5 years: While this would meet the 5-year retention\nrequirement, Aurora managed automated backups are not designed for long-term archiving like this. Using\nonly automated backups would also not address indefinite audit logging. Using AWS Backup is a best practice\nfor long-term retention and compliance of DB clusters.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.Concepts.PostgreSQL.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/postgresql_cloudwatch_logs.html",
      "https://aws.amazon.com/backup/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is optimizing a website for an upcoming musical event. Videos of the performances will be\nstreamed in real time and then will be available on demand. The event is expected to attract a global online\naudience.\nWhich service will improve the performance of both the real-time and on-demand streaming?",
    "options": {
      "A": "Amazon CloudFront",
      "B": "AWS Global Accelerator",
      "C": "Amazon Route 53",
      "D": "Amazon S3 Transfer Acceleration"
    },
    "answer": "A",
    "explanation": "Amazon CloudFront is the optimal choice because it's a content delivery network (CDN) designed to distribute\ncontent with low latency and high transfer speeds globally. For real-time streaming, CloudFront caches the\nstreaming content at edge locations closer to users, minimizing latency and buffering. This is achieved\nthrough its extensive network of edge servers strategically located worldwide. For on-demand streaming,\nCloudFront also caches video files, ensuring quick delivery to users regardless of their location.\nAWS Global Accelerator primarily focuses on improving TCP and UDP performance for applications over long\ndistances by routing traffic through optimized AWS network paths. While it could improve network\nperformance, it doesn't offer the caching benefits of CloudFront.\nAmazon Route 53 is a highly available and scalable DNS web service. It translates domain names into IP\naddresses, but it doesn't cache content or improve content delivery performance directly. While it can be\nintegrated with CloudFront, it is not a solution for improving both real-time and on-demand streaming\nperformance.\nAmazon S3 Transfer Acceleration accelerates transfers into and out of S3 buckets, but it doesn't directly\naddress the needs of live and on-demand streaming to a global audience in the same way a CDN like\nCloudFront does. It's more suitable for accelerating data uploads to S3 rather than content delivery.\nTherefore, CloudFronts caching capabilities, global edge locations, and integration with other AWS services\nmake it ideal for optimizing both real-time and on-demand streaming experiences for a global audience.\nHere are some authoritative links for further research:\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nAWS Global Accelerator: https://aws.amazon.com/global-accelerator/\nAmazon Route 53: https://aws.amazon.com/route53/\nAmazon S3 Transfer Acceleration: https://aws.amazon.com/s3/transfer-acceleration/",
    "links": [
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/global-accelerator/",
      "https://aws.amazon.com/route53/",
      "https://aws.amazon.com/s3/transfer-acceleration/"
    ]
  },
  {
    "question": "CertyIQ\nA company is running a publicly accessible serverless application that uses Amazon API Gateway and AWS\nLambda. The applications traffic recently spiked due to fraudulent requests from botnets.\nWhich steps should a solutions architect take to block requests from unauthorized users? (Choose two.)",
    "options": {
      "A": "Create a usage plan with an API key that is shared with genuine users only.",
      "B": "Integrate logic within the Lambda function to ignore the requests from fraudulent IP addresses: While",
      "C": "Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out.",
      "D": "Convert the existing public API to a private API. Update the DNS records to redirect users to the new API"
    },
    "answer": "A",
    "explanation": "The correct answer is AC. Here's a detailed justification:\nA. Create a usage plan with an API key that is shared with genuine users only.\nAPI Keys in Amazon API Gateway can be used to control access to your API and prevent unauthorized access.\nBy creating a usage plan and associating an API key with it, you can distribute the key to legitimate users\nonly. API Gateway then verifies the API key on each request and only allows requests with valid API keys to\nproceed. This approach helps block traffic from bots that do not possess the correct API key.\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html\nC. Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out.\nAWS WAF (Web Application Firewall) protects your web applications from common web exploits and bots\nthat could affect availability, compromise security, or consume excessive resources. WAF allows you to\ncreate custom rules that target specific request patterns, such as those originating from known botnet IP\naddresses or those that exhibit suspicious behavior. By setting up WAF rules to identify and block these\nmalicious requests, you can effectively filter out the fraudulent traffic hitting your API Gateway endpoint. The\nactions triggered could include blocking the requests, allowing the requests after inspection, or logging the\nrequests for further analysis. https://aws.amazon.com/waf/\nWhy other options are incorrect:\nB. Integrate logic within the Lambda function to ignore the requests from fraudulent IP addresses: While\nfeasible, this approach is less efficient and scalable. It adds complexity to the Lambda function, consumes\nLambda execution time checking IP addresses, and requires constant updates to the Lambda function as\nbotnets evolve. It's better to handle this at the API Gateway or WAF layer.\nD. Convert the existing public API to a private API. Update the DNS records to redirect users to the new API\nendpoint: Converting to a private API would indeed block public access. However, it doesn't solve the\nfundamental problem of distinguishing between legitimate and fraudulent users within the now-controlled\naccess. It would severely affect genuine public users, which is not desirable according to the question.\nE. Create an IAM role for each user attempting to access the API. A user will assume the role when making\nthe API call: Creating individual IAM roles for each user is overkill for a publicly accessible application dealing\nwith botnet attacks. IAM roles are better suited for internal or controlled access, not for scaling to potentially\nthousands or millions of users. Furthermore, bots could still potentially assume these roles if they can gain\naccess to user credentials.",
    "links": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html",
      "https://aws.amazon.com/waf/"
    ]
  },
  {
    "question": "CertyIQ\nAn ecommerce company hosts its analytics application in the AWS Cloud. The application generates about 300 MB\nof data each month. The data is stored in JSON format. The company is evaluating a disaster recovery solution to\nback up the data. The data must be accessible in milliseconds if it is needed, and the data must be kept for 30\ndays.\nWhich solution meets these requirements MOST cost-effectively?",
    "options": {
      "A": "Amazon OpenSearch Service (Amazon Elasticsearch Service)",
      "B": "Amazon S3 Glacier",
      "C": "Amazon S3 Standard",
      "D": "Amazon RDS for PostgreSQL"
    },
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C, Amazon S3 Standard, is the most cost-effective solution for\nthe given scenario, along with supporting concepts and authoritative links:\nThe problem requires a disaster recovery backup solution for 300 MB of JSON data generated monthly by an\nanalytics application. Key requirements are millisecond access time and a 30-day retention period. Cost-\neffectiveness is paramount.\nAmazon S3 Standard offers high durability, availability, and performance for frequently accessed data. It\nprovides low latency, making it suitable for the required millisecond access time. Its cost is relatively low\ncompared to alternatives that are designed for ultra-low latency transactional workloads. For 300MB, the\ncost is negligible. https://aws.amazon.com/s3/storage-classes/\nAmazon OpenSearch Service (Amazon Elasticsearch Service) is primarily used for search and analytics.\nWhile it provides fast access, it is significantly more expensive than S3 for simply storing and retrieving\n300MB of data. It also involves more operational overhead (managing clusters, scaling). It's overkill for just a\nbackup requirement.\nAmazon S3 Glacier is designed for long-term archival storage with infrequent access. Retrieval times range\nfrom minutes to hours, violating the millisecond access requirement. While Glacier is cheap for storage,\nretrieval costs can be substantial if the data is needed frequently.\nAmazon RDS for PostgreSQL is a relational database service, not suitable for storing JSON data as a simple\nbackup. Storing JSON in a database would introduce unnecessary complexity and cost for this backup\npurpose. Additionally, managing a database for a 300MB backup is extremely inefficient.\nConsidering the speed requirement and the need for cost optimization, Amazon S3 Standard emerges as the\nmost fitting choice. It offers the necessary performance while remaining budget-friendly for a small dataset.\nIt aligns perfectly with the characteristics of data backup where quick retrieval capabilities are important,\nmaking it a cost-effective solution to this disaster recovery concerns.",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/"
    ]
  },
  {
    "question": "CertyIQ\nA company has a small Python application that processes JSON documents and outputs the results to an on-\npremises SQL database. The application runs thousands of times each day. The company wants to move the\napplication to the AWS Cloud. The company needs a highly available solution that maximizes scalability and\nminimizes operational overhead.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Place the JSON documents in an Amazon S3 bucket. Run the Python code on multiple Amazon EC2 instances",
      "B": "Place the JSON documents in an Amazon S3 bucket. Create an AWS Lambda function that runs the Python",
      "C": "Place the JSON documents in an Amazon Elastic Block Store (Amazon EBS) volume. Use the EBS Multi-",
      "D": "Place the JSON documents in an Amazon Simple Queue Service (Amazon SQS) queue as messages. Deploy"
    },
    "answer": "B",
    "explanation": "Option B is the most suitable solution because it leverages serverless technologies and managed services to\nmeet the requirements of high availability, scalability, and minimal operational overhead. Placing JSON\ndocuments in an Amazon S3 bucket triggers an AWS Lambda function upon arrival of a new document.\nLambda functions are event-driven and automatically scale to handle incoming requests, eliminating the need\nfor manual server management. Lambda's pay-per-use model minimizes cost when the application is idle.\nThe Python code executed within the Lambda function processes the JSON documents. Storing the results in\nan Amazon Aurora DB cluster offers high availability and scalability. Aurora automatically manages database\ninfrastructure, including replication and failover, reducing operational burden. Aurora is also compatible with\nMySQL and PostgreSQL, simplifying migration from on-premises SQL databases.\nOption A requires managing EC2 instances, including patching, scaling, and ensuring high availability, which\nadds operational overhead. EBS Multi-Attach (Option C) has limitations on the number of instances and can\nintroduce complexity in managing shared storage. Amazon SQS with ECS (Option D) is a viable solution for\nasynchronous processing, but using EC2 launch type adds operational overhead for managing the underlying\nEC2 instances. Lambda is generally more cost-effective and requires less management for this use case.\nIn summary, Option B is the best choice as it combines the scalability and event-driven nature of Lambda with\nthe high availability and managed nature of Aurora, offering a serverless solution that minimizes operational\noverhead and maximizes scalability.\nReferences:\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon S3: https://aws.amazon.com/s3/\nAmazon Aurora: https://aws.amazon.com/rds/aurora/",
    "links": [
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/rds/aurora/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to use high performance computing (HPC) infrastructure on AWS for financial risk modeling. The\ncompanys HPC workloads run on Linux. Each HPC workflow runs on hundreds of Amazon EC2 Spot Instances, is\nshort-lived, and generates thousands of output files that are ultimately stored in persistent storage for analytics\nand long-term future use.\nThe company seeks a cloud storage solution that permits the copying of on-premises data to long-term persistent\nstorage to make data available for processing by all EC2 instances. The solution should also be a high performance\nfile system that is integrated with persistent storage to read and write datasets and output files.\nWhich combination of AWS services meets these requirements?",
    "options": {
      "A": "Amazon FSx for Lustre integrated with",
      "B": "Amazon FSx for Windows File Server: While FSx for Windows File Server is a fully managed, highly",
      "C": "Amazon S3 Glacier integrated with Amazon EBS: S3 Glacier is designed for long-term archival storage",
      "D": "Amazon S3 bucket with a VPC endpoint integrated with an Amazon Elastic Block Store (Amazon EBS)"
    },
    "answer": "A",
    "explanation": "The best solution for the company's HPC workload requirements is A. Amazon FSx for Lustre integrated with\nAmazon S3. Here's why:\nHigh-Performance File System: FSx for Lustre is designed for high-performance computing, machine\nlearning, and media processing workloads. It provides a parallel distributed file system that can handle the\ndemands of HPC applications. https://aws.amazon.com/fsx/lustre/\nIntegration with S3: FSx for Lustre seamlessly integrates with Amazon S3. You can configure it to\nautomatically copy data from S3 buckets to the file system for processing, and then export the output files\nback to S3 for long-term storage and analytics. This addresses the requirement of moving on-premises data\n(via S3) and storing output. https://docs.aws.amazon.com/fsx/latest/LustreGuide/import-data-repo.html\nScalability and Spot Instances: FSx for Lustre can scale to meet the demands of hundreds of EC2 Spot\nInstances, providing the necessary performance for the company's short-lived, intensive workloads.\nLinux Compatibility: Lustre is a Linux-based file system, which aligns with the company's requirement of\nrunning HPC workloads on Linux.\nPersistent Storage: Amazon S3 provides highly durable and scalable object storage, ensuring long-term\npersistence for the output files. https://aws.amazon.com/s3/\nWhy other options are not ideal:\nB. Amazon FSx for Windows File Server: While FSx for Windows File Server is a fully managed, highly\nreliable, and scalable file storage, it's optimized for Windows-based workloads, not Linux HPC workloads.\nC. Amazon S3 Glacier integrated with Amazon EBS: S3 Glacier is designed for long-term archival storage\nand is not suitable for the high-performance read/write requirements of HPC workflows. EBS is block storage\nattached to an EC2 instance, not designed for shared high performance access required by multiple\ninstances.\nD. Amazon S3 bucket with a VPC endpoint integrated with an Amazon Elastic Block Store (Amazon EBS)\nGeneral Purpose SSD (gp2) volume: While S3 is persistent, directly using S3 and EBS would not provide the\nhigh-performance parallel file system needed for the HPC workload. The general purpose SSD also isn't\nsuited for this task.\nIn conclusion, Amazon FSx for Lustre integrated with Amazon S3 provides the best combination of high\nperformance, scalability, Linux compatibility, and integration with persistent storage to meet the company's\nHPC workload requirements.",
    "links": [
      "https://aws.amazon.com/fsx/lustre/",
      "https://docs.aws.amazon.com/fsx/latest/LustreGuide/import-data-repo.html",
      "https://aws.amazon.com/s3/"
    ]
  },
  {
    "question": "CertyIQ\nA company is building a containerized application on premises and decides to move the application to AWS. The\napplication will have thousands of users soon after it is deployed. The company is unsure how to manage the\ndeployment of containers at scale. The company needs to deploy the containerized application in a highly available\narchitecture that minimizes operational overhead.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon",
      "B": "Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon",
      "C": "Store container images in a repository that runs on an Amazon EC2 instance. Run the containers on EC2",
      "D": "Create an Amazon EC2 Amazon Machine Image (AMI) that contains the container image. Launch EC2"
    },
    "answer": "A",
    "explanation": "The correct answer is A because it provides a fully managed, highly scalable, and highly available solution\nwith minimal operational overhead. Here's a breakdown:\nAmazon ECR (Elastic Container Registry): Storing container images in ECR provides a secure, scalable, and\nreliable registry for storing and managing your Docker container images. It eliminates the need to manage\nyour own container registry on EC2, reducing operational overhead. https://aws.amazon.com/ecr/\nAmazon ECS (Elastic Container Service) with Fargate Launch Type: ECS is a fully managed container\norchestration service. Using the Fargate launch type abstracts away the underlying EC2 infrastructure\nmanagement. Fargate manages the scaling, patching, and security of the underlying compute resources,\nsignificantly reducing operational overhead. https://aws.amazon.com/ecs/fargate/\nTarget Tracking Scaling: ECS allows you to automatically scale your services based on metrics like CPU\nutilization or request count. Target tracking automatically adjusts the number of tasks to maintain a specified\ntarget value for a chosen metric. This simplifies scaling and ensures optimal resource utilization.\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-autoscaling-targettracking.html\nOption B is incorrect because using the EC2 launch type requires you to manage the underlying EC2\ninstances, which introduces significant operational overhead, contradicting the problem's requirements.\nOptions C and D are even less suitable. They require manual management of EC2 instances for container\ndeployment and scaling, negating the requirement to minimize operational overhead. They also lack the\ninherent scalability and availability of managed container orchestration services like ECS with Fargate.\nBuilding container images into AMIs is not a best practice for container deployments; it's better to build\nimages separately and deploy them via container orchestration. Also, Option C suggests a container image\nrepository on an EC2 instance which could become a single point of failure.",
    "links": [
      "https://aws.amazon.com/ecr/",
      "https://aws.amazon.com/ecs/fargate/",
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-autoscaling-targettracking.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has two applications: a sender application that sends messages with payloads to be processed and a\nprocessing application intended to receive the messages with payloads. The company wants to implement an AWS\nservice to handle messages between the two applications. The sender application can send about 1,000 messages\neach hour. The messages may take up to 2 days to be processed: If the messages fail to process, they must be\nretained so that they do not impact the processing of any remaining messages.\nWhich solution meets these requirements and is the MOST operationally efficient?",
    "options": {
      "A": "Set up an Amazon EC2 instance running a Redis database. Configure both applications to use the instance.",
      "B": "Use an Amazon Kinesis data stream to receive the messages from the sender application. Integrate the",
      "C": "Integrate the sender and processor applications with an Amazon Simple Queue Service (Amazon SQS) queue.",
      "D": "Subscribe the processing application to an Amazon Simple Notification Service (Amazon SNS) topic to"
    },
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the best solution, and why the other options are less suitable:\nWhy Option C (Amazon SQS with Dead-Letter Queue) is the best:\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service. Its core purpose is\ndecoupling applications and enabling asynchronous communication. This directly addresses the company's\nrequirement to handle messages between the sender and processing applications. SQS is highly scalable,\nreliable, and requires minimal operational overhead.\nAsynchronous Communication: SQS allows the sender application to send messages without waiting for the\nprocessing application to be immediately available. This decoupling improves application resilience.\nMessage Retention: SQS retains messages until they are successfully processed and deleted, guaranteeing\nmessage delivery. The 2-day processing requirement falls well within SQS's configurable message retention\nperiod.\nFault Tolerance: The crucial aspect of a dead-letter queue (DLQ) makes this solution stand out. When a\nmessage fails to process after a specified number of retries, SQS moves it to the DLQ. This ensures that failed\nmessages don't block the processing of other messages, and allows for later analysis and reprocessing of the\nfailed messages. This aligns perfectly with the requirement to retain failed messages without impacting the\nrest.\nOperational Efficiency: SQS is a fully managed service, removing the need for the company to manage\nservers, scaling, or patching.\nCost-Effective: SQS pricing is based on usage, making it a cost-effective solution for handling 1,000\nmessages per hour.\nWhy Option A (Amazon EC2 with Redis) is less suitable:\nWhile Redis is a fast in-memory data store, using it as a message queue hosted on an EC2 instance introduces\nsignificant operational overhead.\nManagement Overhead: The company needs to manage the EC2 instance, including OS patching, scaling, and\nensuring high availability for Redis.\nComplexity: Implementing message queuing functionality in Redis requires custom code and logic, adding\ncomplexity.\nScalability Concerns: Scaling Redis effectively requires expertise and may involve sharding or clustering.\nNo Built-in DLQ: Redis does not have a built-in DLQ mechanism, requiring custom implementation to handle\nfailed messages.\nWhy Option B (Amazon Kinesis Data Streams) is less suitable:\nKinesis Data Streams is designed for real-time streaming data, like logs or sensor data, and isn't optimal for\nasynchronous message processing with specific ordering requirements or individual message handling in this\nscenario.\nComplexity: Kinesis Client Library (KCL) adds complexity to the processing application.\nOrdering focus: Kinesis shines at ordering large volumes of data within shards but the question specifies low\nthroughput (1000 messages/hour).\nNo native DLQ: Implementing DLQ functionality with Kinesis requires custom logic and extra storage.\nWhy Option D (Amazon SNS) is less suitable:\nAmazon SNS is primarily a publish/subscribe service for notifications, not a message queue.\nPush-Based: SNS pushes messages to subscribers, which is less reliable than SQS's pull-based model for\nguaranteed delivery.\nMessage Loss: If a subscriber is unavailable, the message may be lost. SNS is not ideal for applications that\nrequire guaranteed message delivery.\nNo DLQ: SNS does not have a built-in DLQ.\nAuthoritative Links:\nAmazon SQS: https://aws.amazon.com/sqs/\nSQS Dead-Letter Queues:\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-\nqueues.html\nAmazon Kinesis Data Streams: https://aws.amazon.com/kinesis/data-streams/\nAmazon SNS: https://aws.amazon.com/sns/\nIn summary, option C (SQS with a DLQ) is the most operationally efficient and best-suited solution because it\nprovides guaranteed message delivery, automatic handling of failed messages, and requires minimal\nmanagement overhead.",
    "links": [
      "https://aws.amazon.com/sqs/",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-",
      "https://aws.amazon.com/kinesis/data-streams/",
      "https://aws.amazon.com/sns/"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect must design a solution that uses Amazon CloudFront with an Amazon S3 origin to store a\nstatic website. The companys security policy requires that all website traffic be inspected by AWS WAF.\nHow should the solutions architect comply with these requirements?",
    "options": {
      "A": "Configure an S3 bucket policy to accept requests coming from the AWS WAF Amazon Resource Name (ARN)",
      "B": "Configure Amazon CloudFront to forward all incoming requests to AWS WAF before requesting content from",
      "C": "Configure a security group that allows Amazon CloudFront IP addresses to access Amazon S3 only.",
      "D": "Configure Amazon CloudFront and Amazon S3 to use an origin access identity (OAI) to restrict access to the"
    },
    "answer": "D",
    "explanation": "The correct answer is D because it addresses both security requirements: restricting S3 access and\ninspecting traffic with AWS WAF.\nRestricting S3 Access: Using an Origin Access Identity (OAI) with CloudFront is the recommended way to\nprevent users from directly accessing the S3 bucket hosting the static website. The OAI is a special\nCloudFront user that you grant permission to read objects in your S3 bucket. This ensures that all requests for\nyour content must go through CloudFront.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-\nto-s3.html\nInspecting Traffic with AWS WAF: AWS WAF is integrated with CloudFront, meaning you can associate a\nWAF web ACL with your CloudFront distribution. This allows AWS WAF to inspect incoming HTTP(S) requests\nto your website before they reach CloudFront, filtering out malicious traffic and protecting your application.\nhttps://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-integration.html\nWhy other options are incorrect:\nA: S3 bucket policies can restrict access based on various criteria, including source IP, but AWS WAF doesn't\nhave a fixed ARN that you can reliably use in an S3 bucket policy. Moreover, this approach doesn't actually\ninspect the traffic, it just tries to allow requests originating from where WAF is sitting. The problem WAF\nsolves is understanding the content of the traffic, not just its origin.\nB: CloudFront does not \"forward\" requests to WAF. WAF inspects requests made to the CloudFront endpoint\nand acts (allow/block) accordingly. There isn't a mechanism in CloudFront to forward traffic to WAF before\nretrieving the content.\nC: Security Groups are instance-level firewalls, not appropriate for S3 bucket access control or CloudFront\ntraffic filtering. While you can control access from EC2 instances to S3 buckets with security groups,\nCloudFront is a CDN, and its edge locations' IP addresses are not static or manageable through a security\ngroup in this manner. Plus, Security groups do not provide HTTP inspection capabilities like AWS WAF.\nTherefore, answer D directly addresses both the security and access control requirements most effectively\nand aligns with AWS best practices for securing static websites hosted on S3 behind CloudFront.",
    "links": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-",
      "https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-integration.html"
    ]
  },
  {
    "question": "CertyIQ\nOrganizers for a global event want to put daily reports online as static HTML pages. The pages are expected to\ngenerate millions of views from users around the world. The files are stored in an Amazon S3 bucket. A solutions\narchitect has been asked to design an efficient and effective solution.\nWhich action should the solutions architect take to accomplish this?",
    "options": {
      "A": "Generate presigned URLs for the files: Presigned URLs grant temporary access to objects in S3. While",
      "B": "Use cross-Region replication to all Regions: Cross-Region Replication (CRR) is beneficial for disaster",
      "C": "Use the geoproximity feature of Amazon Route 53: Route 53's geoproximity routing directs users to",
      "D": "Use Amazon CloudFront with the S3 bucket as its origin."
    },
    "answer": "D",
    "explanation": "The correct answer is D. Use Amazon CloudFront with the S3 bucket as its origin.\nHere's why:\nThe scenario describes a global event generating millions of views of static HTML pages stored in S3. The\nprimary concerns are performance, scalability, and cost-effectiveness in serving this content globally.\nAmazon CloudFront is a Content Delivery Network (CDN) service offered by AWS. It excels at distributing\nstatic content to users around the world with low latency and high transfer speeds. CloudFront caches the\ncontent in edge locations closer to users, reducing the distance data needs to travel, resulting in faster load\ntimes and a better user experience. By using the S3 bucket as the origin for CloudFront, the static HTML files\nare efficiently served to users worldwide. CloudFront also handles request routing, scaling, and security.\nLet's examine why the other options are less suitable:\nA. Generate presigned URLs for the files: Presigned URLs grant temporary access to objects in S3. While\nthey are useful for controlling access, they don't inherently improve performance or scalability for globally\naccessed static content. They also introduce overhead in generating and managing these URLs.\nB. Use cross-Region replication to all Regions: Cross-Region Replication (CRR) is beneficial for disaster\nrecovery or data sovereignty, not primarily for improving content delivery performance for globally distributed\nusers. It replicates data between S3 buckets in different regions. While it could technically reduce latency for\nsome users by having copies closer to them, it's much less efficient and more expensive than using a CDN like\nCloudFront. You'd need a complex routing mechanism to direct users to the nearest replicated bucket, which\nCloudFront handles automatically.\nC. Use the geoproximity feature of Amazon Route 53: Route 53's geoproximity routing directs users to\nresources based on their geographic location. This works well for dynamic content or applications running in\ndifferent regions. However, for static content already stored in S3, it is not enough by itself to optimize\ndistribution. You would still need a way to efficiently serve the content from those locations, which is precisely\nwhat CloudFront does by caching content at its edge locations. Routing to a single S3 bucket closer to the\nuser wouldn't provide the same level of performance as a globally distributed CDN.\nIn summary, CloudFront is the ideal solution because it is specifically designed for efficient and scalable\nglobal content delivery. It offers superior performance, scalability, and cost-effectiveness compared to the\nother options.\nSupporting Documentation:\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nAmazon S3: https://aws.amazon.com/s3/\nAmazon Route 53: https://aws.amazon.com/route53/",
    "links": [
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/route53/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a production application on a fleet of Amazon EC2 instances. The application reads the data from\nan Amazon SQS queue and processes the messages in parallel. The message volume is unpredictable and often\nhas intermittent traffic. This application should continually process messages without any downtime.\nWhich solution meets these requirements MOST cost-effectively?",
    "options": {
      "A": "Use Spot Instances exclusively to handle the maximum capacity required.",
      "B": "Use Reserved Instances exclusively to handle the maximum capacity required.",
      "C": "Use Reserved Instances for the baseline capacity and use Spot Instances to handle additional capacity.",
      "D": "Use Reserved Instances for the baseline capacity and use On-Demand Instances to handle additional"
    },
    "answer": "D",
    "explanation": "The correct answer is D: Use Reserved Instances for the baseline capacity and use On-Demand Instances to\nhandle additional capacity.\nHere's why:\nReserved Instances (RIs): Reserved Instances provide significant cost savings (up to 75%) compared to On-\nDemand Instances, but require a commitment to a specific instance type and region for a term of one or three\nyears. They are ideal for consistent, predictable workloads. In this scenario, the \"baseline capacity\" represents\nthe minimum EC2 resources always needed to process the SQS queue messages, making RIs a cost-effective\nchoice for this portion of the workload. (https://aws.amazon.com/ec2/pricing/reserved-instances/)\nOn-Demand Instances: On-Demand Instances allow you to pay for compute capacity by the hour or second\n(depending on the instance type and operating system) with no long-term commitments. They are well-suited\nfor unpredictable workloads, spikes in traffic, or applications that need to scale quickly. The problem\ndescription states \"unpredictable and often has intermittent traffic,\" implying the need for scalability to\nhandle these message volume variations. Using On-Demand instances for additional capacity during peak\ntimes addresses this without requiring a long-term commitment. (https://aws.amazon.com/ec2/pricing/on-\ndemand/)\nWhy not Spot Instances exclusively (A)? Spot Instances offer substantial cost savings (up to 90% compared\nto On-Demand), but they can be interrupted with little notice if the Spot price exceeds your bid. The problem\nstates the need for \"continually process messages without any downtime,\" which contradicts the potential for\ninterruption with Spot Instances if used exclusively.\nWhy not Reserved Instances exclusively (B)? While RIs guarantee capacity and save money for the baseline\nworkload, they might lead to over-provisioning if used exclusively to handle the maximum capacity required\nbecause the application only occasionally hits this maximum. This would result in unnecessary costs during\nperiods of low traffic.\nWhy not Reserved Instances for baseline and Spot for additional capacity (C)? While potentially cheaper\nthan pure On-Demand, using Spot Instances for handling unpredictable and intermittent traffic introduces the\nrisk of interruptions. If the Spot price spikes and the instances are terminated, the application could\nexperience downtime, violating the requirement to \"continually process messages without any downtime.\"\nThe small cost saving doesn't outweigh the risk to availabilty for a production workload that requires\ncontinuous message processing.\nTherefore, combining RIs for the steady-state baseline workload with On-Demand Instances for scaling during\npeaks provides the most cost-effective solution that also ensures continuous processing and avoids\ndowntime.",
    "links": [
      "https://aws.amazon.com/ec2/pricing/reserved-instances/)",
      "https://aws.amazon.com/ec2/pricing/on-"
    ]
  },
  {
    "question": "CertyIQ\nA security team wants to limit access to specific services or actions in all of the teams AWS accounts. All\naccounts belong to a large organization in AWS Organizations. The solution must be scalable and there must be a\nsingle point where permissions can be maintained.\nWhat should a solutions architect do to accomplish this?",
    "options": {
      "A": "Create an ACL to provide access to the services or actions.",
      "B": "Create a security group to allow accounts and attach it to user groups.",
      "C": "Create cross-account roles in each account to deny access to the services or actions.",
      "D": "Create a service control policy in the root organizational unit to deny access to the services or actions."
    },
    "answer": "D",
    "explanation": "The correct answer is D: Create a service control policy (SCP) in the root organizational unit (OU) to deny\naccess to the services or actions. Here's why:\nSCPs are the cornerstone of centralized permission management within AWS Organizations. They allow you to\ndefine guardrails that govern the maximum permissions available to accounts within an OU. By attaching an\nSCP to the root OU, the policy applies to all accounts in the organization, ensuring consistent access\nrestrictions across all AWS accounts. This centralized approach fulfills the requirement of a single point of\npermission maintenance, promoting scalability and reducing administrative overhead.\nSpecifically, in this scenario, the security team wants to limit access. SCPs are ideal for denying permissions,\neffectively creating a boundary of permissible actions within the organization. This contrasts with Identity and\nAccess Management (IAM) policies, which grant permissions. SCPs don't grant permissions; they restrict\nthem.\nOptions A, B, and C are not suitable solutions. Access Control Lists (ACLs) are used to control network traffic,\nnot IAM permissions. Security groups are also network-level controls and are not relevant to this scenario of\nlimiting access to specific AWS services or actions. Creating cross-account roles in each account to deny\naccess would be cumbersome, error-prone, and wouldn't scale effectively. It would also negate the single\npoint of maintenance requirement.\nTherefore, SCPs deployed at the root OU provide the most scalable and maintainable solution for\nimplementing consistent access restrictions across an AWS Organization. This approach leverages the power\nof AWS Organizations to enforce organizational-wide governance.\nFor further research, consult the AWS documentation on SCPs:\nAWS Organizations and service control policies (SCPs)\nService control policies (SCPs)",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is concerned about the security of its public web application due to recent web attacks. The application\nuses an Application Load Balancer (ALB). A solutions architect must reduce the risk of DDoS attacks against the\napplication.\nWhat should the solutions architect do to meet this requirement?",
    "options": {
      "A": "Add an Amazon Inspector agent to the ALB.",
      "B": "Amazon Macie (Option B) is a data security and data privacy service that uses machine learning and pattern",
      "C": "Enable AWS Shield Advanced to prevent attacks.",
      "D": "Configure Amazon GuardDuty to monitor the ALB."
    },
    "answer": "C",
    "explanation": "The correct answer is C: Enable AWS Shield Advanced to prevent attacks. Here's why:\nThe primary goal is to mitigate DDoS attacks against a public web application fronted by an Application Load\nBalancer (ALB). AWS Shield is a managed DDoS protection service that safeguards applications running on\nAWS.\nAWS Shield Advanced provides enhanced DDoS protection capabilities beyond AWS Shield Standard, which\nis automatically enabled for all AWS customers. Shield Advanced offers more sophisticated detection and\nmitigation techniques tailored to the specific application's traffic patterns and infrastructure. It provides 24x7\naccess to the AWS Shield Response Team (SRT) who can assist during a DDoS event. Crucially, it offers cost\nprotection against usage spikes during DDoS attacks.\nAmazon Inspector (Option A) is a vulnerability management service that automates security assessments and\nidentifies software vulnerabilities and unintended network exposure in EC2 instances and container images. It\ndoesn't directly prevent or mitigate DDoS attacks against an ALB.\nAmazon Macie (Option B) is a data security and data privacy service that uses machine learning and pattern\nmatching to discover and protect sensitive data. It focuses on identifying and securing sensitive information,\nnot preventing DDoS attacks.\nAmazon GuardDuty (Option D) is a threat detection service that monitors your AWS accounts and workloads\nfor malicious activity and unauthorized behavior. While GuardDuty can detect suspicious activity that might\nindicate a DDoS attack, it doesn't actively prevent or mitigate it. It's a detective control, not a preventative\none.\nShield Advanced integrates seamlessly with ALBs and provides Layer 7 protection, which is critical for\nmitigating application-layer DDoS attacks that target specific URLs or API endpoints. Therefore, AWS Shield\nAdvanced is the most appropriate solution to proactively reduce the risk of DDoS attacks against the\ncompany's web application.\nFurther Reading:\nAWS Shield: https://aws.amazon.com/shield/\nAWS Shield Advanced: https://aws.amazon.com/shield/advanced/",
    "links": [
      "https://aws.amazon.com/shield/",
      "https://aws.amazon.com/shield/advanced/"
    ]
  },
  {
    "question": "CertyIQ\nA companys web application is running on Amazon EC2 instances behind an Application Load Balancer. The\ncompany recently changed its policy, which now requires the application to be accessed from one specific country\nonly.\nWhich configuration will meet this requirement?",
    "options": {
      "A": "Security",
      "B": "Configure the security group on the Application Load Balancer.",
      "C": "Here's a detailed justification:",
      "D": "Configure the network ACL for the subnet that contains the EC2 instances."
    },
    "answer": "C",
    "explanation": "The correct answer is C: Configure AWS WAF on the Application Load Balancer in a VPC.\nHere's a detailed justification:\nAWS WAF (Web Application Firewall) allows you to control access to your web applications based on\nspecified rules. One of its key features is geographic filtering, enabling you to allow or block traffic\noriginating from specific countries. By configuring AWS WAF on the Application Load Balancer (ALB), you can\ncreate a rule that permits traffic only from the required country. WAF operates at the application layer (Layer\n7), providing granular control over HTTP/HTTPS requests.\nOption A, configuring the security group for the EC2 instances, is less effective. While security groups provide\nbasic ingress/egress filtering, they primarily operate on IP addresses and ports. Geolocation is not inherently\nsupported within security groups. You would need a mechanism to constantly update the security group with\nIP address ranges for the desired country, which is a complex and unreliable approach due to the constantly\nchanging IP address space.\nOption B, configuring the security group on the ALB, suffers from the same limitations as Option A. Security\ngroups cannot perform native geolocation filtering.\nOption D, configuring the network ACL (NACL) for the subnet, is also insufficient. NACLs operate at the\nsubnet level (Layer 3 and 4) and filter traffic based on IP addresses and ports. While it is possible to configure\nNACLs based on IP ranges, maintaining a current list of all IP ranges for a specific country would be\nchallenging, complex, and not highly reliable, similar to the security group limitation. Also, NACLs don't have\nthe application-layer inspection capabilities of WAF.\nAWS WAF's Country-based rules offer a much simpler and more effective solution for geo-filtering. WAF\nintegrates directly with the ALB and can readily identify the country of origin for incoming requests, allowing\nyou to enforce the specified policy requirement. WAF rules also have the advantage of being easily updated\nand managed through the AWS Management Console or AWS CLI/SDK. Running the ALB in a VPC ensures a\nsecure and isolated networking environment.\nFurther resources:\nAWS WAF: https://aws.amazon.com/waf/\nAWS WAF documentation: https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\nSecurity Groups: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\nNetwork ACLs: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html",
    "links": [
      "https://aws.amazon.com/waf/",
      "https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html"
    ]
  },
  {
    "question": "CertyIQ\nA company provides an API to its users that automates inquiries for tax computations based on item prices. The\ncompany experiences a larger number of inquiries during the holiday season only that cause slower response\ntimes. A solutions architect needs to design a solution that is scalable and elastic.\nWhat should the solutions architect do to accomplish this?",
    "options": {
      "A": "Provide an API hosted on an Amazon EC2 instance: EC2 instances require manual scaling, which might not",
      "B": "Design a REST API using Amazon API Gateway that accepts the item",
      "C": "Create an Application Load Balancer that has two Amazon EC2 instances behind it: Similar to option A,",
      "D": "Design a REST API using Amazon API Gateway that connects with an API hosted on an Amazon EC2"
    },
    "answer": "B",
    "explanation": "The most appropriate solution is B. Design a REST API using Amazon API Gateway that accepts the item\nnames. API Gateway passes item names to AWS Lambda for tax computations.\nHere's why:\nScalability and Elasticity: API Gateway and Lambda are inherently designed for scalability and elasticity. API\nGateway can handle a large number of concurrent requests and automatically scales to meet demand without\nany manual intervention. Lambda functions automatically scale by running copies of your function in parallel\nto handle each incoming request. During the holiday season, when the number of inquiries surges, these\nservices can effortlessly handle the increased load.\nCost-Effectiveness: Lambda functions are priced based on the actual compute time consumed. This pay-per-\nuse model is ideal for workloads with intermittent spikes in demand, such as during holiday seasons. You only\npay for the compute time used to process the API requests, making it more cost-effective compared to\nrunning EC2 instances continuously.\nReduced Operational Overhead: Lambda eliminates the need to manage servers, apply patches, or perform\ncapacity planning. API Gateway simplifies API management tasks such as authentication, authorization, rate\nlimiting, and monitoring. This reduces the operational overhead associated with managing the API.\nBetter Performance: AWS Lambda runs your code in response to events and automatically manages the\nunderlying compute resources for you, ensuring optimal performance.\nLet's examine why the other options are less suitable:\nA. Provide an API hosted on an Amazon EC2 instance: EC2 instances require manual scaling, which might not\nbe fast enough to accommodate sudden spikes in demand. Also, even when idle, EC2 instances incur costs.\nEC2 instances do not provide the level of inherent scalability and elasticity that API Gateway and Lambda\noffer.\nC. Create an Application Load Balancer that has two Amazon EC2 instances behind it: Similar to option A,\nEC2 instances behind an ALB require more management and cost more, especially during low demand\nperiods. Load Balancing is a great step, but doesn't scale to zero.\nD. Design a REST API using Amazon API Gateway that connects with an API hosted on an Amazon EC2\ninstance: This option introduces API Gateway but then still relies on an EC2 instance for computation. While\nAPI Gateway can handle the incoming traffic, the EC2 instance will become a bottleneck and require manual\nscaling. The benefits of API Gateway (e.g., security, rate limiting) are present, but the fundamental scalability\nissue remains.\nIn summary, API Gateway and Lambda provide the best combination of scalability, elasticity, cost-\neffectiveness, and reduced operational overhead, making option B the most suitable solution.\nAuthoritative Links:\nAWS API Gateway: https://aws.amazon.com/api-gateway/\nAWS Lambda: https://aws.amazon.com/lambda/",
    "links": [
      "https://aws.amazon.com/api-gateway/",
      "https://aws.amazon.com/lambda/"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is creating a new Amazon CloudFront distribution for an application. Some of the information\nsubmitted by users is sensitive. The application uses HTTPS but needs another layer of security. The sensitive\ninformation should.be protected throughout the entire application stack, and access to the information should be\nrestricted to certain applications.\nWhich action should the solutions architect take?",
    "options": {
      "A": "Configure a CloudFront signed URL.",
      "B": "Configure a CloudFront signed cookie.",
      "C": "Configure a CloudFront field-level encryption profile.",
      "D": "Configure CloudFront and set the Origin Protocol Policy setting to HTTPS Only for the Viewer Protocol"
    },
    "answer": "C",
    "explanation": "The correct answer is C: Configure a CloudFront field-level encryption profile.\nHere's a detailed justification:\nThe scenario requires protecting sensitive user data throughout the application stack and restricting access\nto it for specific applications. Field-level encryption (FLE) within CloudFront addresses this directly. FLE\nallows encrypting specific data fields at the edge (CloudFront) so that only the applications with the\ncorresponding decryption key can decrypt and access the data. This ensures that even if the data is\nintercepted en route, it remains unreadable without the key.\nOption A (CloudFront signed URLs) and Option B (CloudFront signed cookies) primarily control access to\ncontent, not the encryption of specific data fields within the content. Signed URLs/cookies are used for\nauthentication and authorization, allowing only authenticated users access to the resources. They don't\nencrypt the data itself. While they enhance security by controlling who can access the application, they don't\nprovide data-level encryption.\nOption D (Origin Protocol Policy to HTTPS Only) ensures that the communication between CloudFront and the\norigin server is encrypted, which is a good security practice, but it doesn't encrypt the specific sensitive fields\nsubmitted by the users. It ensures data in transit between CloudFront and the origin is secure but doesn't\nrestrict access to the data once it reaches the origin, nor does it prevent access to the data by all applications\non the origin.\nField-level encryption is precisely designed to encrypt specific sensitive data at the edge before it even\nreaches the origin, offering end-to-end protection and restricting access to only the intended applications\nthat possess the decryption key. It addresses the core requirements of the question regarding protecting\nsensitive user data and restricting application access.\nFor further research, consult these AWS resources:\nAWS CloudFront Field-Level Encryption:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html\nUsing Signed URLs: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-\ncontent-signed-urls.html\nUsing Signed Cookies: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-\ncontent-signed-cookies.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-"
    ]
  },
  {
    "question": "CertyIQ\nA gaming company hosts a browser-based application on AWS. The users of the application consume a large\nnumber of videos and images that are stored in Amazon S3. This content is the same for all users.\nThe application has increased in popularity, and millions of users worldwide accessing these media files. The\ncompany wants to provide the files to the users while reducing the load on the origin.\nWhich solution meets these requirements MOST cost-effectively?",
    "options": {
      "A": "Deploy an AWS Global Accelerator accelerator in front of the web servers.",
      "B": "Deploy an Amazon CloudFront web distribution in front of the S3 bucket.",
      "C": "Deploy an Amazon ElastiCache for Redis instance in front of the web servers.",
      "D": "Deploy an Amazon ElastiCache for Memcached instance in front of the web servers."
    },
    "answer": "B",
    "explanation": "The correct answer is B. Deploy an Amazon CloudFront web distribution in front of the S3 bucket.\nHere's a detailed justification:\nThe scenario describes a situation where static content (videos and images) from an S3 bucket is being\nheavily accessed globally, leading to potential load issues on the origin (S3). The primary goal is to reduce the\nload on S3 and provide low-latency access to users worldwide, while optimizing for cost.\nCloudFront is a Content Delivery Network (CDN) service. A CDN caches content at edge locations distributed\nglobally, which allows users to retrieve content from a server that's geographically closer to them. This\nsignificantly reduces latency and improves the user experience. By placing CloudFront in front of the S3\nbucket, the most frequently accessed videos and images will be cached at CloudFront edge locations. When a\nuser requests content, CloudFront will first check its cache. If the content is available (a \"cache hit\"),\nCloudFront will serve the content directly from the edge location, without hitting the S3 bucket. This\ndrastically reduces the load on S3 and lowers data transfer costs because data is served from the closest\nlocation.\nOption A (AWS Global Accelerator) is not the best choice for this scenario. While Global Accelerator also\nimproves performance by routing user traffic to optimal endpoints, its primary focus is on accelerating\ndynamic content and improving the reliability of applications. It doesn't cache content like CloudFront,\ntherefore won't reduce the load on S3 for static assets.\nOptions C (ElastiCache for Redis) and D (ElastiCache for Memcached) are in-memory data caching services.\nThey are typically used to cache database query results or frequently accessed data within the application\nlayer, not to serve static content directly from S3. They are more suited for improving the performance of the\napplication logic rather than reducing the load on static content delivery from an object store like S3.\nDeploying them in front of web servers would cache data generated by the servers, not the S3 content.\nTherefore, CloudFront is the most cost-effective solution because it directly addresses the issue of high\ntraffic to S3 for static content, reduces latency through edge caching, and minimizes S3 data transfer costs.\nFurther research:\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nAWS Global Accelerator: https://aws.amazon.com/global-accelerator/\nAmazon ElastiCache: https://aws.amazon.com/elasticache/",
    "links": [
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/global-accelerator/",
      "https://aws.amazon.com/elasticache/"
    ]
  },
  {
    "question": "CertyIQ\nA company has a multi-tier application that runs six front-end web servers in an Amazon EC2 Auto Scaling group in\na single Availability Zone behind an Application Load Balancer (ALB). A solutions architect needs to modify the\ninfrastructure to be highly available without modifying the application.\nWhich architecture should the solutions architect choose that provides high availability?",
    "options": {
      "A": "Create an Auto Scaling group that uses three instances across each of two Regions: This introduces",
      "B": "Modify the Auto Scaling group to use three instances across each of two",
      "C": "Create an Auto Scaling template that can be used to quickly create more instances in another Region:",
      "D": "Change the ALB in front of the Amazon EC2 instances in a round-robin configuration to balance traffic to"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Modify the Auto Scaling group to use three instances across each of two\nAvailability Zones.\nHigh availability fundamentally means ensuring your application remains accessible even if a component fails.\nIn the given scenario, the application currently resides within a single Availability Zone (AZ). If that AZ\nexperiences an outage, the entire application becomes unavailable. To mitigate this risk, it's crucial to\ndistribute the application across multiple AZs within the same AWS Region.\nOption B achieves this by modifying the Auto Scaling group to span two AZs. This ensures that if one AZ fails,\nthe remaining instances in the other AZ can continue to serve traffic, thus maintaining application availability.\nThe Application Load Balancer (ALB) is inherently designed to distribute traffic across multiple targets (EC2\ninstances) registered with it, even if those targets are in different AZs within the same Region.\nLet's examine why the other options are less suitable:\nA. Create an Auto Scaling group that uses three instances across each of two Regions: This introduces\ncomplexity and potentially higher latency due to cross-region communication. While it offers disaster\nrecovery capabilities, it's overkill for the high availability requirement stated in the question, which typically\nfocuses on single-region resilience. Implementing cross-region Auto Scaling also requires mechanisms for\ndata replication and traffic management across regions, adding to the complexity.\nC. Create an Auto Scaling template that can be used to quickly create more instances in another Region:\nThis only provides a mechanism for recovery after a failure. It doesn't inherently provide high availability\nbecause the application will still be unavailable until the new instances are provisioned in the other region.\nHigh availability implies continuous operation with minimal downtime.\nD. Change the ALB in front of the Amazon EC2 instances in a round-robin configuration to balance traffic to\nthe web tier: The ALB already balances traffic. Round-robin is a balancing algorithm that ALB supports, but\nchanging the balancing configuration doesn't address the fundamental issue of single-AZ dependency. The\nALB can't route traffic to instances that don't exist or are unavailable within another AZ. Furthermore, ALB\nconfiguration is not the focus of the question; the question asks about high availability achieved through\ninstance distribution.\nTherefore, the best approach is to modify the existing Auto Scaling group to distribute instances across\nmultiple AZs, leveraging the ALB's built-in capabilities to distribute traffic among them. This provides the\nrequired high availability with the least amount of modification to the existing infrastructure.\nSupporting Links:\nAWS Auto Scaling: https://aws.amazon.com/autoscaling/\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\nAWS Regions and Availability Zones: https://aws.amazon.com/about-aws/global-infrastructure/",
    "links": [
      "https://aws.amazon.com/autoscaling/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
      "https://aws.amazon.com/about-aws/global-infrastructure/"
    ]
  },
  {
    "question": "CertyIQ\nAn ecommerce company has an order-processing application that uses Amazon API Gateway and an AWS Lambda\nfunction. The application stores data in an Amazon Aurora PostgreSQL database. During a recent sales event, a\nsudden surge in customer orders occurred. Some customers experienced timeouts, and the application did not\nprocess the orders of those customers.\nA solutions architect determined that the CPU utilization and memory utilization were high on the database\nbecause of a large number of open connections. The solutions architect needs to prevent the timeout errors while\nmaking the least possible changes to the application.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Configure provisioned concurrency for the Lambda function. Modify the database to be a global",
      "B": "Use Amazon RDS Proxy to create a proxy for the database. Modify the Lambda",
      "C": "Create a read replica for the database in a different AWS Region. Use query string parameters in API",
      "D": "Migrate the data from Aurora PostgreSQL to Amazon DynamoDB by using AWS Database Migration"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Use Amazon RDS Proxy to create a proxy for the database. Modify the Lambda\nfunction to use the RDS Proxy endpoint instead of the database endpoint.\nHere's a detailed justification:\nThe problem is high CPU and memory utilization on the Aurora PostgreSQL database due to a large number of\nopen connections from the Lambda function during peak load. This leads to timeouts for customers because\nthe database cannot handle the connection surge.\nWhy RDS Proxy is the best solution:\nConnection Pooling: RDS Proxy sits between the Lambda function and the database, effectively pooling\ndatabase connections. Instead of the Lambda function directly opening and closing connections to the\ndatabase for each invocation, it requests a connection from the RDS Proxy. The RDS Proxy maintains a pool of\nconnections to the database and reuses them efficiently. This dramatically reduces the overhead on the\ndatabase, freeing up CPU and memory resources.\nConnection Multiplexing: RDS Proxy can multiplex connections from the Lambda function to the database,\nallowing a smaller number of database connections to serve a larger number of Lambda function invocations.\nThis is particularly important in serverless architectures like Lambda, where frequent invocations can\noverwhelm the database with connection requests.\nReduced Database Load: By reducing the number of open connections, RDS Proxy alleviates the CPU and\nmemory pressure on the database, preventing timeouts and improving overall application performance.\nMinimal Code Changes: Using RDS Proxy requires minimal changes to the application. Only the database\nendpoint in the Lambda function needs to be updated to point to the RDS Proxy endpoint.\nLeast Disruptive: The RDS Proxy approach is the least disruptive to the existing architecture. It avoids major\nchanges like migrating databases or introducing complex routing logic.\nWhy other options are less suitable:\nA. Configure provisioned concurrency for the Lambda function. Modify the database to be a global\ndatabase in multiple AWS Regions. Provisioned concurrency for Lambda addresses cold starts and helps\nwith consistent performance, but it doesn't directly solve the database connection overload issue. A global\ndatabase provides disaster recovery and low latency access to geographically distributed users, but it doesn't\naddress the connection management problem. It also introduces more complexity and cost.\nC. Create a read replica for the database in a different AWS Region. Use query string parameters in API\nGateway to route traffic to the read replica. This solution involves using query string parameters to direct\ntraffic to a read replica. While read replicas can offload read traffic, the described problem involves\ntransactional order processing which requires write operations, which must still be routed to the primary\ndatabase. Also, complex routing in API Gateway can increase latency and complexity.\nD. Migrate the data from Aurora PostgreSQL to Amazon DynamoDB by using AWS Database Migration\nService (AWS DMS). Modify the Lambda function to use the DynamoDB table. Migrating to DynamoDB is a\nsignificant architectural change. DynamoDB is suitable for different use cases. An RDBMS like Aurora\nPostgres is better for transactional workloads, and it introduces complexity and cost. DynamoDB is not\nsuitable for this workload, and this option is a disproportionately complex solution.\nSupporting Links:\nAmazon RDS Proxy: https://aws.amazon.com/rds/proxy/\nUsing Amazon RDS Proxy with AWS Lambda: https://aws.amazon.com/blogs/database/using-amazon-rds-\nproxy-with-aws-lambda/",
    "links": [
      "https://aws.amazon.com/rds/proxy/",
      "https://aws.amazon.com/blogs/database/using-amazon-rds-"
    ]
  },
  {
    "question": "CertyIQ\nAn application runs on Amazon EC2 instances in private subnets. The application needs to access an Amazon\nDynamoDB table.\nWhat is the MOST secure way to access the table while ensuring that the traffic does not leave the AWS network?",
    "options": {
      "A": "Use a VPC endpoint for DynamoDB.",
      "B": "Use a NAT gateway in a public subnet.",
      "C": "Use a NAT instance in a private subnet.",
      "D": "Use the internet gateway attached to the VPC."
    },
    "answer": "A",
    "explanation": "The most secure way for EC2 instances in private subnets to access a DynamoDB table without the traffic\nleaving the AWS network is to use a VPC endpoint for DynamoDB (Option A).\nHere's why:\nVPC Endpoints: VPC endpoints enable private connectivity to supported AWS services, including DynamoDB,\nfrom within your VPC without requiring an internet gateway, NAT device, VPN connection, or AWS Direct\nConnect connection. They keep all traffic within the AWS network, enhancing security.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\nSecurity: VPC endpoints eliminate the need to route traffic through the public internet, mitigating exposure\nto potential threats and vulnerabilities. They also support VPC endpoint policies, enabling granular control\nover which DynamoDB resources can be accessed by which principals.\nCost: While there's a cost associated with VPC endpoints (per Availability Zone), it is generally more cost-\neffective than using NAT gateways for high-volume traffic, especially when considering data transfer costs to\nand from NAT gateways.\nNAT Gateway (Option B): A NAT gateway allows instances in a private subnet to connect to the internet or\nother AWS services, but it requires routing traffic through a public subnet. This approach is less secure than a\nVPC endpoint because it involves internet connectivity.\nNAT Instance (Option C): A NAT instance serves a similar purpose to a NAT gateway but requires manual\nmanagement and configuration, making it less reliable and more complex. It also introduces a single point of\nfailure and can become a performance bottleneck. Furthermore, it requires traffic to traverse to the public\ninternet.\nInternet Gateway (Option D): An internet gateway allows instances in the VPC to access the internet.\nHowever, it exposes the instances to the public internet, making it the least secure option.\nTherefore, utilizing a VPC endpoint for DynamoDB is the preferred and most secure method because it\nmaintains network traffic within the AWS infrastructure, reduces the attack surface, and offers granular\naccess control.\nHere is a breakdown of why other options aren't secure:\nB, C, D Routing traffic through the public internet, which violates the requirement of keeping traffic within\nAWS.",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html"
    ]
  },
  {
    "question": "CertyIQ\nAn entertainment company is using Amazon DynamoDB to store media metadata. The application is read intensive\nand experiencing delays. The company does not have staff to handle additional operational overhead and needs to\nimprove the performance efficiency of DynamoDB without reconfiguring the application.\nWhat should a solutions architect recommend to meet this requirement?",
    "options": {
      "A": "Use Amazon ElastiCache for Redis.",
      "B": "This violates the requirement to avoid reconfiguring the application. Similarly,",
      "C": "Replicate data by using DynamoDB global tables.",
      "D": "Use Amazon ElastiCache for Memcached with Auto Discovery enabled."
    },
    "answer": "B",
    "explanation": "The correct answer is B. Use Amazon DynamoDB Accelerator (DAX).\nDynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB. It's\nspecifically designed to improve read performance for read-heavy workloads, which directly addresses the\nproblem described. DAX integrates seamlessly with existing DynamoDB applications without requiring code\nchanges, fulfilling the requirement to avoid application reconfiguration. It significantly reduces read latency\n(often from milliseconds to microseconds) by caching frequently accessed data, which will alleviate the\nobserved delays. Importantly, DAX offloads read traffic from the DynamoDB tables, improving overall\nperformance and reducing pressure on the DynamoDB infrastructure, and because it's a managed service, it\nminimizes operational overhead.\nOption A, ElastiCache for Redis, while a powerful caching solution, requires application-level modifications to\nintegrate with DynamoDB. This violates the requirement to avoid reconfiguring the application. Similarly,\noption D, ElastiCache for Memcached, also necessitates application changes and might require auto-\ndiscovery configuration, adding operational complexity.\nOption C, DynamoDB global tables, provides multi-region replication for availability and disaster recovery, not\nprimarily for improving read performance within a single region. While replication could indirectly improve\nread performance in certain multi-region scenarios, it introduces significantly more operational overhead and\ncomplexity compared to DAX, and does not directly address the read latency issues in the single region the\ncompany uses. Global tables also don't solve the latency issue within a specific region and primarily focus on\ndata redundancy and availability across geographically dispersed locations.\nTherefore, DAX is the most suitable solution because it's a managed, in-memory cache specifically designed\nfor DynamoDB that enhances read performance without application changes or increased operational burden.\nFurther research:\nAmazon DynamoDB Accelerator (DAX): https://aws.amazon.com/dynamodb/dax/\nDAX Use Cases: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html",
    "links": [
      "https://aws.amazon.com/dynamodb/dax/",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html"
    ]
  },
  {
    "question": "CertyIQ\nA companys infrastructure consists of Amazon EC2 instances and an Amazon RDS DB instance in a single AWS\nRegion. The company wants to back up its data in a separate Region.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Use AWS Backup to copy EC2 backups and RDS backups to the separate Region.",
      "B": "Use Amazon Data Lifecycle Manager (Amazon DLM) to copy EC2 backups and RDS backups to the separate",
      "C": "Create Amazon Machine Images (AMIs) of the EC2 instances. Copy the AMIs to the separate Region. Create a",
      "D": "Create Amazon Elastic Block Store (Amazon EBS) snapshots. Copy the EBS snapshots to the separate"
    },
    "answer": "A",
    "explanation": "The correct answer is A: Use AWS Backup to copy EC2 backups and RDS backups to the separate Region.\nHere's why:\nAWS Backup offers a centralized and automated way to manage backups across multiple AWS services,\nincluding EC2 and RDS, simplifying backup management and reducing operational overhead. It allows you to\ndefine backup policies and schedules centrally, and then apply them to your resources. A key feature is its\nability to copy backups across regions, satisfying the requirement of backing up data in a separate region.\nThis eliminates the need for service-specific backup solutions and scripting.\nOption B, using Amazon Data Lifecycle Manager (Amazon DLM), primarily manages the lifecycle of EBS\nsnapshots. While DLM can create EBS snapshots, it doesn't directly handle RDS backups or cross-region\nbackup copying. Thus, this approach requires additional configuration and scripting for the RDS component,\nincreasing operational overhead.\nOption C, creating AMIs and RDS read replicas, meets the DR requirements, but it isn't a backup solution.\nAMIs primarily capture the state of EC2 instances for disaster recovery or instance replication, and read\nreplicas are for read scaling and disaster recovery. This doesn't provide point-in-time backup capabilities for\nboth services and involves more operational overhead to maintain the replicas and keep them in sync.\nOption D, creating EBS and RDS snapshots and using S3 CRR, is more complex and time-consuming than\nusing AWS Backup. It requires configuring snapshot schedules for EC2 using EBS snapshots, RDS snapshots,\nexporting RDS snapshots to S3, and then setting up S3 Cross-Region Replication. Also managing snapshots,\nconfiguring CRR, and ensuring consistent backups are all operational overheads to avoid.\nAWS Backup is the most streamlined solution because it's designed for this specific purpose: centralized\nbackup management across different AWS services, including cross-region copying.\nSupporting Links:\nAWS Backup: https://aws.amazon.com/backup/\nAmazon RDS Backups:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html\nAmazon EC2 Backups: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html",
    "links": [
      "https://aws.amazon.com/backup/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect needs to securely store a database user name and password that an application uses to\naccess an Amazon RDS DB instance. The application that accesses the database runs on an Amazon EC2 instance.\nThe solutions architect wants to create a secure parameter in AWS Systems Manager Parameter Store.\nWhat should the solutions architect do to meet this requirement?",
    "options": {
      "A": "Here's why:",
      "B": "Create an IAM policy that allows read access to the Parameter Store parameter. Allow Decrypt access to an",
      "C": "Create an IAM trust relationship between the Parameter Store parameter and the EC2 instance. Specify",
      "D": "Create an IAM trust relationship between the DB instance and the EC2 instance. Specify Systems Manager"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's why:\nThe scenario requires securely storing database credentials within Systems Manager Parameter Store and\nallowing an EC2 instance to access them. IAM roles are the preferred method for granting permissions to EC2\ninstances, as they provide temporary credentials and are more secure than using long-term access keys.\nOption A correctly leverages this. It creates an IAM role (not just a policy) that grants ssm:GetParameter (read\naccess) permission to the specific parameter within Parameter Store. Crucially, it also allows kms:Decrypt\npermission to the KMS key used to encrypt the secure string parameter. Since the parameter is stored as a\nsecure string (encrypted), the EC2 instance needs permission to decrypt it. Assigning this IAM role to the EC2\ninstance allows the EC2 instance's applications to retrieve and decrypt the database credentials.\nOption B is incorrect because while policies are essential, they must be attached to roles for EC2 instances.\nAttaching a policy directly is not the proper way to grant permissions to EC2 instances, as roles provide the\nnecessary temporary credentials.\nOptions C and D are incorrect because IAM trust relationships are used to allow entities in one AWS account\nto assume roles in another account or for services to assume roles. They are not directly used for granting\naccess to Parameter Store or RDS instances from an EC2 instance within the same account. Specifying\nAmazon RDS or Systems Manager as principals in a trust policy related to an EC2 instance is conceptually\nflawed in this context. The communication is EC2 -> SSM Parameter Store, not EC2 being \"trusted\" by SSM or\nRDS.\nTherefore, option A provides the most secure and correct way to grant the necessary permissions to the EC2\ninstance to access the encrypted credentials stored in Parameter Store.\nSupporting links:\nIAM Roles: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\nSystems Manager Parameter Store: https://docs.aws.amazon.com/systems-\nmanager/latest/userguide/systems-manager-parameter-store.html\nKMS Encryption: https://docs.aws.amazon.com/kms/latest/developerguide/overview.html\nGranting Applications Access to Secrets: https://aws.amazon.com/blogs/security/how-to-manage-secrets-\nfor-amazon-ec2-using-aws-systems-manager-parameter-store/",
    "links": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html",
      "https://docs.aws.amazon.com/systems-",
      "https://docs.aws.amazon.com/kms/latest/developerguide/overview.html",
      "https://aws.amazon.com/blogs/security/how-to-manage-secrets-"
    ]
  },
  {
    "question": "CertyIQ\nA company is designing a cloud communications platform that is driven by APIs. The application is hosted on\nAmazon EC2 instances behind a Network Load Balancer (NLB). The company uses Amazon API Gateway to provide\nexternal users with access to the application through APIs. The company wants to protect the platform against\nweb exploits like SQL injection and also wants to detect and mitigate large, sophisticated DDoS attacks.\nWhich combination of solutions provides the MOST protection? (Choose two.)",
    "options": {
      "A": "Use AWS WAF to protect the NLB.",
      "B": "Use AWS Shield Advanced with the NLB.",
      "C": "Use AWS WAF to protect Amazon API Gateway.",
      "D": "Use Amazon GuardDuty with AWS Shield Standard"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why options B and C (AWS Shield Advanced with the NLB and AWS WAF to\nprotect Amazon API Gateway) provide the most protection in this scenario:\nUnderstanding the Threats: The question highlights two primary threats: web exploits (like SQL injection) and\nDDoS attacks. Web exploits target the application layer (Layer 7), attempting to manipulate the application\nlogic or data. DDoS attacks aim to overwhelm the infrastructure, making the application unavailable.\nAWS WAF for Application Layer Protection (API Gateway): AWS WAF (Web Application Firewall) is\nspecifically designed to protect web applications from common web exploits by inspecting HTTP traffic.\nPlacing WAF in front of Amazon API Gateway (option C) allows it to inspect incoming requests for malicious\npatterns (e.g., SQL injection attempts, cross-site scripting). API Gateway acts as the entry point to the\napplication APIs, making it a prime target for such attacks. https://aws.amazon.com/waf/\nAWS Shield Advanced for DDoS Mitigation (NLB): AWS Shield provides DDoS protection. Standard Shield is\nautomatically enabled and free, but offers basic protection. Shield Advanced (option B) provides more\nsophisticated detection and mitigation capabilities, including 24/7 access to the AWS DDoS Response Team\n(DRT) during attacks. Protecting the Network Load Balancer (NLB) with Shield Advanced is crucial because\nthe NLB distributes incoming traffic to the EC2 instances hosting the application. By protecting the NLB, you\nprotect the entire infrastructure from being overwhelmed by DDoS attacks. Shield Advanced is designed to\nprotect resources like NLBs, Elastic Load Balancers (ELB), and CloudFront distributions.\nhttps://aws.amazon.com/shield/\nWhy other options are less ideal:\nA: Using AWS WAF with the NLB offers some protection but less strategically since the API Gateway sits in\nfront of it. The API Gateway is where the actual API requests are handled and exposed, making it the prime\ntarget.\nD: GuardDuty is a threat detection service that monitors your AWS environment for malicious activity. It's\nhelpful for security monitoring and alerting, but it doesn't directly mitigate DDoS attacks or prevent web\nexploits like WAF and Shield Advanced do. Shield Standard provides basic DDoS protection but not as\ngranular or tailored as Shield Advanced.\nE: Shield Standard is a base service, while API Gateway is a prime location for using WAF.\nComprehensive Protection: By combining WAF at the API Gateway level with Shield Advanced protecting the\nNLB, the company achieves a layered security approach. WAF handles application-layer attacks, while Shield\nAdvanced handles network-layer DDoS attacks. This combination offers the most robust protection for the\ncloud communications platform.",
    "links": [
      "https://aws.amazon.com/waf/",
      "https://aws.amazon.com/shield/"
    ]
  },
  {
    "question": "CertyIQ\nA company has a legacy data processing application that runs on Amazon EC2 instances. Data is processed\nsequentially, but the order of results does not matter. The application uses a monolithic architecture. The only way\nthat the company can scale the application to meet increased demand is to increase the size of the instances.\nThe companys developers have decided to rewrite the application to use a microservices architecture on Amazon\nElastic Container Service (Amazon ECS).\nWhat should a solutions architect recommend for communication between the microservices?",
    "options": {
      "A": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Add code to the data producers, and send",
      "B": "DynamoDB Streams",
      "C": "Create an AWS Lambda function to pass messages. Add code to the data producers to call the Lambda",
      "D": "Create an Amazon DynamoDB table. Enable DynamoDB Streams. Add code to the data producers to insert"
    },
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A is the best choice, and why the other options are less suitable\nfor inter-microservice communication in this scenario:\nJustification for Option A (Amazon SQS):\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that allows you to decouple\nand scale microservices, distributed systems, and serverless applications. In this scenario, SQS provides\nasynchronous communication between the microservices performing the data processing. Producers\n(microservices generating data) can send data to the queue without needing to know who the consumers are.\nConsumers (microservices processing data) can independently pull messages from the queue and process\nthem at their own pace. This decoupling allows the system to handle varying loads efficiently. Since the order\nof the results doesn't matter, SQS Standard Queues are a suitable choice. If the application required strictly\nfirst-in, first-out processing, SQS FIFO Queues could be considered. SQS inherently provides message\nbuffering, which is crucial in handling situations where producers generate data faster than consumers can\nprocess it. The queue acts as a buffer, preventing data loss and allowing the system to gracefully handle\nspikes in demand. The \"at-least-once\" delivery guarantee of SQS ensures that each message is processed.\nWhy other options are not ideal:\nOption B (Amazon SNS): Amazon Simple Notification Service (SNS) is primarily designed for push-based\nnotifications to multiple subscribers. While it can be used for microservice communication, it's better suited\nfor fan-out scenarios where one producer needs to notify many consumers. In this case, the data processing is\nsequential (producer generates, consumer processes), so a queue-based approach (SQS) is more appropriate.\nSNS would also require each consumer to subscribe to the topic and would add unnecessary complexity if the\nintention is solely to process data. It's designed for notifications, not general purpose asynchronous\nprocessing of data.\nOption C (AWS Lambda): Using Lambda to directly pass messages would create synchronous dependencies\nbetween the microservices and limit scalability. Lambda functions have execution time limits and might not be\nsuitable for handling potentially large or long-running data processing tasks. This approach would also\nintroduce unnecessary overhead and complexity compared to a simple queue. Furthermore, the service\nmaking the request would need to wait for Lambda to execute and pass on the response, defeating the\npurpose of asynchronous processing and decoupling.\nOption D (Amazon DynamoDB Streams): DynamoDB Streams are primarily designed for triggering actions\nbased on data changes in a DynamoDB table. While it can be used for inter-microservice communication, it's\nnot its core purpose. It would introduce unnecessary complexity and overhead, as you'd have to treat the\nDynamoDB table as a message queue. It tightly couples the microservices to DynamoDB. DynamoDB Streams\nare better suited for things like auditing changes in a database, or triggering related actions based on specific\ndata modifications. This approach is generally avoided unless you need to persist the data to a database and\ntrigger some kind of action.\nIn conclusion: SQS provides a simple, scalable, and cost-effective solution for asynchronous communication\nbetween microservices in a data processing application, aligning perfectly with the company's goal of\ndecoupling its legacy monolithic application and leveraging a microservices architecture.\nAuthoritative Links:\nAmazon SQS: https://aws.amazon.com/sqs/\nMicroservices architecture on AWS: https://aws.amazon.com/microservices/\nDecoupling microservices using SQS: https://aws.amazon.com/blogs/compute/decoupling-applications-with-\namazon-sqs-message-queues/",
    "links": [
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/microservices/",
      "https://aws.amazon.com/blogs/compute/decoupling-applications-with-"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to migrate its MySQL database from on premises to AWS. The company recently experienced a\ndatabase outage that significantly impacted the business. To ensure this does not happen again, the company\nwants a reliable database solution on AWS that minimizes data loss and stores every transaction on at least two\nnodes.\nWhich solution meets these requirements?",
    "options": {
      "A": "Create an Amazon RDS DB instance with synchronous replication to three nodes in three Availability Zones.",
      "B": "Let's break down why:",
      "C": "Create an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that",
      "D": "Create an Amazon EC2 instance with a MySQL engine installed that triggers an AWS Lambda function to"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Let's break down why:\nThe core requirements are high reliability, minimal data loss, and synchronous data replication on at least two\nnodes within AWS.\nOption A (RDS with synchronous replication to three nodes in three AZs): While seemingly good, standard\nRDS doesn't natively offer synchronous replication to three nodes across three AZs. Multi-AZ provides\nsynchronous replication between two nodes. While MySQL can be configured to replicate to multiple read\nreplicas synchronously, doing so requires significant configuration beyond just creating an RDS instance, and\nisn't typically the best approach for basic high availability within a single region.\nOption B (RDS MySQL with Multi-AZ): This precisely addresses the requirements. RDS Multi-AZ provides a\nsynchronous, automated failover mechanism. Data is synchronously replicated between the primary instance\nand a standby instance in a different Availability Zone. In case of failure, the standby instance automatically\ntakes over with minimal data loss. This configuration stores every transaction on at least two nodes.\nOption C (RDS MySQL with a cross-Region read replica): Cross-region read replicas are asynchronous. While\nthey improve disaster recovery capabilities, they do not provide the required synchronous data replication.\nTherefore, this option does not meet the criteria of minimizing data loss due to its asynchronous nature. Cross-\nRegion replication has latency which goes against the need to minimize data loss.\nOption D (EC2-based MySQL with Lambda-triggered replication to RDS): This solution is overly complex and\nintroduces significant overhead. Manually managing replication using Lambda functions is not an efficient or\nreliable solution compared to RDS's built-in Multi-AZ feature. It also has the burden of needing to manage the\nEC2 instance, OS patching, etc. Synchronous replication implemented in this manner also isn't guaranteed\nwithout significant custom coding and testing, and would be prone to errors. This introduces a very high level\nof complexity.\nIn summary, RDS Multi-AZ provides a managed, reliable, and synchronous replication solution, fulfilling the\ncompany's requirements for high availability and minimal data loss. It is also the simplest and most cost-\neffective option.\nSupporting Links:\nAmazon RDS Multi-AZ: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\nAmazon RDS Read Replicas:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.ReadReplicas.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.ReadReplicas.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is building a new dynamic ordering website. The company wants to minimize server maintenance and\npatching. The website must be highly available and must scale read and write capacity as quickly as possible to\nmeet changes in user demand.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Here's why:",
      "B": "Host static content in Amazon S3. Host dynamic content by using Amazon API Gateway and AWS Lambda.",
      "C": "Host all the website content on Amazon EC2 instances. Create an Auto Scaling group to scale the EC2",
      "D": "Host all the website content on Amazon EC2 instances. Create an Auto Scaling group to scale the EC2"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's why:\nOption A offers a serverless architecture which directly addresses the requirement to minimize server\nmaintenance and patching. Amazon S3 is used for static content hosting, eliminating the need to manage\nservers for these assets. Amazon API Gateway and AWS Lambda handle dynamic content, abstracting away\nthe underlying infrastructure management. DynamoDB with on-demand capacity is a key component,\nproviding automatic scaling for read and write operations without the need for manual capacity planning,\nfulfilling the rapid scaling requirement. CloudFront ensures global content delivery with low latency and high\navailability.\nOption B uses Amazon Aurora with Auto Scaling. While Aurora Auto Scaling is good, it involves managing\nrelational databases, which incurs operational overhead and database maintenance. This contradicts the\nrequirement to minimize server maintenance. Additionally, DynamoDB offers faster, more predictable scaling\nthan Aurora for dynamic workloads.\nOptions C and D rely on EC2 instances for hosting content. This introduces significant server management\noverhead for patching, scaling, and maintenance. Using EC2 Auto Scaling helps, but it does not eliminate the\nunderlying server management burden, directly contradicting the prompt.\nTherefore, Option A offers the most serverless, scalable, and maintainable architecture aligning with the\nrequirements. DynamoDB with on-demand capacity is optimal for unpredictable workloads needing immediate\nscaling.\nKey Concepts:\nServerless Computing: API Gateway, Lambda, and DynamoDB abstract away server management.\nScalability: DynamoDB on-demand capacity scales automatically.\nHigh Availability: S3, CloudFront, API Gateway, Lambda and DynamoDB inherently support high availability.\nSupporting Links:\nAmazon S3\nAmazon API Gateway\nAWS Lambda\nAmazon DynamoDB\nAmazon CloudFront",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has an AWS account used for software engineering. The AWS account has access to the companys on-\npremises data center through a pair of AWS Direct Connect connections. All non-VPC traffic routes to the virtual\nprivate gateway.\nA development team recently created an AWS Lambda function through the console. The development team\nneeds to allow the function to access a database that runs in a private subnet in the companys data center.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Here's a detailed justification:",
      "B": "Set up a VPN connection from AWS to the data center. Route the traffic from the Lambda function through",
      "C": "Option A directly addresses this. Configuring the Lambda function to run within the VPC allows it to leverage",
      "D": "Create an Elastic IP address. Configure the Lambda function to send traffic through the Elastic IP address"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's a detailed justification:\nThe core requirement is to allow a Lambda function to access a database located in a private subnet within\nthe company's on-premises data center, which is connected to AWS via Direct Connect. Lambda functions, by\ndefault, run in a secure, isolated environment outside your VPC. To access resources within a VPC or\nconnected on-premises networks, the Lambda function must be configured to run within the VPC.\nOption A directly addresses this. Configuring the Lambda function to run within the VPC allows it to leverage\nthe existing Direct Connect connection to reach the on-premises database. By associating the Lambda\nfunction with a security group that permits outbound traffic to the database's port and IP address range, and\nensuring the VPC's route tables direct traffic destined for the on-premises network to the virtual private\ngateway (VGW) associated with the Direct Connect connection, the Lambda function can successfully\ncommunicate with the database.\nOption B is incorrect because establishing a VPN connection in addition to the existing Direct Connect adds\nunnecessary complexity and cost. Direct Connect already provides a dedicated, private network connection.\nOption C, while seemingly relevant, misses a crucial step. Simply updating route tables isn't sufficient. The\nLambda function must be running within the VPC to utilize those route tables. Lambda functions executing\noutside a VPC cannot be governed by VPC route tables.\nOption D is incorrect because Lambda functions do not directly use Elastic IP addresses (EIPs). EIPs are static\nIPv4 addresses designed for instances and network interfaces to maintain a consistent public IP. Lambda\nfunctions run within the AWS managed infrastructure and do not have dedicated EIPs assigned to them.\nConfiguring the Lambda function to use an EIP without an ENI is not possible or how Lambda utilizes IP\naddressing.\nIn summary, running the Lambda function within the VPC (Option A) is the most direct, cost-effective, and\nsecure solution. It leverages the existing Direct Connect connection and allows the function to access on-\npremises resources by associating it with the appropriate VPC, subnet, and security group, while ensuring\nproper routing.\nFurther research:\nAWS Lambda VPC Networking: https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html\nAWS Direct Connect: https://aws.amazon.com/directconnect/\nAWS Security Groups: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html",
    "links": [
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html",
      "https://aws.amazon.com/directconnect/",
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an application using Amazon ECS. The application creates resized versions of an original image\nand then makes Amazon S3 API calls to store the resized images in Amazon S3.\nHow can a solutions architect ensure that the application has permission to access Amazon S3?",
    "options": {
      "A": "Update the S3 role in AWS IAM to allow read/write access from Amazon ECS, and then relaunch the",
      "B": "Create an IAM role with S3 permissions, and then specify that role as the",
      "C": "Create a security group that allows access from Amazon ECS to Amazon S3, and update the launch",
      "D": "Create an IAM user with S3 permissions, and then relaunch the Amazon EC2 instances for the ECS"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Create an IAM role with S3 permissions, and then specify that role as the\ntaskRoleArn in the task definition.\nThis is the most secure and recommended method for granting ECS tasks access to AWS services like S3.\nIAM roles provide temporary security credentials to applications running on AWS, eliminating the need to\nembed or manage long-term credentials within the application or containers.\nHere's why the other options are incorrect:\nA. Update the S3 role in AWS IAM to allow read/write access from Amazon ECS, and then relaunch the\ncontainer: S3 buckets don't have IAM roles. IAM roles are assigned to entities (like ECS tasks or EC2\ninstances) that need to access S3. While bucket policies control who can access the bucket itself, they don't\ngrant permissions to specific tasks.\nC. Create a security group that allows access from Amazon ECS to Amazon S3, and update the launch\nconfiguration used by the ECS cluster: Security groups control network traffic, not IAM permissions. S3\naccess is controlled by IAM policies, not network rules. Security groups are important for allowing your ECS\ntasks to communicate with other resources (like databases), but they are not relevant for S3 API\nauthorization.\nD. Create an IAM user with S3 permissions, and then relaunch the Amazon EC2 instances for the ECS\ncluster while logged in as this account: Using an IAM user for this purpose introduces the risk of hardcoding\ncredentials within your EC2 instances, which is a security anti-pattern. Furthermore, \"logging in\" as an IAM\nuser on the EC2 instance doesn't automatically grant your ECS tasks those permissions.\nThe taskRoleArn parameter in the ECS task definition specifies the IAM role that ECS will use to provide\ncredentials to the containers running within that task. When the application makes an S3 API call, the AWS\nSDK automatically retrieves temporary credentials from the IAM role associated with the task. These\ncredentials are then used to authenticate and authorize the request to S3, ensuring that only authorized\napplications can access S3 resources. The Task Role offers better security because the ECS agent manages\nthe retrieval and rotation of temporary credentials, so you don't have to manage credentials yourself.This\npractice adheres to the principle of least privilege, ensuring that only the required permissions are granted to\nthe application for accessing specific resources.\nAWS Documentation - IAM Roles for TasksAWS Security Best Practices",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has a Windows-based application that must be migrated to AWS. The application requires the use of a\nshared Windows file system attached to multiple Amazon EC2 Windows instances that are deployed across\nmultiple Availability Zone:\nWhat should a solutions architect do to meet this requirement?",
    "options": {
      "A": "Configure AWS Storage Gateway in volume gateway mode. Mount the volume to each Windows instance.",
      "B": "Configure Amazon FSx for Windows File Server. Mount the Amazon FSx file system",
      "C": "Configure a file system by using Amazon Elastic File System (Amazon EFS). Mount the EFS file system to",
      "D": "Configure an Amazon Elastic Block Store (Amazon EBS) volume with the required size. Attach each EC2"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Configure Amazon FSx for Windows File Server. Mount the Amazon FSx file system\nto each Windows instance.\nHere's a detailed justification:\nThe requirement is to provide a shared Windows file system across multiple EC2 Windows instances in\nmultiple Availability Zones. This implies the need for a network file system accessible by all instances\nsimultaneously.\nAmazon FSx for Windows File Server is a fully managed Windows file system built on Windows Server. It\nsupports the SMB protocol natively, which is the standard file-sharing protocol for Windows environments.\nThis makes it easy to integrate with existing Windows applications. Moreover, it's designed for multi-AZ\ndeployments, ensuring high availability and durability, which perfectly aligns with the requirement of\ninstances residing across multiple Availability Zones.\nAWS Storage Gateway (volume gateway mode) primarily provides block storage volumes and is not designed\nfor native file sharing across multiple instances simultaneously like a network file system. It presents iSCSI\ntargets to EC2 instances.\nAmazon EFS (Elastic File System) is a fully managed NFS (Network File System) designed for Linux-based\nworkloads. While it offers shared storage, it is not optimized or native to the Windows environment and does\nnot natively support SMB, which is critical for Windows-based applications. While technically possible to\nmake it work with workarounds, it's far from ideal.\nAmazon EBS (Elastic Block Store) volumes are block storage devices attached to a single EC2 instance. EBS\nvolumes cannot be simultaneously attached to multiple EC2 instances, making this option unsuitable for\nshared file system needs across multiple instances. While EBS Multi-Attach exists, it's mainly intended for\nclustered applications with specific write coordination.\nTherefore, Amazon FSx for Windows File Server is the most appropriate service because it directly addresses\nthe need for a shared, highly available, and fully managed Windows file system, fitting seamlessly into a\nWindows-based environment deployed across multiple Availability Zones.\nSupporting documentation:\nAmazon FSx for Windows File Server\nCompare AWS Storage Options",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is developing an ecommerce application that will consist of a load-balanced front end, a container-\nbased application, and a relational database. A solutions architect needs to create a highly available solution that\noperates with as little manual intervention as possible.\nWhich solutions meet these requirements? (Choose two.)",
    "options": {
      "A": "Create an Amazon RDS DB instance in Multi-AZ mode.",
      "B": "Create an Amazon RDS DB instance and one or more replicas in another Availability Zone.",
      "C": "While ECS simplifies container orchestration compared to",
      "D": "Create an Amazon Elastic Container Service (Amazon ECS) cluster with a Fargate launch type to handle the"
    },
    "answer": "A",
    "explanation": "The question focuses on creating a highly available, low-maintenance e-commerce application spanning load\nbalancing, containers, and a relational database. The optimal solutions prioritize automated failover and\nsimplified management.\nOption A is correct because running Amazon RDS in Multi-AZ mode provides automatic failover to a standby\ninstance in another Availability Zone in case of a primary instance failure. This minimizes manual intervention\nand ensures high availability for the database layer, directly addressing the prompt's requirements.\n[https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html]\nOption D is also correct because Amazon ECS with the Fargate launch type abstracts away the underlying\ninfrastructure management. Fargate handles scaling, patching, and provisioning the compute resources for\nthe containers, significantly reducing operational overhead and manual intervention. It also promotes high\navailability by distributing containers across multiple Availability Zones.\n[https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html]\nOption B is incorrect because while read replicas can improve performance, they do not provide automatic\nfailover for the primary database instance. Manual intervention would be needed to promote a read replica to\nthe primary instance in case of failure.\nOption C is incorrect because managing an EC2-based Docker cluster requires significant manual effort for\ntasks such as instance management, scaling, patching, and orchestration. This contradicts the requirement\nfor minimal manual intervention.\nOption E is incorrect for similar reasons to Option C. While ECS simplifies container orchestration compared to\na raw Docker cluster, using the EC2 launch type still requires managing the underlying EC2 instances,\nincluding scaling, patching, and maintenance. This adds operational overhead and manual intervention.",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html]",
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html]"
    ]
  },
  {
    "question": "CertyIQ\nA company uses Amazon S3 as its data lake. The company has a new partner that must use SFTP to upload data\nfiles. A solutions architect needs to implement a highly available SFTP solution that minimizes operational\noverhead.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Use AWS Transfer Family to configure an SFTP-enabled server with a publicly accessible endpoint. Choose",
      "B": "Share the NLB hostname with the new partner.",
      "C": "Launch an Amazon EC2 instance in a private subnet in a VPInstruct the new partner to upload files to the EC2",
      "D": "Launch Amazon EC2 instances in a private subnet in a VPC. Place a Network Load Balancer (NLB) in front of"
    },
    "answer": "A",
    "explanation": "The correct answer is A, utilizing AWS Transfer Family for a secure and scalable SFTP solution directly\nintegrated with S3.\nHere's why:\nAWS Transfer Family is specifically designed for secure file transfers into and out of AWS storage services\nlike S3. It natively supports SFTP, FTP, and FTPS protocols.\nMinimizes Operational Overhead: Transfer Family is a fully managed service. This means AWS handles the\nunderlying infrastructure, patching, scaling, and high availability. This drastically reduces the operational\nburden on the company compared to managing EC2 instances.\nHigh Availability: Transfer Family provides built-in high availability. You don't need to configure and maintain\nload balancers or redundant EC2 instances.\nDirect S3 Integration: Transfer Family can directly integrate with the S3 data lake. Files uploaded via SFTP\nare automatically stored in the designated S3 bucket.\nSecurity: Transfer Family supports various authentication methods and encryption to ensure secure data\ntransfers. The publicly accessible endpoint allows partners to connect easily, while security is maintained\nthrough authentication and authorization policies.\nOption B (S3 File Gateway): S3 File Gateway is intended for on-premises applications to access S3, not\ntypically for external partners using SFTP. It also involves more local infrastructure management.\nOptions C and D (EC2 Instances): Using EC2 instances, even with VPNs and Network Load Balancers (NLB),\nrequires significant manual configuration, patching, scaling, and monitoring. This defeats the requirement of\nminimizing operational overhead. These options are complex and less secure without proper hardening. They\nalso necessitate managing cron jobs and custom scripts for data transfer, adding to operational burden. They\ndo not offer inherent high availability without additional configuration.\nIn summary, AWS Transfer Family offers the most straightforward, highly available, secure, and operationally\nefficient solution for providing SFTP access to an S3 data lake for external partners.\nAuthoritative Links:\nAWS Transfer Family: https://aws.amazon.com/transfer/\nAWS Transfer Family Documentation: https://docs.aws.amazon.com/transfer/latest/userguide/what-is-\ntransfer.html",
    "links": [
      "https://aws.amazon.com/transfer/",
      "https://docs.aws.amazon.com/transfer/latest/userguide/what-is-"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to store contract documents. A contract lasts for 5 years. During the 5-year period, the company\nmust ensure that the documents cannot be overwritten or deleted. The company needs to encrypt the documents\nat rest and rotate the encryption keys automatically every year.\nWhich combination of steps should a solutions architect take to meet these requirements with the LEAST\noperational overhead? (Choose two.)",
    "options": {
      "A": "Store the documents in Amazon S3. Use S3 Object Lock in governance mode.",
      "B": "Store the documents in Amazon S3. Use S3 Object Lock in compliance mode.",
      "C": "Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Configure key rotation.",
      "D": "Use server-side encryption with AWS Key Management Service (AWS KMS) customer managed keys."
    },
    "answer": "B",
    "explanation": "The best solution combines data immutability with managed encryption and automatic key rotation for\nminimal operational overhead.\nB. Store the documents in Amazon S3. Use S3 Object Lock in compliance mode.\nS3 Object Lock in compliance mode is crucial. Compliance mode provides the highest level of data\nimmutability. Once an object is locked in compliance mode, it cannot be deleted or overwritten, even by the\nroot user, until the retention period expires. This satisfies the requirement that documents cannot be\noverwritten or deleted during the 5-year contract period. Governance mode, on the other hand, allows certain\nprivileged users to bypass the lock, which doesn't meet the stringent requirement of complete immutability.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html\nD. Use server-side encryption with AWS Key Management Service (AWS KMS) customer managed keys.\nConfigure key rotation.\nUsing KMS customer managed keys allows for automatic key rotation every year as required. This automated\nprocess reduces operational overhead and aligns with security best practices. While SSE-S3 (option C)\nhandles encryption, it provides less control over key management than KMS. Option E, using customer-\nprovided keys, adds significant operational overhead because the company is responsible for generating,\nstoring, and securely delivering the keys to AWS. KMS simplifies key management. Using KMS with CMKs\nallows you to enable automatic key rotation and maintain audit trails related to encryption key usage. SSE-S3\nalso provides key rotation, but KMS offers better key management overall.\nhttps://docs.aws.amazon.com/kms/latest/developerguide/rotate-\nkeys.htmlhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html\nTherefore, storing the documents in S3 with Object Lock in compliance mode ensures immutability, while\nusing KMS with customer-managed keys provides automated encryption and key rotation, minimizing\noperational overhead and meeting all requirements.",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html",
      "https://docs.aws.amazon.com/kms/latest/developerguide/rotate-",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has a web application that is based on Java and PHP. The company plans to move the application from\non premises to AWS. The company needs the ability to test new site features frequently. The company also needs\na highly available and managed solution that requires minimum operational overhead.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Create an Amazon S3 bucket. Enable static web hosting on the S3 bucket. Upload the static content to the",
      "B": "Deploy the web application to an AWS Elastic Beanstalk environment. Use URL swapping to switch between",
      "C": "Deploy the web application to Amazon EC2 instances that are configured with Java and PHP. Use Auto",
      "D": "Containerize the web application. Deploy the web application to Amazon EC2 instances. Use the AWS Load"
    },
    "answer": "B",
    "explanation": "The correct answer is B: Deploy the web application to an AWS Elastic Beanstalk environment. Use URL\nswapping to switch between multiple Elastic Beanstalk environments for feature testing.\nHere's a detailed justification:\nElastic Beanstalk is a Platform-as-a-Service (PaaS) that simplifies deploying and managing web applications\nin AWS. It supports various programming languages, including Java and PHP, meeting the application's\nrequirements. It provides a managed environment, reducing operational overhead, which aligns with the\ncompany's need for a solution with minimal operational overhead.\nThe key advantage of Elastic Beanstalk in this scenario is its support for easy deployment of multiple\nenvironments and URL swapping. This feature is crucial for frequent testing of new site features. URL\nswapping allows you to quickly direct traffic to a different environment hosting the new features, enabling\nquick A/B testing or staging deployments without modifying DNS records. This minimizes downtime and\nfacilitates continuous integration and continuous delivery (CI/CD) practices.\nOption A is incorrect because Amazon S3 is designed for static content hosting. While Lambda can handle\ndynamic content, this solution would require significant architectural changes and more operational overhead\nto manage the dynamic aspects of the entire application.\nOption C involves deploying the application to Amazon EC2 instances managed by Auto Scaling groups and an\nApplication Load Balancer (ALB). This is a viable solution but requires more manual configuration and\nmanagement compared to Elastic Beanstalk. Setting up a robust testing environment and implementing\nfeature toggles or blue/green deployments on EC2 would increase operational overhead.\nOption D suggests containerizing the application and deploying it to EC2 instances. While containerization\noffers benefits like portability, it also introduces additional complexity in managing the container\norchestration. The AWS Load Balancer Controller is designed for Kubernetes, which is not explicitly stated as\nbeing used by the company. Therefore, setting up this infrastructure with dynamic routing would involve more\neffort compared to the Elastic Beanstalk approach.\nIn summary, Elastic Beanstalk offers the best balance between simplicity, managed services, and support for\nfeature testing via URL swapping, making it the most appropriate solution for this scenario.\nRelevant AWS documentation:\nAWS Elastic Beanstalk: https://aws.amazon.com/elasticbeanstalk/\nElastic Beanstalk URL Swapping: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-\nfeatures.CNAMESwap.html",
    "links": [
      "https://aws.amazon.com/elasticbeanstalk/",
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-"
    ]
  },
  {
    "question": "CertyIQ\nA company has an ordering application that stores customer information in Amazon RDS for MySQL. During\nregular business hours, employees run one-time queries for reporting purposes. Timeouts are occurring during\norder processing because the reporting queries are taking a long time to run. The company needs to eliminate the\ntimeouts without preventing employees from performing queries.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Create a read replica. Move reporting queries to the read replica.",
      "B": "Create a read replica. Distribute the ordering application to the primary DB instance and the read replica.",
      "C": "Migrate the ordering application to Amazon DynamoDB with on-demand capacity.",
      "D": "Schedule the reporting queries for non-peak hours."
    },
    "answer": "A",
    "explanation": "The best solution to eliminate timeouts caused by reporting queries on an Amazon RDS for MySQL database\nwithout restricting employee access is to create a read replica and move the reporting queries to it.\nOption A, creating a read replica, directly addresses the problem by offloading read-intensive reporting\nqueries from the primary RDS instance. Read replicas allow you to scale beyond the capacity constraints of a\nsingle database instance for read-heavy database workloads. The primary RDS instance will then be\ndedicated to order processing, eliminating timeouts.\nOption B, distributing the application across the primary and read replica is incorrect. The ordering application\nneeds consistent writes, which must occur on the primary instance to maintain data integrity. Splitting writes\nacross the primary and read replica is not possible due to the read-only nature of the replica and would\nintroduce data inconsistency issues.\nOption C, migrating to DynamoDB, might be a viable long-term solution, but it's a complex undertaking\ninvolving a complete application rewrite and data migration. Also, DynamoDB is a NoSQL database and might\nnot suit the relational nature of the customer information or the reporting requirements. The cost and effort\ninvolved far outweigh the benefits for this immediate problem.\nOption D, scheduling the reporting queries for non-peak hours, is a temporary workaround, not a permanent\nsolution. During those non-peak hours, timeouts might still occur if the queries remain resource-intensive.\nAlso, it imposes limitations on the employees' ability to perform queries on demand.\nTherefore, Option A provides the most effective and least disruptive solution. It allows continued reporting\nwith minimal latency and doesn't interrupt order processing during business hours.\nAmazon RDS Read ReplicasScaling with Amazon RDS Read Replicas",
    "links": []
  },
  {
    "question": "CertyIQ\nA hospital wants to create digital copies for its large collection of historical written records. The hospital will\ncontinue to add hundreds of new documents each day. The hospitals data team will scan the documents and will\nupload the documents to the AWS Cloud.\nA solutions architect must implement a solution to analyze the documents, extract the medical information, and\nstore the documents so that an application can run SQL queries on the data. The solution must maximize\nscalability and operational efficiency.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "options": {
      "A": "Write the document information to an Amazon EC2 instance that runs a MySQL database.",
      "B": "Write the document information to an Amazon S3 bucket. Use Amazon Athena to query the data. This",
      "C": "Create an Auto Scaling group of Amazon EC2 instances to run a custom application that processes the",
      "D": "Create an AWS Lambda function that runs when new documents are uploaded. Use Amazon Rekognition to"
    },
    "answer": "B",
    "explanation": "The correct answer is BE. Here's why:\nB. Write the document information to an Amazon S3 bucket. Use Amazon Athena to query the data. This\noption offers a highly scalable and cost-effective solution for storing and querying the document information.\nAmazon S3 provides virtually unlimited storage capacity and high durability, suitable for storing the large\ncollection of historical records. Amazon Athena, a serverless query service, allows running SQL queries\ndirectly on data stored in S3, eliminating the need to manage a database server. This approach maximizes\noperational efficiency because no database maintenance is needed. Athena's pay-per-query pricing model\noptimizes cost based on actual usage.\nE. Create an AWS Lambda function that runs when new documents are uploaded. Use Amazon Textract to\nconvert the documents to raw text. Use Amazon Comprehend Medical to detect and extract relevant\nmedical information from the text. This option provides a scalable and efficient way to analyze the\ndocuments and extract the medical information. AWS Lambda allows running code without provisioning or\nmanaging servers, ensuring scalability and operational efficiency. Amazon Textract is specifically designed\nfor extracting text and data from scanned documents. Amazon Comprehend Medical can then efficiently\nprocess this raw text to identify and extract relevant medical information, such as diagnoses, medications,\nand procedures. This is a serverless and automated approach for data extraction and analysis, ideal for\nprocessing hundreds of new documents daily.\nWhy other options are incorrect:\nA: Using an EC2 instance with MySQL for this volume and growth is not scalable or operationally efficient.\nManaging a database server requires ongoing maintenance and scaling efforts.\nC: Creating an Auto Scaling group of EC2 instances to run a custom application for processing the scanned\nfiles adds complexity and operational overhead. It requires developing, deploying, and managing the custom\napplication, which deviates from maximizing operational efficiency.\nD: Amazon Rekognition is designed for image and video analysis, not OCR from documents. Amazon\nTranscribe Medical is for audio transcription and extraction, not for processing text from scanned documents.\nTextract and Comprehend Medical are tools specifically tailored for document processing needs.\nSupporting Documentation:\nAmazon S3: https://aws.amazon.com/s3/\nAmazon Athena: https://aws.amazon.com/athena/\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon Textract: https://aws.amazon.com/textract/\nAmazon Comprehend Medical: https://aws.amazon.com/comprehend/medical/",
    "links": [
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/athena/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/textract/",
      "https://aws.amazon.com/comprehend/medical/"
    ]
  },
  {
    "question": "CertyIQ\nA company is running a batch application on Amazon EC2 instances. The application consists of a backend with\nmultiple Amazon RDS databases. The application is causing a high number of reads on the databases. A solutions\narchitect must reduce the number of database reads while ensuring high availability.\nWhat should the solutions architect do to meet this requirement?",
    "options": {
      "A": "Add Amazon RDS read replicas.",
      "B": "Use Amazon ElastiCache for Redis.",
      "C": "Use Amazon Route 53 DNS caching",
      "D": "Use Amazon ElastiCache for Memcached."
    },
    "answer": "A",
    "explanation": "The most suitable solution is A. Add Amazon RDS read replicas.\nHere's why:\nThe core problem is high read load on the RDS databases. Read replicas are specifically designed to alleviate\nread load from the primary RDS database. They provide read-only copies of the data, allowing read queries to\nbe directed to the replicas instead of the primary database. This reduces the load on the primary database,\nimproving its performance and availability. Because the question mentions a need for HA, using Read Replicas\nfulfills that need also because they act as standby copies of the data if the primary were to fail.\nOption B, using ElastiCache for Redis, is a valid caching solution, but it requires application modifications to\nimplement caching logic. While it could reduce database reads, it's a more complex solution and may not be\nthe best first step without further details on the application.\nOption C, using Route 53 DNS caching, is not relevant to the problem. DNS caching improves the resolution\nspeed of domain names, but it doesn't address the database read load.\nOption D, using ElastiCache for Memcached, is similar to Redis; it's a viable caching solution but requires\napplication changes. Memcached is usually best for caching simple objects that you need with very low\nlatency (usually smaller than Redis use cases).\nTherefore, adding RDS read replicas is the simplest and most direct solution to reduce database read load\nwhile also enhancing availability, without necessitating significant code changes. Read Replicas also\nautomatically handle data replication from the source so theyre easy to manage and can readily be scaled for\nhigher performance.\nFurther reading:\nAmazon RDS Read Replicas:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.ReadReplicas.html\nAmazon ElastiCache: https://aws.amazon.com/elasticache/",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.ReadReplicas.html",
      "https://aws.amazon.com/elasticache/"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to run a critical application on AWS. The company needs to use Amazon EC2 for the applications\ndatabase. The database must be highly available and must fail over automatically if a disruptive event occurs.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Here's why:",
      "B": "Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon",
      "C": "Launch two EC2 instances, each in a different AWS Region. Install the database on both EC2 instances. Set",
      "D": "Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon"
    },
    "answer": "A",
    "explanation": "The most suitable solution for ensuring high availability and automatic failover of a database running on\nAmazon EC2 instances is option A. Here's why:\nOption A proposes launching two EC2 instances in separate Availability Zones (AZs) within the same AWS\nRegion. This leverages the principle of redundancy, a cornerstone of high availability architectures.\nAvailability Zones are physically isolated locations within an AWS Region, designed to be isolated from\nfailures in other AZs. Installing the database on both instances and configuring them as a cluster enables\nactive-passive or active-active replication. Replication ensures data is synchronized between the instances, so\nif one instance fails, the other can immediately take over.\nClustering software, like Pacemaker or Windows Server Failover Clustering, along with database-specific\nreplication tools, orchestrates the failover process. When the primary instance fails, the cluster automatically\npromotes the secondary instance to become the new primary, minimizing downtime and ensuring continuous\napplication availability.\nOption B is less ideal because relying solely on AMIs and CloudFormation for recovery introduces significant\ndowntime. Restoring from an AMI and reprovisioning a new instance are time-consuming processes, not\nsuitable for applications requiring automatic failover.\nOption C suggests using different AWS Regions. While cross-region failover provides disaster recovery\ncapabilities, it typically involves higher latency and complexity compared to multi-AZ setups. Failover across\nRegions often isn't automatic and may require manual intervention or more sophisticated configuration,\nmaking it less suitable for achieving automatic failover as required in the scenario.\nOption D utilizes EC2 automatic recovery. While helpful for some instance-level failures, automatic recovery\nonly restarts the instance on a new underlying hardware within the same AZ. It does not protect against AZ-\nwide events or data loss and it does not guarantee minimal downtime. It's beneficial but not a comprehensive\nHA solution on its own.\nIn summary, Option A delivers the best balance of high availability, automatic failover, and reasonable\ncomplexity by leveraging redundancy within a Region using multiple AZs and database replication.\nRelevant Links:\nAWS High Availability: https://aws.amazon.com/reliability/high-availability/\nAvailability Zones: https://aws.amazon.com/about-aws/global-infrastructure/\nDatabase Replication: Consult your specific database documentation (e.g., MySQL replication, PostgreSQL\nstreaming replication)",
    "links": [
      "https://aws.amazon.com/reliability/high-availability/",
      "https://aws.amazon.com/about-aws/global-infrastructure/"
    ]
  },
  {
    "question": "CertyIQ\nA companys order system sends requests from clients to Amazon EC2 instances. The EC2 instances process the\norders and then store the orders in a database on Amazon RDS. Users report that they must reprocess orders when\nthe system fails. The company wants a resilient solution that can process orders automatically if a system outage\noccurs.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Move the EC2 instances into an Auto Scaling group. Create an Amazon EventBridge (Amazon CloudWatch",
      "B": "Move the EC2 instances into an Auto Scaling group behind an Application Load Balancer (ALB). Update the",
      "C": "Move the EC2 instances into an Auto Scaling group. Configure the order system to send messages to an",
      "D": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Create an AWS Lambda function, and"
    },
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the best solution, along with supporting concepts and\nresources:\nOption C offers the most resilient and decoupled architecture for processing orders in the face of system\noutages. It leverages Amazon SQS to create a buffer between the order system and the EC2 instances,\nachieving asynchronous processing. When an order comes in, instead of directly calling an EC2 instance, the\norder system places a message describing the order into the SQS queue. This action decouples the order\ninitiation from the processing itself.\nThe EC2 instances, which are now part of an Auto Scaling group, are configured to consume messages from\nthe SQS queue. The Auto Scaling group ensures that even if some EC2 instances fail, others will be launched\nautomatically to maintain the processing capacity. Because the order requests are persisted in the SQS\nqueue, even if all EC2 instances are temporarily down, no orders are lost. When the EC2 instances come back\nonline, they start consuming messages from the queue and processing the orders.\nThis approach directly addresses the requirement of automatically processing orders during system outages.\nSQS guarantees at-least-once delivery, ensuring that each order is processed. The Auto Scaling group\nprovides high availability and fault tolerance for the processing instances. The decoupling between the order\nsystem and the processing layer is crucial for resilience.\nOption A is not ideal because EventBridge is typically used for event-driven architectures, not for reliable\nmessage queuing for order processing. ECS might be an unnecessary complication. Option B only adds an\nALB for distribution but does not resolve the problem of orders lost during outages. While the ALB improves\navailability, it does not buffer requests like SQS does. Option D is less suitable because SNS is primarily for\nnotifications, not guaranteed message delivery. Using Run Command to trigger processing on EC2 instances\nintroduces unnecessary complexity and management overhead. SQS is designed explicitly for reliable\nmessage queuing, which perfectly aligns with the problem statement's requirements.\nIn summary, option C leverages the strengths of SQS and Auto Scaling to create a robust and fault-tolerant\norder processing system, ensuring that orders are not lost and are automatically processed even during\nsystem outages.\nSupporting Links:\nAmazon SQS: https://aws.amazon.com/sqs/\nAmazon EC2 Auto Scaling: https://aws.amazon.com/autoscaling/",
    "links": [
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/autoscaling/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an application on a large fleet of Amazon EC2 instances. The application reads and writes entries\ninto an Amazon DynamoDB table. The size of the DynamoDB table continuously grows, but the application needs\nonly data from the last 30 days. The company needs a solution that minimizes cost and development effort.\nWhich solution meets these requirements?",
    "options": {
      "A": "Use an AWS CloudFormation template to deploy the complete solution. Redeploy the CloudFormation stack",
      "B": "Use an EC2 instance that runs a monitoring application from AWS Marketplace. Configure the monitoring",
      "C": "Configure Amazon DynamoDB Streams to invoke an AWS Lambda function when a new item is created in the",
      "D": "Extend the application to add an attribute that has a value of the current timestamp plus 30 days to each"
    },
    "answer": "D",
    "explanation": "The most cost-effective and easiest-to-implement solution is option D, leveraging DynamoDB's Time To Live\n(TTL) feature. TTL allows you to specify an attribute that determines when an item will be automatically\ndeleted from the table.\nHere's why option D is superior to the other options:\nCost Efficiency: TTL incurs no additional charges beyond the standard DynamoDB costs. Option B involves a\nmonitoring application on an EC2 instance, incurring EC2 costs, while option C involves Lambda invocations,\nincurring Lambda costs. Option A requires repeated full stack deployments which is extremely costly and\ntime-consuming.\nDevelopment Effort: Implementing TTL is straightforward. You simply add an attribute to your items\ncontaining the expiration timestamp (current timestamp + 30 days) and configure DynamoDB to recognize this\nattribute as the TTL attribute. The DynamoDB system automatically handles the deletion process.\nImplementing options B and C requires developing and maintaining custom logic for monitoring, deletion, and\npotentially conflict resolution. Option A requires creating a whole stack.\nAutomation: TTL is a fully automated process. Once configured, DynamoDB continuously monitors the TTL\nattribute and removes expired items. Options B and C require scheduled scripts or Lambda functions,\nintroducing complexities with scheduling, error handling, and ensuring data consistency.\nScalability and Reliability: DynamoDB's TTL is a built-in feature, designed to scale with your table size.\nOptions B and C are limited by the scalability and reliability of the EC2 instance or Lambda function,\nrespectively.\nMinimizes Impact on Application: Extending the application to add the TTL attribute is a minimal change with\nlittle to no operational overhead. Options B and C might introduce latency or contention if the monitoring or\ndeletion processes interfere with application operations.\nTTL efficiently handles data retention policies. DynamoDB handles cleanup in the background, minimizing\noperational overhead and complexity. This aligns with cloud computing principles of automation, cost\noptimization, and simplicity.\nFurther research on DynamoDB TTL can be found at:\nDynamoDB TTL",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has a Microsoft .NET application that runs on an on-premises Windows Server. The application stores\ndata by using an Oracle Database Standard Edition server. The company is planning a migration to AWS and wants\nto minimize development changes while moving the application. The AWS application environment should be\nhighly available.\nWhich combination of actions should the company take to meet these requirements? (Choose two.)",
    "options": {
      "A": "Refactor the application as serverless with AWS Lambda functions running .NET Core.",
      "B": "Rehost the application in AWS Elastic Beanstalk with the .NET platform in a Multi-AZ deployment.",
      "C": "Replatform the application to run on Amazon EC2 with the Amazon Linux Amazon Machine Image (AMI).",
      "D": "Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Amazon"
    },
    "answer": "B",
    "explanation": "The requirement is to migrate a .NET application with minimal code changes to a highly available AWS\nenvironment while retaining the Oracle database.\nOption B, rehosting the application in AWS Elastic Beanstalk with the .NET platform in a Multi-AZ deployment,\ndirectly addresses the minimal changes requirement. Elastic Beanstalk simplifies deployment and\nmanagement of web applications. By selecting the .NET platform and deploying it in a Multi-AZ environment,\nthe application gains high availability through automatic instance replacement in case of failure across\ndifferent Availability Zones. This 'lift and shift' approach requires very little to no code changes to the .NET\napplication itself.\nOption E, using AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Oracle\non Amazon RDS in a Multi-AZ deployment, fulfills the database persistence requirement with minimal\ndisruption. DMS allows migration of databases with minimal downtime. Migrating to Oracle RDS maintains\ncompatibility with the existing application architecture. Deploying Oracle on RDS in a Multi-AZ configuration\nprovides high availability for the database.\nOption A is incorrect because refactoring to serverless Lambda functions requires significant code changes to\nthe application, violating the requirement for minimal development changes.\nOption C is incorrect because replatforming to EC2 with Amazon Linux requires changes to the .NET\napplication to be compatible with Linux. It also does not offer any managed service to make deployment and\nmanagement easier, and it increases the overhead of managing the underlying OS.\nOption D is incorrect because migrating to DynamoDB would require significant changes to the application's\ndata access layer to work with a NoSQL database, which is not acceptable since the requirement is to\nminimize code changes. DynamoDB is not compatible with the existing application code.\nTherefore, the correct answer is BE.\nRelevant links:\nAWS Elastic Beanstalk: https://aws.amazon.com/elasticbeanstalk/\nAWS Database Migration Service (DMS): https://aws.amazon.com/dms/\nAmazon RDS Multi-AZ: https://aws.amazon.com/rds/features/multi-az/",
    "links": [
      "https://aws.amazon.com/elasticbeanstalk/",
      "https://aws.amazon.com/dms/",
      "https://aws.amazon.com/rds/features/multi-az/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a containerized application on a Kubernetes cluster in an on-premises data center. The company is\nusing a MongoDB database for data storage. The company wants to migrate some of these environments to AWS,\nbut no code changes or deployment method changes are possible at this time. The company needs a solution that\nminimizes operational overhead.\nWhich solution meets these requirements?",
    "options": {
      "A": "Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes for compute and",
      "B": "In summary, EKS + Fargate provides Kubernetes compatibility and serverless compute, and DocumentDB",
      "C": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes for compute and",
      "D": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute and Amazon"
    },
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the best solution, aligning with the given requirements and\nconstraints:\nThe core requirement is migrating the application to AWS with minimal code and deployment method changes\nwhile minimizing operational overhead. The application uses a containerized setup on Kubernetes and\nMongoDB for data storage.\nOption D, using Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute and\nAmazon DocumentDB (with MongoDB compatibility) for data storage, is the ideal choice. EKS allows the\ncompany to continue using Kubernetes without significant modifications to their existing deployment process,\nfulfilling the \"no deployment method changes\" constraint. Fargate provides serverless compute, eliminating\nthe need to manage EC2 instances, thus reducing operational overhead significantly. This aligns with the\n\"minimizing operational overhead\" requirement.\nCritically, Amazon DocumentDB offers MongoDB compatibility. This is crucial because the prompt explicitly\nstates that \"no code changes\" are possible. Switching to DynamoDB (as suggested in options B and C) would\nrequire significant code changes to interact with the new database API. DocumentDB's MongoDB\ncompatibility allows the application to interact with the database without altering the existing codebase.\nOption A (ECS with EC2 and MongoDB on EC2) fails to minimize operational overhead. Managing EC2\ninstances for both compute and the database adds significant administrative burden compared to Fargate and\nDocumentDB. Option B fails due to the required code changes for DynamoDB integration. Option C also fails\ndue to the need for code changes for DynamoDB.\nIn summary, EKS + Fargate provides Kubernetes compatibility and serverless compute, and DocumentDB\noffers MongoDB compatibility and managed service characteristics, perfectly addressing all constraints and\nrequirements.\nSupporting links:\nAmazon EKS: https://aws.amazon.com/eks/\nAWS Fargate: https://aws.amazon.com/fargate/\nAmazon DocumentDB: https://aws.amazon.com/documentdb/",
    "links": [
      "https://aws.amazon.com/eks/",
      "https://aws.amazon.com/fargate/",
      "https://aws.amazon.com/documentdb/"
    ]
  },
  {
    "question": "CertyIQ\nA telemarketing company is designing its customer call center functionality on AWS. The company needs a\nsolution that provides multiple speaker recognition and generates transcript files. The company wants to query the\ntranscript files to analyze the business patterns. The transcript files must be stored for 7 years for auditing\npurposes.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use",
      "B": "Use Amazon Transcribe for multiple speaker recognition. Use Amazon Athena for transcript file analysis.",
      "C": "Use Amazon Translate for multiple speaker recognition. Store the transcript files in Amazon Redshift. Use",
      "D": "Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use"
    },
    "answer": "B",
    "explanation": "The correct answer is B because it leverages the appropriate AWS services for the specific requirements\noutlined. Amazon Transcribe is specifically designed for converting speech to text, including capabilities for\nmultiple speaker recognition (speaker diarization). This directly addresses the need for transcribing call\ncenter conversations with speaker identification.\nAmazon Athena is a serverless, interactive query service that makes it easy to analyze data directly in Amazon\nS3 using standard SQL. This satisfies the requirement for querying transcript files to analyze business\npatterns.\nStoring the transcript files in Amazon S3 (as implied in the problem description in conjunction with Athena)\nensures cost-effective and durable storage for the required 7-year retention period. S3's storage classes can\nbe configured to optimize cost based on access frequency.\nOption A is incorrect because while Rekognition can analyze images and videos, it's not primarily designed for\nreal-time speech transcription and speaker diarization in the same way as Transcribe. While machine learning\nmodels could be used for analysis, Athena provides a simpler, more readily available solution for querying the\ndata.\nOption C is incorrect because Amazon Translate is used for language translation, not speech-to-text\ntranscription. Amazon Redshift is a data warehouse designed for large-scale data analytics, but using it solely\nfor querying relatively small transcript files is an overkill and less cost-effective than Athena.\nOption D is incorrect because, similar to option A, Amazon Rekognition is primarily designed for image and\nvideo analysis. Amazon Textract is used for extracting text and data from scanned documents and PDFs, not\nfor analyzing speech transcripts.\nIn summary, Amazon Transcribe efficiently handles the audio transcription and speaker recognition, while\nAmazon Athena provides a direct and cost-effective way to query the resulting transcript files stored in\nAmazon S3. This approach aligns perfectly with the prompt's need for transcription, analysis, and long-term\nstorage.\nRelevant Links:\nAmazon Transcribe: https://aws.amazon.com/transcribe/\nAmazon Athena: https://aws.amazon.com/athena/\nAmazon S3: https://aws.amazon.com/s3/",
    "links": [
      "https://aws.amazon.com/transcribe/",
      "https://aws.amazon.com/athena/",
      "https://aws.amazon.com/s3/"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts its application on AWS. The company uses Amazon Cognito to manage users. When users log in\nto the application, the application fetches required data from Amazon DynamoDB by using a REST API that is\nhosted in Amazon API Gateway. The company wants an AWS managed solution that will control access to the\nREST API to reduce development efforts.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Configure an AWS Lambda function to be an authorizer in API Gateway to validate which user made the",
      "B": "For each user, create and assign an API key that must be sent with each request. Validate the key by using an",
      "C": "Send the users email address in the header with every request. Invoke an AWS Lambda function to validate",
      "D": "Configure an Amazon Cognito user pool authorizer in API Gateway to allow Amazon Cognito to validate each"
    },
    "answer": "D",
    "explanation": "The correct answer is D: Configure an Amazon Cognito user pool authorizer in API Gateway to allow Amazon\nCognito to validate each request. Here's why:\nThe problem requires an AWS-managed solution to control API access using Cognito for user management\nwith minimal operational overhead.\nOption D directly leverages the integration between Cognito and API Gateway. Cognito User Pool Authorizers\nwithin API Gateway allow API Gateway to directly validate the identity token (JWT) provided by Cognito upon\nsuccessful user authentication. This eliminates the need for custom code (like Lambda functions) for token\nvalidation, significantly reducing development and operational overhead. API Gateway handles the token\nverification process based on the configured User Pool.\nOption A, using a Lambda function as an authorizer, introduces unnecessary complexity. While it works, it\nrequires writing, deploying, and maintaining the Lambda function. This adds operational overhead that the\nproblem statement seeks to avoid. It also involves potential latency from invoking the Lambda function.\nOption B, using API Keys per user and a Lambda validator, introduces both operational overhead and security\nconcerns. Managing API keys for each user becomes complex, and the need to validate the key with a Lambda\nfunction increases latency and operational burden. API keys are generally for API product access control\nrather than individual user authorization.\nOption C, sending the email address and validating with a Lambda function, is the least secure and most\ninefficient solution. Relying on the email address in a header is easily spoofed. Also, querying a Lambda\nfunction for each API call increases latency and creates an unnecessary dependency. This also likely requires\nfetching user data from DynamoDB within the lambda, adding further overhead.\nCognito User Pool Authorizers offer seamless integration, scalability, and security, making them the ideal\nchoice for managed user-based API authorization when using Cognito for identity management. This approach\nkeeps the solution within the AWS-managed ecosystem, minimizing custom code and operational tasks.\nFurther Research:\nAPI Gateway Authorizers: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-\nlambda-authorizer.html\nCognito User Pools: https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-\npools.html\nAPI Gateway Cognito Authorizer:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html",
    "links": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-",
      "https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is developing a marketing communications service that targets mobile app users. The company needs\nto send confirmation messages with Short Message Service (SMS) to its users. The users must be able to reply to\nthe SMS messages. The company must store the responses for a year for analysis.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Create an Amazon Connect contact flow to send the SMS messages. Use AWS Lambda to process the",
      "B": "Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data",
      "C": "Use Amazon Simple Queue Service (Amazon SQS) to distribute the SMS messages. Use AWS Lambda to",
      "D": "Create an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe an Amazon Kinesis data"
    },
    "answer": "B",
    "explanation": "The correct answer is B: Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an\nAmazon Kinesis data stream for analysis and archiving.\nHere's why:\nAmazon Pinpoint: Pinpoint is specifically designed for marketing communication and user engagement,\nmaking it the ideal choice for sending SMS confirmation messages to mobile app users. It provides features\nfor creating targeted campaigns and journeys.\nSMS Capabilities: Pinpoint supports two-way SMS messaging, allowing users to reply to the confirmation\nmessages. It handles the complexities of sending and receiving SMS at scale.\nEvent Streaming: Pinpoint integrates with Amazon Kinesis Data Streams. This integration is crucial for\ncapturing and analyzing user responses to the SMS messages. The event data stream will contain information\nabout delivered messages, user replies, and other relevant events.\nData Analysis and Archiving: By streaming events to Kinesis Data Streams, the company can process,\nanalyze, and archive the data for a year as required. Kinesis Data Streams enables real-time data ingestion\nand processing, allowing for timely analysis. The stream can then be connected to services like Amazon S3 for\nlong-term storage and archiving. Amazon Athena or Amazon Redshift can be used to query this data for\nanalysis.\nLet's examine why other options are less suitable:\nA (Amazon Connect): Amazon Connect is primarily a contact center solution for voice and chat. While it can\nsend SMS messages, it's not its core function and is not ideal for bulk marketing communications.\nFurthermore, it is more complex to set up for the given scenario.\nC (Amazon SQS): Amazon SQS is a queuing service for decoupling components. It's not designed for sending\nSMS messages directly to users. While Lambda could be used to trigger SMS sends, this is not as streamlined\nor feature-rich as using Pinpoint.\nD (Amazon SNS): Amazon SNS is a notification service for pub/sub messaging. While SNS can send SMS, it's\nprimarily for one-way notifications, not for managing two-way communication and tracking responses. SNS\nFIFO (First-In, First-Out) topics are useful for ordered message delivery, which is not a key requirement here.\nFurther, sending SMS via SNS and routing replies and data streams to SNS is not straightforward or optimal\nas using Pinpoint.\nAuthoritative Links:\nAmazon Pinpoint: https://aws.amazon.com/pinpoint/\nAmazon Kinesis Data Streams: https://aws.amazon.com/kinesis/data-streams/",
    "links": [
      "https://aws.amazon.com/pinpoint/",
      "https://aws.amazon.com/kinesis/data-streams/"
    ]
  },
  {
    "question": "CertyIQ\nA company is planning to move its data to an Amazon S3 bucket. The data must be encrypted when it is stored in\nthe S3 bucket. Additionally, the encryption key must be automatically rotated every year.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Move the data to the S3 bucket. Use server-side encryption with Amazon S3 managed encryption keys (SSE-",
      "B": "Create an AWS Key Management Service (AWS KMS) customer managed key. Enable automatic key rotation.",
      "C": "Create an AWS Key Management Service (AWS KMS) customer managed key. Set the S3 buckets default",
      "D": "Encrypt the data with customer key material before moving the data to the S3 bucket. Create an AWS Key"
    },
    "answer": "B",
    "explanation": "The correct answer is B because it offers the simplest and most automated solution for encrypting data at\nrest in S3 with automatic key rotation, minimizing operational overhead. Let's break down why:\nOption B leverages AWS KMS customer managed keys with automatic key rotation. When you create a KMS\nkey, you can enable automatic key rotation, which automatically rotates the key every year. Setting the S3\nbucket's default encryption to use this KMS key ensures that all new objects uploaded to the bucket are\nautomatically encrypted using the key. This eliminates the need for manual encryption or key management.\nOption A uses SSE-S3. While SSE-S3 encrypts data, the key rotation behavior is managed by AWS and not\ncustomizable. The exact details of their key rotation and control aren't exposed to the user, making it less\ntransparent and potentially not meeting a specific yearly rotation requirement.\nOption C involves manual key rotation, which directly contradicts the requirement for \"LEAST operational\noverhead\". Manual rotation introduces the risk of human error and requires active monitoring and intervention.\nOption D, using customer-provided key material, adds significant complexity. It requires you to manage the\ninitial key material and import it into KMS. While automatic rotation is enabled, managing the initial key\nmaterial lifecycle and its secure import introduces unnecessary overhead compared to Option B, where KMS\ngenerates and rotates the key automatically.\nTherefore, Option B provides the best balance of security, automation, and minimal operational overhead. It\nallows AWS KMS to handle key generation and rotation transparently while ensuring S3 data is encrypted at\nrest.\nSupporting Documentation:\nAWS KMS Key Rotation: https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html\nProtecting Data Using Server-Side Encryption:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html",
    "links": [
      "https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html"
    ]
  },
  {
    "question": "CertyIQ\nThe customers of a finance company request appointments with financial advisors by sending text messages. A\nweb application that runs on Amazon EC2 instances accepts the appointment requests. The text messages are\npublished to an Amazon Simple Queue Service (Amazon SQS) queue through the web application. Another\napplication that runs on EC2 instances then sends meeting invitations and meeting confirmation email messages\nto the customers. After successful scheduling, this application stores the meeting information in an Amazon\nDynamoDB database.\nAs the company expands, customers report that their meeting invitations are taking longer to arrive.\nWhat should a solutions architect recommend to resolve this issue?",
    "options": {
      "A": "Add a DynamoDB Accelerator (DAX) cluster in front of the DynamoDB database. DAX is an in-memory",
      "B": "Add an Amazon API Gateway API in front of the web application that accepts the appointment requests.",
      "C": "Add an Amazon CloudFront distribution. Set the origin as the web application that accepts the",
      "D": "Add an Auto Scaling group for the application that sends meeting invitations."
    },
    "answer": "D",
    "explanation": "The correct answer is D. Add an Auto Scaling group for the application that sends meeting invitations.\nConfigure the Auto Scaling group to scale based on the depth of the SQS queue.\nHere's why:\nThe problem is that meeting invitations are taking longer to arrive as the company expands. This suggests a\nbottleneck in the application responsible for processing messages from the SQS queue and sending out\ninvitations. This application's capacity to handle the increasing workload is insufficient.\nOption D directly addresses this bottleneck. By adding an Auto Scaling group for the application that sends\nmeeting invitations, the system can automatically scale the number of EC2 instances based on the depth of\nthe SQS queue (i.e., the number of messages waiting to be processed). When the queue depth increases (more\nappointment requests), the Auto Scaling group launches more instances to process the messages\nconcurrently, reducing the processing time and ensuring faster delivery of meeting invitations. This aligns\nwith a queue-based load leveling pattern where SQS acts as a buffer and Auto Scaling adjusts the processing\ncapacity based on the backlog.\nHere's why the other options are not the best fit:\nA. Add a DynamoDB Accelerator (DAX) cluster in front of the DynamoDB database. DAX is an in-memory\ncache for DynamoDB. While DAX can improve read performance for data stored in DynamoDB, the bottleneck\nis in processing the messages from the queue and sending invitations, not necessarily in reading the meeting\ninformation from DynamoDB. Therefore, DAX would not directly solve the problem of delayed invitations.\nB. Add an Amazon API Gateway API in front of the web application that accepts the appointment requests.\nAPI Gateway is useful for managing and securing APIs, and could add rate limiting. But the problem is the\ninvitation delivery which occurs after the request is already handled by the web application. This does not\naddress the delay in sending invitations after appointment requests are placed in the SQS queue.\nC. Add an Amazon CloudFront distribution. Set the origin as the web application that accepts the\nappointment requests. CloudFront is a content delivery network (CDN). It's designed to cache and deliver\nstatic and dynamic content closer to users. It's not relevant to the problem of processing SQS messages and\nsending invitations; the performance issue is in the backend processing, not in delivering the web application\nitself.\nIn conclusion, scaling the processing capacity of the application that handles meeting invitations (option D) is\nthe most direct and effective solution to address the increasing delays in meeting invitation delivery as the\ncompany expands. The SQS queue depth serves as a perfect metric to trigger scaling events, ensuring the\napplication can keep up with the growing demand.\nAuthoritative Links:\nAmazon SQS Auto Scaling: https://aws.amazon.com/blogs/compute/automatically-scale-your-amazon-sqs-\nprocessing-using-aws-lambda/\nAmazon Auto Scaling: https://aws.amazon.com/autoscaling/\nQueue-Based Load Leveling Pattern: https://docs.aws.amazon.com/prescriptive-guidance/patterns/queue-\nbased-load-leveling.html",
    "links": [
      "https://aws.amazon.com/blogs/compute/automatically-scale-your-amazon-sqs-",
      "https://aws.amazon.com/autoscaling/",
      "https://docs.aws.amazon.com/prescriptive-guidance/patterns/queue-"
    ]
  },
  {
    "question": "CertyIQ\nAn online retail company has more than 50 million active customers and receives more than 25,000 orders each\nday. The company collects purchase data for customers and stores this data in Amazon S3. Additional customer\ndata is stored in Amazon RDS.\nThe company wants to make all the data available to various teams so that the teams can perform analytics. The\nsolution must provide the ability to manage fine-grained permissions for the data and must minimize operational\noverhead.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Migrate the purchase data to write directly to Amazon RDS. Use RDS access controls to limit access.",
      "B": "Schedule an AWS Lambda function to periodically copy data from Amazon RDS to Amazon S3. Create an",
      "C": "Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS.",
      "D": "Create an Amazon Redshift cluster. Schedule an AWS Lambda function to periodically copy data from"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it provides a centralized, scalable, and secure solution for data analytics with\nfine-grained access control, minimizing operational overhead using AWS Lake Formation. Here's a detailed\njustification:\nAWS Lake Formation for Data Lake: Lake Formation is designed for building, securing, and managing data\nlakes. It centralizes data access management and simplifies the process of setting up a data lake.\nhttps://aws.amazon.com/lake-formation/\nAWS Glue for Data Catalog and ETL: AWS Glue is used to create a data catalog and extract, transform, and\nload (ETL) data. In this solution, a Glue JDBC connection is used to access the data in Amazon RDS, and Glue\ncan catalog the data stored in S3. https://aws.amazon.com/glue/\nFine-grained Access Control: Lake Formation's primary benefit is its fine-grained access control. You can\ndefine permissions at the table and column level, ensuring that different teams only access the data they\nneed. This meets the requirement of managing permissions for the data.\nS3 Bucket Registration: Registering the S3 bucket in Lake Formation makes the purchase data accessible\nand governed by the Lake Formation policies.\nScalability: This solution scales well for 50 million customers and 25,000 orders per day. S3 is highly scalable\nfor data storage, and Lake Formation manages access regardless of data size.\nMinimizing Operational Overhead: Lake Formation automates many of the manual tasks associated with data\nlake setup, security, and management. This reduces operational overhead compared to managing access\ncontrols through S3 policies or RDS access controls manually.\nWhy other options are not ideal:\nA: Migrating purchase data directly to RDS isn't scalable or cost-effective for analytics. RDS is designed for\ntransactional data and is not optimized for large-scale analytics.\nB: Using S3 policies for access control is less granular and more complex to manage than Lake Formation,\nespecially for numerous teams and datasets. Furthermore, copying data from RDS periodically adds\noperational overhead and introduces potential data consistency issues.\nD: Amazon Redshift is a data warehouse, which might be suitable for complex analytics. However, periodically\ncopying large datasets from S3 and RDS to Redshift using Lambda incurs operational overhead, potential\ndata consistency issues, and more complex ETL processes. Redshift access controls also lack the fine-\ngrained control offered by Lake Formation.",
    "links": [
      "https://aws.amazon.com/lake-formation/",
      "https://aws.amazon.com/glue/"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts a marketing website in an on-premises data center. The website consists of static documents and\nruns on a single server. An administrator updates the website content infrequently and uses an SFTP client to\nupload new documents.\nThe company decides to host its website on AWS and to use Amazon CloudFront. The companys solutions\narchitect creates a CloudFront distribution. The solutions architect must design the most cost-effective and\nresilient architecture for website hosting to serve as the CloudFront origin.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Create a virtual server by using Amazon Lightsail. Configure the web server in the Lightsail instance. Upload",
      "B": "Create an AWS Auto Scaling group for Amazon EC2 instances. Use an Application Load Balancer. Upload",
      "C": "Create a private Amazon S3 bucket. Use an S3 bucket policy to allow access from a CloudFront origin access",
      "D": "Create a public Amazon S3 bucket. Configure AWS Transfer for SFTP. Configure the S3 bucket for website"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it offers the most cost-effective and resilient solution for hosting static\nwebsite content as a CloudFront origin. Here's a detailed justification:\nCost-Effectiveness: Amazon S3 offers a very low storage cost compared to virtual servers like Lightsail or\nEC2. S3's pay-as-you-go pricing model for storage and data transfer is ideal for static content with infrequent\nupdates. Lightsail and EC2 incur costs even when the website isn't being heavily accessed.\nResilience: S3 provides high availability and durability due to its distributed nature across multiple\nAvailability Zones. This means that the website content is highly resilient to failures. EC2 instances, while they\ncan be made resilient with Auto Scaling, require more configuration and introduce more points of failure.\nSecurity: Using a private S3 bucket with an Origin Access Identity (OAI) provides a secure way to restrict\ndirect access to the S3 bucket. CloudFront can then access the content through the OAI, while public access\nis blocked. This prevents users from bypassing CloudFront and accessing the S3 bucket directly, enhancing\nsecurity and enabling control over caching.\nSimplified Updates: The AWS CLI is a robust and scriptable tool for uploading content to S3. It provides a\nstreamlined and automated way to update the website content. Using SFTP, as suggested in other options,\nincreases the management overhead.\nOption A (Lightsail): Lightsail, although simple, is not as cost-effective for static content hosting as S3. It also\ninvolves more management of the virtual server.\nOption B (EC2 with ALB): EC2 with an Application Load Balancer (ALB) is more complex and expensive for\nserving static content. It is designed for dynamic applications and requires more configuration.\nOption D (Public S3 with SFTP): Making the S3 bucket public would expose the website content directly,\nbypassing the security benefits of CloudFront's OAI. Using AWS Transfer for SFTP is unnecessary, complex,\nand not cost-effective for this scenario, and S3 website hosting feature is not recommended for production\nworkloads.\nIn summary, utilizing a private S3 bucket with an OAI for CloudFront provides a secure, resilient, cost-\neffective and easier-to-manage solution for hosting static website content.\nAuthoritative Links:\nAmazon S3 Pricing: https://aws.amazon.com/s3/pricing/\nAmazon CloudFront Origin Access Identity (OAI):\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-\nto-s3.html\nAWS CLI: https://aws.amazon.com/cli/",
    "links": [
      "https://aws.amazon.com/s3/pricing/",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-",
      "https://aws.amazon.com/cli/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to manage Amazon Machine Images (AMIs). The company currently copies AMIs to the same\nAWS Region where the AMIs were created. The company needs to design an application that captures AWS API\ncalls and sends alerts whenever the Amazon EC2 CreateImage API operation is called within the companys\naccount.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Create an AWS Lambda function to query AWS CloudTrail logs and to send an alert when a CreateImage API",
      "B": "Configure AWS CloudTrail with an Amazon Simple Notification Service (Amazon SNS) notification that",
      "C": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the CreateImage API call. Configure",
      "D": "Configure an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a target for AWS CloudTrail logs."
    },
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the best solution, along with explanations of why the other\noptions are less suitable:\nJustification for Option C (Amazon EventBridge rule with SNS target):\nThis solution offers the least operational overhead and is the most direct way to address the requirement.\nAmazon EventBridge is a serverless event bus that makes it easy to connect applications with data from a\nvariety of sources.\nReal-time Event Detection: EventBridge allows you to create rules that react to events happening in your\nAWS environment in near real-time. You can define a rule that specifically triggers when the CreateImage API\ncall is made. This ensures immediate detection of the AMI creation.\nDirect Integration with SNS: EventBridge can directly integrate with Amazon SNS (Simple Notification\nService). This eliminates the need for intermediate services or custom coding to handle the event processing\nand notification. The EventBridge rule simply forwards the event data to the SNS topic.\nMinimal Configuration: Setting up an EventBridge rule for the CreateImage event and configuring the SNS\ntarget is relatively straightforward and requires minimal configuration.\nServerless Architecture: Both EventBridge and SNS are serverless services, so you don't need to manage any\nunderlying infrastructure, reducing operational overhead.\nScalability and Reliability: EventBridge and SNS are highly scalable and reliable AWS services, ensuring your\nmonitoring application can handle changes in API call volume.\nWhy other options are less suitable:\nOption A (Lambda querying CloudTrail logs): While feasible, this introduces unnecessary complexity. The\nLambda function would need to poll CloudTrail logs, parse the logs, and search for CreateImage events. This is\nless efficient than a push-based system like EventBridge and requires more coding and operational\nmaintenance. Additionally, CloudTrail logs have a delay, making the alerts potentially not as real-time.\nOption B (CloudTrail with SNS and Athena): This option is overly complex. While CloudTrail can send\nnotifications on log updates to S3 and then trigger SNS, using Athena to query the logs adds significant\noverhead. Athena is designed for analyzing large datasets and is not ideal for real-time event detection. The\nsetup and maintenance of the Athena table and queries would add unnecessary operational burden. The initial\nCloudTrail SNS notification only indicates that a log file has been updated, not necessarily that a CreateImage\ncall has been made.\nOption D (SQS FIFO with Lambda and SNS): This solution also introduces unnecessary complexity. While\nSQS FIFO can ensure message order, it's not needed for this specific requirement. The Lambda function is still\nrequired to poll the SQS queue and parse the CloudTrail logs for the CreateImage event. The use of SQS adds\nan additional layer of infrastructure to manage and monitor, increasing operational overhead.\nAuthoritative Links for Further Research:\nAmazon EventBridge: https://aws.amazon.com/eventbridge/\nAmazon SNS: https://aws.amazon.com/sns/\nAWS CloudTrail: https://aws.amazon.com/cloudtrail/",
    "links": [
      "https://aws.amazon.com/eventbridge/",
      "https://aws.amazon.com/sns/",
      "https://aws.amazon.com/cloudtrail/"
    ]
  },
  {
    "question": "CertyIQ\nA company owns an asynchronous API that is used to ingest user requests and, based on the request type,\ndispatch requests to the appropriate microservice for processing. The company is using Amazon API Gateway to\ndeploy the API front end, and an AWS Lambda function that invokes Amazon DynamoDB to store user requests\nbefore dispatching them to the processing microservices.\nThe company provisioned as much DynamoDB throughput as its budget allows, but the company is still\nexperiencing availability issues and is losing user requests.\nWhat should a solutions architect do to address this issue without impacting existing users?",
    "options": {
      "A": "Add throttling on the API Gateway with server-side throttling limits.",
      "B": "Buffering writes is still necessary in this scenario, and DAX doesn't do",
      "C": "Create a secondary index in DynamoDB for the table with the user requests.",
      "D": "Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer\nwrites to DynamoDB.\nHere's a detailed justification:\nThe problem states that the company is experiencing availability issues and losing user requests due to\nDynamoDB being overwhelmed. This implies that the write throughput to DynamoDB is exceeding the\nprovisioned capacity, even though the company has provisioned as much as their budget allows.\nOption D addresses this directly by introducing a queue (Amazon SQS) in between the Lambda function and\nDynamoDB. When the Lambda function receives a request, instead of immediately writing to DynamoDB, it\nplaces a message in the SQS queue. Another Lambda function (or the same one, configured differently) then\nretrieves messages from the SQS queue and writes them to DynamoDB at a controlled rate.\nBuffering: SQS acts as a buffer, absorbing bursts of traffic and smoothing out the write load to DynamoDB. If\nDynamoDB becomes temporarily unavailable or throttles requests, the messages remain in the queue until\nthey can be successfully processed. This prevents the loss of user requests.\nAsynchronous processing: The API can quickly return a success response after placing the message in the\nqueue, minimizing latency for the user. The actual DynamoDB write happens asynchronously.\nDecoupling: SQS decouples the API and the DynamoDB write process. If DynamoDB is having issues, the API\ncan continue to accept requests without being directly affected.\nOption A is incorrect because throttling at API Gateway only limits the number of incoming requests. It\ndoesn't address the underlying issue of DynamoDB being overwhelmed. While it prevents more requests from\nhitting DynamoDB, it does so by rejecting user requests, which is undesirable as the problem statement\nexplicitly mentions avoiding impacting existing users.\nOption B is incorrect because DAX is a read-through/write-through cache. While DAX can improve read\nperformance and reduce read load on DynamoDB, it doesn't inherently solve the problem of exceeding write\ncapacity. DAX sits in front of DynamoDB, so if the write capacity is exceeded, the bottleneck will still exist\nwhen DAX needs to write to DynamoDB. Buffering writes is still necessary in this scenario, and DAX doesn't do\nthat.\nOption C is incorrect because secondary indexes improve query performance for different access patterns.\nThey do not inherently solve throughput issues related to exceeding write capacity. They can even increase\nthe write load as indexes also need to be updated during writes.\nTherefore, using SQS to buffer writes to DynamoDB is the most effective way to prevent data loss and\nimprove the application's availability without impacting existing users.\nHere are some authoritative links for further research:\nAmazon SQS: https://aws.amazon.com/sqs/\nAmazon DynamoDB: https://aws.amazon.com/dynamodb/\nServerless Architectures with AWS Lambda: https://aws.amazon.com/serverless/\nDynamoDB best practices: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-\npractices.html",
    "links": [
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/dynamodb/",
      "https://aws.amazon.com/serverless/",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to move data from an Amazon EC2 instance to an Amazon S3 bucket. The company must ensure\nthat no API calls and no data are routed through public internet routes. Only the EC2 instance can have access to\nupload data to the S3 bucket.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is located. Attach a",
      "B": "Create a gateway VPC endpoint for Amazon S3 in the Availability Zone where the EC2 instance is located.",
      "C": "Run the nslookup tool from inside the EC2 instance to obtain the private IP address of the S3 buckets",
      "D": "Use the AWS provided, publicly available ip-ranges.json file to obtain the private IP address of the S3"
    },
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A is the correct answer, along with relevant cloud computing\nconcepts and supporting links:\nThe question emphasizes secure data transfer from an EC2 instance to S3 without using the public internet\nand restricting access to only that EC2 instance. Option A directly addresses these requirements using\ninterface VPC endpoints and S3 bucket policies.\nHere's why option A is correct:\nInterface VPC Endpoint for S3: Interface endpoints use AWS PrivateLink to provide private connectivity to\nS3. They expose S3 as a network interface within your VPC, allowing EC2 instances to access S3 without\ntraversing the public internet. Traffic stays within the AWS network. This fulfills the requirement to avoid\npublic internet routes.\nAWS PrivateLink Documentation\nUsing S3 with VPC Endpoints\nS3 Bucket Policy: The S3 bucket policy restricts access to the bucket. By allowing access only from the EC2\ninstance's IAM role, you prevent other resources or users from uploading data to the bucket, enforcing the\nsecurity requirement. The IAM role attached to the EC2 instance provides the necessary permissions to\nupload the data via the private endpoint.\nS3 Bucket Policies\nHere's why the other options are incorrect:\nOption B (Gateway VPC Endpoint): Gateway endpoints only support S3 and DynamoDB and, more\nimportantly, require modifications to the route tables. They don't provide as granular control as interface\nendpoints and might not be suitable when needing to restrict access to a single EC2 instance based on its IAM\nrole via the endpoint. Gateway Endpoints operate on the principle of routing traffic destined for S3 (or\nDynamoDB) through the gateway instead of the internet.\nOptions C and D (Using Public IP Ranges and Route Tables): These options are incorrect because they\ninvolve mapping the EC2 instance directly to the IP addresses of the S3 service. While technically feasible,\nthis approach is highly discouraged for several reasons:\nReliance on Public IP Addresses: The approach relies on the public IP address of the S3 endpoint and routing\ntables, which will use public internet routes, contradicting the requirement of not using the public internet.\nMaintenance Overhead: IP addresses of AWS services can change, which would require constant monitoring\nand updating of your routing tables.\nSecurity Concerns: Bypassing AWS PrivateLink introduces potential security vulnerabilities.\nAWS IP Address Ranges\nIn summary, option A provides the most secure and manageable solution by utilizing interface VPC endpoints\nfor private connectivity and S3 bucket policies for access control, fulfilling all the requirements outlined in\nthe question.",
    "links": []
  },
  {
    "question": "CertyIQ\nA solutions architect is designing the architecture of a new application being deployed to the AWS Cloud. The\napplication will run on Amazon EC2 On-Demand Instances and will automatically scale across multiple Availability\nZones. The EC2 instances will scale up and down frequently throughout the day. An Application Load Balancer\n(ALB) will handle the load distribution. The architecture needs to support distributed session data management.\nThe company is willing to make changes to code if needed.\nWhat should the solutions architect do to ensure that the architecture supports distributed session data\nmanagement?",
    "options": {
      "A": "Use Amazon ElastiCache to manage and store session data.",
      "B": "Session Affinity (Sticky Sessions): Sticky sessions route a user to the same EC2 instance for the duration",
      "C": "Session Manager: Session Manager is a capability of AWS Systems Manager that allows you to manage",
      "D": "GetSessionToken API operation in AWS STS: STS GetSessionToken is used for creating temporary"
    },
    "answer": "A",
    "explanation": "The correct answer is A: Use Amazon ElastiCache to manage and store session data. Here's why:\nDistributed Session Management Requirement: The application scales across multiple Availability Zones.\nSession data needs to be accessible regardless of which EC2 instance handles a user's request. Storing\nsession data locally on an EC2 instance would lead to inconsistencies and a poor user experience if a user is\nrouted to a different instance that doesn't have their session data.\nElastiCache as a Centralized Session Store: ElastiCache (specifically using Memcached or Redis) provides a\ncentralized, in-memory data store that can be accessed by all EC2 instances. This ensures session data is\nconsistent and available regardless of which instance handles the request. Changes to code are required to\nserialize and store session data in ElastiCache and retrieve it upon subsequent requests.\nWhy other options are incorrect:\nB. Session Affinity (Sticky Sessions): Sticky sessions route a user to the same EC2 instance for the duration\nof their session. This violates the principle of scaling across Availability Zones effectively. If the instance the\nuser is stuck to fails, the session is lost. Also, it negates the benefit of load balancing, as some instances may\nbecome overloaded if many users are \"stuck\" to them. While it simplifies session management initially, it\ncreates a single point of failure and hinders scaling.\nC. Session Manager: Session Manager is a capability of AWS Systems Manager that allows you to manage\nyour EC2 instances through a browser-based interactive shell or through the AWS CLI. It is designed for\ninstance management, not session data management for web applications.\nD. GetSessionToken API operation in AWS STS: STS GetSessionToken is used for creating temporary\nsecurity credentials for IAM users or roles. This is relevant to managing AWS access permissions, not for\nmanaging web application session data.\nWhy ElastiCache is suitable: ElastiCache is designed for caching and session management. Redis offers\nadvanced features such as persistence and replication for improved reliability and availability. Memcached\noffers simplicity and high performance for stateless session caching.\nAuthoritative Links:\nAmazon ElastiCache: https://aws.amazon.com/elasticache/\nElastiCache Use Cases (Session Management): https://aws.amazon.com/elasticache/use-cases/\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
    "links": [
      "https://aws.amazon.com/elasticache/",
      "https://aws.amazon.com/elasticache/use-cases/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/"
    ]
  },
  {
    "question": "CertyIQ\nA company offers a food delivery service that is growing rapidly. Because of the growth, the companys order\nprocessing system is experiencing scaling problems during peak traffic hours. The current architecture includes\nthe following:\n A group of Amazon EC2 instances that run in an Amazon EC2 Auto Scaling group to collect orders from the\napplication\n Another group of EC2 instances that run in an Amazon EC2 Auto Scaling group to fulfill orders\nThe order collection process occurs quickly, but the order fulfillment process can take longer. Data must not be\nlost because of a scaling event.\nA solutions architect must ensure that the order collection process and the order fulfillment process can both\nscale properly during peak traffic hours. The solution must optimize utilization of the companys AWS resources.\nWhich solution meets these requirements?",
    "options": {
      "A": "Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling groups. Configure",
      "B": "Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling groups. Configure",
      "C": "Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for",
      "D": "Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for"
    },
    "answer": "D",
    "explanation": "The correct answer is D because it addresses the scaling issues and resource optimization concerns\neffectively.\nHere's a detailed justification:\nQueues for Decoupling: Using Amazon SQS queues for order collection and fulfillment decouples the two\nprocesses. This prevents the slower fulfillment process from overwhelming the order collection process.\nDecoupling allows each process to scale independently.\nReliable Message Storage: SQS guarantees that messages will be delivered, preventing data loss during\nscaling events. If an instance goes down, messages remain in the queue until another instance processes\nthem.\nPolling for Work Distribution: Configuring EC2 instances to poll their respective SQS queues allows them to\nefficiently retrieve and process orders.\nBacklog-Based Scaling: Creating a metric based on the \"backlog per instance\" is crucial for effective Auto\nScaling. This metric reflects the actual demand and helps the Auto Scaling groups to add or remove instances\nonly when needed, optimizing resource utilization. Scaling based on CPU utilization (as suggested in options A\nand B) might not be as effective, as instances could be waiting for I/O or external dependencies.\nOption C uses queue notifications which can trigger scaling events but the queue-based notifications are\nusually not optimal to measure backlog compared to the \"backlog per instance\" mechanism. Option A does\nnot provide a way to scale automatically. Option B is suboptimal because SNS would just create additional\nAuto Scaling Groups on demand, not individual instances to scale with load.\nAuthoritative Links:\nAmazon SQS: https://aws.amazon.com/sqs/\nAmazon EC2 Auto Scaling: https://aws.amazon.com/autoscaling/\nDecoupling Applications with SQS: https://aws.amazon.com/blogs/architecture/queue-based-load-leveling-\narchitecture-for-scaling-compute-resources/",
    "links": [
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/autoscaling/",
      "https://aws.amazon.com/blogs/architecture/queue-based-load-leveling-"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts multiple production applications. One of the applications consists of resources from Amazon\nEC2, AWS Lambda, Amazon RDS, Amazon Simple Notification Service (Amazon SNS), and Amazon Simple Queue\nService (Amazon SQS) across multiple AWS Regions. All company resources are tagged with a tag name of\napplication and a value that corresponds to each application. A solutions architect must provide the quickest\nsolution for identifying all of the tagged components.\nWhich solution meets these requirements?",
    "options": {
      "A": "Use AWS CloudTrail to generate a list of resources with the application tag.",
      "B": "Use the AWS CLI to query each service across all Regions to report the tagged components.",
      "C": "Run a query in Amazon CloudWatch Logs Insights to report on the components with the application tag.",
      "D": "Run a query with the AWS Resource Groups Tag Editor to report on the resources globally with the"
    },
    "answer": "D",
    "explanation": "The correct answer is D because AWS Resource Groups Tag Editor is specifically designed for quickly and\ncentrally managing and querying tagged resources across multiple AWS services and Regions.\nHere's a detailed justification:\nPurpose-Built Tool: The AWS Resource Groups Tag Editor is a dedicated service for managing AWS resource\ntags. Its primary function is to search for resources by tag across different services and Regions. This makes it\nthe most efficient tool for the described task.\nGlobal Reach: It operates across multiple AWS Regions, allowing a single query to identify resources tagged\nwith \"application\" and their corresponding values, regardless of the Region they reside in.\nSpeed and Efficiency: Compared to other options, Tag Editor offers the fastest way to identify tagged\ncomponents as it is purpose-built for this type of search.\nAWS CloudTrail (Option A): CloudTrail records API calls made to AWS services. While it logs resource\ncreation and modification events, it does not offer a direct or efficient way to query resources based on tags.\nIt's more focused on auditing and security.\nAWS CLI (Option B): Using the AWS CLI would require writing scripts to iterate through each AWS service\nand Region, querying resources and filtering by tag. This is a significantly more complex, time-consuming, and\nless efficient solution than using Tag Editor.\nAmazon CloudWatch Logs Insights (Option C): CloudWatch Logs Insights analyzes log data. Tagging\ninformation is not typically directly available within CloudWatch logs unless specifically logged by an\napplication. Therefore, it is not suitable for identifying resources based on their tags.\nIn conclusion, AWS Resource Groups Tag Editor provides the quickest and most efficient solution for\nidentifying all tagged components across multiple AWS Regions, making it the ideal choice for the architect's\nneeds.\nSupporting documentation:\nAWS Resource Groups Tag Editor: https://docs.aws.amazon.com/awsconsole/latest/userguide/resource-\ngroups.html\nTagging AWS Resources: https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html",
    "links": [
      "https://docs.aws.amazon.com/awsconsole/latest/userguide/resource-",
      "https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to export its database once a day to Amazon S3 for other teams to access. The exported object\nsize varies between 2 GB and 5 G",
    "options": {
      "B": "S3 Glacier Instant Retrieval",
      "A": "S3 Intelligent-Tiering",
      "C": "S3 Standard",
      "D": "S3 Standard-Infrequent Access (S3 Standard-IA)"
    },
    "answer": "A",
    "explanation": "Here's a detailed justification for choosing S3 Intelligent-Tiering in this scenario:\nThe core requirement is cost-effectiveness without compromising immediate availability and accessibility for\nup to 3 months, even with varying data access patterns.\nS3 Standard: While offering immediate availability, it's generally more expensive for data that isn't frequently\naccessed. Since the access patterns are variable, always storing in Standard might not be the most cost-\neffective.\nS3 Standard-IA (Infrequent Access): Cheaper storage than Standard, but has retrieval costs and is best for\ndata accessed less frequently. While some teams might access it less frequently, the variable and rapidly\nchanging access patterns make it less ideal than Intelligent-Tiering. It is optimized for data that is accessed\nless frequently but requires rapid access when needed. Since access varies, you could end up paying more in\nretrieval fees.\nS3 Glacier Instant Retrieval: Designed for archival with millisecond retrieval times, it's cheaper than S3\nStandard but more expensive for frequently accessed data than Standard-IA or Intelligent-Tiering and adds\nretrieval cost.\nS3 Intelligent-Tiering: This class automatically moves data between frequent, infrequent, and archive access\ntiers based on usage patterns, without performance impact or operational overhead. It analyzes access\npatterns and transitions objects to the most cost-effective tier automatically. Since the question states the\naccess pattern for the data is variable and changes rapidly, Intelligent-Tiering can dynamically move data to\ncheaper tiers when it is accessed less frequently, and quickly revert to more expensive tiers when the data\naccess pattern changes, maximizing cost savings.\nTherefore, S3 Intelligent-Tiering is the most suitable because it optimizes costs based on the unpredictable\naccess patterns, ensures immediate availability and accessibility, and handles the variable object sizes.\nAuthoritative Links:\nAWS S3 Storage Classes: https://aws.amazon.com/s3/storage-classes/\nS3 Intelligent-Tiering: https://aws.amazon.com/s3/intelligent-tiering/",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://aws.amazon.com/s3/intelligent-tiering/"
    ]
  },
  {
    "question": "CertyIQ\nA company is developing a new mobile app. The company must implement proper traffic filtering to protect its\nApplication Load Balancer (ALB) against common application-level attacks, such as cross-site scripting or SQL\ninjection. The company has minimal infrastructure and operational staff. The company needs to reduce its share of\nthe responsibility in managing, updating, and securing servers for its AWS environment.\nWhat should a solutions architect recommend to meet these requirements?",
    "options": {
      "A": "Configure AWS WAF rules and associate them with the ALB.",
      "B": "Answer: A",
      "C": "Deploy AWS Shield Advanced and add the ALB as a protected resource.",
      "D": "Create a new ALB that directs traffic to an Amazon EC2 instance running a third-party firewall, which then"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Configure AWS WAF rules and associate them with the ALB.\nHere's a detailed justification:\nAWS WAF (Web Application Firewall) is specifically designed to protect web applications from common web\nexploits and bots that can affect availability, compromise security, or consume excessive resources. It allows\nyou to define customizable web security rules that control which traffic to allow or block to your web\napplications. By associating these rules with the ALB, you can filter malicious traffic attempting application-\nlevel attacks like cross-site scripting (XSS) and SQL injection.\nAWS WAF addresses the requirement of proper traffic filtering to protect against these specific attacks\nmentioned in the scenario. The service is tightly integrated with the ALB, making configuration and\ndeployment relatively straightforward. Critically, AWS WAF is a managed service. This aligns perfectly with\nthe requirement to reduce the company's operational burden of managing, updating, and securing servers.\nAWS handles the underlying infrastructure, patching, and scaling of WAF.\nOption B is incorrect because Amazon S3 with public hosting is primarily for static content and doesn't\nprovide the necessary traffic filtering or protection against application-level attacks.\nOption C is incorrect because AWS Shield Advanced provides DDoS protection, primarily against volumetric\nattacks that target network and transport layers (Layers 3 & 4). While valuable for availability, it's not\ndesigned for application-level (Layer 7) attack filtering like WAF. Shield Advanced is also more expensive and\nlikely overkill for the given requirements.\nOption D is incorrect because creating a new ALB and EC2 instance running a third-party firewall increases\noperational overhead and infrastructure management, directly contradicting the requirement to minimize this.\nIt also introduces unnecessary complexity. The company must then manage, patch, and scale the EC2\ninstance and the third-party firewall.\nIn summary, AWS WAF provides the necessary application-level security with minimal management overhead,\nperfectly aligning with the company's requirements.\nAuthoritative Links for Further Research:\nAWS WAF: https://aws.amazon.com/waf/\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\nAWS Shield: https://aws.amazon.com/shield/",
    "links": [
      "https://aws.amazon.com/waf/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
      "https://aws.amazon.com/shield/"
    ]
  },
  {
    "question": "CertyIQ\nA companys reporting system delivers hundreds of .csv files to an Amazon S3 bucket each day. The company\nmust convert these files to Apache Parquet format and must store the files in a transformed data bucket.\nWhich solution will meet these requirements with the LEAST development effort?",
    "options": {
      "A": "Create an Amazon EMR cluster with Apache Spark installed. Write a Spark application to transform the data.",
      "B": "Create an AWS Glue crawler to discover the data. Create an AWS Glue extract, transform, and load (ETL) job",
      "C": "Use AWS Batch to create a job definition with Bash syntax to transform the data and output the data to the",
      "D": "Create an AWS Lambda function to transform the data and output the data to the transformed data bucket."
    },
    "answer": "B",
    "explanation": "Option B, utilizing AWS Glue, is the most efficient solution because it's a fully managed ETL (Extract,\nTransform, Load) service specifically designed for data transformation tasks like converting CSV files to\nParquet and moving them between S3 buckets. Glue crawlers automatically discover the schema of the CSV\nfiles, eliminating the need for manual schema definition. The Glue ETL job simplifies the transformation\nprocess, allowing you to write code (typically in Python or Scala) to convert the data to Parquet format and\nwrite it to the desired S3 bucket. Glue handles the infrastructure management, scaling, and job orchestration,\nminimizing operational overhead and development effort.\nOption A, using Amazon EMR with Apache Spark, is a valid approach but involves significantly more\ndevelopment and operational complexity. It requires setting up and managing an EMR cluster, writing and\ndeploying Spark applications, and configuring EMRFS for S3 access. While powerful, it's overkill for a simple\ndata transformation task.\nOption C, employing AWS Batch with a Bash script, might work for small datasets, but it's not scalable or\nefficient for handling hundreds of CSV files daily. Managing dependencies, error handling, and job\norchestration with Bash scripts becomes cumbersome quickly. Array jobs help with parallelization, but Glue\noffers more sophisticated parallel processing capabilities out-of-the-box.\nOption D, using an AWS Lambda function, is generally unsuitable for large file transformations. Lambda\nfunctions have limitations on execution time and memory, making them impractical for processing hundreds\nof CSV files daily. Lambda is better suited for smaller, event-driven tasks. Furthermore, S3 event notifications\ncould trigger numerous Lambda invocations, potentially leading to concurrency issues and increased costs.\nTherefore, AWS Glue provides a purpose-built, managed service for data transformation, reducing\ndevelopment effort and operational overhead compared to the other options, making it the best choice for this\nscenario.\nSupporting Documentation:\nAWS Glue: https://aws.amazon.com/glue/\nAWS Glue ETL Jobs: https://docs.aws.amazon.com/glue/latest/dg/etl-tutorial-etl.html\nAWS Glue Crawlers: https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html",
    "links": [
      "https://aws.amazon.com/glue/",
      "https://docs.aws.amazon.com/glue/latest/dg/etl-tutorial-etl.html",
      "https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has 700 TB of backup data stored in network attached storage (NAS) in its data center. This backup\ndata need to be accessible for infrequent regulatory requests and must be retained 7 years. The company has\ndecided to migrate this backup data from its data center to AWS. The migration must be complete within 1 month.\nThe company has 500 Mbps of dedicated bandwidth on its public internet connection available for data transfer.\nWhat should a solutions architect do to migrate and store the data at the LOWEST cost?",
    "options": {
      "A": "Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to Amazon",
      "B": "Deploy a VPN connection and AWS CLI to Amazon S3 Glacier: A VPN connection with 500 Mbps",
      "C": "Provision a 500 Mbps AWS Direct Connect to Amazon S3: While Direct Connect provides a dedicated",
      "D": "Use AWS DataSync to transfer the data to Amazon S3 Glacier: Although AWS DataSync accelerates data"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's why:\nA. Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to Amazon\nS3 Glacier Deep Archive.\nData Transfer: With 700 TB of data and only 500 Mbps bandwidth, transferring data over the internet within\none month is practically impossible. Snowball Edge provides a physical way to quickly transfer large amounts\nof data to AWS. Using a physical device bypasses the limitations of the network bandwidth, allowing the\ncompany to meet the 1-month migration requirement.\nStorage Cost: S3 Glacier Deep Archive is the lowest cost storage class, suitable for data that is infrequently\naccessed but requires long-term retention. Since the data is for infrequent regulatory requests and needs to\nbe retained for 7 years, Glacier Deep Archive fits perfectly.\nLifecycle Policy: A lifecycle policy automates the transition of data from S3 Standard (or another S3 storage\nclass) to Glacier Deep Archive after the initial transfer, further reducing storage costs without manual\nintervention.\nCost Optimization: This solution minimizes costs by using a bulk data transfer method (Snowball Edge) and\nthe most cost-effective storage class (Glacier Deep Archive) for the archive data.\nWhy other options are less suitable:\nB. Deploy a VPN connection and AWS CLI to Amazon S3 Glacier: A VPN connection with 500 Mbps\nbandwidth is insufficient for transferring 700 TB of data in one month. The cost of this operation will\ndramatically rise because of the operational cost. Also, transferring directly to Glacier would be inefficient as\nGlacier is designed for archival and restore operations.\nC. Provision a 500 Mbps AWS Direct Connect to Amazon S3: While Direct Connect provides a dedicated\nnetwork connection, 500 Mbps is still too slow to transfer 700 TB in one month. Direct Connect also\nintroduces significant upfront costs and monthly recurring charges, making it less cost-effective for a one-\ntime migration.\nD. Use AWS DataSync to transfer the data to Amazon S3 Glacier: Although AWS DataSync accelerates data\ntransfer over the network, transferring 700 TB of data over a 500 Mbps connection in one month is not\nrealistic. It's not designed to transfer directly to Amazon S3 Glacier.\nSupporting documentation and further reading:\nAWS Snow Family: https://aws.amazon.com/snowball/\nAmazon S3 Storage Classes: https://aws.amazon.com/s3/storage-classes/\nAmazon S3 Lifecycle: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-\ngeneral-considerations.html\nAWS DataSync: https://aws.amazon.com/datasync/",
    "links": [
      "https://aws.amazon.com/snowball/",
      "https://aws.amazon.com/s3/storage-classes/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-",
      "https://aws.amazon.com/datasync/"
    ]
  },
  {
    "question": "CertyIQ\nA company has a serverless website with millions of objects in an Amazon S3 bucket. The company uses the S3\nbucket as the origin for an Amazon CloudFront distribution. The company did not set encryption on the S3 bucket\nbefore the objects were loaded. A solutions architect needs to enable encryption for all existing objects and for all\nobjects that are added to the S3 bucket in the future.\nWhich solution will meet these requirements with the LEAST amount of effort?",
    "options": {
      "A": "Create a new S3 bucket. Turn on the default encryption settings for the new S3 bucket. Download all",
      "B": "Turn on the default encryption settings for the S3 bucket. Use the S3 Inventory feature to create a .csv file",
      "C": "Create a new encryption key by using AWS Key Management Service (AWS KMS). Change the settings on",
      "D": "Navigate to Amazon S3 in the AWS Management Console. Browse the S3 buckets objects. Sort by the"
    },
    "answer": "B",
    "explanation": "The most efficient solution to encrypt existing and future S3 objects for a CloudFront-backed website\ninvolves minimizing manual intervention and leveraging automated features of S3.\nOption B is the best approach because it utilizes S3's built-in features for both ongoing and retroactive\nencryption. Enabling default encryption on the S3 bucket ensures that all new objects uploaded will\nautomatically be encrypted with the specified encryption method (SSE-S3 or SSE-KMS), eliminating future\nmanual effort. S3 Inventory provides a cost-effective and scalable way to identify unencrypted objects. S3\nBatch Operations allows you to perform actions on a large number of S3 objects in an automated and\nauditable manner. Using a copy operation within Batch Operations allows for the re-writing and encryption of\nexisting objects in place without data transfer to an external location. This simplifies the process and reduces\noperational overhead.\nOption A is less efficient. Creating a new bucket involves significant data transfer and potential disruption to\nthe existing CloudFront distribution since the origin would have to be updated. Downloading and uploading all\nobjects also consumes network bandwidth and compute resources.\nOption C is more complex than necessary. While SSE-KMS is a valid encryption method, the problem doesn't\nexplicitly require KMS. Also, it doesn't include a straightforward way to re-encrypt existing objects. Simply\nturning on versioning won't encrypt existing objects.\nOption D is the least efficient. Manually modifying each object through the console is impractical and error-\nprone for a bucket containing millions of objects. This is not a scalable solution.\nTherefore, option B effectively addresses both present and future encryption requirements with the least\namount of manual labor, making use of native S3 capabilities designed for large-scale object management.\nRelevant Links:\nAmazon S3 Default Encryption: https://docs.aws.amazon.com/AmazonS3/latest/userguide/default-bucket-\nencryption.html\nAmazon S3 Inventory: https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory.html\nAmazon S3 Batch Operations: https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-\nbasics.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/default-bucket-",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a global web application on Amazon EC2 instances behind an Application Load Balancer. The\napplication stores data in Amazon Aurora. The company needs to create a disaster recovery solution and can\ntolerate up to 30 minutes of downtime and potential data loss. The solution does not need to handle the load when\nthe primary infrastructure is healthy.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Here's why:",
      "B": "Host a scaled-down deployment of the application in a second AWS Region. Use Amazon Route 53 to",
      "C": "Replicate the primary infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-",
      "D": "Back up data with AWS Backup. Use the backup to create the required infrastructure in a second AWS"
    },
    "answer": "A",
    "explanation": "The best solution to meet the recovery time objective (RTO) of 30 minutes and tolerance for data loss, without\nrequiring full load handling during normal operations, is option A. Here's why:\nOption A Explanation:\nDeployed Infrastructure in DR Region: Having the application infrastructure (EC2 instances, ALBs) pre-\ndeployed in the DR region (but not actively serving traffic) significantly reduces recovery time. Building the\ninfrastructure from scratch during a disaster would exceed the 30-minute RTO.\nActive-Passive Failover with Route 53: Amazon Route 53's active-passive failover configuration allows\ntraffic to be directed to the DR region only when the primary region is unavailable. This aligns with the\nrequirement of not handling load when the primary is healthy.\nAurora Replica in DR Region: Creating an Aurora Replica in the DR region ensures that the data is replicated\nasynchronously from the primary region. In a disaster scenario, the Aurora Replica can be promoted to a\nstandalone instance, minimizing data loss and recovery time. This is a critical advantage since the question\ntolerates limited data loss, asynchronous replication meets that condition, and the warm standby\ninfrastructure is already in place.\nWhy other options are less suitable:\nOption B (Active-Active): Active-active requires constantly handling load in both regions, which isn't needed\nwhen the primary is healthy. While it can provide faster failover, the problem states that it does not need to\nhandle the load when the primary infrastructure is healthy.\nOption C (Active-Active with Snapshot Restore): The infrastructure replication is useful, but using an Aurora\ndatabase restored from a snapshot will take too long to get running (much longer than 30 mins). Replicas\nprovide faster failover.\nOption D (Backup and Restore): AWS Backup is a solid backup strategy, but restoring from backup to build\nthe infrastructure and restore the database will take considerably longer than 30 minutes, violating the RTO.\nAlso, you can't create a \"second primary instance,\" you can only create an Aurora Replica.\nSupporting Cloud Computing Concepts:\nDisaster Recovery Strategies: Option A embodies a \"warm standby\" DR strategy, balancing cost and recovery\ntime.\nRTO and RPO: The solution is tailored to the specific RTO (30 minutes) and acceptable data loss (RPO).\nRoute 53 Failover: Route 53 provides a highly available DNS service for failover.\nAurora Replication: Aurora provides built-in replication for DR and high availability.\nAuthoritative Links:\nAWS Disaster Recovery: https://aws.amazon.com/disaster-recovery/\nAmazon Route 53 Health Checks and Failover:\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html\nAmazon Aurora Global Database: https://aws.amazon.com/rds/aurora/global-database/ (While Global\nDatabase is more advanced than a simple replica, it explains the principles of Aurora replication)",
    "links": [
      "https://aws.amazon.com/disaster-recovery/",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html",
      "https://aws.amazon.com/rds/aurora/global-database/"
    ]
  },
  {
    "question": "CertyIQ\nA company has a web server running on an Amazon EC2 instance in a public subnet with an Elastic IP address. The\ndefault security group is assigned to the EC2 instance. The default network ACL has been modified to block all\ntraffic. A solutions architect needs to make the web server accessible from everywhere on port 443.\nWhich combination of steps will accomplish this task? (Choose two.)",
    "options": {
      "A": "Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0.",
      "B": "Create a security group with a rule to allow TCP port 443 to destination 0.0.0.0/0.",
      "C": "Update the network ACL to allow TCP port 443 from source 0.0.0.0/0.",
      "D": "Update the network ACL to allow inbound/outbound TCP port 443 from source 0.0.0.0/0 and to destination"
    },
    "answer": "A",
    "explanation": "The correct answer is AE because it addresses both the security group and network ACL configurations\nrequired for allowing inbound HTTPS traffic (port 443) to the EC2 instance from the internet.\nOption A correctly creates a security group rule that allows inbound TCP traffic on port 443 from anywhere\n(0.0.0.0/0). Security groups act as a virtual firewall for EC2 instances, controlling inbound and outbound\ntraffic. Allowing inbound traffic on port 443 is essential for HTTPS access. This rule permits any IP address to\nconnect to the EC2 instance on the specified port.\nOption E addresses the necessary changes to the network ACL (NACL). NACLs are stateless, meaning rules\nmust be explicitly defined for both inbound and outbound traffic. The inbound rule allows TCP traffic on port\n443 from any source (0.0.0.0/0). Crucially, it also includes an outbound rule allowing traffic on ephemeral\nports (32768-65535) to any destination (0.0.0.0/0). When a client connects to the web server on port 443, the\nserver's response will originate from an ephemeral port. Without the outbound rule allowing traffic from these\nports, the response will be blocked by the NACL, preventing a successful connection. The ephemeral port\nrange can vary, so it's crucial to allow a broad range.\nOptions B, C, and D are incorrect. B is incorrect because security group rules do not use destinations. C is\nincorrect because it only addresses the inbound traffic in NACL, missing the outbound ephemeral port\nrequirement. D is also incorrect as it implies bi-directional port 443, and does not cover ephemeral port\nrequirements for the outbound connections.\nTherefore, combining the creation of the appropriate security group rule in A with the updating of the network\nACL in E to allow both inbound 443 traffic and outbound ephemeral port traffic ensures that the web server is\naccessible from anywhere on port 443.Security GroupsNetwork ACLs",
    "links": []
  },
  {
    "question": "CertyIQ\nA companys application is having performance issues. The application is stateful and needs to complete in-\nmemory tasks on Amazon EC2 instances. The company used AWS CloudFormation to deploy infrastructure and\nused the M5 EC2 instance family. As traffic increased, the application performance degraded. Users are reporting\ndelays when the users attempt to access the application.\nWhich solution will resolve these issues in the MOST operationally efficient way?",
    "options": {
      "A": "Replace the EC2 instances with T3 EC2 instances that run in an Auto Scaling group. Make the changes by",
      "B": "Modify the CloudFormation templates to run the EC2 instances in an Auto Scaling group. Increase the",
      "C": "Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Use Amazon",
      "D": "Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the"
    },
    "answer": "D",
    "explanation": "The correct answer is D because it addresses both the performance bottleneck and operational efficiency.\nThe application is stateful and requires in-memory tasks, indicating memory constraints. M5 instances are\ngeneral-purpose, while R5 instances are memory-optimized, making them a more suitable choice for this\nworkload. Switching to R5 instances directly addresses the root cause of the performance degradation.\nFurthermore, using CloudFormation for infrastructure changes is operationally efficient as it provides\ninfrastructure-as-code, allowing for repeatable and predictable deployments.\nThe addition of the CloudWatch agent to collect custom application latency metrics is crucial for future\ncapacity planning. Standard EC2 CloudWatch metrics do not track application-specific latency. Custom\nmetrics provide insights into the actual user experience and help proactively identify potential performance\nissues before they impact users. Monitoring application latency allows for targeted scaling decisions based\non real user needs, further optimizing operational efficiency.\nOption A is incorrect because T3 instances are burstable performance instances, which are not ideal for\nconsistent, demanding workloads. Using the AWS Management Console for manual changes lacks\ninfrastructure-as-code benefits. Option B is flawed because manual scaling is not operationally efficient and\ndoes not dynamically respond to traffic fluctuations. While using CloudFormation is good, the scaling\napproach is not. Option C is partially correct by addressing the instance type and using CloudWatch. However,\nit uses built-in EC2 memory metrics, which are not sufficient to track custom application latency and provide\ngranular insights for future capacity planning. Custom metrics offer better visibility into application\nperformance.\nSupporting Links:\nAmazon EC2 Instance Types: https://aws.amazon.com/ec2/instance-types/\nAmazon CloudWatch Custom Metrics:\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html\nAWS CloudFormation: https://aws.amazon.com/cloudformation/\nAuto Scaling Groups: https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html",
    "links": [
      "https://aws.amazon.com/ec2/instance-types/",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html",
      "https://aws.amazon.com/cloudformation/",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is designing a new API using Amazon API Gateway that will receive requests from users. The\nvolume of requests is highly variable; several hours can pass without receiving a single request. The data\nprocessing will take place asynchronously, but should be completed within a few seconds after a request is made.\nWhich compute service should the solutions architect have the API invoke to deliver the requirements at the\nlowest cost?",
    "options": {
      "A": "An AWS Glue job",
      "B": "An AWS Lambda function.",
      "C": "A containerized service hosted in Amazon Elastic Kubernetes Service (Amazon EKS)",
      "D": "A containerized service hosted in Amazon ECS with Amazon EC2"
    },
    "answer": "B",
    "explanation": "The correct answer is B. An AWS Lambda function.\nHere's why:\nVariable Traffic & Cost Optimization: Lambda functions are designed for event-driven, serverless computing.\nThey automatically scale based on the number of requests, and you only pay for the compute time consumed.\nGiven the highly variable traffic with periods of inactivity, Lambda's pay-per-use model makes it the most\ncost-effective choice. No charge occurs if the lambda is not invoked.\nAsynchronous Processing: Lambda functions can be invoked asynchronously. API Gateway can trigger a\nLambda function upon receiving a request and immediately return a response to the user. The Lambda\nfunction can then process the data in the background without blocking the user's request.\nSpeed: AWS Lambda processes requests very quickly which satisfies the need for a low-latency response.\nAlternatives' Drawbacks:\nAWS Glue Job (A): AWS Glue is designed for ETL (Extract, Transform, Load) operations on large datasets. It's\nnot suitable for near real-time processing of individual requests. Glue is a fully-managed extract, transform,\nand load (ETL) service that makes it easy for customers to prepare and load their data for analytics.\nAmazon EKS/ECS with EC2 (C/D): These options involve managing containers and underlying infrastructure\n(EC2 instances). This incurs costs even when the service is idle, making them less cost-effective than Lambda\nfor highly variable traffic. The overhead of managing EC2 instances, container orchestration, and scaling rules\nadds complexity and cost.\nServerless Paradigm: Lambda embraces the serverless paradigm, reducing operational overhead and\nallowing developers to focus solely on writing code.\nIn summary: Lambda provides the perfect balance of cost-effectiveness, scalability, and asynchronous\nprocessing capabilities to meet the requirements.\nAuthoritative Links:\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon API Gateway: https://aws.amazon.com/api-gateway/\nAWS Glue: https://aws.amazon.com/glue/\nAmazon ECS: https://aws.amazon.com/ecs/\nAmazon EKS: https://aws.amazon.com/eks/",
    "links": [
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/api-gateway/",
      "https://aws.amazon.com/glue/",
      "https://aws.amazon.com/ecs/",
      "https://aws.amazon.com/eks/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an application on a group of Amazon Linux EC2 instances. For compliance reasons, the company\nmust retain all application log files for 7 years. The log files will be analyzed by a reporting tool that must be able\nto access all the files concurrently.\nWhich storage solution meets these requirements MOST cost-effectively?",
    "options": {
      "A": "Amazon Elastic Block Store (Amazon EBS): EBS volumes are block storage devices attached to EC2",
      "B": "Amazon Elastic File System (Amazon EFS): EFS provides a shared file system for EC2 instances. While",
      "C": "Amazon EC2 instance store: Instance store provides temporary block storage directly attached to the host",
      "D": "Amazon S3. Here's why:"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Amazon S3. Here's why:\nCost-effectiveness: S3 is significantly more cost-effective for long-term storage of log files than EBS or EFS.\nS3 offers various storage classes (e.g., S3 Standard, S3 Glacier) optimized for different access patterns and\nretention durations. For infrequently accessed logs, S3 Glacier provides very low storage costs.\nDurability and Availability: S3 provides 99.999999999% (11 9's) durability, ensuring that log files are highly\nunlikely to be lost. It's also designed for high availability, meaning the reporting tool can consistently access\nthe files.\nScalability: S3 scales effortlessly to store vast amounts of data. The company can store all 7 years' worth of\nlogs without worrying about storage limitations.\nConcurrency: S3 is designed for concurrent access. The reporting tool can access multiple log files\nsimultaneously without performance bottlenecks.\nData Lifecycle Management: S3 Lifecycle policies automate the process of transitioning log files to lower-\ncost storage tiers (e.g., from S3 Standard to S3 Glacier) based on their age, further optimizing costs.\nWhy the other options are less suitable:\nA. Amazon Elastic Block Store (Amazon EBS): EBS volumes are block storage devices attached to EC2\ninstances. They are expensive for long-term storage of large volumes of log data. EBS is also tied to a specific\navailability zone, complicating data access from other instances or regions.\nB. Amazon Elastic File System (Amazon EFS): EFS provides a shared file system for EC2 instances. While\nsuitable for applications requiring shared file storage, EFS is more expensive than S3 for long-term archival of\nlog files.\nC. Amazon EC2 instance store: Instance store provides temporary block storage directly attached to the host\nEC2 instance. Data on the instance store is lost when the instance is stopped, terminated, or fails, making it\nunsuitable for long-term archival. Also, it is not designed for reporting access where data must be\nconsistently available.\nSupporting Links:\nAmazon S3: https://aws.amazon.com/s3/\nAmazon S3 Storage Classes: https://aws.amazon.com/s3/storage-classes/\nAmazon S3 Lifecycle: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-management-\noverview.html",
    "links": [
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/s3/storage-classes/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-management-"
    ]
  },
  {
    "question": "CertyIQ\nA company has hired an external vendor to perform work in the companys AWS account. The vendor uses an\nautomated tool that is hosted in an AWS account that the vendor owns. The vendor does not have IAM access to\nthe companys AWS account.\nHow should a solutions architect grant this access to the vendor?",
    "options": {
      "A": "Here's a detailed justification:",
      "B": "Create an IAM user in the companys account with a password that meets the password complexity",
      "C": "Create an IAM group in the companys account. Add the tools IAM user from the vendor account to the",
      "D": "Create a new identity provider by choosing AWS account as the provider type in the IAM console. Supply"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's a detailed justification:\nCross-account access is best achieved using IAM roles. IAM roles allow you to delegate access to your AWS\nresources to trusted entities (like the vendor's AWS account) without sharing IAM user credentials. This is a\nmore secure and manageable approach than creating IAM users directly in your account for external parties.\nOption A leverages this principle through a process called \"IAM role trust relationship\". You create an IAM\nrole in your company's AWS account. This role defines the permissions the vendor needs. Crucially, you\nconfigure a trust relationship on this role, specifying the vendor's AWS account ID as the trusted entity. This\nmeans that principals (like IAM roles or users) within the vendor's account can assume the role in your\naccount, provided they have the necessary permissions to do so.\nOn the vendor side, their automated tool will need an IAM role (or user) that is granted permission to assume\nthe role you created in your account. This is accomplished by attaching an IAM policy to the vendor's role (or\nuser) that grants the sts:AssumeRole permission, specifying the ARN of the role in your account.\nOnce this is set up, the vendor's automated tool can programmatically assume the role in your account using\nthe AssumeRole API call. This provides the tool with temporary credentials (access key ID, secret access key,\nand security token) that it can then use to make API calls against your AWS resources, subject to the\npermissions defined in the role.\nOption B is less secure. Creating an IAM user for the vendor means sharing long-term credentials (access key\nID and secret access key). Managing and rotating these credentials becomes complex, and if compromised,\nthey provide persistent access.\nOption C is incorrect. IAM groups are used to manage permissions for users within the same AWS account, not\nto grant cross-account access. You cannot directly add users from another account to a group in your\naccount.\nOption D misinterprets the purpose of identity providers. Identity providers (IdPs) are typically used for\nfederating access for users managed outside of AWS, such as from an Active Directory or other SAML-based\nidentity provider. They are not directly used for cross-account access between AWS accounts in the manner\ndescribed in the question. Specifying the vendor's AWS account ID and user name as an identity provider is\nnot a supported configuration.\nIn summary, using IAM roles for cross-account access is the recommended and most secure approach,\navoiding the need to share long-term credentials and providing granular control over permissions.\nRelevant documentation:\nGranting access to your AWS account to third parties:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\nIAM Roles: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\nAssumeRole API: https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html",
    "links": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html",
      "https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has deployed a Java Spring Boot application as a pod that runs on Amazon Elastic Kubernetes Service\n(Amazon EKS) in private subnets. The application needs to write data to an Amazon DynamoDB table. A solutions\narchitect must ensure that the application can interact with the DynamoDB table without exposing traffic to the\ninternet.\nWhich combination of steps should the solutions architect take to accomplish this goal? (Choose two.)",
    "options": {
      "A": "Attach an IAM role that has sufficient privileges to the EKS pod: This is the best practice for providing",
      "B": "Attach an IAM user that has sufficient privileges to the EKS pod: Attaching an IAM user to an EKS pod is",
      "C": "Allow outbound connectivity to the DynamoDB table through the private subnets network ACLs: While",
      "D": "Create a VPC endpoint for DynamoDB: Creating a VPC endpoint for DynamoDB ensures that the traffic"
    },
    "answer": "A",
    "explanation": "The correct answer is AD. Here's why:\nA. Attach an IAM role that has sufficient privileges to the EKS pod: This is the best practice for providing\nAWS credentials to applications running on Amazon EKS. Instead of embedding credentials directly in the\ncode (which is a security risk), you can associate an IAM role with the pod using the IAM roles for service\naccounts (IRSA) feature. This allows the application to assume the role and obtain temporary AWS credentials\nto access DynamoDB. https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html\nD. Create a VPC endpoint for DynamoDB: Creating a VPC endpoint for DynamoDB ensures that the traffic\nbetween the EKS pod and DynamoDB remains within the AWS network, specifically within your VPC. A VPC\nendpoint eliminates the need for the application to access DynamoDB over the public internet. Instead, the\ntraffic flows through the AWS backbone network, improving security and reducing latency. This is especially\nimportant for applications running in private subnets where internet access is restricted.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\nWhy the other options are incorrect:\nB. Attach an IAM user that has sufficient privileges to the EKS pod: Attaching an IAM user to an EKS pod is\ngenerally not done directly. IAM users are designed for human users or applications running outside of AWS\ninfrastructure. Roles are intended for services.\nC. Allow outbound connectivity to the DynamoDB table through the private subnets network ACLs: While\nyou do need to ensure the network ACLs allow outbound traffic, this is not the primary solution for secure\naccess. This approach relies on your NAT gateway or internet gateway to allow the request, exposing it to the\ninternet. Also, NACLs are stateless, requiring both inbound and outbound rules for a connection.\nE. Embed the access keys in the Java Spring Boot code: Embedding access keys directly in the code is a\nmajor security vulnerability. If the code is compromised or inadvertently exposed, the access keys could be\nused to compromise your AWS account. This is strongly discouraged.",
    "links": [
      "https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html"
    ]
  },
  {
    "question": "CertyIQ\nA company recently migrated its web application to AWS by rehosting the application on Amazon EC2 instances in\na single AWS Region. The company wants to redesign its application architecture to be highly available and fault\ntolerant. Traffic must reach all running EC2 instances randomly.\nWhich combination of steps should the company take to meet these requirements? (Choose two.)",
    "options": {
      "A": "Create an Amazon Route 53 failover routing policy.",
      "B": "Create an Amazon Route 53 weighted routing policy.",
      "C": "Create an Amazon Route 53 multivalue answer routing policy.",
      "D": "Launch three EC2 instances: two instances in one Availability Zone and one instance in another Availability"
    },
    "answer": "C",
    "explanation": "The requirement is to achieve high availability and fault tolerance with traffic distributed randomly across\nEC2 instances in multiple Availability Zones.\nChoice C (Create an Amazon Route 53 multivalue answer routing policy) is correct. Multivalue answer\nrouting returns multiple healthy IP addresses in response to DNS queries. When a client makes a request, it\nwill randomly choose one of the returned IP addresses. This achieves the desired random traffic distribution\nacross instances. Route 53 also performs health checks on the associated endpoints and will only return IP\naddresses that are healthy, contributing to fault tolerance. If an instance fails a health check, its IP will no\nlonger be returned in the DNS response.\n[https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-\nmultivalue]\nChoice E (Launch four EC2 instances: two instances in one Availability Zone and two instances in another\nAvailability Zone) is also correct. Distributing EC2 instances across multiple Availability Zones protects\nagainst failures in a single AZ, increasing availability. Having two instances in each of two AZs ensures that if\none AZ goes down, the application can still function using the instances in the other AZ. The multivalue\nanswer routing will then randomly distribute traffic across these four instances. This architecture provides\nboth high availability and random traffic distribution.\nChoice A (failover routing) is incorrect because it only directs traffic to a secondary endpoint when the\nprimary is unhealthy. It doesn't randomly distribute traffic. Choice B (weighted routing) allows you to specify\nweights for each endpoint, and the traffic is distributed based on these weights, not randomly. Choice D (two\nin one AZ, one in another) doesn't provide adequate redundancy. If the AZ with two instances fails, you lose a\nsignificant portion of your capacity.",
    "links": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-"
    ]
  },
  {
    "question": "CertyIQ\nA media company collects and analyzes user activity data on premises. The company wants to migrate this\ncapability to AWS. The user activity data store will continue to grow and will be petabytes in size. The company\nneeds to build a highly available data ingestion solution that facilitates on-demand analytics of existing data and\nnew data with SQL.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Send activity data to an Amazon Kinesis data stream. Configure the stream to deliver the data to an Amazon",
      "B": "Here's a breakdown of why and why the other options are less suitable:",
      "C": "Place activity data in an Amazon S3 bucket. Configure Amazon S3 to run an AWS Lambda function on the",
      "D": "Create an ingestion service on Amazon EC2 instances that are spread across multiple Availability Zones."
    },
    "answer": "B",
    "explanation": "The correct answer is B. Here's a breakdown of why and why the other options are less suitable:\nWhy Option B (Kinesis Data Firehose to Redshift) is the Best:\nManaged Service and Least Overhead: Kinesis Data Firehose is a fully managed service that automatically\nscales to match the throughput of your data. Redshift is a fully managed data warehouse. These reduce\noperational burden.\nPetabyte Scale: Both Firehose and Redshift are designed to handle petabyte-scale data volumes efficiently.\nData Ingestion and Delivery: Firehose handles the complexities of data ingestion, buffering, and delivery to\nRedshift. It can also transform data en route.\nSQL Analytics: Redshift is a columnar data warehouse optimized for fast SQL queries, meeting the on-\ndemand analytics requirement.\nHigh Availability: Redshift inherently provides high availability through backups and replication. Firehose\nintegrates directly and automatically with Redshift.\nWhy Other Options Are Less Suitable:\nOption A (Kinesis Data Stream to S3): While Kinesis Data Streams can handle high volumes of data, storing\nthe data in S3 alone doesn't directly provide SQL analytics. You'd need to use services like Athena or EMR to\nquery the data in S3, adding operational complexity. Additionally, Data Streams requires more configuration\nand monitoring compared to Data Firehose.\nOption C (S3 to Lambda): Lambda functions are suitable for smaller data transformations and event-driven\nprocessing. Triggering a Lambda function for every data arrival in S3, especially for petabyte-scale data, can\nbe computationally expensive and difficult to manage. It also doesn't directly facilitate SQL analytics without\nan additional service.\nOption D (EC2 Ingestion Service to RDS): Building a custom ingestion service on EC2 instances introduces\nsignificant operational overhead related to managing the instances, scaling, and ensuring fault tolerance.\nAmazon RDS is a relational database service that may not be suitable for analyzing petabyte-scale activity\ndata.\nSupporting Concepts and Links:\nKinesis Data Firehose: https://aws.amazon.com/kinesis/data-firehose/ - Provides a serverless way to ingest\ndata into data stores and analytics services.\nAmazon Redshift: https://aws.amazon.com/redshift/ - A fully managed, petabyte-scale data warehouse\nservice in the cloud.\nAWS Well-Architected Framework (Operational Excellence Pillar): Emphasizes running and monitoring\nsystems to deliver business value and continually improving processes and procedures. The Firehose/Redshift\nsolution aligns well by minimizing operational overhead.",
    "links": [
      "https://aws.amazon.com/kinesis/data-firehose/",
      "https://aws.amazon.com/redshift/"
    ]
  },
  {
    "question": "CertyIQ\nA company collects data from thousands of remote devices by using a RESTful web services application that runs\non an Amazon EC2 instance. The EC2 instance receives the raw data, transforms the raw data, and stores all the\ndata in an Amazon S3 bucket. The number of remote devices will increase into the millions soon. The company\nneeds a highly scalable solution that minimizes operational overhead.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "options": {
      "A": "Use AWS Glue to process the raw data in Amazon S3.",
      "B": "Use Amazon Route 53 to route traffic to different EC2 instances.",
      "C": "Add more EC2 instances to accommodate the increasing amount of incoming data.",
      "D": "Send the raw data to Amazon Simple Queue Service (Amazon SQS). Use EC2 instances to process the data."
    },
    "answer": "A",
    "explanation": "Here's a detailed justification for choosing options A and E as the correct solutions, explaining why other\noptions are less suitable, along with relevant AWS concepts and links:\nJustification for A and E:\nOption A: Use AWS Glue to process the raw data in Amazon S3. AWS Glue is a fully managed extract,\ntransform, and load (ETL) service. Storing raw data in S3 is a good first step. AWS Glue is well-suited to\nprocessing large datasets in S3. This aligns with scalability and minimizing operational overhead. Glue offers\nserverless processing capabilities, automatically scaling resources based on demand, relieving the company\nfrom managing processing infrastructure. Glue is also cost-effective as you only pay for the compute time\nused.\nOption E: Use Amazon API Gateway to send the raw data to an Amazon Kinesis data stream. Configure\nAmazon Kinesis Data Firehose to use the data stream as a source to deliver the data to Amazon S3. API\nGateway provides a scalable and secure front door for the RESTful web services application. It can handle a\nmassive increase in requests from millions of devices. Kinesis Data Streams is designed for ingesting high-\nvelocity, real-time data streams. Using Kinesis Data Firehose automates the delivery of the data to S3. Kinesis\nData Firehose also offers capabilities to transform the data before delivering to S3, which could reduce the\nneed for Glue, depending on the companys architecture preferences. This combination decouples the data\ningestion process from the processing, leading to a more resilient and scalable system.\nWhy other options are less suitable:\nOption B: Use Amazon Route 53 to route traffic to different EC2 instances. While Route 53 can distribute\ntraffic across EC2 instances, simply adding more EC2 instances (as implied by C) doesn't address the\nfundamental scaling bottlenecks of the original design. The EC2 instance is still performing both data\nreception and transformation, creating a single point of contention. Further, managing a scaling group of EC2\ninstances introduces operational overhead that the company wishes to avoid.\nOption C: Add more EC2 instances to accommodate the increasing amount of incoming data. Scaling EC2\ninstances vertically has limitations. Horizontal scaling of EC2 instances requires managing load balancing,\nauto-scaling groups, and potentially database scaling, increasing operational complexity. This doesn't\nminimize overhead.\nOption D: Send the raw data to Amazon Simple Queue Service (Amazon SQS). Use EC2 instances to process\nthe data. While SQS can decouple the data ingestion and processing, the EC2 instances still perform the\ntransformation and data storage. SQS still requires the management and scaling of EC2 instances, negating\nthe operational overhead reduction requirement. Using AWS Glue or Kinesis Data Firehose reduces\noperational overhead more effectively because these services are serverless.\nKey Concepts:\nScalability: The ability of a system to handle increasing workloads.\nOperational Overhead: The effort required to manage and maintain a system.\nDecoupling: Separating components of a system so that they can operate independently.\nServerless Computing: Cloud computing execution model in which the cloud provider dynamically manages\nthe allocation of machine resources.\nAuthoritative Links:\nAWS Glue: https://aws.amazon.com/glue/\nAmazon API Gateway: https://aws.amazon.com/api-gateway/\nAmazon Kinesis Data Streams: https://aws.amazon.com/kinesis/data-streams/\nAmazon Kinesis Data Firehose: https://aws.amazon.com/kinesis/data-firehose/",
    "links": [
      "https://aws.amazon.com/glue/",
      "https://aws.amazon.com/api-gateway/",
      "https://aws.amazon.com/kinesis/data-streams/",
      "https://aws.amazon.com/kinesis/data-firehose/"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to retain its AWS CloudTrail logs for 3 years. The company is enforcing CloudTrail across a set\nof AWS accounts by using AWS Organizations from the parent account. The CloudTrail target S3 bucket is\nconfigured with S3 Versioning enabled. An S3 Lifecycle policy is in place to delete current objects after 3 years.\nAfter the fourth year of use of the S3 bucket, the S3 bucket metrics show that the number of objects has\ncontinued to rise. However, the number of new CloudTrail logs that are delivered to the S3 bucket has remained\nconsistent.\nWhich solution will delete objects that are older than 3 years in the MOST cost-effective manner?",
    "options": {
      "A": "Configure the organizations centralized CloudTrail trail to expire objects after 3 years.",
      "B": "Configure the S3 Lifecycle policy to delete previous versions as well as current versions.",
      "C": "Create an AWS Lambda function to enumerate and delete objects from Amazon S3 that are older than 3",
      "D": "Configure the parent account as the owner of all objects that are delivered to the S3 bucket."
    },
    "answer": "B",
    "explanation": "The problem is that despite having an S3 Lifecycle policy set to delete objects after 3 years, the number of\nobjects in the S3 bucket is still increasing after 4 years. This indicates that the Lifecycle policy is not\neffectively deleting all objects older than 3 years. Since S3 Versioning is enabled, the Lifecycle policy might\nonly be deleting the current versions of objects, leaving behind previous versions.\nOption A is incorrect because CloudTrail itself does not have an inherent mechanism to directly expire objects\nin the S3 bucket where logs are stored. CloudTrail delivers the logs to S3, and the expiration should be\nhandled by S3's lifecycle management features.\nOption B is the most cost-effective solution. The S3 Lifecycle policy needs to be configured to also delete\nprevious versions of objects. S3 Versioning keeps multiple versions of an object. If a new version of an object\nis created (e.g., a new CloudTrail log file with the same name), the previous version remains stored. By\nmodifying the Lifecycle policy to include the deletion of previous versions older than 3 years, all older\nversions will be purged, solving the problem without requiring any additional infrastructure or code. This\nleverages built-in S3 functionality, reducing operational overhead and cost.\nOption C is less efficient and more costly. While a Lambda function can enumerate and delete objects, it\nrequires writing and maintaining code, incurs Lambda execution costs, and consumes S3 API calls, which can\nbecome expensive at scale. It's an unnecessary complication when S3's Lifecycle policies can handle this\nnatively.\nOption D is irrelevant to the problem. Configuring the parent account as the owner of all objects does not\ndirectly address the issue of previous versions accumulating in the bucket. Object ownership affects access\ncontrol and billing, but it doesn't control object lifecycle.\nTherefore, option B is the correct solution because it directly addresses the problem by configuring the S3\nLifecycle policy to manage both current and previous versions of objects, ensuring that all logs older than 3\nyears are deleted in a cost-effective manner.\nSupporting Links:\nS3 Lifecycle Policies: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-\nconcept.html\nS3 Versioning: https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has an API that receives real-time data from a fleet of monitoring devices. The API stores this data in an\nAmazon RDS DB instance for later analysis. The amount of data that the monitoring devices send to the API\nfluctuates. During periods of heavy traffic, the API often returns timeout errors.\nAfter an inspection of the logs, the company determines that the database is not capable of processing the volume\nof write traffic that comes from the API. A solutions architect must minimize the number of connections to the\ndatabase and must ensure that data is not lost during periods of heavy traffic.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Increase the size of the DB instance to an instance type that has more available memory.",
      "B": "Modify the DB instance to be a Multi-AZ DB instance. Configure the application to write to all active RDS DB",
      "C": "Modify the API to write incoming data to an Amazon Simple Queue Service (Amazon SQS) queue. Use an",
      "D": "Modify the API to write incoming data to an Amazon Simple Notification Service (Amazon SNS) topic. Use an"
    },
    "answer": "C",
    "explanation": "The correct answer is C, modifying the API to write incoming data to an Amazon SQS queue and using an AWS\nLambda function to write data from the queue to the database. Here's why:\nThe primary problem is the RDS DB instance's inability to handle the write volume from the API, leading to\ntimeout errors. This suggests the database is overwhelmed with connection requests and write operations.\nOption C addresses this by introducing a decoupling mechanism using Amazon SQS, a fully managed\nmessage queuing service. The API publishes messages containing the real-time data to the SQS queue. This\nimmediately acknowledges the API request, preventing timeout errors even during traffic spikes.\nSQS acts as a buffer, holding the incoming data until the database is ready to process it. This prevents data\nloss during heavy traffic periods, satisfying a key requirement.\nAn AWS Lambda function is then configured to be triggered by new messages in the SQS queue. Lambda\nautomatically scales based on the number of messages, ensuring timely processing. The Lambda function\nretrieves data from the queue in batches and writes it to the RDS database. This reduces the number of\nconcurrent connections to the database compared to the API directly writing data, thus minimizing the load.\nLambda's ability to process messages in batches further optimizes database write operations.\nOption A, increasing the DB instance size, might temporarily alleviate the issue, but it's a reactive approach\nand doesn't address the root cause of high connection load. It also involves downtime during scaling.\nOption B, using Multi-AZ for high availability, doesn't solve the write capacity problem. Writing to multiple\ninstances would actually increase the write load and potentially exacerbate the timeout errors. Also, direct\ndatabase writes from the application to multiple instances introduce complexities in data consistency and\nmanagement.\nOption D, using Amazon SNS, is designed for fan-out scenarios where multiple subscribers need to receive the\nsame message. It's not suitable for buffering and reliable, ordered processing of data intended for a single\ndatabase. SNS's primary purpose is notification, not reliable message queuing for database writes. SQS\nguarantees message delivery, while SNS does not. Using SNS could lead to data loss.\nIn summary, SQS provides the necessary buffering and decoupling to protect the database from\noverwhelming write traffic, while Lambda efficiently processes the queued data, satisfying the requirements\nof minimizing connections and preventing data loss.\nFurther Reading:\nAmazon SQS: https://aws.amazon.com/sqs/\nAWS Lambda: https://aws.amazon.com/lambda/",
    "links": [
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/lambda/"
    ]
  },
  {
    "question": "CertyIQ\nA company manages its own Amazon EC2 instances that run MySQL databases. The company is manually\nmanaging replication and scaling as demand increases or decreases. The company needs a new solution that\nsimplifies the process of adding or removing compute capacity to or from its database tier as needed. The solution\nalso must offer improved performance, scaling, and durability with minimal effort from operations.\nWhich solution meets these requirements?",
    "options": {
      "A": "Migrate the databases to Amazon Aurora Serverless for Aurora MySQL.",
      "B": "Aurora Serverless for PostgreSQL: While Aurora Serverless for PostgreSQL also offers automatic scaling,",
      "C": "Larger MySQL Database on Larger EC2 Instances: This approach is a form of vertical scaling and does not",
      "D": "EC2 Auto Scaling Group: While an EC2 Auto Scaling group automates the scaling of EC2 instances, it does"
    },
    "answer": "A",
    "explanation": "The most suitable solution is A, migrating the databases to Amazon Aurora Serverless for Aurora MySQL.\nHere's why:\nSimplified Scaling: Aurora Serverless automatically scales compute capacity based on the application's\nneeds. This eliminates the manual effort of adding or removing EC2 instances, directly addressing the\ncompany's requirement for simplified capacity management.\nImproved Performance and Durability: Aurora, in general, offers improved performance and durability\ncompared to standard MySQL due to its optimized storage engine and distributed architecture. Aurora\nServerless inherits these benefits. https://aws.amazon.com/rds/aurora/\nMinimal Operational Effort: Aurora Serverless handles the underlying infrastructure management, such as\nprovisioning, patching, and backups, reducing the operational burden on the company.\nMySQL Compatibility: Migrating to Aurora MySQL allows the company to leverage its existing MySQL\nexpertise and application code with minimal changes.\nLet's analyze why the other options are less suitable:\nB. Aurora Serverless for PostgreSQL: While Aurora Serverless for PostgreSQL also offers automatic scaling,\nthe company is currently using MySQL. Migrating to PostgreSQL would require significant application code\nchanges and staff retraining.\nC. Larger MySQL Database on Larger EC2 Instances: This approach is a form of vertical scaling and does not\nprovide the dynamic scaling capabilities required by the company. It also does not reduce the manual effort\ninvolved in managing the database infrastructure. It becomes a single point of failure.\nD. EC2 Auto Scaling Group: While an EC2 Auto Scaling group automates the scaling of EC2 instances, it does\nnot address the underlying database replication and management complexities. The company would still need\nto manually configure and manage database replication across the instances in the Auto Scaling group, and\nscale them.\nIn summary, Aurora Serverless for Aurora MySQL offers the best combination of simplified scaling, improved\nperformance and durability, minimal operational effort, and compatibility with the company's existing MySQL\nenvironment.",
    "links": [
      "https://aws.amazon.com/rds/aurora/"
    ]
  },
  {
    "question": "CertyIQ\nA company is concerned that two NAT instances in use will no longer be able to support the traffic needed for the\ncompanys application. A solutions architect wants to implement a solution that is highly available, fault tolerant,\nand automatically scalable.\nWhat should the solutions architect recommend?",
    "options": {
      "A": "Remove the two NAT instances and replace them with two NAT gateways in the same Availability Zone.",
      "B": "Use Auto Scaling groups with Network Load Balancers for the NAT instances in different Availability Zones.",
      "C": "Remove the two NAT instances and replace them with two NAT gateways in different Availability Zones.",
      "D": "Replace the two NAT instances with Spot Instances in different Availability Zones and deploy a Network"
    },
    "answer": "C",
    "explanation": "The best solution is to replace the two NAT instances with two NAT gateways in different Availability Zones.\nHere's why:\nNAT Gateways vs. NAT Instances: NAT Gateways are AWS-managed, offering high availability, fault\ntolerance, and automatic scaling, relieving the operational burden of managing NAT instances. NAT instances\nrequire manual configuration and maintenance.\nHigh Availability and Fault Tolerance: Deploying NAT Gateways in different Availability Zones (AZs) ensures\nthat if one AZ experiences an outage, traffic can be routed through the NAT Gateway in the other AZ.\nAutomatic Scaling: NAT Gateways automatically scale to handle increased traffic, which addresses the\ncompany's concern about traffic demands.\nCost Optimization: While NAT Gateways incur a per-hour charge and data processing fees, the reduced\noperational overhead and improved availability often offset the cost compared to managing NAT instances,\nespecially when considering the potential cost of downtime.\nOption A is incorrect because using two NAT gateways in the same AZ doesn't provide high availability. If that\nAZ goes down, external connectivity is lost.\nOption B is incorrect because, while using Auto Scaling Groups and NLBs with NAT Instances can improve\navailability, it adds complexity and doesn't match the simplicity and manageability of NAT Gateways. Also, it\ndoes not scale automatically as readily as a NAT Gateway.\nOption D is incorrect because relying on Spot Instances for critical network infrastructure like NAT is risky.\nSpot Instances can be terminated with short notice, potentially disrupting connectivity. Also, the company\nrequired a fault-tolerant solution.\nAuthoritative Links:\nAWS NAT Gateway: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\nNAT Instances: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-instances.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-instances.html"
    ]
  },
  {
    "question": "CertyIQ\nAn application runs on an Amazon EC2 instance that has an Elastic IP address in VPC",
    "options": {
      "A": "B. Configure a VPC peering connection between VPC A and VPC B.",
      "B": "Both VPCs are in the same AWS account.",
      "C": "Make the DB instance publicly accessible. Assign a public IP address to the DB instance.",
      "D": "Launch an EC2 instance with an Elastic IP address into VPC B. Proxy all requests through the new EC2"
    },
    "answer": "B",
    "explanation": "The most secure solution for allowing the application in VPC A to access the database in VPC B within the\nsame AWS account is to establish a VPC peering connection. VPC peering enables private networking\nbetween the two VPCs, allowing resources in each VPC to communicate with each other as if they were within\nthe same network. This eliminates the need for traffic to traverse the public internet, thereby reducing\nexposure to security risks.\nOption A, using the public IP address in a security group rule, exposes the database to potential attacks from\nsources that spoof the EC2 instance's public IP. This is inherently less secure. Option C, making the database\npublicly accessible, is the least secure option as it exposes the database to the entire internet. Assigning a\npublic IP address significantly widens the attack surface and should be avoided whenever possible. Option D,\nusing an EC2 instance as a proxy, adds unnecessary complexity and introduces another potential point of\nfailure and vulnerability. While it does limit direct exposure, it's not as streamlined and inherently secure as\nVPC peering.\nVPC peering offers a direct, private, and secure connection. It allows for fine-grained control through security\ngroups and network ACLs to further restrict and monitor the allowed traffic. The network traffic stays within\nthe AWS network infrastructure, benefiting from AWS's security measures. By configuring appropriate\nrouting tables in both VPCs, you can ensure that traffic destined for the database instance is properly routed\nthrough the VPC peering connection. This approach adheres to the principle of least privilege, granting only\nthe necessary access and minimizing the attack surface.\nAWS VPC Peering Documentation",
    "links": []
  },
  {
    "question": "CertyIQ\nA company runs demonstration environments for its customers on Amazon EC2 instances. Each environment is\nisolated in its own VP",
    "options": {
      "C": "The companys operations team needs to be notified when RDP or SSH access to an"
    },
    "answer": "C",
    "explanation": "The correct answer is C: Publish VPC flow logs to Amazon CloudWatch Logs, create required metric filters,\nand create an Amazon CloudWatch metric alarm with a notification action. Here's why:\nVPC Flow Logs Capture Network Traffic: VPC Flow Logs record information about the IP traffic going to,\nfrom, and within your VPCs. This includes source and destination IP addresses, ports, and the action taken\n(ACCEPT or REJECT). This is critical for detecting RDP (port 3389) and SSH (port 22) access.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html\nCloudWatch Logs as a Central Repository: VPC Flow Logs are published to CloudWatch Logs, providing a\ncentralized location for analysis and monitoring.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html\nMetric Filters Extract Relevant Data: CloudWatch metric filters allow you to search for specific patterns\nwithin the log data. You can create a filter that specifically looks for log entries showing successful\nconnections to port 22 or 3389, indicating SSH or RDP access.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringPolicyExamples.html\nCloudWatch Alarms Trigger Notifications: Once the metric filter identifies the specific log entries, a\nCloudWatch alarm can be created to trigger when the metric value (the number of SSH or RDP connections)\nexceeds a defined threshold. This allows the operations team to be notified immediately.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html\nNotification Action for the Operations Team: The CloudWatch alarm's notification action can be configured\nto send an alert to the operations team, using SNS, for instance.\nWhy other options are incorrect:\nA (CloudWatch Application Insights): Application Insights is better suited for monitoring the health and\nperformance of applications, not necessarily for security-related events like RDP/SSH access. It's not\ndesigned for this granular level of network traffic monitoring out of the box.\nB (IAM Instance Profile with AmazonSSMManagedInstanceCore): This IAM role primarily provides Systems\nManager access for instance management, not for detecting network access events. While Systems Manager\ncan execute commands and potentially log access attempts, it's not the most efficient or direct method for\nthis specific scenario.\nD (EventBridge for EC2 Instance State-change Notification): While EventBridge can track instance state\nchanges (e.g., running, stopped), it doesn't provide information about the reason for the state change or the\nnetwork traffic associated with the instance. Detecting RDP/SSH requires examining the network activity\nwithin the instance, not just its state. It would also require further configuration to determine if the state\nchange was related to RDP/SSH.",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringPolicyExamples.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect has created a new AWS account and must secure AWS account root user access.\nWhich combination of actions will accomplish this? (Choose two.)",
    "options": {
      "A": "Ensure the root user uses a strong password: The root user possesses unrestricted access to the AWS",
      "B": "Enable multi-factor authentication to the root user: MFA adds an extra layer of security by requiring a",
      "C": "Store root user access keys in an encrypted Amazon S3 bucket: It's highly discouraged to actively use",
      "D": "Add the root user to a group containing administrative permissions: The root user already has complete"
    },
    "answer": "A",
    "explanation": "The most secure approach to managing the root user involves enabling multi-factor authentication (MFA) and\nensuring a strong password.\nJustification:\nA. Ensure the root user uses a strong password: The root user possesses unrestricted access to the AWS\naccount. A strong, unique password is the first line of defense against unauthorized access. This password\nshould adhere to best practices for complexity and length.\nB. Enable multi-factor authentication to the root user: MFA adds an extra layer of security by requiring a\nsecond verification factor (e.g., a code from an authenticator app) in addition to the password. Even if the\npassword is compromised, the attacker would still need access to the second factor to gain entry.\nWhy other options are incorrect:\nC. Store root user access keys in an encrypted Amazon S3 bucket: It's highly discouraged to actively use\nroot user access keys. Instead, use IAM roles and users. Storing root user access keys, even in an encrypted\nbucket, is an unnecessary security risk. Best practice is to avoid creating these keys at all.\nD. Add the root user to a group containing administrative permissions: The root user already has complete\nadministrative permissions. Adding them to a group is redundant and doesn't enhance security.\nE. Apply the required permissions to the root user with an inline policy document: Like option D, the root\nuser already has full administrative access. Attaching inline policies is redundant and doesn't contribute to\nsecuring the root account itself. The goal is to limit the usage of the root account, not to further configure its\ninherent superuser privileges.\nIn summary: The best practice is to secure the root user with a strong password and MFA, and then minimize\nits use by creating IAM users and roles for day-to-day operations.\nSupporting Links:\nAWS documentation on securing root user access:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html\nAWS best practices for IAM: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html",
    "links": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is building a new web-based customer relationship management application. The application will use\nseveral Amazon EC2 instances that are backed by Amazon Elastic Block Store (Amazon EBS) volumes behind an\nApplication Load Balancer (ALB). The application will also use an Amazon Aurora database. All data for the\napplication must be encrypted at rest and in transit.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Use AWS Key Management Service (AWS KMS) certificates on the ALB to encrypt data in transit. Use AWS",
      "B": "Use the AWS root account to log in to the AWS Management Console. Upload the companys encryption",
      "C": "Use AWS Key Management Service (AWS KMS) to encrypt the EBS volumes and Aurora database storage at",
      "D": "Use BitLocker to encrypt all data at rest. Import the companys TLS certificate keys to AWS Key"
    },
    "answer": "C",
    "explanation": "The correct solution (C) leverages AWS's native services for encryption at rest and in transit. AWS Key\nManagement Service (KMS) is the recommended service for managing encryption keys and encrypting data at\nrest for both EBS volumes and Aurora databases. KMS allows centralized control over encryption keys,\nintegrates seamlessly with other AWS services, and offers auditing capabilities. EBS volumes can be\nencrypted when they are created, and Aurora supports encryption at rest using KMS keys.\nFor encryption in transit, AWS Certificate Manager (ACM) is the preferred method. ACM allows you to easily\nprovision, manage, and deploy SSL/TLS certificates for use with AWS services like Application Load\nBalancers. Attaching an ACM certificate to the ALB ensures that traffic between clients and the ALB is\nencrypted using HTTPS, and traffic between the ALB and the EC2 instances can also be encrypted.\nOption A is incorrect because ACM is primarily used for managing SSL/TLS certificates for encryption in\ntransit, not for encrypting storage at rest. KMS is the correct service for at-rest encryption for EBS and\nAurora.\nOption B is incorrect because it advocates using the root account, which is generally discouraged due to\nsecurity risks. Also, AWS does not have a single \"turn on encryption for all data\" option. Encryption needs to\nbe configured at the individual service level. Uploading certificates directly to the root account is also not best\npractice; KMS and ACM are specifically designed for this purpose.\nOption D is incorrect. BitLocker is a Microsoft Windows encryption feature and is not the recommended\nmethod for encrypting EBS volumes. While you could technically use it within an EC2 instance, it is far more\ncomplex and less integrated than using native AWS encryption. Moreover, while you can import TLS\ncertificates into KMS, attaching KMS keys directly to the ALB to encrypt data in transit is not the standard\napproach. ACM is specifically designed for managing certificates used by ALBs. The ALB uses ACM to handle\nthe TLS connection.\nIn summary, the chosen answer utilizes the most appropriate AWS services (KMS and ACM) in their intended\nways to provide a comprehensive encryption solution for the application's data at rest and in transit, following\nsecurity best practices.\nFurther research:\nAWS KMS: https://aws.amazon.com/kms/\nAWS ACM: https://aws.amazon.com/certificate-manager/\nEncrypting Amazon EBS volumes:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\nEncrypting Amazon Aurora:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Aurora.Encryption.html",
    "links": [
      "https://aws.amazon.com/kms/",
      "https://aws.amazon.com/certificate-manager/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Aurora.Encryption.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is moving its on-premises Oracle database to Amazon Aurora PostgreSQL. The database has several\napplications that write to the same tables. The applications need to be migrated one by one with a month in\nbetween each migration. Management has expressed concerns that the database has a high number of reads and\nwrites. The data must be kept in sync across both databases throughout the migration.\nWhat should a solutions architect recommend?",
    "options": {
      "A": "Use AWS DataSync for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a",
      "B": "Use AWS DataSync for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a full",
      "C": "Memory-optimized instances are generally better for",
      "D": "Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS) using a compute"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Here's a detailed justification:\nThe scenario requires migrating an Oracle database to Aurora PostgreSQL while maintaining data\nsynchronization during a phased application migration. Key considerations are the high read/write workload\nand the need for continuous data replication.\nOption C is the most suitable solution because it addresses all the requirements. First, the AWS Schema\nConversion Tool (SCT) is crucial for converting the Oracle database schema to a compatible Aurora\nPostgreSQL schema. This includes data types, stored procedures, and other database objects. Without\nschema conversion, the migration will likely fail due to compatibility issues. AWS SCT simplifies this complex\nprocess.\nSecond, AWS Database Migration Service (DMS) handles both the initial data load and ongoing data\nreplication (Change Data Capture - CDC). A \"full load plus CDC\" replication task ensures that the existing data\nis migrated and that subsequent changes on the source database are replicated to the target database in near\nreal-time, keeping the databases synchronized throughout the application migration process. The table\nmapping selection ensures all tables are included in the migration process. The phrase \"memory optimized\nreplication instance\" suggests a focus on handling high throughput, which directly addresses the 'high reads\nand writes' concern. While a compute-optimized instance could also be used, a memory-optimized instance\nhelps avoid bottlenecks in processing the data changes during CDC, especially with a high volume of writes.\nOption A is incorrect because it uses AWS DataSync which is useful for transferring large amounts of data\nbetween on-premises storage and AWS services but isn't designed for continuous database replication with\nschema conversion. It lacks the critical schema conversion step and the ability to efficiently manage ongoing\nchanges in the source database.\nOption B is better than A because it utilizes AWS DMS with CDC, but still lacks schema conversion. DMS alone\ncan't automatically handle the differences in schema between Oracle and PostgreSQL. It would require\nsignificant manual intervention and potentially custom code. DataSync is also not ideal for the initial database\nmigration when considering database-specific migration requirements.\nOption D includes the SCT and DMS with CDC, making it a good option, but it specifically focuses on migrating\nthe \"largest tables\" which does not meet the requirement to migrate all data, and fails to address the 'high\nreads and writes' concern as thoroughly as option C. Memory-optimized instances are generally better for\nhandling CDC, whereas compute-optimized instances are more appropriate for the initial full load.\nIn summary, option C provides the most comprehensive solution by addressing schema conversion, initial data\nmigration, and continuous data synchronization using a memory-optimized replication instance to handle the\ndatabase's high read/write workload.\nRelevant Documentation:\nAWS Schema Conversion Tool (SCT): https://aws.amazon.com/dms/schema-conversion-tool/\nAWS Database Migration Service (DMS): https://aws.amazon.com/dms/",
    "links": [
      "https://aws.amazon.com/dms/schema-conversion-tool/",
      "https://aws.amazon.com/dms/"
    ]
  },
  {
    "question": "CertyIQ\nA company has a three-tier application for image sharing. The application uses an Amazon EC2 instance for the\nfront-end layer, another EC2 instance for the application layer, and a third EC2 instance for a MySQL database. A\nsolutions architect must design a scalable and highly available solution that requires the least amount of change\nto the application.\nWhich solution meets these requirements?",
    "options": {
      "A": "Use Amazon S3 to host the front-end layer. Use AWS Lambda functions for the application layer. Move the",
      "B": "Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the application",
      "C": "Use Amazon S3 to host the front-end layer. Use a fleet of EC2 instances in an Auto Scaling group for the",
      "D": "Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the application"
    },
    "answer": "D",
    "explanation": "The correct answer is D because it provides a scalable and highly available solution with minimal changes to\nthe existing three-tier architecture.\nHere's why:\nElastic Beanstalk for Front-End and Application Layers: Using Elastic Beanstalk handles deployment, load\nbalancing, and auto-scaling automatically, reducing operational overhead for the front-end and application\nlayers. Deploying across multiple Availability Zones (Multi-AZ) within Elastic Beanstalk ensures high\navailability. This requires minimal code changes.\nAmazon RDS Multi-AZ for Database: Migrating the MySQL database to Amazon RDS (Relational Database\nService) Multi-AZ offers automated failover to a standby replica in a different Availability Zone, ensuring high\navailability and data durability. RDS is compatible with MySQL, minimizing code changes.\nAmazon S3 for Image Storage: Using Amazon S3 for storing images leverages its scalability, durability, and\ncost-effectiveness. Serving images directly from S3 reduces the load on the application layer and EC2\ninstances. This offloads image storage and delivery from the database and application servers.\nOption A requires significant architectural changes, including rewriting the application layer to use Lambda\nand changing the database to DynamoDB, which is not compatible with MySQL, increasing development effort\nand risks.\nOption B suggests using read replicas for serving images which is not the intended use of read replicas. The\nmain use is to offload read traffic from the primary database. Also, it doesn't address the scalability and\navailability of the application layer as effectively as Elastic Beanstalk.\nOption C's suggestion to use a memory-optimized instance type for the database to store images is inefficient\nand limits scalability compared to using S3 for image storage.\nSupporting Concepts:\nScalability: The ability of a system to handle increasing workloads.\nHigh Availability: The ability of a system to remain operational even if some components fail.\nLoad Balancing: Distributing traffic across multiple instances to improve performance and availability.\nMulti-AZ Deployment: Deploying resources across multiple Availability Zones to ensure high availability.\nElastic Beanstalk: An AWS service for deploying and managing web applications and services.\nAmazon RDS: A managed relational database service on AWS.\nAmazon S3: A scalable object storage service on AWS.\nAuthoritative Links:\nAWS Elastic Beanstalk: https://aws.amazon.com/elasticbeanstalk/\nAmazon RDS Multi-AZ: https://aws.amazon.com/rds/features/multi-az/\nAmazon S3: https://aws.amazon.com/s3/",
    "links": [
      "https://aws.amazon.com/elasticbeanstalk/",
      "https://aws.amazon.com/rds/features/multi-az/",
      "https://aws.amazon.com/s3/"
    ]
  },
  {
    "question": "CertyIQ\nAn application running on an Amazon EC2 instance in VPC-A needs to access files in another EC2 instance in VPC-",
    "options": {
      "B": "Set up VPC gateway endpoints for the EC2 instance running in VPC-B.",
      "A": "Set up a VPC peering connection between VPC-A and VPC-B.",
      "C": "Option C is incorrect because attaching a virtual private gateway (VGW) to VPC-B typically works in",
      "D": "Create a private virtual interface (VIF) for the EC2 instance running in VPC-B and add appropriate routes"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Set up a VPC peering connection between VPC-A and VPC-B.\nHere's a detailed justification:\nVPC peering establishes a direct networking connection between two VPCs, enabling instances in each VPC\nto communicate with each other as if they were within the same network. This solution directly addresses the\nrequirement of securely connecting EC2 instances across VPCs in different AWS accounts. VPC peering\ndoesn't route traffic through the public internet, providing a secure connection. Because VPC peering\nestablishes direct routing between VPCs, it avoids the bandwidth limitations associated with solutions\ninvolving the public internet or single network gateways. VPC peering is designed to be highly available,\nensuring that connectivity isn't disrupted by single points of failure. You can also implement routing\nconfigurations to ensure that traffic can flow both ways after the peering connection is active.\nOption B is incorrect because VPC gateway endpoints are designed to provide private access to AWS services\nlike S3 and DynamoDB, not to EC2 instances within another VPC.\nOption C is incorrect because attaching a virtual private gateway (VGW) to VPC-B typically works in\nconjunction with a VPN connection or AWS Direct Connect. While technically possible to connect via a VGW\nacross accounts, it's generally more complex and doesn't directly offer the same level of simplicity and high\navailability as VPC peering for this cross-account, intra-AWS connectivity. Additionally, VPN connections\nusing VGWs can introduce bandwidth constraints and a single point of failure.\nOption D is incorrect because creating a private virtual interface (VIF) is relevant for AWS Direct Connect,\nwhich provides a dedicated network connection from your on-premises environment to AWS. It doesn't\ndirectly solve the need for VPC-to-VPC communication within the AWS cloud. Direct Connect is typically used\nfor hybrid cloud scenarios, not for connecting VPCs within AWS accounts.\nFor further research, consult the AWS documentation:\nVPC Peering: https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\nVPC Endpoints: https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html\nAWS Direct Connect: https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html\nVirtual Private Gateways: https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html",
      "https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html",
      "https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html",
      "https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to experiment with individual AWS accounts for its engineer team. The company wants to be\nnotified as soon as the Amazon EC2 instance usage for a given month exceeds a specific threshold for each\naccount.\nWhat should a solutions architect do to meet this requirement MOST cost-effectively?",
    "options": {
      "A": "Use Cost Explorer to create a daily report of costs by service. Filter the report by EC2 instances. Configure",
      "B": "Use Cost Explorer to create a monthly report of costs by service. Filter the report by EC2 instances.",
      "C": "Use AWS Budgets to create a cost budget for each account. Set the period to monthly. Set the scope to EC2",
      "D": "Use AWS Cost and Usage Reports to create a report with hourly granularity. Integrate the report data with"
    },
    "answer": "C",
    "explanation": "The most cost-effective solution for notifying the company when EC2 instance usage exceeds a specific\nthreshold for each account is using AWS Budgets.\nAWS Budgets allows you to set custom budgets to track your AWS costs and usage. You can define budgets\non a monthly basis and scope them specifically to EC2 instances, which directly addresses the requirement.\nThe key advantage is that Budgets support setting alert thresholds. When actual or forecasted costs exceed\nthe defined threshold, Budgets can trigger notifications via Amazon SNS (Simple Notification Service),\nenabling immediate alerting. This eliminates the need for constant manual monitoring.\nOption A and B, while using Cost Explorer, do not provide built-in alerting capabilities that are easily\nconfigurable to trigger notifications when a specific threshold is exceeded. While Cost Explorer reports offer\ninsight, they require external mechanisms or manual review to generate alerts, which is less cost-effective\nand efficient. Amazon SES would need to be manually configured, adding complexity.\nOption D, involving AWS Cost and Usage Reports, Amazon Athena, and Amazon EventBridge, is significantly\nmore complex and costly. Although granular and powerful, it's overkill for a simple usage threshold alert.\nHourly granularity and Athena queries require more resources and associated costs compared to the native\ncapabilities of AWS Budgets.\nAWS Budgets offers the optimal balance of functionality, cost-effectiveness, and ease of use for the\nspecified requirement. It is designed for cost tracking and alerting, making it the right tool for this purpose.\nRelevant link: https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-\ncosts.html",
    "links": [
      "https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect needs to design a new microservice for a companys application. Clients must be able to call\nan HTTPS endpoint to reach the microservice. The microservice also must use AWS Identity and Access\nManagement (IAM) to authenticate calls. The solutions architect will write the logic for this microservice by using a\nsingle AWS Lambda function that is written in Go 1.x.\nWhich solution will deploy the function in the MOST operationally efficient way?",
    "options": {
      "A": "Create an Amazon API Gateway REST API. Configure the method to use the Lambda function. Enable IAM",
      "B": "Create a Lambda function URL for the function. Specify AWS_IAM as the authentication type.",
      "C": "Create an Amazon CloudFront distribution. Deploy the function to [email protected] Integrate IAM",
      "D": "Create an Amazon CloudFront distribution. Deploy the function to CloudFront Functions. Specify AWS_IAM"
    },
    "answer": "A",
    "explanation": "The most operationally efficient solution is to use Amazon API Gateway with Lambda integration and IAM\nauthentication. Here's why:\nAPI Gateway's Purpose: API Gateway is specifically designed to create, publish, maintain, monitor, and\nsecure APIs at any scale. It acts as a front door for applications to access data, logic, or functionality from\nbackend services like Lambda.\nLambda Integration: API Gateway natively integrates with Lambda functions. This allows direct invocation of\nthe Lambda function upon receiving an API request, streamlining the request-response flow.\nIAM Authentication: API Gateway supports IAM authentication, allowing the microservice to leverage AWS's\nrobust identity and access management system. This eliminates the need to implement custom authentication\nlogic within the Lambda function itself, improving security and reducing code complexity.\nOperational Efficiency: Using API Gateway for tasks like authentication and routing centralizes these\nresponsibilities, making management and maintenance more straightforward. It also provides built-in features\nlike caching, throttling, and monitoring that contribute to operational efficiency.\nWhy other options are less efficient:\nLambda Function URLs (Option B): While simpler to set up initially, function URLs lack the advanced features\nof API Gateway, such as request transformation, caching, and comprehensive security policies. They're less\nsuitable for complex scenarios requiring features beyond basic invocation.\nCloudFront with Lambda@Edge or CloudFront Functions (Options C and D): CloudFront is primarily a\nContent Delivery Network (CDN) designed for caching and delivering static content. While it can execute\nLambda functions with Lambda@Edge or CloudFront Functions, these are typically used for edge computing\ntasks like modifying HTTP headers or customizing content delivery. They are not best suited for implementing\nthe core logic of a microservice requiring IAM authentication. Deploying the entire microservice to CloudFront\nwould be an overkill.\nIn summary, API Gateway provides the most comprehensive and efficient solution for creating an HTTPS\nendpoint for a Lambda-backed microservice that requires IAM authentication, due to its features designed for\nAPI management and seamless integration with other AWS services.\nHere are some links for more research:\nAmazon API Gateway: https://aws.amazon.com/api-gateway/\nAWS Lambda: https://aws.amazon.com/lambda/\nIAM Authentication in API Gateway:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-iam-authorizer.html\nLambda Function URLs: https://docs.aws.amazon.com/lambda/latest/dg/lambda-urls.html\nLambda@Edge: https://aws.amazon.com/lambda/edge/\nCloudFront Functions: https://aws.amazon.com/cloudfront/features/cloudfront-functions/",
    "links": [
      "https://aws.amazon.com/api-gateway/",
      "https://aws.amazon.com/lambda/",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-iam-authorizer.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/lambda-urls.html",
      "https://aws.amazon.com/lambda/edge/",
      "https://aws.amazon.com/cloudfront/features/cloudfront-functions/"
    ]
  },
  {
    "question": "CertyIQ\nA company previously migrated its data warehouse solution to AWS. The company also has an AWS Direct Connect\nconnection. Corporate office users query the data warehouse using a visualization tool. The average size of a query\nreturned by the data warehouse is 50 MB and each webpage sent by the visualization tool is approximately 500\nK",
    "options": {
      "B": "Host the visualization tool in the same AWS Region as the data warehouse. Access it over the internet.",
      "A": "Host the visualization tool on premises and query the data warehouse directly over the internet.",
      "C": "Host the visualization tool on premises and query the data warehouse directly over a Direct Connect",
      "D": "Host the visualization tool in the same AWS Region as the data warehouse and access it over a Direct"
    },
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the best solution for minimizing data transfer egress costs in\nthis scenario:\nThe primary goal is to reduce egress costs, which are charges incurred when data leaves an AWS Region.\nOptions A and B involve data traversing the internet, which incurs significant egress charges. The question\nstates that the company already has a Direct Connect connection. Direct Connect offers significantly reduced\ndata transfer costs compared to transferring data over the public internet by establishing a dedicated\nnetwork connection from on-premises to AWS.\nOption C utilizes the Direct Connect connection for querying, which is better than using the internet. However,\nthe visualization tool remains on-premises. This requires the 500 KB webpages generated by the tool to be\nsent back across the Direct Connect connection to the users at the corporate office, leading to unnecessary\negress charges for the visualization content.\nOption D eliminates internet egress charges. By hosting the visualization tool within the same AWS Region as\nthe data warehouse, the 50 MB query results from the data warehouse and the 500 KB web pages generated\nby the tool are transferred within the AWS Region. Intra-region data transfer within AWS is generally free or\nsignificantly cheaper than egressing data outside of AWS. The users then access the visualization tool, and\nthe generated webpages (500 KB) are transmitted over the Direct Connect connection only. This is\nsignificantly less egress data than transferring the 50 MB result sets across Direct Connect or the internet, as\nwould happen with other options. Therefore, D is the most cost-effective because it leverages Direct Connect\nto minimize the egress costs.\nIn summary, option D minimizes egress costs by:\n1. Keeping the majority of data transfer (50 MB query results) within the AWS Region.\n2. Only using the Direct Connect connection to send the smaller webpage results (500 KB) from the\nvisualization tool to on-premises users, resulting in the lowest egress costs.\nAuthoritative Links for Further Research:\nAWS Direct Connect: https://aws.amazon.com/directconnect/\nAWS Pricing: https://aws.amazon.com/pricing/ (Specifically, look for data transfer costs)",
    "links": [
      "https://aws.amazon.com/directconnect/",
      "https://aws.amazon.com/pricing/"
    ]
  },
  {
    "question": "CertyIQ\nAn online learning company is migrating to the AWS Cloud. The company maintains its student records in a\nPostgreSQL database. The company needs a solution in which its data is available and online across multiple AWS\nRegions at all times.\nWhich solution will meet these requirements with the LEAST amount of operational overhead?",
    "options": {
      "A": "Migrate the PostgreSQL database to a PostgreSQL cluster on Amazon EC2 instances.",
      "B": "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance with the Multi-AZ feature",
      "C": "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance.",
      "D": "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Set up DB snapshots to"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance.\nCreate a read replica in another Region.\nHere's why:\nAvailability and Region Redundancy: The primary requirement is for data availability across multiple AWS\nRegions at all times. Option C directly addresses this by using RDS cross-region read replicas. A read replica\nin another region provides a near real-time copy of the data, ensuring data availability even if the primary\nregion experiences an outage.\nOperational Overhead: RDS read replicas are managed services, which significantly reduces operational\noverhead compared to self-managing a PostgreSQL cluster on EC2 (Option A). AWS handles the replication\nand failover processes to the read replica region.\nMulti-AZ vs. Multi-Region: Option B (Multi-AZ) provides high availability within a single region but does not\naddress the requirement of availability across multiple regions. A Multi-AZ deployment automatically\nprovisions a standby database instance in a different Availability Zone within the same region. Thus, it will not\nhelp in case of Regional Outage.\nDB Snapshots: Option D (DB snapshots) provides a point-in-time backup, but restoring from a snapshot in\nanother region involves a manual process that can lead to significant downtime. Snapshots are not suitable\nfor continuous availability.\nRDS Read Replica: RDS read replicas can be promoted to become standalone primary DB instances if the\nprimary instance fails. This is a more seamless failover process than restoring from a snapshot.\nIn summary, using RDS with a cross-region read replica provides the best combination of multi-region data\navailability and minimal operational overhead, fulfilling the company's requirements in a cost-effective and\nmanageable manner.Authoritative Links:\nAmazon RDS Read Replicas\nWorking with Amazon RDS Read Replicas",
    "links": []
  },
  {
    "question": "CertyIQ\nA company hosts its web application on AWS using seven Amazon EC2 instances. The company requires that the IP\naddresses of all healthy EC2 instances be returned in response to DNS queries.\nWhich policy should be used to meet this requirement?",
    "options": {
      "A": "Simple routing policy",
      "B": "Latency routing policy",
      "C": "Multivalue routing policy",
      "D": "Geolocation routing policy"
    },
    "answer": "C",
    "explanation": "The correct answer is Multivalue routing policy (C) because it's designed to return multiple healthy IP\naddresses in response to DNS queries. This directly addresses the company's requirement to receive the IP\naddresses of all healthy EC2 instances.\nHere's a detailed justification:\nMultivalue Answer Routing: This policy is specifically built to return multiple values (like IP addresses) for a\nsingle DNS query. This is essential for high availability and load balancing across multiple resources.\n[https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-\nmultivalue]\nHealth Checks Integration: Multivalue answer routing works in conjunction with Route 53 health checks.\nRoute 53 monitors the health of each EC2 instance (or other endpoint). Only the IP addresses of instances that\npass the health checks are returned in the DNS response.\nWhy other options are incorrect:\nSimple routing policy: Returns only one IP address from a set of records. While it can provide basic failover if\nthe first record is unavailable, it doesn't provide multiple healthy IPs.\nLatency routing policy: Routes traffic based on the lowest latency between the user and the AWS region,\nirrelevant to the health of all instances.\nGeolocation routing policy: Routes traffic based on the geographic location of the user, also irrelevant to the\nhealth of all instances.\nScalability and High Availability: By returning multiple healthy IP addresses, the application can distribute\ntraffic across available EC2 instances, improving scalability and resilience. If one instance fails, traffic is\nautomatically routed to the remaining healthy instances without any DNS propagation delay.\nIn summary: Multivalue answer routing provides the necessary functionality to return the IP addresses of all\nhealthy EC2 instances, aligning with the company's requirements for high availability and load distribution.\nHealth checks ensure only healthy instances are included in the response.",
    "links": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-"
    ]
  },
  {
    "question": "CertyIQ\nA medical research lab produces data that is related to a new study. The lab wants to make the data available with\nminimum latency to clinics across the country for their on-premises, file-based applications. The data files are\nstored in an Amazon S3 bucket that has read-only permissions for each clinic.\nWhat should a solutions architect recommend to meet these requirements?",
    "options": {
      "A": "Deploy an AWS Storage Gateway file gateway as a virtual machine (VM) on premises at each clinic",
      "B": "AWS DataSync: While DataSync can move data to on-premises locations, it's more suited for initial data",
      "C": "AWS Storage Gateway volume gateway: A volume gateway presents cloud-based storage volumes as",
      "D": "Amazon EFS: Amazon EFS is a fully managed elastic NFS file system for use with AWS cloud services and"
    },
    "answer": "A",
    "explanation": "The correct answer is A: Deploy an AWS Storage Gateway file gateway as a virtual machine (VM) on premises\nat each clinic. Here's why:\nLow Latency and On-Premises File Access: The key requirement is minimum latency for file-based\napplications on-premises. AWS Storage Gateway file gateway addresses this directly. It provides a local\ncache of the data, allowing on-premises applications to access frequently used files with very low latency.\nThe files are still durably stored in S3, but the gateway acts as a local \"front-end\" for faster access.\nFile-Based Access: The scenario specifies that the data needs to be accessible to file-based applications. A\nfile gateway presents the data as a standard file share (e.g., NFS, SMB), which is compatible with these\napplications.\nAWS Storage Gateway Overview: AWS Storage Gateway connects an on-premises software appliance with\ncloud-based storage to provide seamless and secure integration between an organization's on-premises IT\nenvironment and the AWS storage infrastructure. https://aws.amazon.com/storagegateway/\nLet's examine why the other options are less suitable:\nB. AWS DataSync: While DataSync can move data to on-premises locations, it's more suited for initial data\nmigration or periodic synchronization, not for providing low-latency, continuous access to frequently changing\ndata. DataSync copies data, introducing delays compared to caching.\nC. AWS Storage Gateway volume gateway: A volume gateway presents cloud-based storage volumes as\niSCSI block devices to on-premises applications. This is not suitable for file-based applications, as it requires\nthe applications to work with block storage instead of files.\nD. Amazon EFS: Amazon EFS is a fully managed elastic NFS file system for use with AWS cloud services and\non-premises resources. While it offers file-based access, directly attaching it to on-premises servers requires\na VPN or Direct Connect connection to AWS. The latency over such a connection would be higher compared to\na local file gateway. Also, EFS is generally optimized for cloud workloads, not necessarily low-latency on-\npremise access to files stored primarily in S3.\nIn summary, a file gateway allows the clinics to directly access the needed files locally, thus reducing latency\nto nearly zero, which is not achievable with any other of the suggested setups.",
    "links": [
      "https://aws.amazon.com/storagegateway/"
    ]
  },
  {
    "question": "CertyIQ\nA company is using a content management system that runs on a single Amazon EC2 instance. The EC2 instance\ncontains both the web server and the database software. The company must make its website platform highly\navailable and must enable the website to scale to meet user demand.\nWhat should a solutions architect recommend to meet these requirements?",
    "options": {
      "A": "Option D: Running the database on a separate EC2 instance without using a managed database service like",
      "B": "Migrate the database to an Amazon Aurora instance with a read replica in the same Availability Zone as the",
      "C": "Move the database to Amazon Aurora with a read replica in another Availability Zone. Create an Amazon",
      "D": "Move the database to a separate EC2 instance, and schedule backups to Amazon S3. Create an Amazon"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it addresses both high availability and scalability in a resilient and automated\nmanner.\nHere's a breakdown of why option C is superior:\nDatabase High Availability and Scalability: Moving the database to Amazon Aurora with a read replica in\nanother Availability Zone provides both high availability (in case of a failure in one AZ) and read scalability\n(offloading read operations to the read replica). Aurora's multi-AZ capabilities ensure data redundancy and\nfailover. https://aws.amazon.com/rds/aurora/\nWeb Server Scalability and Availability: Creating an AMI from the EC2 instance allows for consistent\ndeployments of the web server application. The Application Load Balancer (ALB) distributes incoming traffic\nacross multiple instances in different Availability Zones, providing high availability.\nhttps://aws.amazon.com/elasticloadbalancing/application-load-balancer/\nAutomated Scaling: Attaching an Auto Scaling group to the ALB ensures that the number of EC2 instances\nautomatically adjusts to meet demand, scaling up during peak loads and scaling down during off-peak\nperiods. The Auto Scaling group utilizes the AMI to launch new instances, maintaining consistency. Spreading\ninstances across two Availability Zones provides resilience. https://aws.amazon.com/autoscaling/\nWhy other options are less suitable:\nOption A: Manual launches of EC2 instances and configuring only in one AZ do not provide automatic scaling\nor full high availability across different Availability Zones. Manually launching another EC2 instance is not a\nscalable solution, and the database backup does not provide a high available configuration.\nOption B: Using a read replica in the same Availability Zone does not protect against an AZ failure, therefore\ndoes not achieve high availability. Manually launching EC2 instances has the same drawbacks of option A.\nOption D: Running the database on a separate EC2 instance without using a managed database service like\nRDS or Aurora lacks the built-in high availability features and easier management provided by these services.\nScheduling backups is good practice, but not sufficient for high availability.",
    "links": [
      "https://aws.amazon.com/rds/aurora/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
      "https://aws.amazon.com/autoscaling/"
    ]
  },
  {
    "question": "CertyIQ\nA company is launching an application on AWS. The application uses an Application Load Balancer (ALB) to direct\ntraffic to at least two Amazon EC2 instances in a single target group. The instances are in an Auto Scaling group\nfor each environment. The company requires a development environment and a production environment. The\nproduction environment will have periods of high traffic.\nWhich solution will configure the development environment MOST cost-effectively?",
    "options": {
      "A": "Also, ASG configuration usually",
      "B": "Change the ALB balancing algorithm to least outstanding requests.",
      "C": "Reduce the size of the EC2 instances in both environments.",
      "D": "Reduce the maximum number of EC2 instances in the development environments Auto Scaling group."
    },
    "answer": "A",
    "explanation": "The correct answer is A: \"Reconfigure the target group in the development environment to have only one EC2\ninstance as a target.\"\nHere's a detailed justification:\nThe primary goal is to minimize costs in the development environment while maintaining functionality. Having\nat least two EC2 instances in the production environment's target group and Auto Scaling group is necessary\nfor high availability and handling production traffic spikes. However, the development environment doesn't\ntypically require the same level of resilience or capacity.\nOption A directly addresses cost reduction by minimizing resource usage in the development environment. An\nALB requires at least one healthy instance in a target group to function. Reducing the number of instances\nfrom two to one in the development environment reduces the cost related to EC2 compute resources by half,\nwithout completely disabling the environment. Development teams can still deploy, test, and debug their\ncode.\nOption B, changing the ALB balancing algorithm, doesn't directly reduce costs. Least outstanding requests\ndistributes load differently, but it doesn't eliminate the need for instances. It is mainly helpful in balancing the\nload between the instances; it doesn't inherently reduce the number of instances required and thus doesn't\nsignificantly reduce costs.\nOption C, reducing the size of EC2 instances in both environments, might save costs across both\nenvironments, but is not the most cost-effective for the development environment, as it impacts performance\non the production environment as well, which has periods of high traffic. The prompt is asking specifically for\nwhat configuration would be most cost-effective for development, which means we want to reduce overhead\nwhere the high availability requirements are low.\nOption D, reducing the maximum number of EC2 instances in the development environment's Auto Scaling\ngroup, is a good idea for cost savings. However, the question mentions the application needs at least two\ninstances for functionality to run. Therefore, reducing the maximum without reducing the minimum still will\nrequire 2 instances. Therefore it is less effective compared to Option A. Also, ASG configuration usually\nrequires a minimum value as well, which further mitigates the cost savings and the benefits are only realized\nduring auto-scaling.Therefore, option A is the most appropriate choice. It provides a significant cost reduction\ndirectly in the development environment without crippling its core functionality for developers.\nFurther reading on these AWS services can be found here:\nApplication Load Balancer:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\nAmazon EC2: https://aws.amazon.com/ec2/\nAuto Scaling: https://aws.amazon.com/autoscaling/",
    "links": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://aws.amazon.com/ec2/",
      "https://aws.amazon.com/autoscaling/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a web application on Amazon EC2 instances in multiple Availability Zones. The EC2 instances are\nin private subnets. A solutions architect implements an internet-facing Application Load Balancer (ALB) and\nspecifies the EC2 instances as the target group. However, the internet traffic is not reaching the EC2 instances.\nHow should the solutions architect reconfigure the architecture to resolve this issue?",
    "options": {
      "A": "Replace the ALB with a Network Load Balancer. Configure a NAT gateway in a public subnet to allow",
      "B": "Option B proposes moving the EC2 instances to public subnets, which is generally not recommended for",
      "C": "Update the route tables for the EC2 instances subnets to send 0.0.0.0/0 traffic through the internet gateway",
      "D": "Create public subnets in each Availability Zone. Associate the public subnets with the ALB. Update the route"
    },
    "answer": "D",
    "explanation": "The problem is that an internet-facing Application Load Balancer (ALB) cannot directly route traffic to EC2\ninstances residing in private subnets. ALBs need to be placed in public subnets to receive traffic from the\ninternet.\nOption D correctly addresses this by creating public subnets for the ALB. By associating the ALB with these\npublic subnets, the ALB can receive internet traffic. The route tables for these public subnets are then\nconfigured to route traffic to the private subnets where the EC2 instances are located. This allows the ALB to\nforward the internet traffic to the backend EC2 instances while keeping them isolated in private subnets for\nsecurity.\nOption A is incorrect because while a Network Load Balancer (NLB) can be used, it doesn't inherently solve\nthe problem of the private subnets. A NAT gateway might allow outbound internet access, but doesn't\nfacilitate inbound traffic to the instances through the ALB.\nOption B proposes moving the EC2 instances to public subnets, which is generally not recommended for\nsecurity reasons. Public subnets expose EC2 instances directly to the internet, increasing the attack surface.\nWhile adding a security group rule allows outbound traffic, it doesn't address the fundamental problem of the\nALB's placement.\nOption C suggests updating the route tables for the EC2 instances' subnets to use an internet gateway. This is\nincorrect because private subnets should not have direct routes to the internet gateway. This would\neffectively make the subnets public, negating the purpose of having them private in the first place.\nTherefore, Option D is the most secure and appropriate solution. It utilizes public subnets for the ALB to\nreceive internet traffic and maintains the security posture of the EC2 instances by keeping them in private\nsubnets. The route tables ensure that the traffic is properly routed from the ALB in the public subnets to the\nEC2 instances in the private subnets.\nFurther research:\nAWS Documentation - Application Load Balancers:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\nAWS Documentation - VPC Routing:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html\nAWS Documentation - Security Groups: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-\nsecurity-groups.html",
    "links": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-"
    ]
  },
  {
    "question": "CertyIQ\nA company has deployed a database in Amazon RDS for MySQL. Due to increased transactions, the database\nsupport team is reporting slow reads against the DB instance and recommends adding a read replica.\nWhich combination of actions should a solutions architect take before implementing this change? (Choose two.)",
    "options": {
      "A": "Enable binlog replication on the RDS primary node.",
      "B": "Choose a failover priority for the source DB instance.",
      "C": "Allow long-running transactions to complete on the source DB instance.",
      "D": "Create a global table and specify the AWS Regions where the table will be available."
    },
    "answer": "C",
    "explanation": "The correct answer is CE. Here's a detailed justification:\nC: Allow long-running transactions to complete on the source DB instance.\nBefore creating a read replica, it's crucial to ensure data consistency. Interrupting a long-running transaction\non the primary DB instance could lead to data corruption or an inconsistent state being replicated to the read\nreplica. Allowing the existing transactions to complete ensures a clean cut-off point for replication to begin,\nguaranteeing the read replica is initialized with consistent data. Prematurely interrupting transactions would\nforce a rollback and introduce inconsistencies that might be difficult to resolve after the read replica has\nbeen launched.\nE: Enable automatic backups on the source instance by setting the backup retention period to a value other\nthan 0.\nEnabling automatic backups is a prerequisite for creating a read replica in Amazon RDS for MySQL (and other\nengines). RDS uses the binary logs generated by the backup process to replicate changes to the read replica.\nIf automatic backups are disabled (retention period set to 0), binary logging is also disabled, and replication\ncannot occur. Therefore, configuring a backup retention period greater than zero ensures that binary logs are\navailable for replication.\nLet's discuss why the other options are incorrect:\nA: Enable binlog replication on the RDS primary node. Although related to replication, binlog is enabled\nthrough enabling backups, so it is not a separate step needed as part of the checklist.\nB: Choose a failover priority for the source DB instance. Failover priority is for Multi-AZ deployments and not\nnecessary when creating a read replica, which is meant for read scaling.\nD: Create a global table and specify the AWS Regions where the table will be available. Global tables are\npart of DynamoDB, not RDS MySQL. This is completely irrelevant to the task of creating a MySQL read replica.\nIn summary, ensuring that existing long-running transactions complete and enabling automatic backups are\nvital preparatory steps when creating an RDS for MySQL read replica to guarantee data consistency and\nenable the replication process.\nSupporting documentation:\nCreating a Read Replica - Amazon RDS\nWorking with MySQL and MariaDB Read Replicas",
    "links": []
  },
  {
    "question": "CertyIQ\nA company runs analytics software on Amazon EC2 instances. The software accepts job requests from users to\nprocess data that has been uploaded to Amazon S3. Users report that some submitted data is not being processed\nAmazon CloudWatch reveals that the EC2 instances have a consistent CPU utilization at or near 100%. The\ncompany wants to improve system performance and scale the system based on user load.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Creating a copy of the instance and using an Application Load Balancer: This would distribute the load",
      "B": "Creating an S3 VPC endpoint: This improves security and reduces network latency when accessing S3, but",
      "C": "Stopping and resizing the instance: This provides a temporary performance boost but doesn't scale",
      "D": "Here's why:"
    },
    "answer": "D",
    "explanation": "The correct solution is D. Here's why:\nThe problem describes a system where EC2 instances are CPU-bound due to high user load and data\nprocessing demands. The goal is to improve performance and scale based on user demand. Option D\naddresses this by introducing a queuing mechanism (Amazon SQS) and an Auto Scaling group for the EC2\ninstances.\nHere's a breakdown:\n1. Amazon SQS decouples the request handling from processing: Instead of users directly sending\nrequests to the EC2 instances, they send messages to an SQS queue. This prevents the EC2\ninstances from being overwhelmed by sudden spikes in requests. This addresses the immediate\nproblem of 100% CPU utilization and allows the system to handle bursts of requests.\n2. EC2 Auto Scaling group dynamically scales based on queue size: The Auto Scaling group monitors\nthe number of messages in the SQS queue. When the queue size exceeds a certain threshold, the\nAuto Scaling group automatically launches additional EC2 instances to process the backlog.\nConversely, when the queue size is low, the Auto Scaling group terminates instances to reduce costs.\nThis provides the necessary scaling based on user load. This dynamically adjusts resources to meet\nthe demand.\n3. Software update to read from the queue: The analytics software is modified to consume messages\nfrom the SQS queue instead of directly receiving requests from users. This ensures that all requests\nare processed in an orderly fashion and that no requests are lost.\nWhy the other options are incorrect:\nA. Creating a copy of the instance and using an Application Load Balancer: This would distribute the load\nacross multiple instances, but if the load is consistently high, all instances will eventually become CPU-bound.\nIt doesn't address the underlying issue of CPU-intensive processing and doesn't automatically scale based on\ndemand. It would only provide a limited and static improvement.\nB. Creating an S3 VPC endpoint: This improves security and reduces network latency when accessing S3, but\nit doesn't directly address the CPU bottleneck or the need for scaling based on user load. It improves data\ntransfer speeds, but it doesn't address the processing bottleneck.\nC. Stopping and resizing the instance: This provides a temporary performance boost but doesn't scale\nautomatically with user load. It would resolve the immediate CPU constraint, but it's a manual and temporary\nfix.\nAuthoritative Links:\nAmazon SQS: https://aws.amazon.com/sqs/\nEC2 Auto Scaling: https://aws.amazon.com/autoscaling/\nLoad Balancing: https://aws.amazon.com/elasticloadbalancing/",
    "links": [
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/autoscaling/",
      "https://aws.amazon.com/elasticloadbalancing/"
    ]
  },
  {
    "question": "CertyIQ\nA company is implementing a shared storage solution for a media application that is hosted in the AWS Cloud. The\ncompany needs the ability to use SMB clients to access data. The solution must be fully managed.\nWhich AWS solution meets these requirements?",
    "options": {
      "A": "Create an AWS Storage Gateway volume gateway. Create a file share that uses the required client protocol.",
      "B": "Create an AWS Storage Gateway tape gateway. Configure tapes to use Amazon S3. Connect the application",
      "C": "Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance.",
      "D": "Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server."
    },
    "answer": "D",
    "explanation": "The requirement is a fully managed shared storage solution accessible via SMB clients for a media\napplication. Let's analyze each option.\nOption A utilizes AWS Storage Gateway volume gateway. While Storage Gateway can present block storage\nover iSCSI, which could be formatted with SMB, it is not a fully managed file sharing solution. The\nmanagement of the SMB server and file shares would fall on the user.\nOption B, AWS Storage Gateway tape gateway, is designed for archival storage and is not suitable for\nproviding low-latency access for media applications. It primarily interacts with backup software and emulates\ntape libraries.\nOption C involves manually creating an EC2 Windows instance and configuring the SMB file share. This\napproach is not fully managed. The user is responsible for patching, scaling, and managing the Windows\ninstance and the SMB server.\nOption D, Amazon FSx for Windows File Server, provides a fully managed Windows file system built on\nWindows Server. It natively supports the SMB protocol, which directly fulfills the requirement of SMB client\naccess. It handles patching, backups, and other administrative tasks, reducing operational overhead. FSx for\nWindows File Server allows applications to access shared file storage over SMB, which is exactly what the\nmedia application needs. This provides a readily available, scalable, and managed file-sharing solution.\nTherefore, option D is the most suitable solution due to its fully managed nature and native SMB support.\nSupporting Links:\nAmazon FSx for Windows File Server: https://aws.amazon.com/fsx/windows/\nAWS Storage Gateway: https://aws.amazon.com/storagegateway/",
    "links": [
      "https://aws.amazon.com/fsx/windows/",
      "https://aws.amazon.com/storagegateway/"
    ]
  },
  {
    "question": "CertyIQ\nA companys security team requests that network traffic be captured in VPC Flow Logs. The logs will be frequently\naccessed for 90 days and then accessed intermittently.\nWhat should a solutions architect do to meet these requirements when configuring the logs?",
    "options": {
      "A": "Use Amazon CloudWatch as the target. Set the CloudWatch log group with an expiration of 90 days",
      "B": "Use Amazon Kinesis as the target. Configure the Kinesis stream to always retain the logs for 90 days.",
      "C": "Use AWS CloudTrail as the target. Configure CloudTrail to save to an Amazon S3 bucket, and enable S3",
      "D": "Use Amazon S3 as the target. Enable an S3 Lifecycle policy to transition the logs to S3 Standard-Infrequent"
    },
    "answer": "D",
    "explanation": "The correct answer is D because it provides a cost-effective and compliant solution for storing and accessing\nVPC Flow Logs according to the company's requirements.\nHere's a detailed breakdown:\nThe company needs frequent access for 90 days, followed by infrequent access. This immediately points to\ntiered storage as a suitable solution. Amazon S3 offers different storage classes optimized for various access\npatterns and cost considerations.\nOption A (CloudWatch): CloudWatch Logs are primarily designed for real-time monitoring and alerting, not\nlong-term storage of large volumes of data like VPC Flow Logs. While you can set an expiration, frequent\naccess and cost efficiency for 90 days would be better served by S3.\nOption B (Kinesis): Kinesis is designed for real-time data streaming and processing. Retaining logs in Kinesis\nfor 90 days would be unnecessarily expensive compared to S3, which is designed for storing large volumes of\ndata.\nOption C (CloudTrail): CloudTrail logs API calls and user activity in your AWS account. It's not designed to\ncapture network traffic data like VPC Flow Logs. Furthermore, while CloudTrail logs can be stored in S3 with\nIntelligent-Tiering, using CloudTrail solely for capturing VPC Flow Logs is not the intended purpose.\nCloudTrail focuses on auditing and governance.\nOption D (S3 with Lifecycle Policy): This option aligns perfectly with the requirements. VPC Flow Logs can be\ndirectly stored in an S3 bucket. An S3 Lifecycle policy can then be configured to automatically transition the\nlogs from the more expensive, frequently accessed S3 Standard storage class to the less expensive S3\nStandard-Infrequent Access (S3 Standard-IA) storage class after 90 days. S3 Standard-IA is optimized for\ndata that is infrequently accessed but requires rapid access when needed. This achieves cost efficiency for\ndata accessed after the initial 90-day period. This directly addresses the intermittent access requirement.\nIn conclusion, using Amazon S3 as the target with an S3 Lifecycle policy that transitions logs to S3 Standard-\nIA after 90 days is the most cost-effective and appropriate solution for storing and accessing VPC Flow Logs\nbased on the stated requirements.\nHere are some authoritative links for further research:\nVPC Flow Logs: https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html\nAmazon S3 Storage Classes: https://aws.amazon.com/s3/storage-classes/\nS3 Lifecycle Policies: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-\nexamples.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html",
      "https://aws.amazon.com/s3/storage-classes/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-"
    ]
  },
  {
    "question": "CertyIQ\nAn Amazon EC2 instance is located in a private subnet in a new VP",
    "options": {
      "C": "Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the",
      "A": "Create an internet gateway, and attach it to the VPC. Configure the private subnet route table to use the",
      "B": "Create a NAT gateway, and place it in a public subnet. Configure the private subnet route table to use the",
      "D": "Create an internet gateway, and attach it to the VPC. Create a NAT instance, and place it in the same subnet"
    },
    "answer": "B",
    "explanation": "The correct answer is B: Create a NAT gateway, and place it in a public subnet. Configure the private subnet\nroute table to use the NAT gateway as the default route.\nHere's a detailed justification:\nThe scenario requires outbound internet access for an EC2 instance residing in a private subnet without direct\ninternet connectivity. The instance needs to download security updates. A Network Address Translation (NAT)\ngateway is designed for precisely this purpose. It allows instances in private subnets to initiate outbound\nconnections to the internet (or other AWS services) while preventing the internet from initiating connections\nwith those instances.\nOption A is incorrect because configuring the private subnet route table to use an internet gateway as the\ndefault route effectively turns the private subnet into a public subnet, negating the desired security posture\nof isolating the instance. Internet Gateways allow two-way communication with the Internet.\nOption C is incorrect because while a NAT instance can also provide outbound internet access, it's a less\nscalable and more complex solution than a NAT gateway. NAT instances require manual management,\npatching, and scaling, while NAT gateways are managed by AWS and offer higher availability and\nperformance. Placing the NAT Instance in the same subnet as the EC2 instance would not solve the problem,\nas the EC2 instance would need to route traffic to the NAT instance through an entity with internet access.\nOption D is incorrect because it combines an internet gateway (which exposes the subnet to inbound traffic)\nwith a NAT instance, which is not necessary and over-complicates the solution. The route table of the private\nsubnet should point to the NAT gateway and not the internet gateway, which negates the point of the private\nsubnet.\nBy placing a NAT gateway in a public subnet (a subnet with a route to an internet gateway), the EC2 instance\nin the private subnet can route its outbound traffic to the NAT gateway. The NAT gateway then translates the\nprivate IP address of the EC2 instance to its own public IP address (provided by AWS) and forwards the traffic\nto the internet. The response traffic is then routed back through the NAT gateway to the EC2 instance.\nCritically, no inbound traffic from the internet can directly reach the EC2 instance. The route table\nconfiguration ensures that all outbound traffic from the private subnet is directed through the NAT gateway.\nThe NAT gateway itself handles the connection to the Internet Gateway.\nFor further research, refer to the AWS documentation:\nNAT Gateway\nInternet Gateways\nNAT Instances",
    "links": []
  },
  {
    "question": "CertyIQ\nA solutions architect needs to design a system to store client case files. The files are core company assets and are\nimportant. The number of files will grow over time.\nThe files must be simultaneously accessible from multiple application servers that run on Amazon EC2 instances.\nThe solution must have built-in redundancy.\nWhich solution meets these requirements?",
    "options": {
      "A": "Amazon Elastic File System (Amazon EFS)",
      "B": "Amazon Elastic Block Store (Amazon EBS)",
      "C": "Amazon S3 Glacier Deep Archive",
      "D": "AWS Backup"
    },
    "answer": "A",
    "explanation": "The correct answer is A: Amazon Elastic File System (Amazon EFS). Here's why:\nAmazon EFS is a fully managed, scalable, elastic, and shared file system designed for use with AWS Cloud\nservices and on-premises resources. Its key features directly address the requirements outlined in the\nprompt. Specifically, it provides shared file storage that can be simultaneously accessed by multiple EC2\ninstances, fulfilling the requirement of accessibility from multiple application servers.\nCrucially, EFS offers built-in redundancy by replicating data across multiple Availability Zones (AZs) within a\nregion. This ensures high availability and durability, addressing the need for a solution with built-in\nredundancy to protect important company assets. EFS scales automatically as data is added or removed,\naccommodating the anticipated growth in the number of files over time without requiring manual intervention.\nIn contrast, Amazon EBS (B) is a block storage volume designed for single EC2 instance attachment. While\nEBS volumes can be backed up, they are not inherently designed for simultaneous access from multiple\nservers. S3 Glacier Deep Archive (C) is suitable for long-term archiving and infrequently accessed data, not\nfor active file storage and simultaneous access. AWS Backup (D) is a data protection service providing\ncentralized backup management, but does not provide storage itself. It is best used to create backups of EFS.\nTherefore, EFS provides the best combination of scalability, shared access, redundancy, and ease of\nmanagement for the given scenario.\nAuthoritative links:\nAmazon EFS documentation",
    "links": []
  },
  {
    "question": "CertyIQ\nA solutions architect has created two IAM policies: Policy1 and Policy2. Both policies are attached to an IAM group.\nA cloud engineer is added as an IAM user to the IAM group. Which action will the cloud engineer be able to\nperform?",
    "options": {
      "A": "Deleting IAM users",
      "B": "Deleting directories",
      "C": "Deleting Amazon EC2 instances",
      "D": "Deleting logs from Amazon CloudWatch Logs"
    },
    "answer": "C",
    "explanation": "ec2:* Allows full control of EC2 instances, so C is correct\nThe policy only grants get and list permission on IAM users, so not A ds:Delete deny denies delete-directory,\nso not B, see https://awscli.amazonaws.com/v2/documentation/api/latest/reference/ds/index.html\nThe policy only grants get and describe permission on logs, so not D",
    "links": [
      "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/ds/index.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is reviewing a recent migration of a three-tier application to a VP",
    "options": {
      "C": "Create security group rules using the VPC CIDR blocks as the source or destination.",
      "A": "Create security group rules using the instance ID as the source or destination.",
      "B": "Create security group rules using the security group ID as the source or destination.",
      "D": "Create security group rules using the subnet CIDR blocks as the source or destination."
    },
    "answer": "B",
    "explanation": "The principle of least privilege dictates granting only the minimum necessary permissions. In the context of\nEC2 security groups, this means precisely controlling the traffic allowed between application tiers.\nOption B, creating security group rules using security group IDs as the source or destination, is the correct\napproach. This is because it allows you to specify that only members of a particular security group\n(representing a specific tier) can communicate with another security group. This is a tightly controlled and\nexplicit permission. For example, you can allow traffic only from the web tier security group to the application\ntier security group, and vice versa, applying least privilege.\nOption A, using instance IDs, is not scalable or maintainable. Instance IDs are dynamic and change when\ninstances are replaced, requiring constant updates to security group rules.\nOption C, using VPC CIDR blocks, is too broad. It opens up communication to any instance within the entire\nVPC, violating the principle of least privilege.\nOption D, using subnet CIDR blocks, is also broader than necessary. While it's more restrictive than using the\nVPC CIDR, it still allows communication between any instances within the subnets, even if they don't belong to\nthe application tiers.\nTherefore, using security group IDs directly addresses the security team's concerns by precisely defining\nallowed communication between application tiers, effectively implementing the principle of least privilege.\nThis minimizes the attack surface and simplifies security management.For further research, consult the AWS\ndocumentation on security groups:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has an ecommerce checkout workflow that writes an order to a database and calls a service to process\nthe payment. Users are experiencing timeouts during the checkout process. When users resubmit the checkout\nform, multiple unique orders are created for the same desired transaction.\nHow should a solutions architect refactor this workflow to prevent the creation of multiple orders?",
    "options": {
      "A": "Configure the web application to send an order message to Amazon Kinesis Data Firehose. Set the payment",
      "B": "Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged application path",
      "C": "Store the order in the database. Send a message that includes the order number to Amazon Simple",
      "D": "Let's break down why:"
    },
    "answer": "D",
    "explanation": "The correct solution to prevent duplicate order creation in the e-commerce checkout workflow scenario is\noption D. Let's break down why:\nThe core problem is the timeouts and subsequent retries leading to multiple orders. This indicates a need for\nan asynchronous, idempotent approach. Option D uses Amazon SQS FIFO (First-In, First-Out) queues, which\nare designed to handle messages in the exact order they were sent and ensure that a message is processed\nonly once. This is crucial for preventing duplicate order processing.\nHere's a step-by-step justification:\n1. Database Storage: Storing the order in the database first provides persistence and allows a record to\nbe created regardless of payment processing success.\n2. SQS FIFO Queue: Sending a message containing the order number to an SQS FIFO queue guarantees\nthat the payment service receives the order information in the order it was placed. The FIFO nature\nensures order is preserved.\n3. Payment Service Processing: The payment service retrieves the message from the SQS FIFO queue,\nprocesses the payment using the order number in the message.\n4. Idempotency via De-duplication: The message is deleted from the queue only after successful\nprocessing of the payment. If the payment service fails or times out before deleting the message, the\nmessage remains in the queue and will be retried, ensuring at-least-once delivery. The payment\nservice should be designed to be idempotent; even if the service receives the same request multiple\ntimes, it generates the same result, thus avoiding duplication. This can be ensured using a transaction\nID that is stored in the database and tracked.\n5. Error Handling: In case of a payment processing failure even after retries, the message could be\nmoved to a Dead Letter Queue (DLQ) for further investigation and manual intervention.\nWhy other options are incorrect:\nA (Kinesis Data Firehose): Kinesis Data Firehose is designed for streaming data to destinations like S3 or\nRedshift for analytics. It's not meant for guaranteed, ordered message delivery in a transactional workflow.\nB (CloudTrail and Lambda): CloudTrail logs events after they occur. Using it to trigger Lambda for payment\nprocessing introduces significant delay and doesn't address the issue of preventing duplicate order creation.\nAlso, CloudTrail is not meant for this kind of transactional system.\nC (SNS): Amazon SNS is a pub/sub messaging service suitable for broadcasting messages to multiple\nsubscribers. It doesn't offer the guaranteed, ordered delivery and message de-duplication needed for this\nscenario. There is also the possibility of multiple subscribers being invoked, leading to multiple payments.\nIn summary, using SQS FIFO queues, combined with idempotent payment processing, provides the required\nguarantees to handle timeouts and retries gracefully, preventing the creation of multiple orders for a single\nuser request.\nAuthoritative Links:\nAmazon SQS FIFO Queues:\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\nIdempotency: https://en.wikipedia.org/wiki/Idempotence (general concept)",
    "links": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html",
      "https://en.wikipedia.org/wiki/Idempotence"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is implementing a document review application using an Amazon S3 bucket for storage. The\nsolution must prevent accidental deletion of the documents and ensure that all versions of the documents are\navailable. Users must be able to download, modify, and upload documents.\nWhich combination of actions should be taken to meet these requirements? (Choose two.)",
    "options": {
      "A": "Enable a read-only bucket ACL: A read-only bucket ACL would prevent users from modifying or uploading",
      "B": "Enable versioning on the bucket: Versioning is critical for ensuring that all versions of the documents are",
      "C": "Attach an IAM policy to the bucket: While IAM policies are essential for controlling access to S3",
      "D": "Enable MFA Delete on the bucket: MFA (Multi-Factor Authentication) Delete provides an extra layer of"
    },
    "answer": "B",
    "explanation": "The correct answer is BD. Let's break down why each option is or isn't suitable:\nB. Enable versioning on the bucket: Versioning is critical for ensuring that all versions of the documents are\navailable. When versioning is enabled, every time an object is uploaded to the bucket, S3 stores a copy of the\nobject and assigns it a unique version ID. This allows users to retrieve previous versions of a document if\nneeded after modifications or accidental deletions.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/versioning-workflows.html\nD. Enable MFA Delete on the bucket: MFA (Multi-Factor Authentication) Delete provides an extra layer of\nsecurity against accidental or malicious deletions. When enabled, deleting an object version or suspending\nversioning requires an MFA token. This helps prevent unintended data loss, fulfilling the requirement to\nprevent accidental deletion.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html#MultiFactorAuthenticationDelete\nNow, let's look at why the other options are not suitable:\nA. Enable a read-only bucket ACL: A read-only bucket ACL would prevent users from modifying or uploading\ndocuments, which contradicts the requirement that users must be able to download, modify, and upload\ndocuments.\nC. Attach an IAM policy to the bucket: While IAM policies are essential for controlling access to S3\nresources, simply attaching a policy to the bucket doesn't inherently prevent accidental deletions or ensure\nversion availability. An IAM policy would control who can perform actions, but versioning and MFA delete\ncontrols how actions are handled.\nE. Encrypt the bucket using AWS KMS: Encryption protects the confidentiality of the data at rest. While\nencryption is a security best practice, it does not directly address the requirements of preventing accidental\ndeletion or ensuring that all versions of the documents are available. Encryption is for data protection, not\ndata recovery.\nTherefore, versioning and MFA delete are the most effective and necessary solutions to meet the stated\nrequirements of preventing accidental deletion and making all document versions accessible.",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/versioning-workflows.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html#MultiFactorAuthenticationDelete"
    ]
  },
  {
    "question": "CertyIQ\nA company is building a solution that will report Amazon EC2 Auto Scaling events across all the applications in an\nAWS account. The company needs to use a serverless solution to store the EC2 Auto Scaling status data in\nAmazon S3. The company then will use the data in Amazon S3 to provide near-real-time updates in a dashboard.\nThe solution must not affect the speed of EC2 instance launches.\nHow should the company move the data to Amazon S3 to meet these requirements?",
    "options": {
      "A": "Here's a detailed justification:",
      "B": "Launch an Amazon EMR cluster to collect the EC2 Auto Scaling status data and send the data to Amazon",
      "C": "Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Configure the Lambda",
      "D": "Use a bootstrap script during the launch of an EC2 instance to install Amazon Kinesis Agent. Configure"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's a detailed justification:\nThe requirement is to create a serverless solution to store EC2 Auto Scaling events in S3 for near-real-time\ndashboard updates without affecting EC2 instance launch speed. Let's analyze why each option is suitable or\nunsuitable:\nOption A: Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon\nKinesis Data Firehose. Store the data in Amazon S3.\nCloudWatch metric streams allow you to continuously stream CloudWatch metrics to destinations like Kinesis\nData Firehose. This provides a near-real-time flow of Auto Scaling status data.\nKinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations like\nS3. It handles buffering, batching, and compression, ensuring efficient data delivery.\nThis solution is serverless because both CloudWatch metric streams and Kinesis Data Firehose are managed\nservices that require no server provisioning.\nIt avoids impacting EC2 instance launch speed because the data capture is asynchronous and handled by\nCloudWatch.\nOption B: Launch an Amazon EMR cluster to collect the EC2 Auto Scaling status data and send the data to\nAmazon Kinesis Data Firehose. Store the data in Amazon S3.\nUsing EMR introduces unnecessary complexity and overhead. EMR is designed for big data processing and\nanalysis, which is not needed for this relatively simple data streaming task.\nEMR involves managing an entire cluster, which is not serverless.\nOption C: Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Configure\nthe Lambda function to send the EC2 Auto Scaling status data directly to Amazon S3.\nEventBridge on a schedule will not provide near-real-time updates. EventBridge is event driven and the\nLambda function would need to poll data, thus not serverless.\nPolling the Auto Scaling API from Lambda could potentially impact the API's availability and, indirectly, the\nEC2 instance launch speed.\nOption D: Use a bootstrap script during the launch of an EC2 instance to install Amazon Kinesis Agent.\nConfigure Kinesis Agent to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis\nData Firehose. Store the data in Amazon S3.\nInstalling Kinesis Agent on each EC2 instance using a bootstrap script will increase the EC2 launch time,\ncontrary to the requirements.\nManaging Kinesis Agent on each instance adds complexity, which undermines the desire for a simple,\nserverless solution.\nIn summary, option A offers the best balance of real-time data streaming, serverless architecture, and\nminimal impact on EC2 instance launches.\nHere are some authoritative links for more research:\nCloudWatch Metric Streams:\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metricstreams.html\nKinesis Data Firehose: https://aws.amazon.com/kinesis/data-firehose/\nAmazon EventBridge: https://aws.amazon.com/eventbridge/\nAWS Lambda: https://aws.amazon.com/lambda/",
    "links": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metricstreams.html",
      "https://aws.amazon.com/kinesis/data-firehose/",
      "https://aws.amazon.com/eventbridge/",
      "https://aws.amazon.com/lambda/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an application that places hundreds of .csv files into an Amazon S3 bucket every hour. The files are\n1 GB in size. Each time a file is uploaded, the company needs to convert the file to Apache Parquet format and\nplace the output file into an S3 bucket.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Create an AWS Lambda function to download the .csv files, convert the files to Parquet format, and place the",
      "B": "Create an Apache Spark job to read the .csv files, convert the files to Parquet format, and place the output",
      "C": "Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application places the .csv",
      "D": "Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and"
    },
    "answer": "D",
    "explanation": "The correct answer is D because it leverages AWS Glue, a fully managed ETL service specifically designed for\ndata transformation tasks like converting CSV to Parquet. The process is triggered automatically by S3 PUT\nevents via a Lambda function, thus requiring minimal operational overhead.\nHere's a detailed breakdown:\nAWS Glue ETL Job: Glue ETL jobs are purpose-built for data transformation tasks. They are designed to\nhandle large-scale data processing, making them efficient for converting 1 GB CSV files to Parquet. Glue\nhandles the complexities of managing the compute resources required for the transformation.\nLambda Trigger: The Lambda function triggered by each S3 PUT event provides the real-time aspect,\nensuring immediate processing of each uploaded file. Lambda is a serverless compute service, meaning you\ndon't have to manage any servers, reducing operational overhead.\nParquet Format: Parquet is a columnar storage format optimized for analytical queries, suitable for\ndownstream data analysis.\nLeast Operational Overhead: This solution minimizes operational overhead because Glue is a fully managed\nservice. You don't have to manage servers, configure networking, or perform other infrastructure-related\ntasks. Lambda is also serverless, further reducing management burden.\nLet's analyze why the other options are less optimal:\nA (Lambda only): Lambda functions have execution time and memory limits. Processing 1 GB files might\nexceed these limits. Additionally, managing the transformation logic within Lambda for such large files\nintroduces complexity and operational overhead.\nB (Spark and Lambda): Managing an Apache Spark cluster requires significant operational overhead. While\nSpark is powerful, it's overkill for this relatively simple data transformation task and increases management\ncomplexity.\nC (Glue Crawler, Athena, and Lambda): This option is unnecessarily complex. Using Athena to query CSV files\ndirectly in S3 is less efficient and requires more processing power than using a direct ETL transformation.\nRelying on scheduling instead of event-driven processing increases latency.\nIn summary, leveraging AWS Glue ETL jobs triggered by S3 PUT events through Lambda provides an efficient,\nscalable, and fully managed solution for converting CSV files to Parquet with minimal operational overhead.\nRelevant AWS documentation:\nAWS Glue: https://aws.amazon.com/glue/\nAmazon S3 Event Notifications:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html\nAWS Lambda: https://aws.amazon.com/lambda/",
    "links": [
      "https://aws.amazon.com/glue/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html",
      "https://aws.amazon.com/lambda/"
    ]
  },
  {
    "question": "CertyIQ\nA company is implementing new data retention policies for all databases that run on Amazon RDS DB instances.\nThe company must retain daily backups for a minimum period of 2 years. The backups must be consistent and\nrestorable.\nWhich solution should a solutions architect recommend to meet these requirements?",
    "options": {
      "A": "Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily",
      "B": "Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot retention policy",
      "C": "Configure database transaction logs to be automatically backed up to Amazon CloudWatch Logs with an",
      "D": "Configure an AWS Database Migration Service (AWS DMS) replication task. Deploy a replication instance,"
    },
    "answer": "A",
    "explanation": "The recommended solution is A: Create a backup vault in AWS Backup to retain RDS backups. Create a new\nbackup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB\ninstances to the backup plan.\nHere's why:\nAWS Backup is designed for centralized backup management: It provides a unified console for configuring\nand managing backups across multiple AWS services, including RDS. This simplifies compliance and\ngovernance. https://aws.amazon.com/backup/\nBackup Vaults offer enhanced security: AWS Backup vaults enable you to isolate backups, control access,\nand prevent accidental or malicious deletion of backups. This is crucial for long-term retention requirements\nand compliance.\nBackup Plans provide scheduling and retention policies: AWS Backup allows you to define backup plans with\ncustom schedules (daily, weekly, etc.) and retention periods (2 years in this case). This automates the backup\nprocess and ensures backups are retained for the required duration.\nCentralized retention management: AWS Backup simplifies the management of long-term retention by\nautomatically handling the lifecycle of backups based on the defined policies.\nRDS integration with AWS Backup: RDS is natively integrated with AWS Backup, making it easy to protect\nRDS instances using backup plans.\nLet's analyze why the other options are not as suitable:\nOption B (RDS Snapshots and DLM): While RDS snapshots are a valid backup method, managing snapshot\nretention manually using Amazon DLM can become complex at scale, especially for a large number of\ndatabases. It doesn't offer the centralized management and security benefits of AWS Backup. DLM primarily\nfocuses on EBS volumes and, while it can be used for RDS snapshots, it's not the best practice compared to\nAWS Backup for centralized RDS backup management.\nhttps://docs.aws.amazon.com/dlm/latest/userguide/what-is-dlm.html\nOption C (CloudWatch Logs): CloudWatch Logs are designed for storing application and system logs, not\ndatabase backups. While database transaction logs can be helpful for point-in-time recovery, they are not a\nsubstitute for full database backups. They don't offer a consistent, restorable state like a database snapshot.\nOption D (DMS and S3): AWS DMS is primarily used for database migration and replication, not for long-term\nbackup and retention. While CDC can capture changes, it's not a direct replacement for full backups.\nReconstructing a database from CDC data stored in S3 can be complex and time-consuming. It requires\nmanual reconstruction procedures and doesn't provide the ease of restoration offered by RDS snapshots or\nAWS Backup.\nTherefore, AWS Backup provides the most efficient, secure, and compliant solution for managing RDS\nbackups with a 2-year retention period, offering centralized management, automated scheduling, and\nenhanced security features.",
    "links": [
      "https://aws.amazon.com/backup/",
      "https://docs.aws.amazon.com/dlm/latest/userguide/what-is-dlm.html"
    ]
  },
  {
    "question": "CertyIQ\nA companys compliance team needs to move its file shares to AWS. The shares run on a Windows Server SMB file\nshare. A self-managed on-premises Active Directory controls access to the files and folders.\nThe company wants to use Amazon FSx for Windows File Server as part of the solution. The company must ensure\nthat the on-premises Active Directory groups restrict access to the FSx for Windows File Server SMB compliance\nshares, folders, and files after the move to AWS. The company has created an FSx for Windows File Server file\nsystem.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D, joining the FSx for Windows File Server file system to the existing on-premises\nActive Directory (AD).\nHere's why: The primary requirement is to maintain the existing on-premises Active Directory groups for\naccess control to files and folders hosted on FSx for Windows File Server. This means the file system needs to\nbe integrated with the on-premises AD so that existing group memberships and permissions translate to the\ncloud environment.\nFSx for Windows File Server provides native support for integrating with Active Directory. By joining the FSx\nfile system to the existing on-premises AD, the file system becomes a member of the AD domain. This enables\nusers and groups defined in the on-premises AD to be recognized and authorized by FSx. Users can then\naccess the file shares using their existing AD credentials, and permissions configured on the shares, folders,\nand files using AD groups will be enforced.\nOption A is incorrect because Active Directory Connector is typically used to connect AWS Directory Service\nto on-premises Active Directory, which is not the case here; the company is already running its own on-\npremises Active Directory. Mapping AD groups to IAM groups won't directly translate SMB permissions on\nfiles and folders on FSx.\nOption B is incorrect because tags are used for metadata and resource organization, not for directly\ncontrolling access permissions on file shares using Active Directory groups. Mapping AD groups to IAM\ngroups is not the solution for SMB level permissions.\nOption C is incorrect because IAM service-linked roles grant AWS services permission to call other AWS\nservices on your behalf. While FSx uses service-linked roles, they are not a mechanism for controlling file\nshare access using on-premises Active Directory groups. Service-linked roles will not grant or restrict the\nusage of on-premise AD users.\nIn summary, direct integration with Active Directory is essential for maintaining existing permissions and\nsecurity policies. FSx for Windows File Server's AD joining capability addresses this directly by extending the\non-premises AD environment to the cloud-based file system.\nFor more information, refer to the AWS documentation on:\nFSx for Windows File Server - Active Directory Integration:\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/self-managed-AD.html",
    "links": [
      "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/self-managed-AD.html"
    ]
  },
  {
    "question": "CertyIQ\nA company recently announced the deployment of its retail website to a global audience. The website runs on\nmultiple Amazon EC2 instances behind an Elastic Load Balancer. The instances run in an Auto Scaling group\nacross multiple Availability Zones.\nThe company wants to provide its customers with different versions of content based on the devices that the\ncustomers use to access the website.\nWhich combination of actions should a solutions architect take to meet these requirements? (Choose two.)",
    "options": {
      "A": "Configure Amazon CloudFront to cache multiple versions of the content.",
      "B": "Configure a host header in a Network Load Balancer to forward traffic to different instances. NLBs",
      "C": "Configure a [email protected] function to send specific objects to users based on the User-Agent header.",
      "D": "Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB"
    },
    "answer": "A",
    "explanation": "The correct answer is AC. Here's a detailed justification:\nA. Configure Amazon CloudFront to cache multiple versions of the content.\nCloudFront's ability to cache multiple versions of content based on request headers is crucial for serving\ndevice-specific versions of the website. This is accomplished through \"Cache Key Settings\" and \"Origin\nRequest Policies\" in CloudFront distributions. By including the User-Agent header in the cache key, CloudFront\ncan store and serve different versions of the content for different device types (e.g., desktop, mobile). This\nreduces the load on the origin servers by serving pre-cached content to users. This approach is preferred over\nalways forwarding requests to the\norigin.https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/controlling-the-cache-\nkey.html\nC. Configure a [email protected] function to send specific objects to users based on the User-Agent header.\n[email protected] functions allow you to customize content delivery at the edge based on request attributes,\nincluding the User-Agent header. By inspecting the User-Agent header within the [email protected] function,\nyou can rewrite URLs or redirect requests to specific origin servers that serve optimized content for each\ndevice type. This is a highly flexible approach that allows for fine-grained control over content delivery. [email\nprotected] intercepts requests before they hit the cache (or origin, in a cache miss), providing immediate\ncustomization before content is\nserved.https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cloudfront-functions.html\nWhy other options are incorrect:\nB. Configure a host header in a Network Load Balancer to forward traffic to different instances. NLBs\noperate at the transport layer (Layer 4) and do not inspect HTTP headers like the host header. NLBs primarily\nforward traffic based on IP addresses and ports, making them unsuitable for routing based on the User-Agent\nheader.\nD & E. Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB)...: While\nGlobal Accelerator improves application availability and performance, it is primarily used for routing traffic to\nthe nearest endpoint, not for device-specific content delivery. NLBs are still not suited for routing based on\nthe User-Agent. Path-based routing in the NLB would also be unrelated to device type. The problem requires\ninspection of the User-Agent header, which neither Global Accelerator nor NLB natively supports.",
    "links": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/controlling-the-cache-",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cloudfront-functions.html"
    ]
  },
  {
    "question": "CertyIQ\nA company plans to use Amazon ElastiCache for its multi-tier web application. A solutions architect creates a\nCache VPC for the ElastiCache cluster and an App VPC for the applications Amazon EC2 instances. Both VPCs are\nin the us-east-1 Region.\nThe solutions architect must implement a solution to provide the applications EC2 instances with access to the\nElastiCache cluster.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "A": "Create a peering connection between the VPCs. Add a route table entry for the peering connection in both",
      "B": "Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route traffic",
      "C": "Configure an inbound rule for the Transit VPCs security group to allow inbound",
      "D": "Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route traffic"
    },
    "answer": "A",
    "explanation": "The most cost-effective solution for connecting EC2 instances in one VPC (App VPC) to an ElastiCache cluster\nin another VPC (Cache VPC) within the same AWS Region (us-east-1) is to use VPC Peering.\nOption A is the correct solution because it establishes a direct network connection between the two VPCs\nusing VPC peering. This allows network traffic to flow directly between the EC2 instances and the\nElastiCache cluster without traversing the public internet or intermediate network infrastructure. The route\ntable entries in each VPC are essential to direct traffic destined for the other VPC through the peering\nconnection. The security group rule allows the EC2 instances to initiate connections to the ElastiCache\ncluster.\nOption B and D are incorrect because a Transit VPC is typically used for connecting multiple VPCs together or\nfor centralizing network services. In this simple scenario involving only two VPCs within the same region, a\nTransit VPC adds unnecessary complexity and cost. A Transit VPC involves additional EC2 instances running\nrouting software, increasing management overhead and operational costs.\nOption C is incorrect because security groups can be directly associated with network interfaces and\nresources. There is no \"peering connection's security group\" to configure. You want to permit traffic from the\napplication's security group directly.\nTherefore, VPC peering (Option A) is the most efficient and cost-effective way to enable communication\nbetween the EC2 instances and the ElastiCache cluster in this scenario.\nSupporting links:\nVPC Peering\nSecurity Groups",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is building an application that consists of several microservices. The company has decided to use\ncontainer technologies to deploy its software on AWS. The company needs a solution that minimizes the amount of\nongoing effort for maintenance and scaling. The company cannot manage additional infrastructure.\nWhich combination of actions should a solutions architect take to meet these requirements? (Choose two.)",
    "options": {
      "A": "Deploy an Amazon Elastic Container Service (Amazon ECS) cluster: ECS is a fully managed container",
      "B": "Deploy the Kubernetes control plane on Amazon EC2 instances that span multiple Availability Zones:",
      "C": "Deploy an Amazon Elastic Container Service (Amazon ECS) service with an Amazon EC2 launch type.",
      "D": "Deploy an Amazon Elastic Container Service (Amazon ECS) service with a Fargate launch type. Specify a"
    },
    "answer": "A",
    "explanation": "The correct answer is AD. Here's why:\nA. Deploy an Amazon Elastic Container Service (Amazon ECS) cluster: ECS is a fully managed container\norchestration service that allows you to run, stop, and manage containers on a cluster. By using ECS, the\ncompany can avoid managing the underlying infrastructure. ECS handles the orchestration and scheduling of\nthe containers, reducing operational overhead.\nD. Deploy an Amazon Elastic Container Service (Amazon ECS) service with a Fargate launch type. Specify a\ndesired task number level of greater than or equal to 2: Fargate is a serverless compute engine for\ncontainers. By using the Fargate launch type with ECS, the company eliminates the need to manage EC2\ninstances for their containers, further reducing the operational burden. Specifying a desired task number of 2\nor more ensures high availability and fault tolerance, as multiple instances of the service will be running. If\none container fails, another will take its place.\nWhy the other options are incorrect:\nB. Deploy the Kubernetes control plane on Amazon EC2 instances that span multiple Availability Zones:\nKubernetes (K8s) is a robust container orchestration system, but deploying the control plane on EC2\ninstances introduces significant operational overhead. The company would be responsible for managing the\nhealth, scaling, and patching of these EC2 instances, directly contradicting the requirement to minimize\nongoing effort and avoid managing additional infrastructure. Furthermore, Amazon EKS (Elastic Kubernetes\nService) is a managed service for Kubernetes.\nC. Deploy an Amazon Elastic Container Service (Amazon ECS) service with an Amazon EC2 launch type.\nSpecify a desired task number level of greater than or equal to 2: While ECS with the EC2 launch type could\nwork, it would require the company to manage the underlying EC2 instances, including patching, scaling, and\nmaintenance. This goes against the requirement to minimize maintenance and avoid infrastructure\nmanagement.\nE. Deploy Kubernetes worker nodes on Amazon EC2 instances that span multiple Availability Zones: Similar\nto option B, deploying worker nodes on EC2 instances adds an operational burden, forcing the company to\nmanage and maintain the underlying infrastructure, which conflicts with the problem's requirements. Again,\nutilizing EKS and Fargate reduces overhead.\nSupporting Documentation:\nAmazon ECS: https://aws.amazon.com/ecs/\nAWS Fargate: https://aws.amazon.com/fargate/",
    "links": [
      "https://aws.amazon.com/ecs/",
      "https://aws.amazon.com/fargate/"
    ]
  },
  {
    "question": "CertyIQ\nA company has a web application hosted over 10 Amazon EC2 instances with traffic directed by Amazon Route 53.\nThe company occasionally experiences a timeout error when attempting to browse the application. The networking\nteam finds that some DNS queries return IP addresses of unhealthy instances, resulting in the timeout error.\nWhat should a solutions architect implement to overcome these timeout errors?",
    "options": {
      "A": "Create a Route 53 simple routing policy record for each EC2 instance. Associate a health check with each",
      "B": "In conclusion, the ALB provides a robust and efficient solution for managing instance health and ensuring that",
      "C": "Create an Amazon CloudFront distribution with EC2 instances as its origin. Associate a health check with the",
      "D": "Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances. Route to the"
    },
    "answer": "D",
    "explanation": "The best solution to prevent timeout errors caused by unhealthy EC2 instances in a web application is to use\nan Application Load Balancer (ALB) with health checks in front of the instances and then route traffic to the\nALB from Route 53.\nHere's why:\nALB Health Checks: ALBs have built-in health check capabilities. They periodically send requests to the EC2\ninstances and determine their health based on the response. Only healthy instances receive traffic.\nDynamic Instance Management: If an instance fails the health check, the ALB automatically stops routing\ntraffic to it, preventing timeouts for users. When the instance recovers, the ALB resumes traffic routing.\nRoute 53 Integration: Route 53 can be configured to point to the ALB's DNS name. This ensures that all\ntraffic is directed to the ALB, which then intelligently routes traffic to healthy instances.\nScalability and Availability: ALBs offer built-in scalability and high availability. They can handle traffic spikes\nand ensure that the application remains available even if some instances fail.\nSimplicity: Compared to alternatives like creating individual Route 53 records with health checks for each\ninstance, using an ALB is a simpler and more manageable solution.\nLet's analyze why the other options are less suitable:\nA: Route 53 simple routing policy: While health checks can be associated, simple routing policy does not\ninherently remove unhealthy instances from the rotation as effectively as an ALB. It relies on DNS\npropagation, which might take time.\nB: Route 53 failover routing policy: This is intended for active/passive setups. It doesn't dynamically\ndistribute traffic based on health within a group of active servers like the given scenario.\nC: CloudFront distribution with EC2 instances as the origin: CloudFront primarily focuses on caching static\ncontent and improving website performance by distributing content closer to users. While health checks can\nbe associated with origins, CloudFront is not the best solution for dynamic traffic routing based on instance\nhealth, especially within a single region. Also, it doesn't manage routing traffic to a group of active instances\nfor high availability like an ALB.\nIn conclusion, the ALB provides a robust and efficient solution for managing instance health and ensuring that\nusers are only directed to healthy instances, preventing timeout errors.\nSupporting Links:\nApplication Load Balancers:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\nRoute 53 Health Checks: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-\ncreating-deleting.html",
    "links": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect needs to design a highly available application consisting of web, application, and database\ntiers. HTTPS content delivery should be as close to the edge as possible, with the least delivery time.\nWhich solution meets these requirements and is MOST secure?",
    "options": {
      "A": "Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in public",
      "B": "Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in private",
      "C": "Here's why:",
      "D": "Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in public"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Here's why:\nRequirements Breakdown:\nHighly Available: Achieved by using multiple EC2 instances behind an Application Load Balancer (ALB). The\nALB distributes traffic across healthy instances, ensuring service availability.\nHTTPS Content Delivery Close to the Edge: Amazon CloudFront is a Content Delivery Network (CDN) that\ncaches content in edge locations globally, reducing latency for users accessing the application. Serving\ncontent via HTTPS ensures secure communication.\nLeast Delivery Time: CloudFront's edge locations minimize the distance data travels to reach users.\nMost Secure: Placing EC2 instances in private subnets enhances security. Private subnets don't have direct\ninternet access, reducing the attack surface.\nJustification:\nOption C optimally fulfills all the requirements.\n1. Public ALB: The ALB needs to be public to receive incoming HTTPS requests from users.\n2. Redundant EC2 instances in private subnets: Placing EC2 instances in private subnets significantly\nimproves security by preventing direct access from the internet. The ALB acts as a secure\nintermediary, routing traffic to the EC2 instances.\n3. CloudFront with ALB as origin: CloudFront serves HTTPS content from edge locations. Configuring\nthe public ALB as the origin directs CloudFront to fetch content from the ALB, which then distributes\nrequests to the backend EC2 instances. This ensures HTTPS delivery closer to the users.\nWhy other options are incorrect:\nOption A & D (EC2 instances in public subnets): Exposing EC2 instances directly to the internet in public\nsubnets increases the security risk. This violates the \"most secure\" requirement.\nOption B (EC2 instances as origin): While EC2 instances can technically be origins, it is not as practical and\nrecommended as using an ALB since it is much harder to manage redundancy, scaling, and health checks.\nCloudFront is designed to work well with load balancers.\nOptions B & D (Directly using EC2 as origin without ALB): It loses the advantage of the ALB's health checks,\ntraffic distribution, and improved manageability in front of the EC2 instances. The EC2 instances would then\nneed to be directly accessible from CloudFront, further compromising security.\nSupporting Links:\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nApplication Load Balancer (ALB): https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\nAmazon VPC Subnets: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html",
    "links": [
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has a popular gaming platform running on AWS. The application is sensitive to latency because latency\ncan impact the user experience and introduce unfair advantages to some players. The application is deployed in\nevery AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups configured behind\nApplication Load Balancers (ALBs). A solutions architect needs to implement a mechanism to monitor the health of\nthe application and redirect traffic to healthy endpoints.\nWhich solution meets these requirements?",
    "options": {
      "A": "Configure an accelerator in AWS Global Accelerator. Add a listener for the port",
      "B": "Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the cache",
      "C": "Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure the cache",
      "D": "Configure an Amazon DynamoDB database to serve as the data store for the application. Create a DynamoDB"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Configure an accelerator in AWS Global Accelerator. Add a listener for the port\nthat the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the\nendpoint.\nHere's a detailed justification:\nAWS Global Accelerator is designed to improve the availability and performance of applications by directing\nuser traffic to the optimal endpoint based on factors such as location, health, and configured weights. It uses\nthe AWS global network, which is less congested and more reliable than the public internet, to route traffic.\nThis is vital for latency-sensitive applications like the gaming platform described.\nGlobal Accelerator operates by advertising static IP addresses from AWS edge locations around the world.\nWhen a user connects to the application using one of these IPs, Global Accelerator intelligently routes the\ntraffic to the nearest healthy endpoint. By attaching Application Load Balancers (ALBs) in each Region as\nendpoints, the application benefits from regional load balancing (handled by the ALBs) and global traffic\nmanagement (handled by Global Accelerator).\nGlobal Accelerator performs continuous health checks on the ALBs in each Region. If an ALB or the\nunderlying EC2 instances within a Region become unhealthy, Global Accelerator automatically redirects\ntraffic to healthy endpoints in other Regions. This provides high availability and resilience. The listener\nconfigured on the Global Accelerator is associated with the port on which the game application runs and is\nmapped to ALBs acting as endpoints.\nOptions B and C are incorrect because CloudFront is primarily designed for caching static and dynamic\ncontent. While CloudFront can improve performance, it's not the primary solution for routing traffic based on\nreal-time health checks and minimizing latency across multiple regions in the way that Global Accelerator\ncan. The caching that CloudFront provides will not be effective if the origin (ALB) is unhealthy. Origin cache\nheaders are related to caching behavior and not health checks. Option C uses S3 as the origin, which is even\nless appropriate for a dynamic, interactive gaming application.\nOption D is incorrect because DynamoDB and DAX are database-related services and are not suitable for\nrouting traffic or performing health checks on application endpoints.\nIn summary, Global Accelerator provides the optimal solution by using the AWS global network to minimize\nlatency and provide continuous health checks to route traffic to healthy endpoints in different regions, which\nis crucial for the gaming platform's performance and user experience.\nRelevant links:\nAWS Global Accelerator: https://aws.amazon.com/global-accelerator/\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
    "links": [
      "https://aws.amazon.com/global-accelerator/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/"
    ]
  },
  {
    "question": "CertyIQ\nA company has one million users that use its mobile app. The company must analyze the data usage in near-real\ntime. The company also must encrypt the data in near-real time and must store the data in a centralized location in\nApache Parquet format for further processing.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon Kinesis Data",
      "B": "Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon EMR cluster to",
      "C": "Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon",
      "D": "Here's why:"
    },
    "answer": "D",
    "explanation": "The best solution to meet the company's requirements with the least operational overhead is option D.\nHere's why:\nData Ingestion & Storage (Kinesis Data Firehose): Kinesis Data Firehose is designed to reliably load\nstreaming data into data lakes, data stores, and analytics services. It automatically scales to match the\nthroughput of your data and requires no ongoing administration. It can directly deliver data to S3 in Parquet\nformat. This addresses the requirements of storing data in S3 in Parquet format with encryption.\nhttps://aws.amazon.com/kinesis/data-firehose/\nNear-Real-Time Analysis (Kinesis Data Analytics): Kinesis Data Analytics is a serverless service that allows\nyou to process and analyze streaming data in real time using standard SQL or Java. It has less operational\noverhead than managing an EMR cluster. It aligns perfectly with the need for near-real-time data usage\nanalysis.\nhttps://aws.amazon.com/kinesis/data-analytics/\nEncryption: Kinesis Data Firehose supports encryption of data at rest in S3 using AWS KMS or server-side\nencryption with Amazon S3-managed keys (SSE-S3).\nOperational Overhead: Compared to using an EMR cluster (options B and C), Kinesis Data Analytics is a fully\nmanaged service, eliminating the need for cluster management, scaling, and patching. Using Lambda to send\ndata to either EMR or Kinesis Data Analytics (options A and B) adds unnecessary complexity. Firehose\nsimplifies the data loading process directly to S3.\nParquet Conversion: Kinesis Firehose can automatically convert the data into Parquet format before storing it\nin S3.\nIn contrast:\nOptions A, B, and C involve more manual steps and resource management. Setting up and managing an EMR\ncluster involves more operational overhead compared to Kinesis Data Analytics. Using Kinesis Data Streams\nwithout Firehose would require additional logic for data buffering, transformation, and storage in Parquet\nformat. Using Lambda to ingest data adds an extra layer of complexity.",
    "links": [
      "https://aws.amazon.com/kinesis/data-firehose/",
      "https://aws.amazon.com/kinesis/data-analytics/"
    ]
  },
  {
    "question": "CertyIQ\nA gaming company has a web application that displays scores. The application runs on Amazon EC2 instances\nbehind an Application Load Balancer. The application stores data in an Amazon RDS for MySQL database. Users\nare starting to experience long delays and interruptions that are caused by database read performance. The\ncompany wants to improve the user experience while minimizing changes to the applications architecture.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Use Amazon ElastiCache in front of the database: While ElastiCache (e.g., Memcached or Redis) can",
      "B": "Use RDS Proxy between the application and the database.",
      "C": "Migrate the application from EC2 instances to AWS Lambda: Migrating the application to Lambda is a",
      "D": "Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB: Migrating to DynamoDB is"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Use RDS Proxy between the application and the database.\nHere's a detailed justification:\nThe problem is database read performance, leading to user delays and interruptions. The requirement is to\nimprove the user experience with minimal architectural changes.\nWhy RDS Proxy is the Best Fit:\nConnection Pooling: RDS Proxy sits between the application and the RDS database and manages database\nconnections efficiently. Instead of each application instance establishing and maintaining its own connection,\nRDS Proxy maintains a pool of connections. This reduces the overhead of creating new connections, which is a\ncommon performance bottleneck. This directly addresses the read performance issue by optimizing how the\napplication interacts with the database.\nConnection Multiplexing: RDS Proxy allows multiple application requests to share a single database\nconnection, further optimizing resource usage and reducing connection-related overhead.\nReduced Database Load: By reducing the number of connections the database has to manage, RDS Proxy\nlessens the load on the RDS instance, improving its overall responsiveness and read performance.\nMinimal Architectural Changes: RDS Proxy is a relatively transparent layer. The application's code doesn't\nneed significant modification to utilize it. The application simply needs to point to the RDS Proxy endpoint\ninstead of the database endpoint. This aligns with the requirement to minimize changes to the existing\narchitecture.\nFocus on Read Performance: While RDS Proxy helps with all database operations, its connection pooling and\nmultiplexing particularly benefit read-heavy workloads, which is the core issue in this scenario.\nWhy other options are less suitable:\nA. Use Amazon ElastiCache in front of the database: While ElastiCache (e.g., Memcached or Redis) can\nimprove read performance by caching frequently accessed data, it requires significant changes to the\napplication to implement caching logic. It is an architectural change that the question is trying to avoid.\nC. Migrate the application from EC2 instances to AWS Lambda: Migrating the application to Lambda is a\nmajor architectural change. It also doesn't directly address the database read performance issue. It's a much\nmore complex undertaking than implementing RDS Proxy.\nD. Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB: Migrating to DynamoDB is\nalso a significant architectural change. DynamoDB is a NoSQL database, and requires a redesign of the data\nmodel and application code to use effectively. This is a complex solution and doesnt meet the minimization of\nchanges requirement.\nIn conclusion, RDS Proxy provides a targeted solution that optimizes database connections, reduces load on\nthe database, and requires minimal changes to the application's architecture. Therefore, it is the best choice\nto address the database read performance issues.\nAuthoritative Links:\nAWS RDS Proxy: https://aws.amazon.com/rds/proxy/\nBenefits of RDS Proxy: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy.html#rds-\nproxy.benefits",
    "links": [
      "https://aws.amazon.com/rds/proxy/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy.html#rds-"
    ]
  },
  {
    "question": "CertyIQ\nAn ecommerce company has noticed performance degradation of its Amazon RDS based web application. The\nperformance degradation is attributed to an increase in the number of read-only SQL queries triggered by\nbusiness analysts. A solutions architect needs to solve the problem with minimal changes to the existing web\napplication.\nWhat should the solutions architect recommend?",
    "options": {
      "A": "Export the data to Amazon DynamoDB and have the business analysts run their queries.",
      "B": "Load the data into Amazon ElastiCache and have the business analysts run their queries.",
      "C": "Create a read replica of the primary database and have the business analysts run their queries.",
      "D": "Copy the data into an Amazon Redshift cluster and have the business analysts run their queries."
    },
    "answer": "C",
    "explanation": "The recommended solution is to create a read replica of the primary RDS database and direct business\nanalysts' read-only queries to it. This approach addresses the performance degradation without requiring\nsignificant changes to the existing web application architecture. Read replicas allow you to offload read\ntraffic from the primary database instance, reducing the load on the primary and improving its performance\nfor transactional operations.\nOption A, exporting data to DynamoDB, is not suitable because DynamoDB is a NoSQL database, and\nmigrating SQL queries to it would require substantial code changes. Option B, using ElastiCache, is primarily\nfor caching frequently accessed data, not for running ad-hoc queries by analysts. Option D, copying data to\nRedshift, is better suited for complex analytical workloads over large datasets, which is overkill for simple\nread-only queries and introduces unnecessary complexity.\nRead replicas are designed specifically for read-heavy workloads and are easy to set up and manage within\nthe RDS ecosystem. They maintain near real-time data synchronization with the primary database, ensuring\nthe business analysts have access to up-to-date information. This solution provides minimal disruption to the\nexisting application while improving its performance and scalability by isolating the analytical workload.\nUsing read replicas is a cost-effective and efficient way to address the increased read-only query load.\nAuthoritative Links:\nAWS RDS Read Replicas:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.ReadReplicas.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.ReadReplicas.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is using a centralized AWS account to store log data in various Amazon S3 buckets. A solutions\narchitect needs to ensure that the data is encrypted at rest before the data is uploaded to the S3 buckets. The\ndata also must be encrypted in transit.\nWhich solution meets these requirements?",
    "options": {
      "A": "Use client-side encryption to encrypt the data that is being uploaded to the S3 buckets.",
      "B": "Use server-side encryption to encrypt the data that is being uploaded to the S3 buckets.",
      "C": "Create bucket policies that require the use of server-side encryption with S3 managed encryption keys (SSE-",
      "D": "Enable the security option to encrypt the S3 buckets through the use of a default AWS Key Management"
    },
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A is the most suitable answer:\nThe question requires encryption at rest before data is uploaded to S3 and encryption in transit. Client-side\nencryption addresses both requirements directly. By encrypting the data on the client-side (i.e., before it\nleaves the company's environment) before uploading it to S3, you ensure that the data is already encrypted at\nrest. This addresses the requirement of encryption at rest prior to upload. Then, to meet the encryption-in-\ntransit requirement, ensure that HTTPS (TLS) is used during the upload process. This encrypts the data as it\ntravels across the network to S3.\nOptions B, C, and D involve server-side encryption. While server-side encryption (SSE) methods address\nencryption at rest within S3, they don't fulfill the requirement of encrypting the data before it's uploaded. In\nthose scenarios, the data transits to S3 unencrypted and then gets encrypted by S3 itself. Furthermore,\nsimply creating bucket policies (option C) or enabling a KMS key (option D) does not guarantee data is\nencrypted before it's uploaded.\nClient-side encryption gives the company more control over the encryption process, particularly the\nmanagement of encryption keys. The company can manage its keys, rotate them, and maintain ownership of\nthe encryption process from end to end.\nIn summary, using client-side encryption with HTTPS meets both stated requirements: encrypting the data at\nrest before upload and encrypting the data in transit. This provides a strong security posture and full control\nover the encryption process.\nRelevant links:\nAmazon S3 Data Protection: https://docs.aws.amazon.com/AmazonS3/latest/userguide/data-protection.html\nProtecting Data Using Server-Side Encryption:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html\nProtecting Data Using Client-Side Encryption:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/data-protection.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect observes that a nightly batch processing job is automatically scaled up for 1 hour before the\ndesired Amazon EC2 capacity is reached. The peak capacity is the same every night and the batch jobs always\nstart at 1 AM. The solutions architect needs to find a cost-effective solution that will allow for the desired EC2\ncapacity to be reached quickly and allow the Auto Scaling group to scale down after the batch jobs are complete.\nWhat should the solutions architect do to meet these requirements?",
    "options": {
      "A": "Increase the minimum capacity for the Auto Scaling group.",
      "B": "Increase the maximum capacity for the Auto Scaling group.",
      "C": "Configure scheduled scaling to scale up to the desired compute level.",
      "D": "Change the scaling policy to add more EC2 instances during each scaling operation."
    },
    "answer": "C",
    "explanation": "The correct answer is C, configuring scheduled scaling to scale up to the desired compute level. Here's why:\nThe core problem is that the Auto Scaling group takes too long to reach the required capacity for the nightly\nbatch job, resulting in an unnecessary scaling-up period before the peak is reached. Since the peak capacity\nand the start time are consistent every night, a predictable scaling pattern exists.\nScheduled scaling allows you to predefine scaling actions that occur at specific times. By configuring a\nscheduled action to increase the capacity of the Auto Scaling group to the required compute level at 1 AM,\nthe architect can ensure that the desired capacity is available immediately when the batch jobs start. This\neliminates the gradual scaling process and the unnecessary 1-hour scale-up period. After the batch job\nfinishes, another scheduled action can scale the group down, optimizing cost.\nOption A (increasing the minimum capacity) is not cost-effective. It will keep more instances running\ncontinuously, even when they are not needed, leading to unnecessary expenses.\nOption B (increasing the maximum capacity) only allows the Auto Scaling group to scale up higher if needed,\nbut it does not guarantee a faster initial scaling to the desired level. The gradual scaling issue would persist.\nOption D (changing the scaling policy) would affect the responsiveness of scaling events based on real-time\nmetrics (CPU utilization, etc.), which are not directly related to the predictable nightly demand. Furthermore, it\ndoesn't address the specific timing requirement. It might make the scaling more aggressive but doesn't\nguarantee the capacity will be available exactly at 1 AM.\nTherefore, scheduled scaling provides the most cost-effective and precise solution to address the consistent\nand time-bound scaling needs of the nightly batch job. It leverages the predictability of the workload to\nensure resources are available exactly when needed and are released when no longer required.\nFurther research:\nAWS Auto Scaling Scheduled Actions:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\nAWS Auto Scaling Concepts:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html",
    "links": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html"
    ]
  },
  {
    "question": "CertyIQ\nA company serves a dynamic website from a fleet of Amazon EC2 instances behind an Application Load Balancer\n(ALB). The website needs to support multiple languages to serve customers around the world. The websites\narchitecture is running in the us-west-1 Region and is exhibiting high request latency for users that are located in\nother parts of the world.\nThe website needs to serve requests quickly and efficiently regardless of a users location. However, the company\ndoes not want to recreate the existing architecture across multiple Regions.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Replace the existing architecture with a website that is served from an Amazon S3 bucket. Configure an",
      "B": "This",
      "C": "Create an Amazon API Gateway API that is integrated with the ALB. Configure the API to use the HTTP",
      "D": "Launch an EC2 instance in each additional Region and configure NGINX to act as a cache server for that"
    },
    "answer": "B",
    "explanation": "The correct solution is to configure an Amazon CloudFront distribution with the Application Load Balancer\n(ALB) as the origin and set the cache behavior settings to cache based on the Accept-Language request header\n(Option B).\nHere's why:\nCloudFront for Global Content Delivery: CloudFront is AWS's Content Delivery Network (CDN). It caches\ncontent at edge locations around the world, reducing latency for users regardless of their geographic\nlocation. This directly addresses the requirement of serving requests quickly and efficiently worldwide\nwithout recreating the architecture in multiple regions. https://aws.amazon.com/cloudfront/\nALB as Origin: The existing website architecture utilizes an ALB. Configuring CloudFront with the ALB as the\norigin allows CloudFront to cache the dynamic content generated by the EC2 instances behind the ALB. This\navoids the need for significant architectural changes.\nAccept-Language Header Caching: The Accept-Language header in an HTTP request specifies the user's\npreferred language. By configuring CloudFront to cache based on this header, the CDN can store different\nversions of the website's content for each language. When a user requests the website, CloudFront will serve\nthe cached version corresponding to their preferred language, ensuring a localized experience.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/content-based-config.html\nLet's analyze why the other options are less suitable:\nOption A (S3 and CloudFront): Replacing the existing dynamic website architecture with a static website\nserved from S3 is not practical, as the website is defined as dynamic. S3 is ideal for static content but not for\ndynamic content requiring server-side processing.\nOption C (API Gateway and ALB): API Gateway is generally used for managing APIs, not for caching dynamic\nwebsite content. While it can be configured for caching, CloudFront is a more efficient and purpose-built\nsolution for content delivery and caching for web applications. Furthermore, introducing API Gateway adds\nunnecessary complexity.\nOption D (EC2 cache servers and Route 53): Launching EC2 instances in each region for caching is\neffectively recreating the architecture in multiple regions, which the company wants to avoid. While Route 53\ngeolocation routing would direct users to the closest cache server, this approach is more complex, costly, and\ndifficult to manage than using a CDN like CloudFront.\nIn summary, Option B leverages CloudFront's global CDN capabilities and the Accept-Language header to\ndeliver localized content efficiently without requiring significant architectural changes or regional replication.",
    "links": [
      "https://aws.amazon.com/cloudfront/",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/content-based-config.html"
    ]
  },
  {
    "question": "CertyIQ\nA rapidly growing ecommerce company is running its workloads in a single AWS Region. A solutions architect must\ncreate a disaster recovery (DR) strategy that includes a different AWS Region. The company wants its database to\nbe up to date in the DR Region with the least possible latency. The remaining infrastructure in the DR Region needs\nto run at reduced capacity and must be able to scale up if necessary.\nWhich solution will meet these requirements with the LOWEST recovery time objective (RTO)?",
    "options": {
      "A": "Use an Amazon Aurora global database with a pilot light deployment.",
      "B": "In summary, Aurora Global Database provides continuous replication with minimal latency across regions, and",
      "C": "Use an Amazon RDS Multi-AZ DB instance with a pilot light deployment.",
      "D": "Use an Amazon RDS Multi-AZ DB instance with a warm standby deployment."
    },
    "answer": "B",
    "explanation": "The question requires a DR strategy with the lowest RTO for a rapidly growing e-commerce company. The\ndatabase must be up-to-date with minimal latency in the DR region, and other infrastructure should run at\nreduced capacity, scaling up as needed.\nOption B, \"Use an Amazon Aurora global database with a warm standby deployment,\" offers the best solution.\nAurora Global Database replicates data with low latency across AWS Regions, ensuring the DR database is\nnear real-time synchronized, minimizing data loss and RTO. The warm standby approach involves having the\nDR infrastructure running at a reduced capacity, ready to scale up quickly.\nOption A, using Aurora Global Database with a pilot light approach, would require more time to bring up all\nresources, increasing the RTO compared to a warm standby. While Aurora Global Database ensures low\nlatency replication, the pilot light method necessitates provisioning and configuring resources during a\ndisaster, increasing downtime.\nOptions C and D, using Amazon RDS Multi-AZ DB instances, do not offer cross-region replication like Aurora\nGlobal Database. RDS Multi-AZ provides high availability within a single region but does not provide a quick\nDR solution to a separate region with minimal latency. Therefore, they would have significantly higher RTOs\nthan option B.\nIn summary, Aurora Global Database provides continuous replication with minimal latency across regions, and\na warm standby configuration minimizes the time required to recover in the DR region, achieving the lowest\nRTO compared to other options.\nAmazon Aurora Global Databases: https://aws.amazon.com/rds/aurora/global-database/AWS Disaster\nRecovery Options: https://d1.awsstatic.com/whitepapers/aws-disaster-recovery.pdf",
    "links": [
      "https://aws.amazon.com/rds/aurora/global-database/AWS",
      "https://d1.awsstatic.com/whitepapers/aws-disaster-recovery.pdf"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an application on Amazon EC2 instances. The company needs to implement a disaster recovery\n(DR) solution for the application. The DR solution needs to have a recovery time objective (RTO) of less than 4\nhours. The DR solution also needs to use the fewest possible AWS resources during normal operations.\nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "options": {
      "A": "Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS",
      "B": "Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS",
      "C": "Launch EC2 instances in a secondary AWS Region. Keep the EC2 instances in the secondary Region active at",
      "D": "Launch EC2 instances in a secondary Availability Zone. Keep the EC2 instances in the secondary Availability"
    },
    "answer": "B",
    "explanation": "Option B is the most operationally efficient DR solution that meets the requirements. Here's why:\nAMIs for Backup: Creating AMIs provides a consistent and point-in-time backup of the EC2 instances,\nincluding the OS, applications, and data. This facilitates a quicker recovery process compared to rebuilding\nservers from scratch.\nCross-Region Replication: Copying AMIs to a secondary AWS Region addresses the disaster recovery aspect\nby ensuring that the necessary server images are available in a separate geographical location in case the\nprimary region becomes unavailable.\nInfrastructure as Code (IaC) with CloudFormation: AWS CloudFormation allows defining and managing\ninfrastructure as code. This enables automating the deployment of EC2 instances and other necessary AWS\nresources in the secondary region during a disaster recovery event. The automated nature helps achieve the\n4-hour RTO target.\nCost Efficiency: This approach minimizes resource utilization during normal operations. The infrastructure in\nthe secondary region remains inactive until a disaster event occurs. Only storage costs for the replicated AMIs\nand CloudFormation templates incur during normal operation.\nOperational Efficiency: CloudFormation provides version control, rollback capabilities, and dependency\nmanagement, making it significantly more operationally efficient than using custom scripts and Lambda\nfunctions.\nOption A is less operationally efficient because Lambda and custom scripts require more management and\nare less robust than using CloudFormation's infrastructure-as-code approach.\nOption C involves launching and maintaining EC2 instances in the secondary region at all times, which is the\nmost costly and inefficient approach, especially when the disaster event is not happening. It goes against the\nrequirement of using the fewest possible AWS resources during normal operations.\nOption D only uses a secondary Availability Zone within the same region. This does not fully address disaster\nrecovery since an entire region outage would affect both availability zones.\nSupporting Links:\nAWS CloudFormation: https://aws.amazon.com/cloudformation/\nAmazon Machine Images (AMIs): https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\nDisaster Recovery with AWS: https://aws.amazon.com/disaster-recovery/",
    "links": [
      "https://aws.amazon.com/cloudformation/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html",
      "https://aws.amazon.com/disaster-recovery/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an internal browser-based application. The application runs on Amazon EC2 instances behind an\nApplication Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability\nZones. The Auto Scaling group scales up to 20 instances during work hours, but scales down to 2 instances\novernight. Staff are complaining that the application is very slow when the day begins, although it runs well by\nmid-morning.\nHow should the scaling be changed to address the staff complaints and keep costs to a minimum?",
    "options": {
      "A": "Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens.",
      "B": "Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown period.",
      "C": "Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period.",
      "D": "Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before the office"
    },
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the best answer:\nThe problem is application slowdown during the morning rush due to EC2 Auto Scaling needing time to spin\nup instances and handle the increased load. The goal is to improve initial responsiveness while minimizing\ncosts.\nWhy C is the best approach (Target Tracking Scaling with adjusted thresholds and cooldown):\nTarget tracking scaling: This is ideal for maintaining consistent performance. By targeting a specific metric\nlike CPU utilization, it dynamically adjusts capacity to meet demand, ensuring the application remains\nresponsive even during peak hours.\nLower CPU threshold: Triggering scaling at a lower CPU threshold ensures that new instances are launched\nbefore the existing instances become overloaded, thus preventing the initial slowdown. This is proactive, not\nreactive.\nDecreased cooldown period: The cooldown period is the amount of time after a scaling activity completes\nbefore another scaling activity can start. Reducing it allows the Auto Scaling group to react more quickly to\nfluctuating demand, enabling a faster response to the morning load spike. This ensures resources are added\npromptly when needed.\nCost-effectiveness: Target tracking only adds capacity when needed, unlike scheduled actions that might\nover-provision resources even if demand doesn't fully materialize.\nWhy other options are less suitable:\nA (Scheduled action setting desired capacity to 20): This approach doesn't account for actual load. If the\nload isn't high enough to require 20 instances, resources are wasted. It's inflexible to unexpected variations in\ndemand.\nB (Step Scaling with adjusted thresholds and cooldown): Step scaling is based on pre-defined steps. This\nrequires more effort to configure and fine-tune than target tracking, and it's less adaptive to dynamic load\nchanges. Although it uses a threshold and reduced cooldown, it is not as dynamically responsive as target\ntracking which continuously adjusts based on the tracked metric.\nD (Scheduled action setting minimum and maximum capacity to 20): This is the most expensive option\nbecause it maintains 20 instances regardless of actual load. This eliminates cost savings during off-peak\nhours when only a few instances are needed. It's also unnecessarily restrictive, preventing the Auto Scaling\ngroup from exceeding 20 instances if needed in unforeseen circumstances.\nIn summary: Target tracking with a lower CPU threshold and decreased cooldown provides a balanced\napproach, proactively scaling up resources to maintain performance, reacting quickly to demand changes,\nand avoiding unnecessary costs during off-peak hours.\nAuthoritative Links for Further Research:\nAWS Auto Scaling: https://aws.amazon.com/autoscaling/\nTarget Tracking Scaling: https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-\ntracking.html\nCooldown Period: https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-\ntermination.html#instance-cooldown",
    "links": [
      "https://aws.amazon.com/autoscaling/",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-"
    ]
  },
  {
    "question": "CertyIQ\nA company has a multi-tier application deployed on several Amazon EC2 instances in an Auto Scaling group. An\nAmazon RDS for Oracle instance is the application s data layer that uses Oracle-specific PL/SQL functions. Traffic\nto the application has been steadily increasing. This is causing the EC2 instances to become overloaded and the\nRDS instance to run out of storage. The Auto Scaling group does not have any scaling metrics and defines the\nminimum healthy instance count only. The company predicts that traffic will continue to increase at a steady but\nunpredictable rate before leveling off.\nWhat should a solutions architect do to ensure the system can automatically scale for the increased traffic?\n(Choose two.)",
    "options": {
      "A": "Configure storage Auto Scaling on the RDS for Oracle instance: RDS Storage Auto Scaling allows the",
      "B": "Migrate the database to Amazon Aurora to use Auto Scaling storage: While Aurora does offer auto-",
      "C": "Configure an alarm on the RDS for Oracle instance for low free storage space: While configuring an alarm",
      "D": "Configure the Auto Scaling group to use the average CPU as the scaling metric: Using average CPU"
    },
    "answer": "A",
    "explanation": "The correct answer is AD. Here's why:\nA. Configure storage Auto Scaling on the RDS for Oracle instance: RDS Storage Auto Scaling allows the\ndatabase to automatically scale its storage capacity in response to growing data needs. This directly\naddresses the problem of the RDS instance running out of storage due to increasing traffic and data. Since\nthe company predicts traffic will increase, proactive storage scaling is essential. This is a crucial consideration\nfor relational databases where running out of storage can lead to application downtime and data loss.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html#USER_PIOPS.Autoscaling\nD. Configure the Auto Scaling group to use the average CPU as the scaling metric: Using average CPU\nutilization as a scaling metric for the Auto Scaling group allows new EC2 instances to be launched\nautomatically when the existing instances are under heavy load. This addresses the problem of EC2 instances\nbecoming overloaded due to increasing traffic. Monitoring CPU utilization provides a clear indication of\nresource consumption and allows the application to scale horizontally based on demand. This ensures the\napplication remains responsive and available as traffic increases.\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-capacity.html\nWhy other options are incorrect:\nB. Migrate the database to Amazon Aurora to use Auto Scaling storage: While Aurora does offer auto-\nscaling storage, migrating a database, especially one using Oracle-specific PL/SQL, is a significant\nundertaking. It involves compatibility assessments, code changes, and data migration, which introduces risk\nand complexity. Given the existing infrastructure's reliance on Oracle-specific functionality, migrating to\nAurora might not be the most efficient or immediate solution for the current scaling issue. It's a major\narchitectural change better suited for a long-term strategic decision rather than a quick fix to the overload\nproblem.\nC. Configure an alarm on the RDS for Oracle instance for low free storage space: While configuring an alarm\nis a good practice for monitoring, it only provides notification when the storage is running low. It doesn't\nautomatically scale the storage. It requires manual intervention to increase storage, which is not ideal for\nautomatically scaling for unpredictable traffic.\nE. Configure the Auto Scaling group to use the average free memory as the scaling metric: While monitoring\nmemory usage is important, CPU utilization is generally a better metric for scaling web application tiers. Low\nfree memory can indicate an issue, but high CPU utilization directly reflects the workload demand on the\ninstances. Therefore, CPU is often the more responsive metric for scaling the application tier in this scenario.",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html#USER_PIOPS.Autoscaling",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-capacity.html"
    ]
  },
  {
    "question": "CertyIQ\nA company provides an online service for posting video content and transcoding it for use by any mobile platform.\nThe application architecture uses Amazon Elastic File System (Amazon EFS) Standard to collect and store the\nvideos so that multiple Amazon EC2 Linux instances can access the video content for processing. As the popularity\nof the service has grown over time, the storage costs have become too expensive.\nWhich storage solution is MOST cost-effective?",
    "options": {
      "A": "Use AWS Storage Gateway for files to store and process the video content.",
      "B": "Use AWS Storage Gateway for volumes to store and process the video content.",
      "C": "Use Amazon EFS for storing the video content. Once processing is complete, transfer the files to Amazon",
      "D": "Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block"
    },
    "answer": "D",
    "explanation": "The most cost-effective solution for storing and processing video content in this scenario is option D: using\nAmazon S3 for storage and temporarily moving files to an EBS volume attached to the processing server.\nHere's why:\nCost of EFS Standard: EFS Standard is designed for active file system workloads requiring frequent access.\nAs the service's popularity increases, the storage costs with EFS Standard become prohibitively expensive\nbecause it charges based on the amount of storage used, regardless of access frequency.\nS3 for Cost-Effectiveness: Amazon S3 offers various storage classes optimized for different access patterns.\nIn this case, the infrequent access nature of the video content once processed makes S3 a more cost-\neffective primary storage solution. S3's storage classes like S3 Standard-IA (Infrequent Access) or S3 Glacier\n(for archival) offer significantly lower storage costs than EFS Standard.\nEBS for Processing: Amazon EBS volumes provide block storage optimized for EC2 instances. Temporarily\nmoving the video files to an EBS volume attached to the processing server allows for efficient processing due\nto its low latency and high throughput. EBS volumes offer consistent performance for video transcoding\ntasks.\nWorkflow Optimization: The workflow would involve retrieving the video file from S3 to the EBS volume,\nperforming the transcoding on the EC2 instance, and then potentially deleting the file from EBS (if no longer\nneeded) after transcoding.\nAWS Storage Gateway limitations: AWS Storage Gateway is primarily used for hybrid cloud scenarios to\nintegrate on-premises storage with AWS. While it can connect to S3, using it as the primary storage solution\nis not ideal in this completely cloud-based architecture. Both file and volume gateways have limitations that\nmake them less suitable than a direct S3 to EC2 to EBS workflow.\nOption C is suboptimal: Storing everything in EFS and then moving to EBS after processing doesn't address\nthe core cost issue. EFS would still be the primary, expensive storage location.\nTherefore, leveraging S3 for cost-effective archival storage and EBS for temporary, high-performance\nprocessing offers the optimal balance of cost and performance.\nRelevant links:\nAmazon S3 Storage Classes: https://aws.amazon.com/s3/storage-classes/\nAmazon EBS: https://aws.amazon.com/ebs/\nAmazon EFS: https://aws.amazon.com/efs/\nAWS Storage Gateway: https://aws.amazon.com/storagegateway/",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://aws.amazon.com/ebs/",
      "https://aws.amazon.com/efs/",
      "https://aws.amazon.com/storagegateway/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to create an application to store employee data in a hierarchical structured relationship. The\ncompany needs a minimum-latency response to high-traffic queries for the employee data and must protect any\nsensitive data. The company also needs to receive monthly email messages if any financial information is present\nin the employee data.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "options": {
      "A": "Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3 every month.",
      "B": "Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every",
      "C": "Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly",
      "D": "Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon QuickSight"
    },
    "answer": "B",
    "explanation": "The correct answer is BE. Let's break down why:\nB: Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every\nmonth. DynamoDB is a NoSQL database that can store hierarchical data using techniques like adjacency lists\nor materialized paths. It provides extremely low-latency responses for high-traffic queries, meeting the\nprimary requirement. While DynamoDB isn't inherently hierarchical, the data can be structured hierarchically\nwithin the NoSQL schema. Exporting the data to S3 monthly allows for the subsequent analysis and\nmonitoring.\nE: Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send\nmonthly notifications through an Amazon Simple Notification Service (Amazon SNS) subscription. Amazon\nMacie is designed to discover and protect sensitive data stored in S3. By configuring it for the AWS account\nand integrating it with EventBridge, you can trigger an SNS notification when Macie identifies financial\ninformation. This meets the requirement of receiving monthly email notifications about the presence of\nsensitive data. Macie scanning in S3 (where the exported DynamoDB data resides) can trigger SNS\nnotifications via EventBridge, aligning with the monthly requirement.\nWhy other options are incorrect:\nA: Redshift is a data warehouse optimized for analytical queries, not low-latency transactional reads. Storing\nhierarchical data and querying it for real-time access is not its strength.\nC: While configuring Macie is useful, directly sending events to Lambda is not a notification mechanism for\nmonthly email messages to humans without additional steps, like implementing an email service within\nLambda. Lambda also has execution time limits that might cause it to fail for large datasets. Additionally,\nintegrating macie directly with event bridge would create notifications as soon as the data is stored in s3.\nD: Athena is suitable for analyzing data in S3, but it's not needed for the initial storage and retrieval of\nemployee data. QuickSight is a visualization tool, not a direct solution for detecting financial\ninformation.Athena + QuickSight do not provide any mechanism to alert when sensitive financial information\nappears in the employee data. This requires some automated scanning and notification service like Macie and\nSNS.\nAuthoritative Links:\nAmazon DynamoDB: https://aws.amazon.com/dynamodb/\nAmazon Macie: https://aws.amazon.com/macie/\nAmazon EventBridge: https://aws.amazon.com/eventbridge/\nAmazon SNS: https://aws.amazon.com/sns/",
    "links": [
      "https://aws.amazon.com/dynamodb/",
      "https://aws.amazon.com/macie/",
      "https://aws.amazon.com/eventbridge/",
      "https://aws.amazon.com/sns/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an application that is backed by an Amazon DynamoDB table. The companys compliance\nrequirements specify that database backups must be taken every month, must be available for 6 months, and must\nbe retained for 7 years.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Here's why:",
      "B": "It allows you to define backup plans with schedules and retention policies.",
      "C": "Use the AWS SDK to develop a script that creates an on-demand backup of the DynamoDB table. Set up an",
      "D": "Use the AWS CLI to create an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's why:\nAWS Backup is the ideal service for centrally managing and automating backups across AWS services,\nincluding DynamoDB. It allows you to define backup plans with schedules and retention policies.\nA directly addresses all requirements:\nMonthly Backups: An AWS Backup plan can be configured to run on the first day of each month.\nAvailability for 6 Months: The lifecycle policy allows for transitioning backups to cold storage after 6 months,\nensuring backups are still available for restore but at a lower storage cost.\nRetention for 7 Years: The retention period can be set to 7 years, satisfying the long-term retention\nrequirement.\nWhy the other options are less suitable:\nB: While you can create DynamoDB on-demand backups, transitioning them directly to S3 Glacier Flexible\nRetrieval is not the most efficient way, and the AWS Backup service has better options. Backup should be\nmaintained in the AWS Backup vault for better integrity.\nC and D: While automating on-demand backups via the AWS SDK or CLI and EventBridge is possible, it\nrequires more manual scripting and management compared to the AWS Backup service. Furthermore, the\ncommand/scripts do not transition backups to cold storage and delete old backups, which is less ideal than\nusing the AWS Backup. The second scripts will also incur the cost of development, running, and maintaining.\nUsing AWS Backup centralizes the backup process and ensures that compliance requirements are met in an\nautomated and auditable manner. It simplifies lifecycle management and retention, reducing the operational\noverhead.\nSupporting Documentation:\nAWS Backup: https://aws.amazon.com/backup/\nDynamoDB Backup and Restore:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html\nAWS Backup Plans: https://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html",
    "links": [
      "https://aws.amazon.com/backup/",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html",
      "https://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is using Amazon CloudFront with its website. The company has enabled logging on the CloudFront\ndistribution, and logs are saved in one of the companys Amazon S3 buckets. The company needs to perform\nadvanced analyses on the logs and build visualizations.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the",
      "B": "Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the",
      "C": "Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize",
      "D": "Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize"
    },
    "answer": "B",
    "explanation": "The correct solution is to use Amazon Athena for analyzing CloudFront logs in S3 and Amazon QuickSight for\nvisualization.\nHere's why:\nAmazon Athena: Athena is a serverless, interactive query service that makes it easy to analyze data in\nAmazon S3 using standard SQL. CloudFront logs are stored as files in S3, making Athena a natural fit for\nquerying and analyzing them directly. You can define a schema on the log files and then use SQL to extract\ninsights, filter data, and perform aggregations. https://aws.amazon.com/athena/\nAmazon QuickSight: QuickSight is a fast, cloud-powered business intelligence (BI) service that makes it easy\nto build visualizations, perform ad-hoc analysis, and get business insights from your data. It integrates\nseamlessly with Athena and can directly visualize the results of Athena queries.\nhttps://aws.amazon.com/quicksight/\nWhy other options are incorrect:\nDynamoDB: DynamoDB is a NoSQL database designed for high-performance key-value and document\nstorage. It's not well-suited for directly querying log files stored in S3. CloudFront logs are typically stored in\na flat file format. Loading log data into DynamoDB just for analysis would be an unnecessary complexity.\nAWS Glue: While Glue is a useful ETL (extract, transform, load) service, it is not the ideal choice for visualizing\ndata. Glue excels at preparing data for analysis, but Athena and QuickSight provide a more direct path for\nquerying and visualizing data stored in S3. Glue's primary purpose is data cataloging, transformation, and job\norchestration, rather than data visualization.\nCombining Athena and Glue vs. Athena and Quicksight: Athena handles the query execution on the log files\nin S3 and QuickSight specializes in visualization. This combination offers a more seamless experience for the\nstated objective. Using Glue to prepare for visualizations adds unnecessary complexity.",
    "links": [
      "https://aws.amazon.com/athena/",
      "https://aws.amazon.com/quicksight/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a fleet of web servers using an Amazon RDS for PostgreSQL DB instance. After a routine\ncompliance check, the company sets a standard that requires a recovery point objective (RPO) of less than 1\nsecond for all its production databases.\nWhich solution meets these requirements?",
    "options": {
      "A": "Enable a Multi-AZ deployment for the DB instance.",
      "B": "Enable auto scaling for the DB instance in one Availability Zone.",
      "C": "Configure the DB instance in one Availability Zone, and create multiple read replicas in a separate",
      "D": "Configure the DB instance in one Availability Zone, and configure AWS Database Migration Service (AWS"
    },
    "answer": "A",
    "explanation": "The requirement is to achieve an RPO of less than 1 second for an Amazon RDS for PostgreSQL database. This\nnecessitates a solution that minimizes data loss in the event of a failure.\nOption A, enabling a Multi-AZ deployment for the RDS instance, provides synchronous replication to a\nstandby instance in a different Availability Zone. In case of a failure in the primary instance, RDS\nautomatically fails over to the standby instance. Because the replication is synchronous, the standby has near\nreal-time data, minimizing data loss to only what was in-flight at the time of the failure. This fulfills the <1\nsecond RPO.\nOption B, enabling auto-scaling, addresses performance and scalability, not data recovery and RPO.\nOption C, using read replicas, provides asynchronous replication. While read replicas can improve read\nperformance, they do not guarantee minimal data loss. In the event of a primary instance failure, data may be\nlost that hasn't yet been replicated to the read replicas. This violates the <1 second RPO.\nOption D, using AWS DMS CDC, also employs asynchronous replication. While DMS can capture changes in\nnear real-time, it involves more overhead and complexity than Multi-AZ for simple failover scenarios.\nAdditionally, the lag between the primary database and the DMS target (another database) can easily exceed\n1 second, failing the RPO requirement. It also introduces more points of failure.\nTherefore, Multi-AZ offers the simplest and most effective way to meet the <1 second RPO requirement\nthrough synchronous replication and automatic failover.\nAuthoritative Links:\nAmazon RDS Multi-AZ deployments:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a web application that is deployed on Amazon EC2 instances in the private subnet of a VP",
    "options": {
      "C": "An",
      "A": "Configure a route in a route table to direct traffic from the internet to the private IP addresses of the EC2",
      "B": "It also",
      "D": "Configure the security group for the ALB to allow any TCP traffic on any port."
    },
    "answer": "B",
    "explanation": "The correct answer is B. Configure the security group for the EC2 instances to only allow traffic that comes\nfrom the security group for the ALB.\nHere's a detailed justification:\nThe requirement is to restrict inbound traffic to the EC2 instances in the private subnet only from the ALB.\nThis means blocking all other sources, both internal and external. Security Groups act as virtual firewalls at\nthe instance level.\nOption B directly addresses this requirement. By configuring the EC2 instances' security group to only allow\ninbound traffic from the ALB's security group, you ensure that only the ALB can reach the instances. This\nleverages Security Group source rules. If the ALB is compromised, or if an attacker manages to launch\nresources inside the same VPC, they still won't be able to directly reach the EC2 instances unless they are\nassociated with the ALB's Security Group. This prevents unauthorized access from other sources within or\noutside the private subnet.\nOption A is incorrect because routing traffic from the internet to the private IP addresses of EC2 instances is\nnot possible without a NAT gateway or similar setup, which would defeat the requirement of blocking external\naccess. Directly routing internet traffic to private IPs is a security risk and not the intended function.\nOption C contradicts the requirement. Moving EC2 instances into a public subnet and assigning Elastic IPs\nwould expose them directly to the internet, which violates the restriction of preventing access from any other\nsource outside the ALB. Elastic IPs are typically used for resources needing direct internet accessibility,\nwhich we want to avoid here.\nOption D is incorrect because allowing any TCP traffic on any port for the ALB's security group would not\nrestrict access to the EC2 instances. It will widen the attack surface on ALB, leading to security\nvulnerabilities. It doesn't address the core requirement of isolating the EC2 instances to only the ALB. It also\npotentially allows other sources within the VPC to communicate with the ALB unnecessarily.\nIn essence, Security Groups are the ideal tool for implementing fine-grained access control at the instance\nlevel in AWS, and referencing other Security Groups in your Security Group rules is a security best practice.\nFurther reading:\nAWS Security Groups\nControlling Traffic to Resources Using Security Groups",
    "links": []
  },
  {
    "question": "CertyIQ\nA research company runs experiments that are powered by a simulation application and a visualization application.\nThe simulation application runs on Linux and outputs intermediate data to an NFS share every 5 minutes. The\nvisualization application is a Windows desktop application that displays the simulation output and requires an SMB\nfile system.\nThe company maintains two synchronized file systems. This strategy is causing data duplication and inefficient\nresource usage. The company needs to migrate the applications to AWS without making code changes to either\napplication.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Migrate both applications to AWS Lambda. Create an Amazon S3 bucket to exchange data between the",
      "B": "C (EC2 & SQS): SQS is a message queuing service, not a file system. It cannot directly replace the NFS and",
      "C": "Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to",
      "D": "Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to"
    },
    "answer": "D",
    "explanation": "The correct solution is D because it addresses the core requirements of providing both NFS and SMB file\nsharing capabilities without requiring code changes to the simulation and visualization applications.\nHere's a detailed justification:\nRequirement 1: Linux NFS Share: The simulation application requires an NFS share. Amazon FSx for NetApp\nONTAP supports NFS file shares, allowing the Linux EC2 instances running the simulation application to write\ndata to it. https://aws.amazon.com/fsx/netapp-ontap/features/\nRequirement 2: Windows SMB File System: The visualization application requires an SMB file system.\nAmazon FSx for NetApp ONTAP supports SMB file shares, enabling the Windows EC2 instances running the\nvisualization application to read data from it. https://aws.amazon.com/fsx/netapp-ontap/features/\nRequirement 3: No Code Changes: By using FSx for NetApp ONTAP, the applications can continue to use\ntheir existing file access patterns (NFS and SMB) without requiring any code modifications.\nRequirement 4: Eliminate Data Duplication: Using a single FSx for NetApp ONTAP file system eliminates the\nneed for two synchronized file systems, thus reducing data duplication and improving resource efficiency.\nLet's examine why the other options are less suitable:\nA (Lambda & S3): While S3 can store data, it doesn't natively support NFS or SMB file sharing protocols. This\nwould require significant code changes to both applications to interact with S3's object storage API instead of\nfile systems.\nB (ECS & FSx File Gateway): FSx File Gateway caches data locally and uploads it to S3. The visualization\napplication cannot access the data from the Linux NFS share directly via SMB.\nC (EC2 & SQS): SQS is a message queuing service, not a file system. It cannot directly replace the NFS and\nSMB file sharing requirements of the applications. It would necessitate significant code changes to transfer\nsimulation data as messages and reassemble them for visualization.\nIn summary, solution D using Amazon FSx for NetApp ONTAP provides the most seamless migration path with\nminimal disruption and no code changes, while meeting all the specified requirements for NFS and SMB file\nsharing.",
    "links": [
      "https://aws.amazon.com/fsx/netapp-ontap/features/",
      "https://aws.amazon.com/fsx/netapp-ontap/features/"
    ]
  },
  {
    "question": "CertyIQ\nAs part of budget planning, management wants a report of AWS billed items listed by user. The data will be used\nto create department budgets. A solutions architect needs to determine the most efficient way to obtain this\nreport information.\nWhich solution meets these requirements?",
    "options": {
      "A": "Run a query with Amazon Athena to generate the report.",
      "B": "Create a report in Cost Explorer and download the report. Here's why:",
      "C": "Access the bill details from the billing dashboard and download the bill.",
      "D": "Modify a cost budget in AWS Budgets to alert with Amazon Simple Email Service (Amazon SES)."
    },
    "answer": "B",
    "explanation": "The correct answer is B. Create a report in Cost Explorer and download the report. Here's why:\nCost Explorer is designed specifically for analyzing AWS costs and usage. It allows you to generate detailed\nreports, filter them by various dimensions (including users through tags), and download the data. This aligns\ndirectly with the requirement of obtaining a report of AWS billed items listed by user for budget planning.\nCost Explorer provides a user-friendly interface and built-in functionality for this purpose.\nOption A, using Amazon Athena, would require setting up a complex data pipeline involving AWS Cost and\nUsage Reports (CUR), configuring S3 buckets, and writing SQL queries. While powerful, it's an unnecessarily\ncomplex solution for a simple reporting requirement. It would be overkill to use Athena.\nOption C, accessing the bill details from the billing dashboard and downloading the bill, would provide a\ndetailed bill, but it wouldn't readily offer the granularity needed to easily generate a report organized by user.\nExtracting user-specific data from the raw bill would involve significant manual processing or custom\nscripting, which isn't efficient.\nOption D, modifying a cost budget in AWS Budgets to alert with Amazon SES, focuses on setting up alerts\nwhen costs exceed a certain threshold. It does not generate a report of billed items listed by user, which is the\ncore requirement. Budgets help with notifications, not reporting on past usage patterns.\nCost Explorer offers the easiest and most efficient way to generate the required report directly within the\nAWS console and download the results. Using Cost Explorer is the most optimized method.\nhttps://aws.amazon.com/aws-cost-management/aws-cost-explorer/",
    "links": [
      "https://aws.amazon.com/aws-cost-management/aws-cost-explorer/"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts its static website by using Amazon S3. The company wants to add a contact form to its webpage.\nThe contact form will have dynamic server-side components for users to input their name, email address, phone\nnumber, and user message. The company anticipates that there will be fewer than 100 site visits each month.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "A": "Host a dynamic contact form page in Amazon Elastic Container Service (Amazon ECS). Set up Amazon",
      "B": "Create an Amazon API Gateway endpoint with an AWS Lambda",
      "C": "Convert the static webpage to dynamic by deploying Amazon Lightsail. Use client-side scripting to build the",
      "D": "Create a t2.micro Amazon EC2 instance. Deploy a LAMP (Linux, Apache, MySQL, PHP/Perl/Python) stack to"
    },
    "answer": "B",
    "explanation": "The most cost-effective solution is B. Create an Amazon API Gateway endpoint with an AWS Lambda\nbackend that makes a call to Amazon Simple Email Service (Amazon SES).\nHere's why:\nCost Efficiency: For fewer than 100 site visits per month, the \"serverless\" approach of API Gateway and\nLambda offers the lowest cost. Lambda functions are billed only for the compute time they consume, and API\nGateway has a free tier for low volumes. SES is also very cost-effective for sending emails.\nScalability and Management: API Gateway and Lambda automatically scale to handle traffic fluctuations\nwithout requiring manual configuration. This removes the operational overhead of managing servers.\nServerless Architecture: This solution aligns with serverless architecture, which minimizes infrastructure\nmanagement and allows the company to focus on the contact form's functionality.\nAlternatives Analysis:\nA (ECS): Hosting a dynamic page in ECS involves container orchestration and persistent compute resources,\nmaking it overkill and more expensive for the expected low traffic.\nC (Lightsail): While Lightsail is simpler than EC2, it still involves a fixed monthly cost, even when the contact\nform isn't being used. Using client-side scripting for the form's logic is not ideal for security or data\nprocessing on the server-side. WorkMail is not well-suited for this use-case.\nD (EC2 with LAMP): Running a t2.micro EC2 instance incurs a constant hourly cost, even if the contact form is\nrarely used. LAMP stack administration also adds complexity.\nIn contrast, the API Gateway/Lambda/SES approach only charges when the form is submitted, making it the\nmost cost-effective and operationally efficient solution for this low-traffic scenario. This approach avoids the\ncost of maintaining a dedicated server or virtual machine.\nAuthoritative Links:\nAWS Lambda Pricing: https://aws.amazon.com/lambda/pricing/\nAmazon API Gateway Pricing: https://aws.amazon.com/api-gateway/pricing/\nAmazon SES Pricing: https://aws.amazon.com/ses/pricing/",
    "links": [
      "https://aws.amazon.com/lambda/pricing/",
      "https://aws.amazon.com/api-gateway/pricing/",
      "https://aws.amazon.com/ses/pricing/"
    ]
  },
  {
    "question": "CertyIQ\nA company has a static website that is hosted on Amazon CloudFront in front of Amazon S3. The static website\nuses a database backend. The company notices that the website does not reflect updates that have been made in\nthe websites Git repository. The company checks the continuous integration and continuous delivery (CI/CD)\npipeline between the Git repository and Amazon S3. The company verifies that the webhooks are configured\nproperly and that the CI/CD pipeline is sending messages that indicate successful deployments.\nA solutions architect needs to implement a solution that displays the updates on the website.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Add an Application Load Balancer.",
      "B": "Add Amazon ElastiCache for Redis or Memcached to the database layer of the web application.",
      "C": "Invalidate the CloudFront cache.",
      "D": "Use AWS Certificate Manager (ACM) to validate the websites SSL certificate."
    },
    "answer": "C",
    "explanation": "The correct solution is to invalidate the CloudFront cache. Here's why:\nThe problem states that the website updates are not reflected, despite successful deployments to S3. This\nstrongly suggests that CloudFront is serving cached versions of the website files. CloudFront caches content\nat edge locations to improve latency and performance. While it eventually checks for updates from the origin\n(S3 in this case), the Time To Live (TTL) setting determines how long content is cached before being\nrevalidated.\nInvalidating the cache forces CloudFront to retrieve the latest version of the website files from the origin (S3).\nThis ensures that users see the most up-to-date content after the CI/CD pipeline has successfully deployed\nthe updates. There are several methods to invalidate the cache, including using the AWS Management\nConsole, AWS CLI, or CloudFront API. This is a common practice in CI/CD pipelines for static websites hosted\non S3 and fronted by CloudFront.\nOption A (Application Load Balancer) is incorrect because it is primarily used for distributing traffic to\napplication servers, which is not the core issue here. The problem involves static content caching, not\napplication load balancing.\nOption B (Amazon ElastiCache) is not relevant because the problem pertains to static website content served\nthrough CloudFront, not the database layer. ElastiCache is used for caching database query results or other\ndynamic data, not static assets.\nOption D (AWS Certificate Manager) is unrelated to the caching issue. ACM is used for managing SSL/TLS\ncertificates for secure communication. While having a valid certificate is crucial, it doesn't address the\nproblem of outdated content being served from the CloudFront cache.\nTherefore, invalidating the CloudFront cache is the most direct and effective solution to ensure that website\nupdates are immediately reflected to users.\nRelevant Documentation:\nInvalidating Files: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html\nHow CloudFront Delivers Content:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to migrate a Windows-based application from on premises to the AWS Cloud. The application\nhas three tiers: an application tier, a business tier, and a database tier with Microsoft SQL Server. The company\nwants to use specific features of SQL Server such as native backups and Data Quality Services. The company also\nneeds to share files for processing between the tiers.\nHow should a solutions architect design the architecture to meet these requirements?",
    "options": {
      "A": "Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing between the",
      "B": "Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file sharing",
      "C": "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon",
      "D": "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the best solution and why the other options are less suitable:\nWhy Option B is Correct: Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File\nServer for file sharing between the tiers.\nComplete Control: Hosting all three tiers on EC2 instances grants the company complete control over the\nenvironment, including the SQL Server database. This is vital for utilizing specific features of SQL Server like\nnative backups and Data Quality Services, which may not be fully supported or accessible with a managed\nservice like Amazon RDS.\nSQL Server Feature Compatibility: RDS for SQL Server offers a managed SQL Server instance but can have\nlimitations on certain administrative features. Native backups are fully supported with EC2-hosted SQL server\ninstances. https://aws.amazon.com/rds/sqlserver/\nWindows Environment: The application is Windows-based. FSx for Windows File Server is a fully managed,\nhighly reliable, and scalable file storage solution built on Windows Server. It's natively compatible, offering\nfeatures like SMB protocol support and integration with Active Directory, which are likely required for the\napplication. https://aws.amazon.com/fsx/windows/\nFile Sharing Requirements: FSx for Windows File Server allows seamless file sharing between the\napplication, business, and database tiers due to its Windows-native nature.\nWhy Other Options are Incorrect:\nOption A: Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing\nbetween the tiers. FSx File Gateway is for on-premises access to FSx file shares. It does not make sense for\napplications fully migrated to AWS.\nOption C: Host the application tier and the business tier on Amazon EC2 instances. Host the database tier\non Amazon RDS. Use Amazon Elastic File System (Amazon EFS) for file sharing between the tiers. EFS is a\nLinux-based file system and is not well-suited for sharing files in a native Windows environment. Additionally,\nhosting the database on RDS might restrict access to SQL Server's native features.\nOption D: Host the application tier and the business tier on Amazon EC2 instances. Host the database tier\non Amazon RDS. Use a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume for\nfile sharing between the tiers. EBS volumes are block storage and not designed for file sharing between\nmultiple instances. While EBS can be attached to an EC2 instance, creating a shared file system requires\nadditional configuration (e.g., setting up NFS or SMB), adding complexity. Also, as with Option C, using RDS\nlimits SQL Server feature access.\nIn summary, Option B provides the best balance of control, compatibility, and ease of use for migrating a\nWindows-based application to AWS while meeting the specific requirements for SQL Server features and file\nsharing.",
    "links": [
      "https://aws.amazon.com/rds/sqlserver/",
      "https://aws.amazon.com/fsx/windows/"
    ]
  },
  {
    "question": "CertyIQ\nA company is migrating a Linux-based web server group to AWS. The web servers must access files in a shared file\nstore for some content. The company must not make any changes to the application.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Create an Amazon S3 Standard bucket with access to the web servers. While S3 is excellent for object",
      "B": "Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin. CloudFront is a",
      "C": "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS",
      "D": "Configure a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. Mount the"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS\nfile system on all web servers.\nHere's a detailed justification:\nThe problem states that a Linux-based web server group needs to access files in a shared file store without\nrequiring application changes. This immediately suggests the need for a network file system that the existing\napplications can use seamlessly.\nAmazon EFS (Elastic File System) is a fully managed, scalable, elastic, cloud-native NFS (Network File\nSystem) for Linux-based workloads. It can be mounted on multiple EC2 instances simultaneously, providing a\nshared file system that behaves just like a local file system from the application's perspective. This fulfills the\nrequirement of not needing application changes. The web servers can access and modify files in the EFS file\nsystem as if they were locally stored. EFS offers high availability and durability, making it a suitable choice for\nproduction workloads.\nNow, let's analyze why the other options are incorrect:\nA. Create an Amazon S3 Standard bucket with access to the web servers. While S3 is excellent for object\nstorage, it's not a file system. Applications would need to be rewritten to use S3's APIs to interact with the\nstored objects. This violates the \"no application changes\" constraint. S3 is also object storage and not\ndesigned to act as a directly mounted file system for Linux web servers.\nB. Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin. CloudFront is a\ncontent delivery network (CDN) designed for caching and distributing static content. While it uses S3 as an\norigin, it doesn't provide a shared file system for the web servers to write to or directly access in a file system\nmanner. It's primarily for delivering static content to end-users, not for shared storage amongst web servers.\nThis also requires application changes.\nD. Configure a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. Mount the\nEBS volume to all web servers. EBS volumes can only be attached to a single EC2 instance at a time, unless\nusing EBS Multi-Attach, which is only supported for specific instances and use cases (and typically requires\nclustering knowledge). Mounting a single EBS volume to multiple web servers would be impossible without\ncomplex configuration and potentially data corruption, defeating the purpose of a shared file system. It would\nalso introduce a single point of failure.\nIn summary, EFS directly addresses the problem of providing a shared file system accessible by multiple\nLinux web servers without requiring any changes to the application code. EBS is single-instance attached\n(without more specialized configurations), and S3/CloudFront involve object storage, requiring application-\nlevel changes.\nAuthoritative Links:\nAmazon EFS: https://aws.amazon.com/efs/\nAmazon EBS: https://aws.amazon.com/ebs/\nAmazon S3: https://aws.amazon.com/s3/\nAmazon CloudFront: https://aws.amazon.com/cloudfront/",
    "links": [
      "https://aws.amazon.com/efs/",
      "https://aws.amazon.com/ebs/",
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/cloudfront/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an AWS Lambda function that needs read access to an Amazon S3 bucket that is located in the\nsame AWS account.\nWhich solution will meet these requirements in the MOST secure manner?",
    "options": {
      "A": "Apply an S3 bucket policy that grants read access to the S3 bucket.",
      "B": "Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to the S3",
      "C": "Embed an access key and a secret key in the Lambda functions code to grant the required IAM permissions",
      "D": "Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to all S3"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the most secure solution for granting an AWS Lambda\nfunction read access to an Amazon S3 bucket in the same account:\nOption B leverages IAM roles, which are the recommended way to grant permissions to AWS services like\nLambda. An IAM role is an identity that can be assumed by an AWS service, allowing it to make API requests\nto other AWS services. By assigning an IAM role to the Lambda function, you provide it with temporary\nsecurity credentials without embedding long-term secrets (like access keys) within the code.\nThe IAM policy attached to the role specifically grants s3:GetObject (or s3:ListBucket if needed for listing\nobjects) permissions to the particular S3 bucket. This adheres to the principle of least privilege, granting only\nthe necessary access to the function. This means that the Lambda function can only perform read operations\non that specific bucket and nothing else.\nOption A, using an S3 bucket policy, could technically grant access, but it's less flexible and harder to\nmanage, especially if you need to grant access to multiple Lambda functions or other services. Also, it makes\nthe bucket dependent on the function's needs, which violates separation of concerns.\nOption C, embedding access keys in the code, is a major security risk. If the code is compromised (e.g.,\nthrough a vulnerability or accidental exposure), the access keys could be used to compromise the entire AWS\naccount. Hardcoding credentials is a well-documented anti-pattern.\nOption D is less secure than option B because it grants read access to all S3 buckets in the account. This\nviolates the principle of least privilege. The Lambda function only needs access to a specific S3 bucket. Giving\nit broader access increases the potential blast radius if the function is compromised.\nIn summary, using an IAM role with a narrowly scoped IAM policy is the most secure and recommended\napproach for granting permissions to AWS Lambda functions. It avoids hardcoding credentials, adheres to the\nprinciple of least privilege, and simplifies permission management.\nRelevant Links:\nIAM Roles: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\nIAM Policies: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\nLambda Execution Role: https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html\nPrinciple of Least Privilege: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-\nleast-privilege",
    "links": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts a web application on multiple Amazon EC2 instances. The EC2 instances are in an Auto Scaling\ngroup that scales in response to user demand. The company wants to optimize cost savings without making a long-\nterm commitment.\nWhich EC2 instance purchasing option should a solutions architect recommend to meet these requirements?",
    "options": {
      "A": "Dedicated Instances only: Dedicated Instances are isolated at the hardware level, which incurs a higher",
      "B": "On-Demand Instances only: On-Demand Instances provide the flexibility to pay only for what you use and",
      "C": "A mix of On-Demand Instances and Spot Instances: This option offers the best balance between cost",
      "D": "A mix of On-Demand Instances and Reserved Instances: Reserved Instances offer significant cost savings"
    },
    "answer": "C",
    "explanation": "The correct answer is C. A mix of On-Demand Instances and Spot Instances.\nHere's a detailed justification:\nThe company needs a cost-optimized solution for EC2 instances that scale with user demand without a long-\nterm commitment. Let's analyze why each option is or isn't suitable:\nA. Dedicated Instances only: Dedicated Instances are isolated at the hardware level, which incurs a higher\ncost and doesn't directly contribute to cost optimization for fluctuating workloads. They're more suitable for\ncompliance or security requirements.\nB. On-Demand Instances only: On-Demand Instances provide the flexibility to pay only for what you use and\nare suitable for short-term, spiky, or unpredictable workloads where interruptions are not acceptable. While\nflexible, they are more expensive than other options like Spot Instances.\nC. A mix of On-Demand Instances and Spot Instances: This option offers the best balance between cost\nsavings and availability. On-Demand Instances can handle the baseline capacity and critical workload\ncomponents that cannot be interrupted. Spot Instances allow the company to bid on spare EC2 capacity,\noffering significant cost savings (up to 90% compared to On-Demand). The Auto Scaling group can be\nconfigured to launch Spot Instances when available and fall back to On-Demand Instances if Spot Instances\nare terminated due to price fluctuations. This dynamic approach maximizes cost savings while maintaining\navailability and scalability.\nD. A mix of On-Demand Instances and Reserved Instances: Reserved Instances offer significant cost savings\ncompared to On-Demand, but they require a commitment of 1 or 3 years. Since the company wants to avoid\nlong-term commitments, Reserved Instances are not the ideal choice for the entire scaling infrastructure.\nWhile some core baseline resources could be reserved, the auto-scaling portion is better served with spot and\non-demand.\nTherefore, the combination of On-Demand Instances (for guaranteed capacity) and Spot Instances (for\nopportunistic cost savings) allows the company to optimize costs without a long-term commitment, while\nleveraging the Auto Scaling group's ability to dynamically adapt to user demand.\nAuthoritative Links for Further Research:\nAWS EC2 Purchasing Options: https://aws.amazon.com/ec2/pricing/\nAWS Spot Instances: https://aws.amazon.com/ec2/spot/\nAWS Auto Scaling: https://aws.amazon.com/autoscaling/",
    "links": [
      "https://aws.amazon.com/ec2/pricing/",
      "https://aws.amazon.com/ec2/spot/",
      "https://aws.amazon.com/autoscaling/"
    ]
  },
  {
    "question": "CertyIQ\nA media company uses Amazon CloudFront for its publicly available streaming video content. The company wants\nto secure the video content that is hosted in Amazon S3 by controlling who has access. Some of the companys\nusers are using a custom HTTP client that does not support cookies. Some of the companys users are unable to\nchange the hardcoded URLs that they are using for access.\nWhich services or methods will meet these requirements with the LEAST impact to the users? (Choose two.)",
    "options": {
      "A": "Signed Cookies: Incorrect. Signed cookies are used to control access to multiple restricted files. In this",
      "B": "Signed URLs: Correct. Signed URLs allow time-limited access to individual S3 objects (through",
      "C": "AWS AppSync: Incorrect. AWS AppSync is a managed GraphQL service that can be used to build APIs. It's",
      "D": "JSON Web Token (JWT): Incorrect. JWTs are for authentication and authorization, but they require client-"
    },
    "answer": "A",
    "explanation": "The requirement is to secure streaming video content in S3 delivered via CloudFront, controlling access\nwithout relying on cookies (due to limitations of some HTTP clients) and while accommodating users with\nhardcoded URLs.\nA. Signed Cookies: Incorrect. Signed cookies are used to control access to multiple restricted files. In this\ncase, a user is using a custom HTTP client that does not support cookies. Therefore, signed cookies can't meet\nthe requirement.\nB. Signed URLs: Correct. Signed URLs allow time-limited access to individual S3 objects (through\nCloudFront). Since some users have hardcoded URLs, creating signed URLs and distributing them to\nauthorized users before embedding them in applications allows access to restricted content. Each URL\ncontains an expiration timestamp. Therefore, signed URLs allow users access without the use of cookies.\nC. AWS AppSync: Incorrect. AWS AppSync is a managed GraphQL service that can be used to build APIs. It's\nnot directly related to securing streaming video content in S3 through CloudFront using signed URLs and\ndoes not help with users with existing hardcoded URLs.\nD. JSON Web Token (JWT): Incorrect. JWTs are for authentication and authorization, but they require client-\nside handling to send the token in headers. Clients with hardcoded URLs and no cookie support can't easily\nincorporate JWTs, as they likely cannot modify request headers.\nE. AWS Secrets Manager: Incorrect. AWS Secrets Manager is used to store, retrieve, and manage secrets.\nThis is irrelevant to controlling access to content via CloudFront.\nTherefore, the best combination of services and methods is using signed URLs. Signed URLs are the perfect\nsolution, as they are compatible with clients that do not support cookies and provide a secure way to access\nthe videos. Signed URLs can be used together with hardcoded URLs.\nAuthoritative Links:\nCloudFront Signed URLs: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-\ncontent-signed-urls.html\nUsing signed URLs to serve private content: https://aws.amazon.com/premiumsupport/knowledge-\ncenter/cloudfront-signed-urls-troubleshooting/",
    "links": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-",
      "https://aws.amazon.com/premiumsupport/knowledge-"
    ]
  },
  {
    "question": "CertyIQ\nA company is preparing a new data platform that will ingest real-time streaming data from multiple sources. The\ncompany needs to transform the data before writing the data to Amazon S3. The company needs the ability to use\nSQL to query the transformed data.\nWhich solutions will meet these requirements? (Choose two.)",
    "options": {
      "A": "Use Amazon Kinesis Data Streams to stream the data. Use Amazon Kinesis Data Analytics to transform the",
      "B": "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use AWS Glue to",
      "C": "Use AWS Database Migration Service (AWS DMS) to ingest the data. Use Amazon EMR to transform the data",
      "D": "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use Amazon Kinesis"
    },
    "answer": "A",
    "explanation": "Let's analyze each option to determine why A and B are the correct solutions.\nOption A: This solution correctly leverages the Kinesis family of services for real-time data ingestion and\ntransformation. Kinesis Data Streams effectively handles the streaming data. Kinesis Data Analytics allows\nfor real-time data transformation using SQL. Kinesis Data Firehose delivers the transformed data to S3.\nAthena then enables SQL queries directly on the data residing in S3. This approach aligns perfectly with the\nrequirements of real-time processing, transformation, and SQL querying.\nhttps://aws.amazon.com/kinesis/data-analytics/ , https://aws.amazon.com/athena/\nOption B: This option also fulfills the stated needs. Amazon MSK, a managed Kafka service, can ingest high-\nvolume streaming data. AWS Glue provides a serverless ETL (Extract, Transform, Load) service that can\ntransform the data and write it to S3. Glue's ability to connect to various data stores makes it suitable for this\ntask. Athena can then be utilized to query the data within the S3 data lake. The combination of MSK, Glue, and\nAthena is a common pattern for building data lakes and performing analytics. https://aws.amazon.com/msk/,\nhttps://aws.amazon.com/glue/\nWhy other options are incorrect:\nOption C: AWS DMS is primarily designed for database migrations, not real-time streaming data ingestion.\nWhile EMR can transform data, it's generally an overkill for simple transformations that Kinesis Data Analytics\nor Glue can handle more efficiently.\nOption D: While MSK and Kinesis Data Analytics are valid components, the Amazon RDS query editor is\ndesigned for querying relational databases (RDS), not data in S3. Athena is the appropriate tool for querying\ndata in S3 using SQL.\nOption E: Similar to Option D, while Kinesis Data Streams, Glue, and Kinesis Data Firehose are suitable\ncomponents, the Amazon RDS query editor is not the right tool to query data in S3. Athena should be used\ninstead.\nIn summary, options A and B utilize appropriate AWS services to ingest real-time streaming data, transform it\nefficiently, store it in S3, and then allow for SQL queries on the transformed data through Athena. Options C,\nD, and E include inappropriate services, such as DMS or the RDS query editor, for the specific requirements of\nthe data platform.",
    "links": [
      "https://aws.amazon.com/kinesis/data-analytics/",
      "https://aws.amazon.com/athena/",
      "https://aws.amazon.com/msk/,",
      "https://aws.amazon.com/glue/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an on-premises volume backup solution that has reached its end of life. The company wants to use\nAWS as part of a new backup solution and wants to maintain local access to all the data while it is backed up on\nAWS. The company wants to ensure that the data backed up on AWS is automatically and securely transferred.\nWhich solution meets these requirements?",
    "options": {
      "A": "Use AWS Snowball to migrate data out of the on-premises solution to Amazon S3. Configure on-premises",
      "B": "Use AWS Snowball Edge to migrate data out of the on-premises solution to Amazon S3. Use the Snowball",
      "C": "Use AWS Storage Gateway and configure a cached volume gateway. Run the Storage Gateway software",
      "D": "Use AWS Storage Gateway and configure a stored volume gateway. Run the"
    },
    "answer": "D",
    "explanation": "The correct solution is D. Use AWS Storage Gateway and configure a stored volume gateway. Run the\nStorage Gateway software appliance on premises and map the gateway storage volumes to on-premises\nstorage. Mount the gateway storage volumes to provide local access to the data.\nHere's a detailed justification:\nThe scenario requires maintaining local access to data while also backing it up to AWS automatically and\nsecurely. AWS Storage Gateway is designed precisely for this hybrid cloud use case, bridging the gap\nbetween on-premises environments and AWS storage services.\nA stored volume gateway is the most suitable configuration because it stores the entire dataset locally on\npremises and asynchronously backs it up to AWS. This directly satisfies the requirement of maintaining local\naccess to all the data. Changes made locally are continuously and asynchronously replicated to AWS for\nbackup and disaster recovery purposes, ensuring data durability and security.\nThe Storage Gateway software appliance is deployed on-premises, connecting to your existing storage\ninfrastructure. You then create gateway storage volumes that map to your on-premises storage. These\nvolumes can be mounted to your applications, providing seamless access to the data. The gateway handles\nthe secure transfer of data to AWS.\nOption A is incorrect because AWS Snowball is primarily a data migration tool, not a continuous backup\nsolution with local access after migration. Mounting the Snowball S3 endpoint directly is not a supported or\npractical configuration for local access.\nOption B is also incorrect because Snowball Edge, while having some compute capabilities, is still primarily a\ndata migration and edge computing device. Its file interface is designed for specific edge workloads, not for\nserving as a primary local storage solution backed up to AWS.\nOption C, using a cached volume gateway, is not ideal because it only caches a percentage of the data locally.\nThis means not all data is readily available on-premises, violating the requirement for local access to all data.\nIt prioritizes frequently accessed data for local caching, but the less frequently accessed data resides\nprimarily in AWS.\nIn summary, the stored volume gateway configuration of AWS Storage Gateway provides a robust and secure\nhybrid cloud solution that maintains local data access while seamlessly backing up data to AWS.\nHere are some authoritative links for further research:\nAWS Storage Gateway: https://aws.amazon.com/storagegateway/\nStored Volumes: https://docs.aws.amazon.com/storagegateway/latest/userguide/stored-volumes.html",
    "links": [
      "https://aws.amazon.com/storagegateway/",
      "https://docs.aws.amazon.com/storagegateway/latest/userguide/stored-volumes.html"
    ]
  },
  {
    "question": "CertyIQ\nAn application that is hosted on Amazon EC2 instances needs to access an Amazon S3 bucket. Traffic must not\ntraverse the internet.\nHow should a solutions architect configure access to meet these requirements?",
    "options": {
      "A": "Create a private hosted zone by using Amazon Route 53.",
      "B": "Set up a gateway VPC endpoint for Amazon S3 in the VPC.",
      "C": "Connecting to S3 via VPN is neither possible nor the correct approach.",
      "D": "Establish an AWS Site-to-Site VPN connection between the VPC and the S3 bucket."
    },
    "answer": "B",
    "explanation": "The correct answer is B, setting up a gateway VPC endpoint for Amazon S3 in the VPC. Here's why:\nA gateway VPC endpoint allows resources within your VPC, such as EC2 instances, to privately access\nAmazon S3 without requiring internet access or using public IPs. This ensures that all traffic remains within\nthe AWS network. A gateway endpoint is a virtual device that is horizontally scaled, redundant, and highly\navailable. It allows communication between your VPC and S3 using AWS's internal network.\nOption A, creating a private hosted zone in Route 53, is used for internal DNS resolution within your VPC.\nWhile important for naming and discovery, it doesn't establish a private connection to S3; EC2 instances\nwould still need a way to reach S3's public endpoints, potentially via the internet.\nOption C, using a NAT gateway, allows EC2 instances without public IPs to initiate outbound internet traffic.\nThis is the opposite of the requirement, as it forces traffic to traverse the internet.\nOption D, establishing an AWS Site-to-Site VPN connection to the S3 bucket is not applicable. S3 is a service\nand doesn't reside in a VPC to connect to via VPN. A Site-to-Site VPN is used to connect your on-premises\nnetwork to a VPC. Connecting to S3 via VPN is neither possible nor the correct approach.\nVPC endpoints are the recommended and most efficient way to provide private connectivity to AWS services\nlike S3. They offer improved security, reduced latency, and lower costs by eliminating internet traffic.\nFurther research:\nAWS VPC Endpoints Documentation: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\nAWS Gateway Endpoints: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html"
    ]
  },
  {
    "question": "CertyIQ\nAn ecommerce company stores terabytes of customer data in the AWS Cloud. The data contains personally\nidentifiable information (PII). The company wants to use the data in three applications. Only one of the applications\nneeds to process the PII. The PII must be removed before the other two applications process the data.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "DynamoDB with a Proxy Application Layer: While feasible, a proxy application layer adds operational",
      "B": "Store the data in an Amazon S3 bucket. Process and transform",
      "C": "Separate S3 Buckets: Storing three copies of the data (one original, two transformed) increases storage",
      "D": "Separate DynamoDB Tables: Similar to option C, this approach multiplies storage costs and management"
    },
    "answer": "B",
    "explanation": "The most operationally efficient solution is B. Store the data in an Amazon S3 bucket. Process and transform\nthe data by using S3 Object Lambda before returning the data to the requesting application.\nHere's why:\nS3 Object Lambda: S3 Object Lambda allows you to add your own code to Amazon S3 GET requests to\nmodify and process data as it is being retrieved from S3. This means data transformation happens on-the-fly,\nwithout the need to store multiple transformed copies of the data. https://aws.amazon.com/s3/object-lambda/\nCentralized Data Storage: This solution centralizes the customer data in a single S3 bucket, simplifying data\ngovernance and management.\nReduced Storage Costs: Avoiding the creation of multiple datasets minimizes storage costs compared to\noptions C and D.\nLeast Operational Overhead: S3 Object Lambda simplifies data transformation compared to managing\nseparate data transformation pipelines and storage for each application. The code within the S3 Object\nLambda can identify the requesting application and apply the appropriate transformation.\nData Security: By removing PII using S3 Object Lambda before it reaches the two applications that do not\nneed it, the overall security posture is improved.\nLet's analyze why the other options are less optimal:\nA. DynamoDB with a Proxy Application Layer: While feasible, a proxy application layer adds operational\ncomplexity and latency. Managing and scaling the proxy layer introduces extra overhead. DynamoDB is also\nmore expensive for large-scale storage compared to S3.\nC. Separate S3 Buckets: Storing three copies of the data (one original, two transformed) increases storage\ncosts and complicates data synchronization and updates. Managing data consistency across multiple buckets\nadds overhead.\nD. Separate DynamoDB Tables: Similar to option C, this approach multiplies storage costs and management\noverhead, and the cost of DynamoDB is already high. DynamoDB is best used for key-value queries and not\nsuited for batch processing of data.\nIn summary, S3 Object Lambda offers a serverless and cost-effective way to transform data on-the-fly as it is\naccessed, minimizing operational overhead and ensuring that only authorized applications have access to the\ncomplete dataset.",
    "links": [
      "https://aws.amazon.com/s3/object-lambda/"
    ]
  },
  {
    "question": "CertyIQ\nA development team has launched a new application that is hosted on Amazon EC2 instances inside a development\nVP",
    "options": {
      "C": "It is also a",
      "A": "10.0.1.0/32",
      "B": "192.168.0.0/24",
      "D": "10.0.1.0/24"
    },
    "answer": "D",
    "explanation": "The correct answer is D, 10.0.1.0/24. Let's break down why.\nThe core requirement is creating a new VPC that can peer with the existing development VPC\n(192.168.0.0/24). A fundamental rule for VPC peering is that the CIDR blocks of the peered VPCs cannot\noverlap. Overlapping CIDR blocks would lead to routing conflicts and prevent proper communication between\nthe VPCs.\nOption B (192.168.0.0/24) is immediately incorrect because it's the same CIDR block as the development VPC,\ncausing a direct overlap.\nOptions A (10.0.1.0/32) and C (192.168.1.0/32) are technically valid CIDR blocks. However, a /32 CIDR block\nrepresents a single IP address. While a VPC can be created with a /32 CIDR, it's not practically useful,\nparticularly in the context of a real-world application where multiple EC2 instances and other resources need\nto communicate within the VPC. A /32 block is effectively useless for hosting an application. The question asks\nfor the smallest CIDR block that meets the requirements. Meeting the requirements implies that the VPC is\npractically usable.\nOption D (10.0.1.0/24) provides a non-overlapping CIDR block with the development VPC (192.168.0.0/24). It\nalso gives a usable address space. A /24 CIDR block provides 256 addresses (251 usable, considering network\nand broadcast addresses) that is a more reasonable and useful size for a VPC. It doesn't overlap with the\nexisting VPC's address space and offers enough IPs to host resources within the peered VPC. It is also a\nprivate IP address block making it a valid choice.\nIn summary, while technically any non-overlapping CIDR would allow for peering, the /32 options are not\npractical for hosting resources. Option D is the smallest usable and valid CIDR block that enables VPC peering\nwithout overlapping address spaces.\nRelevant documentation:\nAWS VPC Peering: https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\nVPC CIDR Blocks: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-ip-addressing.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-ip-addressing.html"
    ]
  },
  {
    "question": "CertyIQ\nA company deploys an application on five Amazon EC2 instances. An Application Load Balancer (ALB) distributes\ntraffic to the instances by using a target group. The average CPU usage on each of the instances is below 10%\nmost of the time, with occasional surges to 65%.\nA solutions architect needs to implement a solution to automate the scalability of the application. The solution\nmust optimize the cost of the architecture and must ensure that the application has enough CPU resources when\nsurges occur.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Create an Amazon CloudWatch alarm that enters the ALARM state when the CPUUtilization metric is less",
      "B": "Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target",
      "C": "Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target",
      "D": "Create two Amazon CloudWatch alarms. Configure the first CloudWatch alarm to enter the ALARM state"
    },
    "answer": "B",
    "explanation": "The correct answer is B because it uses EC2 Auto Scaling with a target tracking scaling policy, a common and\nefficient way to manage application scalability based on a metric like CPU utilization. Here's a breakdown:\nEC2 Auto Scaling Group (ASG): An ASG automatically adjusts the number of EC2 instances in your\napplication based on demand. It maintains a desired capacity and can scale out (add instances) when demand\nincreases and scale in (remove instances) when demand decreases. This directly addresses the need for\nautomated scalability.\nALB Integration: Integrating the ASG with the existing Application Load Balancer (ALB) ensures that traffic is\nautomatically distributed to newly launched instances by the ASG and removed from terminated instances.\nThe ASG automatically registers and deregisters instances from the target group.\nTarget Tracking Scaling Policy: This policy type is ideal for maintaining a specified target value for a metric.\nIn this case, it uses ASGAverageCPUUtilization. The ASG automatically adjusts the number of instances to\nkeep the average CPU utilization of the group around 50%.\nMinimum, Desired, and Maximum Instances: These parameters define the boundaries of the scaling. A\nminimum of 2 ensures that the application remains available even during low traffic. A maximum of 6 limits\nthe costs and prevents over-provisioning. The desired capacity sets the initial number of instances.\nCost Optimization: By automatically scaling based on CPU utilization, the solution only uses the required\nresources, minimizing costs when the average CPU utilization is low. When surges occur, the ASG quickly\nadds instances to maintain performance, preventing performance degradation.\nLet's examine why the other options are less suitable:\nA: Using a CloudWatch alarm and Lambda function to terminate instances when CPU utilization is low is a\nscale-in approach but does not handle scaling out to address surges. Furthermore, terminating instances\nbased on low CPU alone could lead to service disruptions if demand suddenly spikes.\nC: Creating an ASG without a scaling policy only provides automatic instance replacement in case of failure. It\ndoes not automatically adjust the number of instances based on CPU utilization. It only uses the number of\ninstances specified.\nD: Using CloudWatch alarms to send email notifications for manual intervention is not an automated solution.\nManual intervention is time-consuming and can lead to delays in scaling, resulting in performance issues\nduring surges.\nIn summary, option B provides the most automated, cost-effective, and reliable solution for scaling the\napplication based on CPU utilization, ensuring it can handle surges while optimizing resource usage during\nperiods of low demand.\nAuthoritative Links:\nAWS Auto Scaling: https://aws.amazon.com/autoscaling/\nEC2 Auto Scaling Groups: https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-\ngroups.html\nTarget Tracking Scaling Policies: https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-\ntarget-tracking-scaling-policies.html\nCloudWatch Alarms:\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html",
    "links": [
      "https://aws.amazon.com/autoscaling/",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is running a critical business application on Amazon EC2 instances behind an Application Load\nBalancer. The EC2 instances run in an Auto Scaling group and access an Amazon RDS DB instance.\nThe design did not pass an operational review because the EC2 instances and the DB instance are all located in a\nsingle Availability Zone. A solutions architect must update the design to use a second Availability Zone.\nWhich solution will make the application highly available?",
    "options": {
      "A": "Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2",
      "B": "Provision two subnets that extend across both Availability Zones. Configure the Auto Scaling group to",
      "C": "Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2",
      "D": "Provision a subnet that extends across both Availability Zones. Configure the Auto Scaling group to"
    },
    "answer": "C",
    "explanation": "Here's a breakdown of why option C is the correct solution for achieving high availability in the given scenario:\nThe core issue is the single point of failure within a single Availability Zone (AZ). To enhance availability,\nresources need to be distributed across multiple AZs. The EC2 instances must reside in multiple AZs to\ncontinue serving requests if one AZ fails. This requires creating subnets within each desired AZ. The Auto\nScaling group should then be configured to launch instances into these different subnets, ensuring a\ndistribution across the AZs. This provides redundancy for the application tier.\nCritically, for the database tier's availability, Multi-AZ deployment is the key. RDS Multi-AZ creates a standby\nreplica of the database in another AZ. In case of a failure in the primary AZ, RDS automatically fails over to the\nstandby replica, minimizing downtime. Simply configuring connections to each network (as in options A and B)\ndoesn't provide automated failover or replication. Extending a single subnet across multiple AZs (as in options\nB and D) is not a standard or recommended practice; subnets are confined to a single AZ. Furthermore, the\nAuto Scaling Group must be configured to launch instances into distinct subnets that exist in different\nAvailability Zones to ensure proper distribution and high availability.\nTherefore, the complete solution involves:\n1. Creating separate subnets within each AZ.\n2. Configuring the Auto Scaling group to distribute EC2 instances across these AZ-specific subnets.\n3. Enabling RDS Multi-AZ for automatic failover to a standby replica in another AZ.\nThis approach guarantees that if one AZ experiences an outage, the application can continue running from the\nother AZ, with the database automatically failing over to the standby instance.\nHere are some helpful links for further research:\nAmazon RDS Multi-AZ: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\nAmazon EC2 Auto Scaling: https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-\ngroups.html\nAmazon VPC Subnets: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-",
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html"
    ]
  },
  {
    "question": "CertyIQ\nA research laboratory needs to process approximately 8 TB of data. The laboratory requires sub-millisecond\nlatencies and a minimum throughput of 6 GBps for the storage subsystem. Hundreds of Amazon EC2 instances\nthat run Amazon Linux will distribute and process the data.\nWhich solution will meet the performance requirements?",
    "options": {
      "A": "Create an Amazon FSx for NetApp ONTAP file system. Sat each volume tiering policy to ALL. Import the raw",
      "B": "Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses",
      "C": "Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses",
      "D": "Create an Amazon FSx for NetApp ONTAP file system. Set each volumes tiering policy to NONE. Import the"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the correct answer, along with relevant concepts and links\nfor further research:\nThe research laboratory's requirements of sub-millisecond latencies and a minimum throughput of 6 GBps\nnecessitate a high-performance storage solution. Amazon FSx for Lustre, specifically with persistent SSD\nstorage, is designed for such workloads. Lustre is a parallel distributed file system optimized for speed and\nthroughput, commonly used in high-performance computing (HPC) environments. Using SSDs further\nenhances the performance, delivering the required low latency.\nOption B leverages Amazon S3 as a cost-effective and durable repository for the initial raw data. The FSx for\nLustre file system is then linked to the S3 bucket for efficient data ingestion. This import/export functionality\nallows for a separation of storage tiers: S3 for cost-effective archiving and FSx for Lustre for active\nprocessing. The EC2 instances mount the FSx for Lustre file system, enabling them to access and process the\ndata with the necessary performance.\nOption A suggests using Amazon FSx for NetApp ONTAP. While ONTAP provides enterprise-grade features, it\ntypically doesn't match the raw performance of FSx for Lustre, especially concerning throughput-intensive\nHPC workloads. Setting the tiering policy to ALL would further exacerbate the performance due to frequent\ndata movement between tiers.\nOption D also uses FSx for NetApp ONTAP but sets the tiering policy to NONE. This would reduce the\nlikelihood of latency issues from tiering, but ONTAP still isn't optimized for the ultra-high throughput and low\nlatency required, making it a less ideal choice compared to Lustre.\nOption C uses FSx for Lustre but with persistent HDD storage. HDDs cannot deliver the sub-millisecond\nlatencies demanded by the requirement. Though cheaper, using HDDs would be a performance bottleneck,\nfailing to meet the stated needs.\nTherefore, the combination of S3 for storage, FSx for Lustre with persistent SSD storage for performance,\nand the import/export functionality to transfer data from S3 to FSx for Lustre is the most appropriate and\ncost-effective solution.\nKey Concepts:\nAmazon FSx for Lustre: High-performance, parallel file system designed for HPC workloads.\nAmazon S3: Object storage service for scalability, data availability, security, and performance.\nPersistent SSD vs. HDD: SSDs offer significantly lower latency and higher throughput compared to HDDs.\nImport/Export Functionality: Data can be easily moved between S3 and FSx for Lustre.\nAuthoritative Links:\nAmazon FSx for Lustre: https://aws.amazon.com/fsx/lustre/\nAmazon S3: https://aws.amazon.com/s3/\nAmazon FSx for NetApp ONTAP: https://aws.amazon.com/fsx/netapp-ontap/",
    "links": [
      "https://aws.amazon.com/fsx/lustre/",
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/fsx/netapp-ontap/"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to migrate a legacy application from an on-premises data center to the AWS Cloud because of\nhardware capacity constraints. The application runs 24 hours a day, 7 days a week. The applications database\nstorage continues to grow over time.\nWhat should a solutions architect do to meet these requirements MOST cost-effectively?",
    "options": {
      "A": "Migrate the application layer to Amazon EC2 Spot Instances. Migrate the data storage layer to Amazon S3.",
      "B": "Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon",
      "C": "Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon",
      "D": "Migrate the application layer to Amazon EC2 On-Demand Instances. Migrate the data storage layer to"
    },
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the most cost-effective solution, along with supporting\nconcepts and links:\nJustification for Option C:\nOption C suggests migrating the application layer to Amazon EC2 Reserved Instances and the data storage\nlayer to Amazon Aurora Reserved Instances. This approach provides the best balance of cost efficiency and\nperformance for a 24/7 application with growing data storage needs.\nApplication Layer (EC2 Reserved Instances): The application runs constantly (24/7), making Reserved\nInstances (RIs) the most cost-effective choice for the application layer. RIs offer a significant discount (up to\n75%) compared to On-Demand Instances in exchange for a one- or three-year commitment. Since the\napplication is always running, the commitment ensures consistent usage and cost savings. Spot Instances are\nunsuitable for 24/7 operations due to their potential for interruption.\nData Storage Layer (Amazon Aurora Reserved Instances): Amazon Aurora is a fully managed, MySQL- and\nPostgreSQL-compatible relational database engine. Given the application's growing database storage needs,\nAurora is a suitable choice. The use of Reserved Instances for Aurora instances aligns with the constant\noperation of the application and offers similar cost benefits to EC2 RIs. Aurora also provides built-in\nscalability to handle the increasing data volume. RDS On-Demand Instances, while simpler, are less cost-\neffective for long-term, continuous use than reserved instances.\nWhy Other Options Are Less Suitable:\nOption A (EC2 Spot, S3): Spot Instances are unreliable for 24/7 applications due to potential interruptions.\nAmazon S3 is object storage; it's not a relational database and unsuitable for storing transactional application\ndata.\nOption B (EC2 Reserved, RDS On-Demand): While EC2 Reserved Instances are a good choice for the\napplication layer, RDS On-Demand Instances are less cost-effective for a database that needs to run\nconstantly compared to reserved instances.\nOption D (EC2 On-Demand, RDS Reserved): This reverses the cost optimization. On-Demand instances are\nexpensive for 24/7 application runtime, while reserved instances should be applied to the application.\nKey Concepts:\nAmazon EC2 Instance Purchasing Options: Understanding the trade-offs between On-Demand, Reserved,\nand Spot Instances is crucial for cost optimization.\nAmazon Aurora: A fully managed, high-performance relational database service. It's a good choice for\napplications with growing database needs.\nCost Optimization in the Cloud: Choosing the appropriate AWS services and purchasing options to minimize\nexpenses while meeting performance and availability requirements.\nRelational vs. Object storage: Understanding when to use RDS services like Aurora for structured data rather\nthan object storage like S3.\nAuthoritative Links:\nAmazon EC2 Instance Purchasing Options: https://aws.amazon.com/ec2/purchasing-options/\nAmazon Aurora Pricing: https://aws.amazon.com/rds/aurora/pricing/\nAWS Well-Architected Framework - Cost Optimization Pillar:\nhttps://wa.aws.amazon.com/wellarchitected/2020-07-02T19-33-23/cost-optimization/co-introduction.en.html\nIn summary, option C provides the best balance between cost savings and the operational requirements of the\napplication by leveraging reserved instances for both the compute and database layers. This solution\naddresses the continuous operation and growing data needs in a cost-effective manner.",
    "links": [
      "https://aws.amazon.com/ec2/purchasing-options/",
      "https://aws.amazon.com/rds/aurora/pricing/",
      "https://wa.aws.amazon.com/wellarchitected/2020-07-02T19-33-23/cost-optimization/co-introduction.en.html"
    ]
  },
  {
    "question": "CertyIQ\nA university research laboratory needs to migrate 30 TB of data from an on-premises Windows file server to\nAmazon FSx for Windows File Server. The laboratory has a 1 Gbps network link that many other departments in the\nuniversity share.\nThe laboratory wants to implement a data migration service that will maximize the performance of the data\ntransfer. However, the laboratory needs to be able to control the amount of bandwidth that the service uses to\nminimize the impact on other departments. The data migration must take place within the next 5 days.\nWhich AWS solution will meet these requirements?",
    "options": {
      "A": "AWS Snowcone",
      "B": "Amazon FSx File Gateway",
      "C": "AWS DataSync.",
      "D": "AWS Transfer Family"
    },
    "answer": "C",
    "explanation": "The correct answer is C. AWS DataSync.\nHere's a detailed justification:\nAWS DataSync is a data transfer service specifically designed for moving large amounts of data between on-\npremises storage and AWS services like Amazon FSx for Windows File Server. It excels in scenarios where\nyou need to migrate data quickly and efficiently over a network connection. A key feature of DataSync that\nmakes it perfect for this scenario is its built-in bandwidth throttling capability. This allows the laboratory to\ncontrol the amount of bandwidth DataSync utilizes, minimizing the impact on other university departments\nsharing the 1 Gbps network link. DataSync also optimizes data transfer using techniques such as incremental\ntransfers (transferring only changed data after the initial copy), in-line compression, and parallel data\nstreams, maximizing throughput and making the migration within the 5-day timeframe feasible, given\nappropriate bandwidth settings. It also offers encryption in transit and at rest, which is relevant for sensitive\nresearch data.\nAWS Snowcone (A) is a physical device used for data transport, primarily useful when network bandwidth is\nlimited or unavailable. While it could theoretically transfer the data, it involves shipping the device to AWS,\nwhich introduces delays and doesn't allow for bandwidth control.Amazon FSx File Gateway (B) is a service\nthat provides low-latency access to FSx file systems from on-premises applications. It's not designed for the\ninitial migration of large datasets. It facilitates hybrid access to files already residing in FSx.AWS Transfer\nFamily (D) is a suite of services for secure file transfers into and out of Amazon S3, Amazon EFS, and AWS\nStorage Gateway using protocols such as SFTP, FTPS, and FTP. It doesn't directly support transferring data to\nFSx for Windows File Server or offer the same level of optimization and bandwidth control as DataSync for\nthis specific migration use case.\nIn summary, AWS DataSync's optimized data transfer, bandwidth throttling, and direct integration with\nAmazon FSx for Windows File Server make it the most appropriate and efficient solution for the university's\ndata migration requirements.\nRelevant links for further research:\nAWS DataSync: https://aws.amazon.com/datasync/\nAmazon FSx for Windows File Server: https://aws.amazon.com/fsx/windows/",
    "links": [
      "https://aws.amazon.com/datasync/",
      "https://aws.amazon.com/fsx/windows/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to create a mobile app that allows users to stream slow-motion video clips on their mobile\ndevices. Currently, the app captures video clips and uploads the video clips in raw format into an Amazon S3\nbucket. The app retrieves these video clips directly from the S3 bucket. However, the videos are large in their raw\nformat.\nUsers are experiencing issues with buffering and playback on mobile devices. The company wants to implement\nsolutions to maximize the performance and scalability of the app while minimizing operational overhead.\nWhich combination of solutions will meet these requirements? (Choose two.)",
    "options": {
      "A": "Deploy Amazon CloudFront for content delivery and caching:",
      "B": "Use AWS DataSync to replicate the video files across AWS Regions in other S3 buckets: While replication",
      "C": "Use Amazon Elastic Transcoder to convert the video files to more appropriate formats:",
      "D": "AWS Elastic Transcoder is a managed service that is already configured for this purpose"
    },
    "answer": "A",
    "explanation": "The correct answer is A and C. Here's why:\nA. Deploy Amazon CloudFront for content delivery and caching:\nCloudFront is a Content Delivery Network (CDN) that caches content closer to the end users. By deploying\nCloudFront in front of the S3 bucket containing the raw video files, you significantly reduce latency and\nimprove the streaming experience. CloudFront has edge locations distributed globally, ensuring users can\nretrieve video clips from a location geographically closer to them. This minimizes buffering and playback\nissues, fulfilling the requirement to maximize performance and scalability. CloudFront integrates seamlessly\nwith S3 and provides features like geo-restriction, custom SSL certificates, and access\nlogs.https://aws.amazon.com/cloudfront/\nC. Use Amazon Elastic Transcoder to convert the video files to more appropriate formats:\nThe problem statement highlights that videos are in raw format, which are large and cause playback issues.\nElastic Transcoder converts video files from their raw format into more suitable formats for streaming over\nthe internet to mobile devices (e.g., H.264). This involves encoding the videos into smaller file sizes and\nresolutions optimized for mobile viewing. It can also create adaptive bitrate streaming formats (like HLS or\nDASH) to adjust the video quality based on the user's network conditions, further improving playback. This\naddresses the buffering problem directly by reducing the file sizes. Elastic Transcoder is a fully managed\nservice, minimizing operational overhead.https://aws.amazon.com/elastictranscoder/\nWhy other options are incorrect:\nB. Use AWS DataSync to replicate the video files across AWS Regions in other S3 buckets: While replication\nimproves availability, it doesn't directly address the issue of large video file sizes causing buffering on mobile\ndevices. It also increases storage costs.\nD. Deploy an Auto Scaling group of Amazon EC2 instances in Local Zones for content delivery and caching:\nThis option is less efficient and more complex than using CloudFront. It requires managing EC2 instances and\ncaching software, significantly increasing operational overhead. CloudFront is a purpose-built CDN service\nthat handles caching automatically.\nE. Deploy an Auto Scaling group of Amazon EC2 instances to convert the video files to more appropriate\nformats: While using EC2 instances for transcoding is possible, it is not recommended for the same reasons\nlisted for option D. AWS Elastic Transcoder is a managed service that is already configured for this purpose\nand scales automatically. Using EC2 for transcoding would require more resources and higher overhead.",
    "links": [
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/elastictranscoder/"
    ]
  },
  {
    "question": "CertyIQ\nA company is launching a new application deployed on an Amazon Elastic Container Service (Amazon ECS) cluster\nand is using the Fargate launch type for ECS tasks. The company is monitoring CPU and memory usage because it\nis expecting high traffic to the application upon its launch. However, the company wants to reduce costs when\nutilization decreases.\nWhat should a solutions architect recommend?",
    "options": {
      "A": "Incorrect: Amazon EC2 Auto Scaling is designed to scale EC2 instances, not ECS tasks using Fargate.",
      "B": "Incorrect: While a Lambda function could theoretically be used to adjust ECS task counts, it's a less",
      "C": "Incorrect: As with option A, Amazon EC2 Auto Scaling is not applicable to Fargate tasks. ECS tasks running",
      "D": "Correct: AWS Application Auto Scaling is the preferred solution for scaling ECS tasks using the Fargate"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Here's why:\nA. Incorrect: Amazon EC2 Auto Scaling is designed to scale EC2 instances, not ECS tasks using Fargate.\nFargate is a serverless compute engine for containers; you don't manage EC2 instances directly.\nB. Incorrect: While a Lambda function could theoretically be used to adjust ECS task counts, it's a less\nefficient and more complex solution than using Application Auto Scaling. Lambda would require custom code\nfor monitoring metrics and scaling actions, adding overhead and potential points of failure.\nC. Incorrect: As with option A, Amazon EC2 Auto Scaling is not applicable to Fargate tasks. ECS tasks running\non Fargate do not directly involve EC2 instances, making EC2 Auto Scaling irrelevant.\nD. Correct: AWS Application Auto Scaling is the preferred solution for scaling ECS tasks using the Fargate\nlaunch type. It allows you to automatically adjust the desired count of tasks within your ECS service based on\nvarious metrics (like CPU and memory utilization). Target tracking policies are specifically designed to\nmaintain a desired target value for a metric (e.g., keep CPU utilization at 70%). When CloudWatch alarms are\ntriggered because the metric exceeds the target, Application Auto Scaling automatically increases the\nnumber of ECS tasks to handle the load. When utilization drops, it reduces the number of tasks, optimizing\ncosts.\nJustification in Detail:\n1. Fargate and Serverless: Fargate abstracts away the underlying EC2 infrastructure, making EC2 Auto\nScaling irrelevant. Fargate provides serverless compute for containers.\n2. Application Auto Scaling for ECS: AWS Application Auto Scaling integrates directly with ECS\nservices and Fargate tasks. https://docs.aws.amazon.com/autoscaling/application/userguide/what-is-\napplication-auto-scaling.html\n3. Target Tracking Policies: Target tracking policies simplify scaling by automatically adjusting the\nservice's desired count to maintain a target value for a specific metric, such as average CPU or\nmemory utilization. https://docs.aws.amazon.com/autoscaling/application/userguide/application-\nauto-scaling-target-tracking.html\n4. CloudWatch Alarms as Triggers: CloudWatch alarms monitor the specified metrics (CPU, memory)\nand trigger the Application Auto Scaling policies when predefined thresholds are breached.\n5. Cost Optimization: By scaling down task counts when utilization is low, Application Auto Scaling\nensures you only pay for the resources you need, minimizing costs.\n6. Simplified Management: Application Auto Scaling eliminates the need for custom scaling logic (as\nwith Lambda), reducing operational overhead and complexity.\n7. Responsiveness: Target tracking allows the system to quickly adapt to changes in demand,\nincreasing the number of ECS tasks when utilization increases and decreasing the number of tasks\nwhen utilization decreases.\nIn summary, AWS Application Auto Scaling with target tracking policies offers a managed, efficient, and cost-\neffective way to dynamically scale ECS tasks running on Fargate, directly responding to changes in\napplication utilization. The CloudWatch alarms serve as the trigger to initiate the scaling actions.",
    "links": [
      "https://docs.aws.amazon.com/autoscaling/application/userguide/what-is-",
      "https://docs.aws.amazon.com/autoscaling/application/userguide/application-"
    ]
  },
  {
    "question": "CertyIQ\nA company recently created a disaster recovery site in a different AWS Region. The company needs to transfer\nlarge amounts of data back and forth between NFS file systems in the two Regions on a periodic basis.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Use AWS DataSync.",
      "B": "Use AWS Snowball devices.",
      "C": "Set up an SFTP server on Amazon EC2.",
      "D": "Use AWS Database Migration Service (AWS DMS)."
    },
    "answer": "A",
    "explanation": "The correct answer is A. Use AWS DataSync.\nHere's a detailed justification:\nAWS DataSync is a purpose-built data transfer service designed for moving large amounts of data between\non-premises storage and AWS, or between AWS storage services. It simplifies, automates, and accelerates\ndata transfer over the network. In this scenario, where data needs to be moved between NFS file systems in\ntwo different AWS Regions on a periodic basis, DataSync is an ideal solution because it offers significant\nadvantages in terms of speed, reliability, and operational overhead.\nDataSync optimizes network usage through built-in acceleration techniques and parallel data transfer, which\nis essential for large datasets. It also offers built-in security features like encryption and data integrity\nverification. Critically, it simplifies the transfer process, automating scheduling, monitoring, and error\nhandling, which translates to minimal operational effort compared to other solutions.\nOption B (AWS Snowball devices) is more suited for initial bulk data migrations, not for periodic transfers\nbetween regions. Transporting physical devices back and forth introduces logistical complexity and is not\nefficient for recurring data transfers.\nOption C (Setting up an SFTP server on Amazon EC2) would require significant manual configuration and\nmanagement of the server, security settings, and transfer processes. This dramatically increases the\noperational overhead and introduces potential vulnerabilities. It's also not optimized for high-speed data\ntransfer.\nOption D (AWS Database Migration Service) is designed for migrating databases, not file systems. Therefore,\nit is completely inappropriate for this use case involving NFS file systems.\nTherefore, AWS DataSync provides the least operational overhead for periodic, large-scale data transfers\nbetween NFS file systems in different AWS Regions because it is specifically designed for this purpose with\nbuilt-in automation, optimization, and security features.\nFurther research:\nAWS DataSync: https://aws.amazon.com/datasync/",
    "links": [
      "https://aws.amazon.com/datasync/"
    ]
  },
  {
    "question": "CertyIQ\nA company is designing a shared storage solution for a gaming application that is hosted in the AWS Cloud. The\ncompany needs the ability to use SMB clients to access data. The solution must be fully managed.\nWhich AWS solution meets these requirements?",
    "options": {
      "A": "Create an AWS DataSync task that shares the data as a mountable file system. Mount the file system to the",
      "B": "S3 is also not designed for low-latency file access often needed by applications.",
      "C": "Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server.",
      "D": "Create an Amazon S3 bucket. Assign an IAM role to the application to grant access to the S3 bucket. Mount"
    },
    "answer": "C",
    "explanation": "The correct answer is C, creating an Amazon FSx for Windows File Server file system. Here's why:\nRequirement: SMB Access: The gaming application needs to be accessed via SMB (Server Message Block)\nprotocol. Amazon FSx for Windows File Server natively supports SMB, allowing Windows-based applications\nand clients to interact seamlessly with the file system using standard Windows file sharing protocols.\nRequirement: Fully Managed: The solution needs to be fully managed. FSx for Windows File Server is a fully\nmanaged service, meaning AWS handles the underlying infrastructure, patching, backups, and maintenance.\nThis significantly reduces the operational overhead for the company.\nWhy other options are incorrect:\nA (AWS DataSync): AWS DataSync is primarily used for data transfer between on-premises storage and AWS\nstorage services, and it does not provide a direct SMB file share endpoint. It's more suitable for migration or\nbackup scenarios, not for ongoing shared storage access.\nB (EC2 Windows Instance): While an EC2 Windows instance can be configured as a file server, this is not a\nfully managed solution. The company would be responsible for managing the OS, patching, backups, and\nensuring high availability. This introduces significant operational overhead, contradicting the requirement.\nD (Amazon S3): Amazon S3 is an object storage service, not a file system. While S3 can be \"mounted\" using\ntools, it doesn't natively support SMB. Accessing it through SMB would require additional layers of software\nand complexity, and the object storage model isn't designed for the typical file system operations expected\nby applications using SMB. S3 is also not designed for low-latency file access often needed by applications.\nIn summary, Amazon FSx for Windows File Server directly fulfills the requirements of providing a fully\nmanaged SMB file share solution for the gaming application, aligning with the service's core functionality and\nintended use case.\nAuthoritative Links:\nAmazon FSx for Windows File Server: https://aws.amazon.com/fsx/windows/",
    "links": [
      "https://aws.amazon.com/fsx/windows/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to run an in-memory database for a latency-sensitive application that runs on Amazon EC2\ninstances. The application processes more than 100,000 transactions each minute and requires high network\nthroughput. A solutions architect needs to provide a cost-effective network design that minimizes data transfer\ncharges.\nWhich solution meets these requirements?",
    "options": {
      "A": "Launch all EC2 instances in the same Availability Zone within the same AWS Region. Specify a placement",
      "B": "Launch all EC2 instances in different Availability Zones within the same AWS Region. Specify a placement",
      "C": "Deploy an Auto Scaling group to launch EC2 instances in different Availability Zones based on a network",
      "D": "Deploy an Auto Scaling group with a step scaling policy to launch EC2 instances in different Availability"
    },
    "answer": "A",
    "explanation": "The correct answer is A because it prioritizes low latency and cost-effectiveness for a high-throughput, in-\nmemory database within AWS. Let's break down why:\nPlacement Groups: Placement groups influence how EC2 instances are placed on underlying hardware. A\ncluster placement group strategy aims to place instances within a single Availability Zone as closely together\nas possible. This reduces latency and increases network throughput by minimizing the physical distance\nbetween instances.\nAvailability Zones and Latency: Launching instances within the same Availability Zone minimizes latency\ncompared to spreading them across different Availability Zones. Data transfer between Availability Zones\nincurs costs, which should be avoided in a cost-sensitive scenario with high data transfer requirements.\nCost-Effectiveness: Data transfer between Availability Zones is charged, whereas data transfer within an\nAvailability Zone is free. Keeping instances within the same Availability Zone eliminates these cross-AZ data\ntransfer charges.\nAuto Scaling (Option C and D - incorrect): While Auto Scaling is beneficial for scaling resources based on\ndemand, it's not essential for minimizing latency within a tightly coupled, in-memory database application\nwhere consistent, low-latency communication is paramount. Auto Scaling adds complexity without directly\naddressing the latency and data transfer cost concerns in this specific scenario. Launching across multiple\nAZs is not beneficial for latency or cost optimization given the constraints.\nPartition Strategy (Option B - incorrect): Partition placement groups are suitable for distributing large\nnumbers of replicas across distinct racks to minimize correlated hardware failures. They are not optimized for\nlow-latency communication. Moreover, launching across multiple AZs will incur costs due to data transfer.\nTherefore, choosing a cluster placement group within a single Availability Zone provides the best solution for\nminimizing latency, maximizing network throughput, and reducing data transfer costs for the given\napplication requirements.\nFurther Research:\nAWS Placement Groups: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\nAWS Data Transfer Costs: https://aws.amazon.com/ec2/pricing/on-demand/\nAWS Availability Zones: https://aws.amazon.com/about-aws/global-infrastructure/regions_availability-zones/",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
      "https://aws.amazon.com/ec2/pricing/on-demand/",
      "https://aws.amazon.com/about-aws/global-infrastructure/regions_availability-zones/"
    ]
  },
  {
    "question": "CertyIQ\nA company that primarily runs its application servers on premises has decided to migrate to AWS. The company\nwants to minimize its need to scale its Internet Small Computer Systems Interface (iSCSI) storage on premises.\nThe company wants only its recently accessed data to remain stored locally.\nWhich AWS solution should the company use to meet these requirements?",
    "options": {
      "A": "Amazon S3 File Gateway",
      "B": "AWS Storage Gateway Tape Gateway",
      "C": "AWS Storage Gateway Volume Gateway stored volumes",
      "D": "AWS Storage Gateway Volume Gateway cached volumes"
    },
    "answer": "D",
    "explanation": "The company needs a solution that minimizes on-premises iSCSI storage while keeping recently accessed\ndata local. AWS Storage Gateway offers different modes, and understanding these is crucial.\nOption A, Amazon S3 File Gateway, stores data as objects directly in S3. While it reduces on-premises\nstorage, it doesn't cache frequently accessed data locally for low latency access.\nOption B, AWS Storage Gateway Tape Gateway, is designed for virtual tape storage and is not relevant for\ngeneral application server data requiring low latency.\nOption C, AWS Storage Gateway Volume Gateway (stored volumes), stores the entire dataset on-premises\nand asynchronously backs it up to AWS. This contradicts the requirement to minimize on-premises storage\nand keep only recently accessed data locally.\nOption D, AWS Storage Gateway Volume Gateway (cached volumes), addresses the requirements directly.\nCached volumes store the entire dataset in S3 and cache only the frequently accessed data on-premises. This\nreduces the on-premises storage footprint, keeps recently accessed data local for fast access, and provides\ndurable backup in S3. The \"minimize its need to scale its iSCSI storage on premises\" and \"only its recently\naccessed data to remain stored locally\" requirements are perfectly met by this configuration. It strikes a\nbalance between on-premises performance and cloud storage scalability and cost-effectiveness. Therefore,\noption D is the most suitable.\nFurther reading:\nAWS Storage Gateway Documentation\nUnderstanding Volume Gateway Types",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has multiple AWS accounts that use consolidated billing. The company runs several active high\nperformance Amazon RDS for Oracle On-Demand DB instances for 90 days. The companys finance team has\naccess to AWS Trusted Advisor in the consolidated billing account and all other AWS accounts.\nThe finance team needs to use the appropriate AWS account to access the Trusted Advisor check\nrecommendations for RDS. The finance team must review the appropriate Trusted Advisor check to reduce RDS\ncosts.\nWhich combination of steps should the finance team take to meet these requirements? (Choose two.)",
    "options": {
      "A": "Use the Trusted Advisor recommendations from the account where the RDS instances are running.",
      "B": "It would require logging into multiple",
      "C": "Review the Trusted Advisor check for Amazon RDS Reserved Instance Optimization.",
      "D": "Review the Trusted Advisor check for Amazon RDS Idle DB Instances."
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why options B and D are the correct choices, and why the others are\nincorrect:\nOption B: Use the Trusted Advisor recommendations from the consolidated billing account to see all RDS\ninstance checks at the same time.\nExplanation: With consolidated billing, the payer (consolidated billing account) has visibility into the usage\nand cost optimization recommendations for all linked accounts. AWS Trusted Advisor, when accessed from\nthe payer account, provides a single pane of glass to view the recommendations across the entire AWS\norganization. This allows the finance team to efficiently identify potential cost savings opportunities across all\naccounts without logging into each one individually. This centralized view is a key benefit of consolidated\nbilling.\nRelevance: This directly addresses the requirement to have a comprehensive view for cost reduction across\nmultiple accounts.\nSupporting concept: Consolidated Billing within AWS Organizations.\nAuthoritative link: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html\nOption D: Review the Trusted Advisor check for Amazon RDS Idle DB Instances.\nExplanation: Since the company is looking to reduce RDS costs, identifying and addressing idle DB instances\nis a crucial step. The \"Amazon RDS Idle DB Instances\" Trusted Advisor check specifically flags instances that\nhave low CPU utilization and no connection activity over a period, indicating they are potentially underutilized\nand can be stopped or resized to save costs.\nRelevance: Addresses the need to reduce RDS costs by targeting instances that are consuming resources\nwithout significant activity.\nSupporting concept: Cost optimization through resource utilization analysis.\nAuthoritative link: https://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor.html (Look for RDS\nchecks within the Trusted Advisor documentation)\nWhy the other options are incorrect:\nOption A: Use the Trusted Advisor recommendations from the account where the RDS instances are\nrunning. While this would provide recommendations specific to that account, it wouldn't give the finance team\na comprehensive view across all accounts as easily as option B. It would require logging into multiple\naccounts.\nOption C: Review the Trusted Advisor check for Amazon RDS Reserved Instance Optimization. While\nReserved Instances are important for cost optimization, the instances were On-Demand for 90 days.\nTherefore, Reserved Instance Optimization wouldn't directly address the cost reduction for existing usage of\nOn-Demand instances. It's a future optimization strategy rather than a solution for current On-Demand usage.\nOption E: Review the Trusted Advisor check for Amazon Redshift Reserved Node Optimization. This is\nirrelevant because the question specifies Amazon RDS for Oracle instances, not Amazon Redshift. This check\napplies to a different AWS service.",
    "links": [
      "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html",
      "https://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect needs to optimize storage costs. The solutions architect must identify any Amazon S3\nbuckets that are no longer being accessed or are rarely accessed.\nWhich solution will accomplish this goal with the LEAST operational overhead?",
    "options": {
      "A": "Analyze bucket access patterns by using the S3 Storage Lens dashboard for advanced activity metrics.",
      "B": "Analyze bucket access patterns by using the S3 dashboard in the AWS Management Console.",
      "C": "Turn on the Amazon CloudWatch BucketSizeBytes metric for buckets. Analyze bucket access patterns by",
      "D": "Turn on AWS CloudTrail for S3 object monitoring. Analyze bucket access patterns by using CloudTrail logs"
    },
    "answer": "A",
    "explanation": "The correct answer is A because S3 Storage Lens is specifically designed to provide organization-wide\nvisibility into object storage, trends, and generate actionable recommendations to optimize costs and apply\ndata protection best practices. S3 Storage Lens does this with advanced metrics, trends, and visualizations. It\nprovides a central dashboard to analyze data usage and activity trends across your entire S3 estate,\nidentifying buckets that are infrequently accessed or potentially abandoned. This approach requires minimal\noperational overhead as it is a managed service with built-in capabilities for analyzing access patterns.\nOption B is incorrect because the S3 dashboard in the AWS Management Console provides basic storage\nusage metrics, but it lacks the advanced activity metrics and trend analysis capabilities of S3 Storage Lens. It\ndoes not offer the same level of detailed insights into access patterns required for identifying rarely accessed\nbuckets effectively.\nOption C is less efficient. While CloudWatch can track bucket size, analyzing access patterns using only\nbucket size and Athena requires more manual configuration and analysis. It would not directly reveal access\nfrequency like S3 Storage Lens. This involves writing complex queries in Athena to correlate bucket size\nchanges with other potential access logs, resulting in more operational overhead.\nOption D involves high operational overhead. While CloudTrail captures API activity, enabling it for S3 object\nmonitoring generates a large volume of logs. Integrating these logs with CloudWatch Logs and then analyzing\nthem to determine access patterns is a complex and resource-intensive task, making it a less efficient\nsolution for cost optimization.\nTherefore, S3 Storage Lens directly addresses the requirement with the least operational overhead by\nproviding comprehensive analysis of S3 usage and activity, making it easy to identify infrequently accessed\nbuckets.\nRelevant Link:\nAnalyzing storage usage with S3 Storage Lens",
    "links": []
  },
  {
    "question": "CertyIQ\nA company sells datasets to customers who do research in artificial intelligence and machine learning (AI/ML). The\ndatasets are large, formatted files that are stored in an Amazon S3 bucket in the us-east-1 Region. The company\nhosts a web application that the customers use to purchase access to a given dataset. The web application is\ndeployed on multiple Amazon EC2 instances behind an Application Load Balancer. After a purchase is made,\ncustomers receive an S3 signed URL that allows access to the files.\nThe customers are distributed across North America and Europe. The company wants to reduce the cost that is\nassociated with data transfers and wants to maintain or improve performance.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Configure S3 Transfer Acceleration on the existing S3 bucket. Direct customer requests to the S3 Transfer",
      "B": "Deploy an Amazon CloudFront distribution with the existing S3 bucket as the origin. Direct customer",
      "C": "Set up a second S3 bucket in the eu-central-1 Region with S3 Cross-Region Replication between the",
      "D": "Modify the web application to enable streaming of the datasets to end users. Configure the web application"
    },
    "answer": "B",
    "explanation": "The optimal solution to reduce data transfer costs and improve performance for customers across North\nAmerica and Europe accessing datasets in an S3 bucket is to use Amazon CloudFront.\nOption B, deploying CloudFront with the existing S3 bucket as the origin and using CloudFront signed URLs\nfor access control, addresses both the cost and performance concerns. CloudFront is a content delivery\nnetwork (CDN) that caches data at edge locations closer to the users. This reduces latency and improves\ndownload speeds, thus enhancing performance for customers in both North America and Europe.\nFurthermore, CloudFront data transfer costs are generally lower than direct S3 data transfer costs, especially\nfor geographically dispersed users. By caching the datasets closer to users, CloudFront significantly reduces\nthe amount of data transferred directly from the S3 bucket, which in turn lowers costs.\nSwitching to CloudFront signed URLs is crucial for maintaining security. These URLs control access to the\ncontent served through CloudFront, ensuring that only authorized users (those who have purchased access)\ncan download the datasets. CloudFront signed URLs are also a more secure option in this scenario, allowing\nfor features like key rotation and fine-grained access control based on date, time or IP address.\nOption A, using S3 Transfer Acceleration, focuses primarily on accelerating uploads to S3, which is not the\nprimary concern here. The key focus is on optimizing downloads for the customers.\nOption C, using S3 Cross-Region Replication, would incur higher storage costs due to the duplicated data.\nWhile it improves latency for European users, it's a less efficient and more expensive solution compared to\nCloudFront. Also, managing separate buckets and ensuring data consistency can be more complex.\nOption D, streaming from the web application, would add significant overhead to the application instances. It\nrequires more complex application logic and processing, and will likely not be as efficient as using a CDN to\ndistribute the data. Also, implementing access control in the application itself would make it complex and may\nnot be highly scalable.\nTherefore, CloudFront with signed URLs offers the best balance of cost reduction, performance improvement,\nand security for the company's dataset distribution needs.\nSupporting links:\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nCloudFront Signed URLs: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-\ncontent-signed-urls.html",
    "links": [
      "https://aws.amazon.com/cloudfront/",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-"
    ]
  },
  {
    "question": "CertyIQ\nA company is using AWS to design a web application that will process insurance quotes. Users will request quotes\nfrom the application. Quotes must be separated by quote type, must be responded to within 24 hours, and must\nnot get lost. The solution must maximize operational efficiency and must minimize maintenance.\nWhich solution meets these requirements?",
    "options": {
      "C": "Here's a detailed justification:"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Here's a detailed justification:\nOption C leverages the strengths of Amazon SNS and SQS to build a reliable and scalable message queuing\nsystem that effectively addresses the requirements. The application publishes quote requests to a single SNS\ntopic. SNS then fans out the messages to multiple SQS queues based on message filtering. Each SQS queue\nis associated with a specific quote type, and the backend application servers consume messages only from\ntheir corresponding queue. This ensures that quotes are separated by type and processed by the appropriate\nservers. SQS offers message durability and guarantees that messages will not be\nlost.https://aws.amazon.com/sns/https://aws.amazon.com/sqs/\nThe 24-hour processing requirement can be met using SQS's message visibility timeout and dead-letter\nqueues. If a message is not processed within the timeout, it returns to the queue, and after a configured\nnumber of retries, it can be sent to a dead-letter queue for further investigation, ensuring no quotes are lost.\nThis combination maximizes operational efficiency because SNS/SQS are fully managed services, minimizing\nmaintenance overhead. The filtering mechanism within SNS directs traffic efficiently.\nOption A, using Kinesis Data Streams, is more suitable for real-time streaming analytics rather than\nasynchronous task processing like insurance quote generation. It also requires more configuration and\nmanagement via the KCL. Option B, using separate SNS topics and Lambda functions for each quote type,\ncreates a large number of resources and adds complexity. Managing many Lambda functions can be\noperationally inefficient. Option D, using Kinesis Data Firehose and OpenSearch Service, is better suited for\nlog analytics or large-scale data warehousing. Searching OpenSearch service continuously and processing\nmessages isn't the best use case compared to SQS queues and is overkill for processing messages that need\nto be processed in near-real-time.",
    "links": [
      "https://aws.amazon.com/sns/https://aws.amazon.com/sqs/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an application that runs on several Amazon EC2 instances. Each EC2 instance has multiple Amazon\nElastic Block Store (Amazon EBS) data volumes attached to it. The applications EC2 instance configuration and\ndata need to be backed up nightly. The application also needs to be recoverable in a different AWS Region.\nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "options": {
      "A": "Write an AWS Lambda function that schedules nightly snapshots of the applications EBS volumes and",
      "B": "Create a backup plan by using AWS Backup to perform nightly",
      "C": "Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another",
      "D": "Write an AWS Lambda function that schedules nightly snapshots of the application's EBS volumes and"
    },
    "answer": "B",
    "explanation": "The most operationally efficient solution is B. Create a backup plan by using AWS Backup to perform nightly\nbackups. Copy the backups to another Region. Add the applications EC2 instances as resources.\nHere's why:\nAWS Backup's Orchestration: AWS Backup is designed to centralize and automate backup and restore tasks\nacross AWS services, including EC2 and EBS. It provides a managed service for creating backup plans,\nschedules, and retention policies, simplifying the backup process.\nEC2-Level Backup: By adding the EC2 instances as resources to the backup plan, AWS Backup will\nautomatically discover and back up all attached EBS volumes as part of the instance backup. This ensures\nconsistency and simplifies management compared to managing EBS volume snapshots individually.\nCross-Region Copying: AWS Backup natively supports copying backups to another AWS Region as part of\nthe backup plan. This addresses the disaster recovery requirement.\nOperational Efficiency: Using AWS Backup significantly reduces the operational overhead compared to\nwriting and maintaining a custom Lambda function. AWS Backup handles scheduling, orchestration, and\ncompliance aspects, allowing administrators to focus on other tasks.\nRecovery Simplicity: Backing up the entire EC2 instance simplifies recovery. The instance can be restored in\nanother region including the instance configuration and the data on the associated EBS volumes.\nAutomated Discovery: AWS Backup automates the discovery of EBS volumes attached to the EC2 instances,\nremoving the need to manually manage EBS volume identifiers within a Lambda function.\nWhy other options are less optimal:\nA & D: Using Lambda functions for snapshot management is more complex and requires more operational\noverhead to maintain and troubleshoot. While feasible, it lacks the built-in orchestration and centralized\nmanagement benefits of AWS Backup. D is incorrect because copying to a different Availability Zone doesn't\nfulfill the cross-region recovery requirement.\nC: While C uses AWS Backup, adding only the EBS volumes as resources will only backup the data and would\nnot include the important instance configuration.\nSupporting Links:\nAWS Backup: https://aws.amazon.com/backup/\nAWS Backup Documentation: https://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html",
    "links": [
      "https://aws.amazon.com/backup/",
      "https://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is building a mobile app on AWS. The company wants to expand its reach to millions of users. The\ncompany needs to build a platform so that authorized users can watch the companys content on their mobile\ndevices.\nWhat should a solutions architect recommend to meet these requirements?",
    "options": {
      "A": "Publish content to a public Amazon S3 bucket. Use AWS Key Management Service (AWS KMS) keys to",
      "B": "Set up IPsec VPN between the mobile app and the AWS environment to stream content.",
      "C": "Use Amazon CloudFront. Provide signed URLs to stream content.",
      "D": "Set up AWS Client VPN between the mobile app and the AWS environment to stream content."
    },
    "answer": "C",
    "explanation": "The correct answer is C: Use Amazon CloudFront. Provide signed URLs to stream content.\nHere's a detailed justification:\nTo deliver content to millions of users securely and efficiently, a Content Delivery Network (CDN) is essential.\nAmazon CloudFront is AWS's CDN service, designed to distribute content globally with low latency and high\ntransfer speeds. Option C leverages CloudFront's capabilities to cache content at edge locations closer to\nusers, significantly improving the viewing experience by reducing latency. Signed URLs provide a secure\nmechanism for authorizing access to content. They grant time-limited access to specific resources for\nauthorized users. This approach allows the company to control who can view the content and for how long,\npreventing unauthorized access. The company controls distribution through the signing process, ensuring\nonly authorized users can watch the mobile app content.\nOption A is incorrect because storing content in a public S3 bucket without a CDN does not address\nscalability or performance for millions of users. Also, KMS keys are for encrypting data at rest, not for\nstreaming authorization.\nOption B and D are incorrect because setting up VPNs (IPsec or Client VPN) for millions of mobile users is not\na scalable or practical solution. VPNs are designed for secure network connections between defined points,\nnot for content delivery to a vast user base. They will also cause performance issues because you're\nessentially routing all traffic from the mobile app to the AWS environment through the VPN, creating a\nbottleneck.Therefore, using CloudFront with signed URLs offers the best balance of scalability, performance,\nand security for streaming content to millions of users.\nRelevant Links:\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nCloudFront Signed URLs and Signed Cookies:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html",
    "links": [
      "https://aws.amazon.com/cloudfront/",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has an on-premises MySQL database used by the global sales team with infrequent access patterns.\nThe sales team requires the database to have minimal downtime. A database administrator wants to migrate this\ndatabase to AWS without selecting a particular instance type in anticipation of more users in the future.\nWhich service should a solutions architect recommend?",
    "options": {},
    "answer": "B",
    "explanation": "The recommended service is Amazon Aurora Serverless for MySQL because it directly addresses the\nscenario's key requirements: minimal downtime, infrequent access patterns, and avoidance of pre-defined\ninstance types due to uncertain future capacity needs.\nAmazon Aurora Serverless provides automatic scaling, starting, and stopping of the database cluster based\non application needs. This eliminates the need for manual capacity provisioning or selection of a specific\ninstance type upfront. Its \"pay-per-use\" model aligns perfectly with the infrequent access patterns,\noptimizing costs when the database is not actively in use. Aurora Serverless automatically starts up when a\nconnection is requested and scales compute and memory capacity to handle the workload.\nAurora MySQL (Option A) would require selecting a specific instance type, which contradicts the requirement\nto avoid doing so in anticipation of future needs. While RDS for MySQL (Option D) offers a managed MySQL\nservice, it also requires specifying an instance type and doesn't provide the automatic scaling and cost\noptimization benefits of Aurora Serverless for infrequent workloads. Amazon Redshift Spectrum (Option C) is\na data warehousing solution optimized for analytics, not a direct replacement for a transactional MySQL\ndatabase.\nAurora Serverless provides high availability by storing data across multiple Availability Zones. Downtime is\nminimized during scaling operations, ensuring uninterrupted service to the global sales team. The automatic\nscaling capabilities ensure the database can easily handle more users in the future without manual\nintervention or the need for instance type upgrades. This meets the requirement of supporting potential\ngrowth and preventing operational overhead.\nIn summary, Aurora Serverless offers the ideal balance of minimal downtime, automatic scaling, cost\noptimization for infrequent access, and avoidance of pre-defined instance types, making it the best solution\nfor this migration scenario.\nRelevant links:\nAmazon Aurora Serverless: https://aws.amazon.com/rds/aurora/serverless/\nAmazon RDS: https://aws.amazon.com/rds/",
    "links": [
      "https://aws.amazon.com/rds/aurora/serverless/",
      "https://aws.amazon.com/rds/"
    ]
  },
  {
    "question": "CertyIQ\nA company experienced a breach that affected several applications in its on-premises data center. The attacker\ntook advantage of vulnerabilities in the custom applications that were running on the servers. The company is now\nmigrating its applications to run on Amazon EC2 instances. The company wants to implement a solution that\nactively scans for vulnerabilities on the EC2 instances and sends a report that details the findings.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Deploy AWS Shield to scan the EC2 instances for vulnerabilities. Create an AWS Lambda function to log any",
      "B": "Deploy Amazon Macie and AWS Lambda functions to scan the EC2 instances for vulnerabilities. Log any",
      "C": "Turn on Amazon GuardDuty. Deploy the GuardDuty agents to the EC2 instances. Configure an AWS Lambda",
      "D": "Turn on Amazon Inspector. Deploy the Amazon Inspector agent to the EC2 instances. Configure an AWS"
    },
    "answer": "D",
    "explanation": "The correct answer is D because it leverages Amazon Inspector, a service specifically designed for\nvulnerability management in AWS environments. Amazon Inspector automatically assesses EC2 instances for\nsecurity vulnerabilities and deviations from security best practices. Deploying the Inspector agent on the EC2\ninstances allows it to scan the operating system and applications for known vulnerabilities. The service then\ngenerates findings that detail the discovered issues.\nOption D also includes an AWS Lambda function to automate report generation and distribution. This\naddresses the requirement to send a report detailing the findings.\nLet's examine why the other options are incorrect:\nA: AWS Shield is a DDoS protection service and does not scan EC2 instances for vulnerabilities in the way that\nInspector does. While CloudTrail logs API calls, it does not directly handle vulnerability scanning results from\nShield.\nB: Amazon Macie is designed to discover and protect sensitive data in Amazon S3 buckets. It is not the\nappropriate service for scanning EC2 instances for vulnerabilities.\nC: Amazon GuardDuty is a threat detection service that analyzes logs and network activity for malicious or\nunauthorized behavior. While GuardDuty provides valuable security insights, it's not specifically designed for\nactive vulnerability scanning of EC2 instances at the OS and application level. Deploying agents for\nGuardDuty onto EC2 instances is not standard practice.\nTherefore, Amazon Inspector provides the most comprehensive solution for active vulnerability scanning of\nEC2 instances and reporting the findings, making option D the best choice.\nAuthoritative Links:\nAmazon Inspector: https://aws.amazon.com/inspector/\nAWS Lambda: https://aws.amazon.com/lambda/",
    "links": [
      "https://aws.amazon.com/inspector/",
      "https://aws.amazon.com/lambda/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses an Amazon EC2 instance to run a script to poll for and process messages in an Amazon Simple\nQueue Service (Amazon SQS) queue. The company wants to reduce operational costs while maintaining its ability\nto process a growing number of messages that are added to the queue.\nWhat should a solutions architect recommend to meet these requirements?",
    "options": {
      "A": "Increase the size of the EC2 instance to process messages faster.",
      "B": "Use Amazon EventBridge to turn off the EC2 instance when the instance is underutilized.",
      "C": "Migrate the script on the EC2 instance to an AWS Lambda function with the appropriate runtime.",
      "D": "Use AWS Systems Manager Run Command to run the script on demand."
    },
    "answer": "C",
    "explanation": "The correct answer is C, migrating the script to an AWS Lambda function. Here's why:\nThe primary goal is to reduce operational costs while handling a potentially growing number of messages in\nthe SQS queue. The existing EC2 instance is continuously running and polling the queue, incurring costs even\nwhen there are no messages to process. This is inefficient.\nLambda functions are serverless compute services that execute code only when triggered. By triggering the\nLambda function based on new messages arriving in the SQS queue, you eliminate the need for a constantly\nrunning EC2 instance. This significantly reduces costs because you only pay for the compute time used when\nprocessing messages.\nSpecifically, you can configure the Lambda function to be triggered by SQS using SQS as an event source.\nWhen messages are available, Lambda automatically scales to handle the workload. This makes the solution\ncost-effective and inherently scalable to accommodate the growing number of messages.\nOption A, increasing the EC2 instance size, would increase costs without necessarily solving the underlying\ninefficiency of continuous operation. Option B, using EventBridge to turn off the EC2 instance, is complex to\nimplement reliably and might still lead to delays in processing messages. Option D, using Systems Manager\nRun Command, doesn't automatically scale the processing based on the number of messages, so there is no\nguarantee that on-demand instances can handle the workload quickly without a pre-warmed pool of\ninstances, which also increases cost and is against the need for minimizing it. The best approach is to leverage\nthe serverless and event-driven architecture that Lambda provides for optimal cost and scalability.\nFurther research:\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon SQS as a Lambda Event Source: https://docs.aws.amazon.com/lambda/latest/dg/services-sqs.html\nServerless Architectures: https://aws.amazon.com/serverless/",
    "links": [
      "https://aws.amazon.com/lambda/",
      "https://docs.aws.amazon.com/lambda/latest/dg/services-sqs.html",
      "https://aws.amazon.com/serverless/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses a legacy application to produce data in CSV format. The legacy application stores the output data\nin Amazon S3. The company is deploying a new commercial off-the-shelf (COTS) application that can perform\ncomplex SQL queries to analyze data that is stored in Amazon Redshift and Amazon S3 only. However, the COTS\napplication cannot process the .csv files that the legacy application produces.\nThe company cannot update the legacy application to produce data in another format. The company needs to\nimplement a solution so that the COTS application can use the data that the legacy application produces.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule.",
      "B": "Python script on EC2: This requires managing EC2 instances, writing and maintaining Python code, and",
      "C": "Lambda and DynamoDB: DynamoDB is a NoSQL database and not suitable for complex SQL queries. While",
      "D": "EMR: EMR is powerful for big data processing, but using it for simple CSV conversion is overkill. It has"
    },
    "answer": "A",
    "explanation": "The best solution is A. Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule.\nConfigure the ETL job to process the .csv files and store the processed data in Amazon Redshift.\nHere's why:\nAWS Glue is a fully managed ETL service. This significantly reduces operational overhead compared to\nmanaging EC2 instances, EMR clusters, or Lambda functions for complex data transformations.\nDirect Integration with Amazon Redshift: Glue is designed to work seamlessly with Redshift. It can efficiently\nload and transform data directly into Redshift tables, making it readily accessible for the COTS application's\nSQL queries.\nAutomatic Schema Discovery (Crawler): Glue can automatically infer the schema of the CSV files stored in\nS3, simplifying the ETL process.\nScalability and Reliability: Glue provides scalable and reliable ETL processing with minimal operational\neffort. The scheduled ETL job will handle the CSV files as they are produced.\nWhy other options are less suitable:\nB. Python script on EC2: This requires managing EC2 instances, writing and maintaining Python code, and\nhandling potential scaling issues. Converting CSV to SQL and storing as .sql files is not an efficient approach\nfor data analysis in Redshift.\nC. Lambda and DynamoDB: DynamoDB is a NoSQL database and not suitable for complex SQL queries. While\nLambda is serverless, it's less ideal for large-scale data transformation tasks compared to Glue, and\nDynamoDB isn't the target data store for the COTS application's SQL queries.\nD. EMR: EMR is powerful for big data processing, but using it for simple CSV conversion is overkill. It has\nhigher operational overhead and cost compared to Glue for this specific use case. Also, a weekly schedule\nmight not be sufficient if data needs to be available more frequently.\nIn summary, AWS Glue provides the most straightforward, cost-effective, and operationally efficient solution\nfor transforming the CSV data and loading it into Amazon Redshift for use by the COTS application.\nAuthoritative Links:\nAWS Glue Documentation: https://aws.amazon.com/glue/\nAmazon Redshift Documentation: https://aws.amazon.com/redshift/",
    "links": [
      "https://aws.amazon.com/glue/",
      "https://aws.amazon.com/redshift/"
    ]
  },
  {
    "question": "CertyIQ\nA company recently migrated its entire IT environment to the AWS Cloud. The company discovers that users are\nprovisioning oversized Amazon EC2 instances and modifying security group rules without using the appropriate\nchange control process. A solutions architect must devise a strategy to track and audit these inventory and\nconfiguration changes.\nWhich actions should the solutions architect take to meet these requirements? (Choose two.)",
    "options": {
      "A": "Enable AWS CloudTrail and use it for auditing: CloudTrail is an AWS service that enables governance,",
      "B": "Use data lifecycle policies for the Amazon EC2 instances: Data lifecycle policies are primarily used for",
      "C": "Enable AWS Trusted Advisor and reference the security dashboard: Trusted Advisor provides",
      "D": "Enable AWS Config and create rules for auditing and compliance purposes: AWS Config continuously"
    },
    "answer": "A",
    "explanation": "The correct answer is AD. Here's why:\nA. Enable AWS CloudTrail and use it for auditing: CloudTrail is an AWS service that enables governance,\ncompliance, operational auditing, and risk auditing of your AWS account. By enabling CloudTrail, you record\nAPI calls made within your AWS environment. This includes actions like launching EC2 instances, modifying\nsecurity group rules, and other configuration changes. You can then analyze the CloudTrail logs to track who\nmade the changes, when they were made, and from where. This addresses the need to track and audit\nconfiguration changes. https://aws.amazon.com/cloudtrail/\nD. Enable AWS Config and create rules for auditing and compliance purposes: AWS Config continuously\nmonitors and records the configuration of your AWS resources. It allows you to create rules that\nautomatically check whether resources comply with desired configurations. For instance, you can create a\nConfig rule to check if EC2 instances meet certain size criteria or if security groups have overly permissive\nrules. Config can also trigger remediation actions when resources are non-compliant. This satisfies the\nrequirement to track inventory and configuration changes and ensure compliance with company policies.\nhttps://aws.amazon.com/config/\nNow, let's consider why the other options are incorrect:\nB. Use data lifecycle policies for the Amazon EC2 instances: Data lifecycle policies are primarily used for\nmanaging the lifecycle of data stored in services like Amazon S3 or EBS volumes. They are not directly\nrelevant to tracking and auditing configuration changes or enforcing instance sizing and security group\npolicies.\nC. Enable AWS Trusted Advisor and reference the security dashboard: Trusted Advisor provides\nrecommendations on security, cost optimization, performance, and fault tolerance based on best practices.\nWhile it offers general security advice, it does not provide detailed, auditable logs of specific configuration\nchanges made by users. It is more of a reactive advisory service than a continuous auditing and compliance\ntool.\nE. Restore previous resource configurations with an AWS CloudFormation template: While CloudFormation\ncan be used for infrastructure as code and versioning of configurations, using it solely for restoring previous\nconfigurations is not a proactive solution for tracking and auditing changes as they happen. It's more of a\ndisaster recovery approach than a continuous monitoring and auditing system. CloudTrail and Config are\nmuch more suitable for those purposes.",
    "links": [
      "https://aws.amazon.com/cloudtrail/",
      "https://aws.amazon.com/config/"
    ]
  },
  {
    "question": "CertyIQ\nA company has hundreds of Amazon EC2 Linux-based instances in the AWS Cloud. Systems administrators have\nused shared SSH keys to manage the instances. After a recent audit, the companys security team is mandating\nthe removal of all shared keys. A solutions architect must design a solution that provides secure access to the EC2\ninstances.\nWhich solution will meet this requirement with the LEAST amount of administrative overhead?",
    "options": {
      "A": "Use AWS Systems Manager Session Manager to connect to the EC2 instances.",
      "B": "Use AWS Security Token Service (AWS STS) to generate one-time SSH keys on demand.",
      "C": "Allow shared SSH access to a set of bastion instances. Configure all other instances to allow only SSH",
      "D": "Use an Amazon Cognito custom authorizer to authenticate users. Invoke an AWS Lambda function to"
    },
    "answer": "A",
    "explanation": "The correct answer is A: Use AWS Systems Manager Session Manager to connect to the EC2 instances.\nHere's a detailed justification:\nThe security team requires the removal of shared SSH keys due to security risks. Session Manager addresses\nthis by eliminating the need for SSH keys altogether. Session Manager uses the AWS Systems Manager\nAgent (SSM Agent) on the EC2 instance to establish a secure connection to the EC2 instance through the\nAWS cloud, removing the need for open inbound SSH ports (port 22) and managed SSH keys.\nOption B, using AWS STS to generate one-time SSH keys, would be a more complex solution that still involves\nmanaging SSH keys, albeit temporary ones. This adds administrative overhead compared to Session\nManager's keyless approach. Moreover, integrating STS for temporary SSH key generation would necessitate\ncustom scripting and key management, significantly increasing complexity.\nOption C, bastion hosts, introduces a single point of failure and requires configuring security groups to allow\nSSH access from the bastion hosts, adding to the administrative overhead. While it restricts access compared\nto shared keys across all servers, it doesn't eliminate the need for SSH keys entirely and presents a\nmaintenance burden.\nOption D, using Cognito with a custom authorizer and Lambda for SSH keys, is far more complex and requires\nsignificant development and operational overhead to maintain and manage. It's an overkill for the stated\nrequirement.\nSession Manager offers the least administrative overhead because it leverages the existing SSM Agent\n(which is often already installed for patching and other management tasks), provides a secure and auditable\nconnection, and eliminates the complexities associated with managing SSH keys. It centralizes access\nmanagement through IAM roles, providing granular control over who can access which instances.\nTherefore, Session Manager presents the most straightforward and secure solution, adhering to security\nrequirements while minimizing administrative overhead.\nRelevant links:\nAWS Systems Manager Session Manager: https://docs.aws.amazon.com/systems-\nmanager/latest/userguide/session-manager.html\nEliminating the need for SSH keys: https://aws.amazon.com/blogs/security/simplify-secure-access-to-ec2-\ninstances-with-amazon-ec2-instance-connect-ssh/ (while this article talks about EC2 Instance Connect, the\nprinciples of eliminating direct SSH key management are similar to Session Manager)",
    "links": [
      "https://docs.aws.amazon.com/systems-",
      "https://aws.amazon.com/blogs/security/simplify-secure-access-to-ec2-"
    ]
  },
  {
    "question": "CertyIQ\nA company is using a fleet of Amazon EC2 instances to ingest data from on-premises data sources. The data is in\nJSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is rebooted, the data in-flight is\nlost. The companys data science team wants to query ingested data in near-real time.\nWhich solution provides near-real-time data querying that is scalable with minimal data loss?",
    "options": {
      "A": "Publish data to Amazon Kinesis Data Streams, Use Kinesis Data Analytics to query the data.",
      "B": "Publish data to Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift",
      "C": "Store ingested data in an EC2 instance store. Publish data to Amazon Kinesis Data Firehose with Amazon S3",
      "D": "Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Publish data to Amazon"
    },
    "answer": "A",
    "explanation": "The best solution is A: Publish data to Amazon Kinesis Data Streams, Use Kinesis Data Analytics to query\nthe data.\nHere's why:\nNear Real-Time Querying: Kinesis Data Streams is designed for real-time data streaming and processing.\nKinesis Data Analytics enables you to query the streamed data with SQL in near real-time. This directly\naddresses the requirement for near-real-time querying.\nScalability: Kinesis Data Streams is highly scalable, capable of handling high ingestion rates like 1 MB/s by\nadjusting the number of shards. This ensures the solution can scale as data volume increases.\nMinimal Data Loss: Kinesis Data Streams provides data durability and fault tolerance. Even if an EC2 instance\nreboots, the data in the stream is not lost (within the configured retention period). This minimizes data loss\nduring instance restarts.\nAlternative A weaknesses:\nKinesis Data Firehose is a loading service, not a querying service. While it can load data into data stores, it\ndoesn't allow for interactive, near-real-time queries.\nAmazon Redshift is a data warehouse designed for analytical workloads. It's not optimized for near-real-time\nquerying of streaming data.\nEC2 instance store are ephemeral. Data is lost when instance stops or terminates.\nAmazon S3 is an object store. Querying this type of storage for near-real-time querying could potentially add\nlatency.\nAmazon Athena can query the data. But it will be on a more ad hoc basis.\nAmazon Elastic Block Store are durable but storing data on EBS and using ElastiCache is not a good\narchitectural fit.\nTherefore, Kinesis Data Streams and Kinesis Data Analytics together provide a scalable, near-real-time\nquerying solution with minimal data loss, making option A the most suitable.\nAmazon Kinesis Data Streams:https://aws.amazon.com/kinesis/data-streams/\nAmazon Kinesis Data Analytics:https://aws.amazon.com/kinesis/data-analytics/",
    "links": [
      "https://aws.amazon.com/kinesis/data-streams/",
      "https://aws.amazon.com/kinesis/data-analytics/"
    ]
  },
  {
    "question": "CertyIQ\nWhat should a solutions architect do to ensure that all objects uploaded to an Amazon S3 bucket are encrypted?",
    "options": {
      "C": "The server-side encryption applied depends on the particular encryption key specified. This header is"
    },
    "answer": "D",
    "explanation": "The correct answer is D because it directly enforces server-side encryption for all objects uploaded to the S3\nbucket. The x-amz-server-side-encryption header, when present in a PutObject request, instructs S3 to encrypt\nthe object using server-side encryption (SSE). A bucket policy configured to deny uploads lacking this header\neffectively mandates that all new objects are encrypted upon storage.\nOption A and B focus on Access Control Lists (ACLs), not encryption. While ACLs control access permissions,\nthey do not provide data encryption. Option A is incorrect because requiring any ACL doesn't guarantee\nencryption. Option B restricting to \"private\" ACL also controls access and not encryption.\nOption C, referencing aws:SecureTransport, is related to requiring HTTPS for data transmission. While using\nHTTPS secures data in transit, it doesn't guarantee that the data is encrypted at rest in S3 storage. Therefore,\nit does not fulfill the requirement of ensuring that all objects uploaded to an Amazon S3 bucket are\nencrypted.\nBy denying any PutObject request that does not include the x-amz-server-side-encryption header, the policy\nenforces that all objects are encrypted at rest using server-side encryption, such as SSE-S3, SSE-KMS, or\nSSE-C. The server-side encryption applied depends on the particular encryption key specified. This header is\nthe mechanism S3 provides for enforcing server-side encryption through bucket policies.\nFor more information on S3 bucket policies and server-side encryption, refer to the following AWS\ndocumentation:\nUsing server-side encryption with bucket policies:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-policy-using-sse.html\nProtecting data using server-side encryption:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html\nSpecifying Encryption in a Request: https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-policy-using-sse.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is designing a multi-tier application for a company. The application's users upload images\nfrom a mobile device. The application generates a thumbnail of each image and returns a message to the user to\nconfirm that the image was uploaded successfully.\nThe thumbnail generation can take up to 60 seconds, but the company wants to provide a faster response time to\nits users to notify them that the original image was received. The solutions architect must design the application to\nasynchronously dispatch requests to the different application tiers.\nWhat should the solutions architect do to meet these requirements?",
    "options": {
      "A": "Write a custom AWS Lambda function to generate the thumbnail and alert the user. Use the image upload",
      "B": "Create an AWS Step Functions workflow. Configure Step Functions to handle the orchestration between the",
      "C": "Create an Amazon Simple Queue Service (Amazon SQS) message queue. As images are uploaded, place a",
      "D": "Create Amazon Simple Notification Service (Amazon SNS) notification topics and subscriptions. Use one"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it provides the most efficient and decoupled method for asynchronous\nprocessing of image uploads and thumbnail generation, enabling a faster response time for the user.\nHere's why:\nDecoupling with SQS: Amazon SQS (Simple Queue Service) decouples the image upload process from the\nthumbnail generation process. The application places a message containing information about the uploaded\nimage onto the SQS queue. This allows the image upload process to complete quickly and immediately\nacknowledge the user. The user receives confirmation of receipt without waiting for the resource-intensive\nthumbnail generation. https://aws.amazon.com/sqs/\nAsynchronous Processing: The thumbnail generation process, triggered by messages in the SQS queue, can\nthen proceed asynchronously. A separate worker (e.g., an EC2 instance, Lambda function, or ECS task) can\nconsume messages from the queue and generate the thumbnail at its own pace. This addresses the\nrequirement to handle potentially time-consuming thumbnail generation without impacting the user's\nperceived performance.\nScalability and Reliability: SQS is a highly scalable and reliable messaging service. It can handle a large\nvolume of messages, ensuring that no image upload requests are lost. The queue also provides buffering\ncapabilities, allowing the thumbnail generation process to keep up with varying workloads.\nAlternative A's drawbacks: While Lambda can generate thumbnails, using the image upload as a direct event\nsource tightly couples the upload process with the thumbnail generation. This defeats the purpose of\nproviding a fast response to the user. Lambda invocations have time limits, which may be exceeded by long\nthumbnail generations.\nAlternative B's drawbacks: Step Functions are more suited for complex workflows with multiple steps and\ndecision points. For a simple scenario like this, SQS offers a more lightweight and efficient solution. Step\nfunctions would introduce unnecessary complexity and overhead.\nAlternative D's drawbacks: SNS is primarily designed for fan-out notifications to multiple subscribers. While\nit could be used for this scenario, it's less suitable than SQS for asynchronous task processing, especially\nwhen message reliability and guaranteed delivery are important. SNS delivers messages to all subscribers\nimmediately, which doesn't create a queue for later processing.\nIn summary, SQS enables decoupling, asynchronous processing, scalability, and reliability, providing the best\nsolution for achieving the desired faster response time for users while managing potentially lengthy\nthumbnail generation processes.",
    "links": [
      "https://aws.amazon.com/sqs/"
    ]
  },
  {
    "question": "CertyIQ\nA companys facility has badge readers at every entrance throughout the building. When badges are scanned, the\nreaders send a message over HTTPS to indicate who attempted to access that particular entrance.\nA solutions architect must design a system to process these messages from the sensors. The solution must be\nhighly available, and the results must be made available for the companys security team to analyze.\nWhich system architecture should the solutions architect recommend?",
    "options": {
      "A": "Launch an Amazon EC2 instance to serve as the HTTPS endpoint and to process the messages. Configure the",
      "B": "Create an HTTPS endpoint in Amazon API Gateway. Configure the API Gateway endpoint to invoke an AWS",
      "C": "Use Amazon Route 53 to direct incoming sensor messages to an AWS Lambda function. Configure the",
      "D": "Create a gateway VPC endpoint for Amazon S3. Configure a Site-to-Site VPN connection from the facility"
    },
    "answer": "B",
    "explanation": "The correct answer is B, which leverages API Gateway and Lambda for message processing and DynamoDB\nfor data storage. Here's why:\nHigh Availability: API Gateway is a fully managed service that automatically scales and provides high\navailability. Similarly, Lambda functions are automatically scaled by AWS based on demand, ensuring the\nsystem can handle a large volume of messages from the badge readers without manual intervention.\nDynamoDB is also a fully managed, highly available NoSQL database.\nHTTPS Endpoint: API Gateway allows you to easily create secure HTTPS endpoints for receiving data from\nthe badge readers. This is a crucial requirement as the problem statement specified HTTPS communication\nfrom the sensors.\nServerless Architecture: Lambda allows the solution to be serverless. You don't need to provision or manage\nservers, reducing operational overhead and cost. The Lambda function contains the code to process the\nmessages from the badge readers.\nData Persistence: DynamoDB is a NoSQL database that is well-suited for storing structured or semi-\nstructured data from the badge readers. It offers fast read and write speeds and the flexibility to handle\nvarying data structures.\nScalability: API Gateway, Lambda, and DynamoDB are all designed for scalability. As the number of badge\nreaders or the frequency of scans increases, the system can automatically scale to handle the increased load.\nSecurity: API Gateway offers features like authentication and authorization to protect the endpoint from\nunauthorized access. Lambda can use IAM roles to access DynamoDB securely.\nWhy other options are less suitable:\nA (EC2 instance): EC2 instances require manual management (patching, scaling) which is not ideal for high\navailability and creates operational overhead. Also, EC2 is more expensive to run continuously compared to\nserverless options.\nC (Route 53 and Lambda): Route 53 is primarily for DNS routing, not for directly receiving and processing\nHTTPS requests. It doesn't act as an HTTPS endpoint.\nD (S3 and VPN): While S3 is good for storage, the sensor data likely needs processing before storage. Also,\nwriting data directly to S3 without any processing in between introduces security concerns (data validation,\netc.). It also requires a VPN which adds complexity. This method is only suitable if the sensor data can be\ndirectly and securely persisted without modification.\nAuthoritative Links:\nAmazon API Gateway: https://aws.amazon.com/api-gateway/\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon DynamoDB: https://aws.amazon.com/dynamodb/",
    "links": [
      "https://aws.amazon.com/api-gateway/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/dynamodb/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to implement a disaster recovery plan for its primary on-premises file storage volume. The file\nstorage volume is mounted from an Internet Small Computer Systems Interface (iSCSI) device on a local storage\nserver. The file storage volume holds hundreds of terabytes (TB) of data.\nThe company wants to ensure that end users retain immediate access to all file types from the on-premises\nsystems without experiencing latency.\nWhich solution will meet these requirements with the LEAST amount of change to the company's existing\ninfrastructure?",
    "options": {
      "A": "Provision an Amazon S3 File Gateway as a virtual machine (VM) that is hosted on premises. Set the local",
      "B": "Provision an AWS Storage Gateway tape gateway. Use a data backup solution to back up all existing data to",
      "C": "Provision an AWS Storage Gateway Volume Gateway cached volume. Set the local cache to 10 TB. Mount the",
      "D": "Here's why:"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Here's why:\nRequirement: The company needs a disaster recovery plan for a large (hundreds of TB) on-premises file\nstorage volume with immediate access and minimal latency. They want to minimize changes to their existing\ninfrastructure.\nWhy D is the best choice (Volume Gateway - Stored Volume):\nStored Volume Configuration: A Stored Volume Gateway stores all data locally on-premises first. This is\ncrucial for maintaining immediate access and low latency, as the data is directly available on-site. The initial\nsynchronization moves all the existing hundreds of TB of data to AWS Storage Gateway.\niSCSI Integration: It utilizes iSCSI, the same protocol the company is already using, minimizing infrastructure\nchanges. The gateway integrates with the existing iSCSI device, providing seamless compatibility.\nSnapshots for DR: Scheduled snapshots capture point-in-time copies of the data and store them in AWS. This\nprovides a reliable mechanism for recovery.\nDR Recovery: In a disaster, the snapshots can be restored to an Amazon EBS volume and attached to an EC2\ninstance, enabling a functional file server in AWS. This ensures business continuity.\nWhy other options are incorrect:\nA (S3 File Gateway): File Gateway uses S3 as the primary storage, which introduces latency to access the\ndata. Furthermore, it requires modifying existing applications to use the NFS protocol, adding complexity.\nB (Tape Gateway): Tape Gateway is for archival purposes, not for immediate access disaster recovery.\nRestoring from tapes is a slow process and wouldn't meet the \"immediate access\" requirement.\nC (Volume Gateway - Cached Volume): While it uses iSCSI, a cached volume only stores the frequently\naccessed data locally. With hundreds of TB, a 10 TB cache would mean that most data would reside in AWS,\nleading to high latency for access.\nIn conclusion, the Stored Volume Gateway configuration maintains local data access for low latency,\nintegrates seamlessly with existing iSCSI infrastructure, and leverages AWS snapshots for reliable disaster\nrecovery.\nAWS Storage Gateway Documentation:\nhttps://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html\nVolume Gateway - Stored Volume details:\nhttps://docs.aws.amazon.com/storagegateway/latest/userguide/stored-volumes.html",
    "links": [
      "https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html",
      "https://docs.aws.amazon.com/storagegateway/latest/userguide/stored-volumes.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is hosting a web application from an Amazon S3 bucket. The application uses Amazon Cognito as an\nidentity provider to authenticate users and return a JSON Web Token (JWT) that provides access to protected\nresources that are stored in another S3 bucket.\nUpon deployment of the application, users report errors and are unable to access the protected content. A\nsolutions architect must resolve this issue by providing proper permissions so that users can access the protected\ncontent.\nWhich solution meets these requirements?",
    "options": {
      "A": "Update the Amazon Cognito identity pool to assume the proper IAM role for access to the protected content.",
      "B": "Update the S3 ACL to allow the application to access the protected content.",
      "C": "Redeploy the application to Amazon S3 to prevent eventually consistent reads in the S3 bucket from",
      "D": "Update the Amazon Cognito pool to use custom attribute mappings within the identity pool and grant users"
    },
    "answer": "A",
    "explanation": "The correct answer is A: Update the Amazon Cognito identity pool to assume the proper IAM role for access\nto the protected content.\nHere's a detailed justification:\nThe core problem is users, authenticated through Cognito, are failing to access protected S3 content. Cognito\nIdentity Pools are designed precisely to grant temporary AWS credentials to users, allowing them to access\nAWS resources. The crucial step is ensuring that the Cognito Identity Pool is configured with an IAM role that\nhas the necessary permissions to read (or otherwise access) the protected S3 bucket.\nCognito provides two types of identities: authenticated and unauthenticated. Regardless of identity type,\nCognito assumes an IAM role on behalf of the user. This IAM role dictates what AWS resources the user can\naccess. If the IAM role assigned to the Cognito Identity Pool lacks S3 read permissions for the protected\nbucket, users will be denied access, hence the errors.\nUpdating the Identity Pool to assume an IAM role with the required s3:GetObject (and potentially other actions\nlike s3:ListBucket if listing the contents of the bucket is needed) permission on the protected S3 bucket\nresolves the issue. This approach adheres to the principle of least privilege: the user only gets the permissions\nneeded to access the protected resources, and the permissions are temporary, granted by AWS STS (Security\nToken Service) behind the scenes.\nOption B is incorrect because S3 ACLs are a legacy access control mechanism and are generally not\nrecommended for granting access to users authenticated via Cognito. IAM roles and policies offer much finer-\ngrained control and better integration with identity providers.\nOption C is unrelated to the authentication and authorization problem. S3's eventual consistency is a factor to\nconsider when designing applications, but it's not the root cause here. The problem is that the authenticated\nusers lack the permissions to read the data in the first place.\nOption D is less efficient and potentially less secure than using IAM roles directly. While custom attribute\nmappings within Cognito can pass user attributes to IAM policies, it's generally simpler and more\nmaintainable to assign users to Cognito Identity Pools that assume predefined IAM roles. Moreover, simply\nmapping attributes doesn't automatically grant permissions; the policies still need to be defined and\nassociated with the role. Directly updating the IAM role associated with the Identity Pool is the most direct\nand appropriate solution.\nIn summary, Cognito Identity Pools linked to appropriate IAM roles provide a robust and secure mechanism for\ngranting authenticated users access to protected AWS resources.\nSupporting links:\nAWS Cognito Identity Pools: https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html\nIAM Roles: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\nS3 Permissions Overview: https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html",
    "links": [
      "https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html"
    ]
  },
  {
    "question": "CertyIQ\nAn image hosting company uploads its large assets to Amazon S3 Standard buckets. The company uses multipart\nupload in parallel by using S3 APIs and overwrites if the same object is uploaded again. For the first 30 days after\nupload, the objects will be accessed frequently. The objects will be used less frequently after 30 days, but the\naccess patterns for each object will be inconsistent. The company must optimize its S3 storage costs while\nmaintaining high availability and resiliency of stored assets.\nWhich combination of actions should a solutions architect recommend to meet these requirements? (Choose two.)",
    "options": {
      "A": "Move assets to S3 Intelligent-Tiering after 30 days:",
      "B": "Configure an S3 Lifecycle policy to clean up incomplete multipart uploads:",
      "C": "Configure an S3 Lifecycle policy to clean up expired object delete markers: Object delete markers are",
      "D": "Move assets to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days: This is less optimal than"
    },
    "answer": "A",
    "explanation": "Here's a detailed justification for why options A and B are the correct choices, and why the others are not:\nA. Move assets to S3 Intelligent-Tiering after 30 days:\nS3 Intelligent-Tiering is designed for data with unknown or changing access patterns. It automatically moves\ndata between frequent, infrequent, and archive access tiers based on actual access patterns, optimizing costs\nwithout performance impact.\nThe question states that access patterns for each object are inconsistent after 30 days. Intelligent-Tiering is\nideal for this scenario, as it automatically adapts to these changing patterns.\nStandard-IA is suitable for data accessed less frequently, but requires you to know when you need to change\ntier. Intelligent-Tiering automates this decision.\nB. Configure an S3 Lifecycle policy to clean up incomplete multipart uploads:\nThe company uses multipart upload and overwrites objects. Incomplete multipart uploads (if a part fails to\nupload or the upload is aborted) consume storage space unnecessarily, increasing costs.\nAn S3 Lifecycle policy can automatically clean up these incomplete uploads after a specified period.\nMultipart uploads are used for large objects and are prone to issues, making this a crucial optimization.\nWhy the other options are incorrect:\nC. Configure an S3 Lifecycle policy to clean up expired object delete markers: Object delete markers are\nsmall and their cost is insignificant. It is not an area that needs optimisation.\nD. Move assets to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days: This is less optimal than\nIntelligent-Tiering because it requires you to know the access pattern.\nE. Move assets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days: While One Zone-IA is\ncheaper, it compromises data availability and resiliency by storing data in a single availability zone. The\nquestion specifies a need for \"high availability and resiliency,\" making this option unsuitable.\nSupporting Links:\nS3 Intelligent-Tiering: https://aws.amazon.com/s3/storage-classes/intelligent-tiering/\nS3 Lifecycle Policies: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-\nexamples.html\nMultipart Upload: https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/intelligent-tiering/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 instances contain\nhighly sensitive data and run in a private subnet. According to company policy, the EC2 instances that run in the\nVPC can access only approved third-party software repositories on the internet for software product updates that\nuse the third partys URL. Other internet traffic must be blocked.\nWhich solution meets these requirements?",
    "options": {
      "B": "Use a URL-based rule listener in the ALBs target group for outbound access to the internet."
    },
    "answer": "A",
    "explanation": "The correct answer is A: Update the route table for the private subnet to route the outbound traffic to an AWS\nNetwork Firewall firewall. Configure domain list rule groups.\nHere's why:\nRequirement for granular control: The scenario necessitates restricting outbound traffic to specific,\napproved third-party software repositories identified by their URLs. Security groups and NACLs, while useful\nfor basic traffic filtering, don't offer URL-based filtering. AWS WAF is designed for protecting web\napplications from common exploits and doesn't typically handle generic outbound traffic filtering from EC2\ninstances. An ALB is primarily designed for handling inbound application traffic and is not suited for routing\nand filtering general outbound internet traffic from EC2 instances.\nAWS Network Firewall: AWS Network Firewall provides centralized network protection for VPCs. It allows\ndefining fine-grained rules based on domain names, URLs, and other criteria, making it ideal for meeting the\nrequirement of allowing access only to specific software repositories.\nDomain list rule groups: Network Firewall utilizes domain list rule groups, which enable specifying a list of\nallowed or denied domain names. This aligns directly with the requirement of permitting access only to the\nURLs of the approved software repositories.\nPrivate Subnet & Route Table: The EC2 instances reside in a private subnet, meaning they don't have direct\naccess to the internet. Configuring the route table to route outbound traffic to the Network Firewall ensures\nthat all traffic from these instances is inspected and filtered according to the defined rules.\nSecurity Best Practices: Using a centralized firewall solution like Network Firewall promotes consistent\nsecurity policies across the entire VPC, adhering to security best practices for network segmentation and\ntraffic inspection.\nWhy other options are incorrect:\nB (AWS WAF): AWS WAF is designed for web application protection and primarily deals with HTTP/HTTPS\ntraffic targeting web servers. It's not suitable for filtering all outbound traffic from EC2 instances.\nC (Security Groups): Security groups operate at the instance level and are stateful firewalls that allow or\ndeny traffic based on IP addresses and ports. They lack the ability to filter traffic based on URLs.\nD (ALB): Application Load Balancers are for distributing incoming application traffic. They are not designed\nfor routing and filtering general outbound traffic from EC2 instances in a private subnet.\nAuthoritative Links:\nAWS Network Firewall: https://aws.amazon.com/network-firewall/\nAWS Network Firewall Rule Groups: https://docs.aws.amazon.com/network-\nfirewall/latest/developerguide/rule-groups.html",
    "links": [
      "https://aws.amazon.com/network-firewall/",
      "https://docs.aws.amazon.com/network-"
    ]
  },
  {
    "question": "CertyIQ\nA company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts the website on\nAmazon S3 and integrates the website with an API that handles sales requests. The company hosts the API on\nthree Amazon EC2 instances behind an Application Load Balancer (ALB). The API consists of static and dynamic\nfront-end content along with backend workers that process sales requests asynchronously.\nThe company is expecting a significant and sudden increase in the number of sales requests during events for the\nlaunch of new products.\nWhat should a solutions architect recommend to ensure that all the requests are processed successfully?",
    "options": {
      "A": "Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2 instances to",
      "B": "Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto Scaling",
      "C": "Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache instance in",
      "D": "Here's why:"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Here's why:\nThe problem describes a scenario where a company anticipates a surge in sales requests during product\nlaunches, potentially overwhelming their API hosted on EC2 instances. The goal is to ensure all requests are\nprocessed successfully despite the load spike.\nOption D addresses this problem effectively by:\n1. Caching static content using Amazon CloudFront: Serving static content (like images, CSS,\nJavaScript) from CloudFront reduces the load on the EC2 instances, freeing them up to handle\ndynamic API requests. CloudFront distributes the content globally, improving website performance\nfor users worldwide. https://aws.amazon.com/cloudfront/\n2. Using Amazon SQS to decouple the web application from the API: Introducing an SQS queue\nbetween the website and the API creates a buffer. The website can quickly submit sales requests to\nthe SQS queue without waiting for the API to process them immediately. This prevents the website\nfrom being overwhelmed and ensures requests are not lost. https://aws.amazon.com/sqs/\n3. EC2 instances process messages from the SQS queue asynchronously: The EC2 instances, acting\nas workers, pull requests from the SQS queue at their own pace. Even during peak loads, the queue\nstores requests until the instances can process them, preventing request loss. This asynchronous\nprocessing ensures that all requests are eventually handled. This pattern implements a form of\nmessage queuing, promoting system resilience and scalability.\nWhy other options are less suitable:\nA: Caching dynamic content isn't typically beneficial as it changes frequently. Increasing the number of EC2\ninstances alone might not be enough to handle the sudden surge and doesn't guarantee requests won't be\ndropped.\nB: While caching static content with CloudFront is good, simply using an Auto Scaling group might not\nprevent the EC2 instances from being overwhelmed if the traffic spike is too sudden and large. Auto Scaling\ntakes time to provision new instances.\nC: Caching dynamic content with CloudFront is not suitable. ElastiCache can help with database caching but\ndoesn't directly address the problem of front-end request overload. It won't stop the EC2 instances from\nbeing overwhelmed by the initial surge of requests.\nIn summary, Option D combines caching static content with asynchronous processing via SQS to effectively\nhandle the expected traffic spikes and ensure all sales requests are successfully processed. The use of SQS\noffers resilience and scalability, which are crucial for handling unpredictable workloads.",
    "links": [
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/sqs/"
    ]
  },
  {
    "question": "CertyIQ\nA security audit reveals that Amazon EC2 instances are not being patched regularly. A solutions architect needs to\nprovide a solution that will run regular security scans across a large fleet of EC2 instances. The solution should\nalso patch the EC2 instances on a regular schedule and provide a report of each instances patch status.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Set up Amazon Macie to scan the EC2 instances for software vulnerabilities. Set up a cron job on each EC2",
      "B": "Turn on Amazon GuardDuty in the account. Configure GuardDuty to scan the EC2 instances for software",
      "C": "Set up Amazon Detective to scan the EC2 instances for software vulnerabilities. Set up an Amazon",
      "D": "Turn on Amazon Inspector in the account. Configure Amazon Inspector to scan the EC2 instances for"
    },
    "answer": "D",
    "explanation": "The correct answer is D because it leverages AWS services specifically designed for vulnerability scanning\nand patch management.\nAmazon Inspector is a vulnerability management service that automatically assesses EC2 instances for\nvulnerabilities and deviations from best practices. It provides detailed findings with severity levels and\nremediation recommendations. https://aws.amazon.com/inspector/\nAWS Systems Manager Patch Manager automates the process of patching managed instances, including EC2\ninstances, with security-related updates. It allows for scheduled patching, compliance scanning, and\nreporting on patch status. Patch Manager directly addresses the need for regular patching and reporting on\npatch status. https://aws.amazon.com/systems-manager/features/patch-manager/\nOption A is incorrect because Amazon Macie focuses on discovering and protecting sensitive data stored in\nAmazon S3. It doesn't scan EC2 instances for general software vulnerabilities. Cron jobs on individual\ninstances are less manageable and scalable compared to Systems Manager Patch Manager for a large fleet.\nOption B is incorrect because Amazon GuardDuty is a threat detection service that monitors for malicious\nactivity and unauthorized behavior. While it identifies threats, it does not provide vulnerability scanning or\npatch management capabilities. Session Manager facilitates secure instance access but doesn't handle the\npatching process itself.\nOption C is incorrect because Amazon Detective analyzes log data to investigate security incidents. It doesn't\nscan for vulnerabilities or manage patching. While EventBridge can trigger actions on a schedule, it lacks the\nintegrated patch management capabilities of Systems Manager Patch Manager, such as dependency\nresolution, reboot management, and reporting.",
    "links": [
      "https://aws.amazon.com/inspector/",
      "https://aws.amazon.com/systems-manager/features/patch-manager/"
    ]
  },
  {
    "question": "CertyIQ\nA company is planning to store data on Amazon RDS DB instances. The company must encrypt the data at rest.\nWhat should a solutions architect do to meet this requirement?",
    "options": {
      "A": "Create a key in AWS Key Management Service (AWS KMS). Enable encryption for the DB instances.",
      "B": "Create an encryption key. Store the key in AWS Secrets Manager. Use the key to encrypt the DB instances.",
      "C": "Generate a certificate in AWS Certificate Manager (ACM). Enable SSL/TLS on the DB instances by using the",
      "D": "Generate a certificate in AWS Identity and Access Management (IAM). Enable SSL/TLS on the DB instances"
    },
    "answer": "A",
    "explanation": "The correct answer is A because it leverages AWS KMS, the recommended and secure way to encrypt data at\nrest in RDS. AWS KMS allows you to create and manage encryption keys centrally and securely. Enabling\nencryption for an RDS DB instance using a KMS key means the data is encrypted at rest using industry-\nstandard encryption algorithms. RDS integrates seamlessly with KMS, handling the encryption and decryption\noperations transparently.\nOption B is incorrect. While AWS Secrets Manager is used for storing secrets like database credentials, it's\nnot the ideal tool for managing encryption keys for RDS data at rest. KMS is specifically designed for key\nmanagement and offers features like key rotation and auditing, which are critical for security and compliance.\nOptions C and D are incorrect because ACM and IAM certificates are used for encrypting data in transit using\nSSL/TLS, not for encrypting data at rest. SSL/TLS secures the connection between the client and the\ndatabase server, preventing eavesdropping, but it doesn't protect the data stored on the disk. Encryption at\nrest protects data if the underlying storage is compromised.\nTherefore, leveraging KMS for RDS encryption addresses the security requirement of encrypting data at rest\nin a secure, manageable, and compliant manner. This is best practice and directly aligned with AWS's\nrecommended architecture.\nhttps://docs.aws.amazon.com/kms/latest/developerguide/overview.htmlhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/data-\nat-rest-encryption.html",
    "links": [
      "https://docs.aws.amazon.com/kms/latest/developerguide/overview.htmlhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/data-"
    ]
  },
  {
    "question": "CertyIQ\nA company must migrate 20 TB of data from a data center to the AWS Cloud within 30 days. The companys\nnetwork bandwidth is limited to 15 Mbps and cannot exceed 70% utilization.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Use AWS Snowball.",
      "B": "Use AWS DataSync: DataSync is a good choice for ongoing replication of data, especially when there's a",
      "C": "Use a secure VPN connection: A VPN secures the data transfer between the on-premises data center and",
      "D": "Use Amazon S3 Transfer Acceleration: S3 Transfer Acceleration uses Amazon CloudFront's globally"
    },
    "answer": "A",
    "explanation": "Here's a detailed justification for why the correct answer is A (Use AWS Snowball) and why the other options\nare less suitable, considering the constraints of the scenario:\nWhy AWS Snowball is the Best Choice:\nThe primary bottleneck is the limited network bandwidth (15 Mbps with a 70% utilization cap) and the large\ndata volume (20 TB) within a strict timeframe (30 days). Calculating the transfer time using the available\nbandwidth is crucial:\nUsable bandwidth: 15 Mbps * 70% = 10.5 Mbps\nData volume: 20 TB = 20 1024 GB = 20480 GB = 20480 8 bits = 163840 Gb\nEstimated transfer time: 163840 Gb / 10.5 Mbps = ~15603 seconds/Gb = 15603 * (1/3600) hours/Gb = ~4.33\nhours/Gb\nTotal time: 4.33 hours/Gb * 20480 Gb = ~88700 hours = ~3696 days\nThis calculation clearly shows that transferring 20 TB of data over a 10.5 Mbps connection would take far\nlonger than the allowed 30 days. AWS Snowball (now part of AWS Snow Family) offers a physical appliance\nto securely transfer large amounts of data into and out of AWS. You ship the appliance, load it with your data\nin your data center, and then ship it back to AWS, where the data is uploaded to S3. This bypasses the\nnetwork constraint. Snowball offers significant advantages in cost and time when dealing with substantial\ndata volumes and limited bandwidth, fitting the scenario perfectly. The data is also encrypted in transit and at\nrest for security.\nWhy Other Options are Less Suitable:\nB. Use AWS DataSync: DataSync is a good choice for ongoing replication of data, especially when there's a\nreasonable network connection. However, with only 10.5 Mbps available and 20 TB to transfer initially, it's\nsimply too slow to meet the 30-day deadline, as demonstrated by the calculations above. DataSync utilizes\nthe network, so it is constrained by the bandwidth limitation.\nC. Use a secure VPN connection: A VPN secures the data transfer between the on-premises data center and\nAWS. However, it doesn't address the fundamental problem of limited bandwidth. Using a VPN on top of an\nalready constrained 15 Mbps connection only adds overhead (reducing the effective bandwidth further),\nmaking the transfer even slower.\nD. Use Amazon S3 Transfer Acceleration: S3 Transfer Acceleration uses Amazon CloudFront's globally\ndistributed edge locations to accelerate uploads to S3. While it can improve transfer speeds, it still relies on\nthe internet connection. With a very constrained bandwidth (10.5 Mbps), S3 Transfer Acceleration won't\nprovide nearly enough acceleration to transfer 20 TB within 30 days. The initial bandwidth limitations still\napply.\nAuthoritative Links for Further Research:\nAWS Snow Family: https://aws.amazon.com/snowball/\nAWS DataSync: https://aws.amazon.com/datasync/\nAmazon S3 Transfer Acceleration: https://aws.amazon.com/s3/transfer-acceleration/",
    "links": [
      "https://aws.amazon.com/snowball/",
      "https://aws.amazon.com/datasync/",
      "https://aws.amazon.com/s3/transfer-acceleration/"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to provide its employees with secure access to confidential and sensitive files. The company\nwants to ensure that the files can be accessed only by authorized users. The files must be downloaded securely to\nthe employees devices.\nThe files are stored in an on-premises Windows file server. However, due to an increase in remote usage, the file\nserver is running out of capacity.\n.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Migrate the file server to an Amazon EC2 instance in a public subnet. Configure the security group to limit",
      "B": "Migrate the files to an Amazon FSx for Windows File Server file system. Integrate",
      "C": "Migrate the files to Amazon S3, and create a private VPC endpoint. Create a signed URL to allow download.",
      "D": "Migrate the files to Amazon S3, and create a public VPC endpoint. Allow employees to sign on with AWS IAM"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Migrate the files to an Amazon FSx for Windows File Server file system. Integrate\nthe Amazon FSx file system with the on-premises Active Directory. Configure AWS Client VPN.\nHere's a detailed justification:\n1. Security Requirement: The company needs secure access to confidential files, limiting access to\nauthorized users only. Answer B addresses this by integrating Amazon FSx for Windows File Server\nwith the on-premises Active Directory. Active Directory is a directory service that allows central\nmanagement of users and security policies.\n2. Authorization: By integrating FSx with Active Directory, existing user accounts and permissions\ndefined in the on-premises environment are extended to the cloud file system. Only authorized users\nwill have access based on their defined roles.\n3. Secure Download: Using AWS Client VPN ensures that all data transferred between the employee's\ndevice and AWS is encrypted over a secure VPN tunnel. This satisfies the requirement of secure\ndownloads to employees' devices.\n4. Scalability: Amazon FSx for Windows File Server is a fully managed Windows file server in the cloud.\nMigrating the files to FSx solves the capacity issue of the on-premises server, as FSx can scale to\nmeet the company's growing storage needs.\n5. Public vs. Private Subnets (Option A): Option A suggests migrating to an EC2 instance in a public\nsubnet. Public subnets expose resources to the internet, creating security risks. Limiting inbound\ntraffic by IP address is not a scalable or secure solution, especially with remote employees who have\ndynamic IP addresses.\n6. S3 with Signed URLs/Public VPC Endpoint (Options C & D): Options C and D suggest storing files in\nAmazon S3. While S3 is excellent for object storage, it requires additional configuration to mimic the\nbehavior of a file server, such as shared access and file locking. Signed URLs can be complex to\nmanage at scale and are typically used for temporary access. Public VPC endpoints are generally\ndiscouraged for sensitive data.\n7. AWS IAM Identity Center (Option D): While using AWS IAM Identity Center (formerly AWS Single\nSign-On) can provide centralized authentication, it doesn't inherently guarantee secure file\ndownloads or integrate seamlessly with existing on-premises Active Directory permissions for file\naccess control without significant additional configuration.\n8. FSx for Windows File Server as a File Server: FSx for Windows File Server offers features such as\nSMB protocol support, Active Directory integration, and NTFS permissions that makes it appropriate\nfor organizations looking to move existing Windows file server workloads to AWS, without requiring\nsignificant changes to application code.\nIn summary, Option B is the only solution that meets all the requirements, offering secure access via VPN,\nintegration with existing Active Directory for authorization, and scalability through Amazon FSx for Windows\nFile Server.\nSupporting Links:\nAmazon FSx for Windows File Server: https://aws.amazon.com/fsx/windows/\nAWS Client VPN: https://aws.amazon.com/vpn/client-vpn/",
    "links": [
      "https://aws.amazon.com/fsx/windows/",
      "https://aws.amazon.com/vpn/client-vpn/"
    ]
  },
  {
    "question": "CertyIQ\nA companys application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances\nrun in an Amazon EC2 Auto Scaling group across multiple Availability Zones. On the first day of every month at\nmidnight, the application becomes much slower when the month-end financial calculation batch runs. This causes\nthe CPU utilization of the EC2 instances to immediately peak to 100%, which disrupts the application.\nWhat should a solutions architect recommend to ensure the application is able to handle the workload and avoid\ndowntime?",
    "options": {
      "A": "Configure an Amazon CloudFront distribution in front of the ALB.",
      "B": "Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization.",
      "C": "Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly",
      "D": "Configure Amazon ElastiCache to remove some of the workload from the EC2 instances."
    },
    "answer": "C",
    "explanation": "The correct answer is C. Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly\nschedule.\nHere's why:\nThe problem is a predictable, recurring spike in CPU utilization due to a month-end financial calculation that\nhappens on a fixed schedule. An Auto Scaling scheduled scaling policy directly addresses this predictable\nworkload. It allows you to proactively increase the number of EC2 instances before the workload hits,\nensuring enough capacity to handle the increased demand and prevent the CPU from maxing out. By scaling\nup in anticipation, the application remains responsive and avoids downtime. Scheduled scaling is ideal for\nworkloads that have consistent and repeating patterns.\nOption A (CloudFront) isn't suitable because CloudFront is a content delivery network (CDN). It helps\ndistribute static content and doesn't address the underlying processing bottleneck on the EC2 instances\nperforming the financial calculations. The issue isn't about content delivery speed but about compute\ncapacity.\nOption B (Simple scaling policy) isn't optimal. A simple scaling policy reacts after CPU utilization hits a\nthreshold. In this case, by the time the Auto Scaling group responds to the high CPU, the application has\nalready become slow or unresponsive, as the CPU has already peaked. While reactive scaling is useful,\nproactive scaling through scheduled actions is better for predictable spikes.\nOption D (ElastiCache) might help if the financial calculations involve frequent reads of the same data, which\nisn't specified. However, it doesn't address the core problem of needing more compute power to process the\nfinancial calculations themselves. Furthermore, implementing ElastiCache could involve significant\napplication code changes, making it a more complex solution than scheduled scaling. Even if caching\nimproves some aspects, the underlying computational demand still exists and needs to be managed with\ncapacity. Scheduled scaling handles the capacity increase preemptively, directly addressing the workload\nspike.\nIn summary, scheduled scaling is the most efficient and targeted solution because it precisely addresses the\npredictable workload and avoids the reactive nature of other Auto Scaling policies.\nAuthoritative Links:\nAWS Auto Scaling Scheduled Scaling:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\nAWS Auto Scaling Concepts:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html",
    "links": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to give a customer the ability to use on-premises Microsoft Active Directory to download files\nthat are stored in Amazon S3. The customers application uses an SFTP client to download the files.\nWhich solution will meet these requirements with the LEAST operational overhead and no changes to the\ncustomers application?",
    "options": {
      "A": "Set up AWS Transfer Family with SFTP for Amazon S3. Configure integrated Active Directory authentication.",
      "B": "Set up AWS Database Migration Service (AWS DMS) to synchronize the on-premises client with Amazon S3.",
      "C": "Set up AWS DataSync to synchronize between the on-premises location and the S3 location by using AWS",
      "D": "Set up a Windows Amazon EC2 instance with SFTP to connect the on-premises client with Amazon S3."
    },
    "answer": "A",
    "explanation": "The correct answer is A because it directly addresses the requirements with the least operational overhead\nand no changes to the customer's application. AWS Transfer Family with SFTP for Amazon S3 natively\nsupports SFTP protocol to access S3 buckets. It also provides built-in integration with Microsoft Active\nDirectory for authentication. This integration allows users to use their existing Active Directory credentials to\nauthenticate and access the S3 bucket via SFTP. It avoids the need for custom authentication mechanisms or\napplication changes.\nOption B is incorrect because AWS DMS is for database migrations and does not support SFTP or file transfer\nscenarios. It is unsuitable for synchronizing files between an on-premises client and S3.\nOption C is incorrect because AWS DataSync is used to synchronize data between on-premises storage and\nAWS storage services. While it can sync with S3, it does not directly support SFTP. Also, configuring AWS\nIAM Identity Center (formerly AWS SSO) would require changes to the client application and is not designed\nfor native SFTP authentication against Active Directory.\nOption D is incorrect because setting up an EC2 instance involves more operational overhead than using AWS\nTransfer Family. It would require managing the EC2 instance, installing and configuring an SFTP server, and\nhandling authentication against Active Directory manually using more complex configurations compared to\nAWS Transfer Family's built-in Active Directory integration. Also, this option requires more management and\npotentially higher costs than using the fully managed AWS Transfer Family service.\nIn conclusion, AWS Transfer Family provides a managed service that integrates directly with Active Directory\nand S3 via SFTP, fulfilling all requirements with minimal configuration and operational overhead.\nHere are some authoritative links for further research:\nAWS Transfer Family: https://aws.amazon.com/transfer/\nUsing Active Directory authentication with AWS Transfer Family:\nhttps://docs.aws.amazon.com/transfer/latest/userguide/active-directory.html",
    "links": [
      "https://aws.amazon.com/transfer/",
      "https://docs.aws.amazon.com/transfer/latest/userguide/active-directory.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is experiencing sudden increases in demand. The company needs to provision large Amazon EC2\ninstances from an Amazon Machine Image (AMI). The instances will run in an Auto Scaling group. The company\nneeds a solution that provides minimum initialization latency to meet the demand.\nWhich solution meets these requirements?",
    "options": {
      "A": "Use the aws ec2 register-image command to create an AMI from a snapshot. Use AWS Step Functions to",
      "B": "Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by",
      "C": "Enable AMI creation and define lifecycle rules in Amazon Data Lifecycle Manager (Amazon DLM). Create an",
      "D": "Use Amazon EventBridge to invoke AWS Backup lifecycle policies that provision AMIs. Configure Auto"
    },
    "answer": "B",
    "explanation": "The question targets minimizing EC2 instance initialization latency during demand spikes when using Auto\nScaling. This is primarily achieved by reducing the time it takes to create instances from the AMI.\nOption B, enabling Amazon EBS fast snapshot restore (FSR) and using a snapshot to provision the AMI,\ndirectly addresses the latency requirement. EBS fast snapshot restore significantly reduces the time to\nprovision volumes from a snapshot. When you create an AMI from a snapshot with FSR enabled, subsequent\nEC2 instances launched using that AMI will have volumes that are quickly provisioned, minimizing initialization\nlatency. By replacing the AMI in the Auto Scaling group with the new AMI based on FSR, new instances\nlaunched by Auto Scaling will benefit from the rapid EBS volume provisioning.\nOption A involves registering an AMI from a snapshot. While this creates an AMI, it doesn't inherently address\nthe EBS volume provisioning time. Creating an AMI from a snapshot is a standard practice, but it does not\ndirectly minimize instance initialization latency when using EBS backed AMI. Step Functions will not reduce\nthe launch time, it will only orchestrate a deployment.\nOption C introduces Amazon Data Lifecycle Manager (DLM) and Lambda. While DLM can automate AMI\ncreation, it doesn't inherently minimize the EBS volume provisioning time. The use of Lambda to modify the\nAuto Scaling group's AMI might work, but it would not solve the fundamental problem of reducing the latency\nwith the launch of each instance.\nOption D employs EventBridge and AWS Backup. While AWS Backup can be used to create EBS snapshots,\nit's not directly geared towards accelerating instance launch times. The use of EventBridge here doesn't\ncontribute to a faster EC2 initialization.\nTherefore, the optimal solution is to enable EBS fast snapshot restore and provision an AMI from the restored\nsnapshot to minimize EBS volume creation latency during instance launch within the Auto Scaling group.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-\nrestore.htmlhttps://aws.amazon.com/ebs/features/fast-snapshot-restore/",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-",
      "https://aws.amazon.com/ebs/features/fast-snapshot-restore/"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage. The\napplication tier is hosted on Amazon EC2 instances. The companys IT security guidelines mandate that the\ndatabase credentials be encrypted and rotated every 14 days.\nWhat should a solutions architect do to meet this requirement with the LEAST operational effort?",
    "options": {
      "A": "Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to",
      "B": "Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a string",
      "C": "Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon",
      "D": "Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon"
    },
    "answer": "A",
    "explanation": "The best solution is A because it leverages AWS Secrets Manager, which is specifically designed for\nmanaging, rotating, and retrieving database credentials securely. Using Secrets Manager minimizes\noperational overhead by automating the rotation process. Creating a new KMS key ensures encryption at rest\nfor the stored credentials. Configuring a custom rotation period of 14 days aligns with the company's security\nrequirements. The application can then retrieve the credentials from Secrets Manager.\nOption B, using Systems Manager Parameter Store, is less suitable because while it can store credentials\nsecurely, automated rotation is not its primary feature. It would require a custom Lambda function to handle\nthe rotation logic, increasing operational effort. Also, implementing rotation logic in Lambda is not as secure\nand straightforward as leveraging the built-in features of Secrets Manager.\nOptions C and D, involving storing credentials in EFS or S3 and using a Lambda function for rotation, are\nconsiderably more complex and introduce more operational overhead. They require managing file system\npermissions, handling file uploads/downloads, and synchronizing credential updates across instances.\nFurthermore, keeping credentials in files on EC2 instances or even in encrypted S3 buckets exposes them to\npotential breaches if not handled properly. Secrets Manager encapsulates these concerns effectively.\nSecrets Manager handles the complexities of secure credential storage, encryption using KMS, and\nautomated rotation, making it the least operational effort solution.\nHere are some authoritative links for further research:\nAWS Secrets Manager: https://aws.amazon.com/secrets-manager/\nAWS Key Management Service (KMS): https://aws.amazon.com/kms/\nAmazon Aurora Security:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Security.html",
    "links": [
      "https://aws.amazon.com/secrets-manager/",
      "https://aws.amazon.com/kms/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Security.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has deployed a web application on AWS. The company hosts the backend database on Amazon RDS for\nMySQL with a primary DB instance and five read replicas to support scaling needs. The read replicas must lag no\nmore than 1 second behind the primary DB instance. The database routinely runs scheduled stored procedures.\nAs traffic on the website increases, the replicas experience additional lag during periods of peak load. A solutions\narchitect must reduce the replication lag as much as possible. The solutions architect must minimize changes to\nthe application code and must minimize ongoing operational overhead.\nWhich solution will meet these requirements?",
    "options": {
      "B": "Provision a large number of read capacity units (RCUs) to"
    },
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A is the best solution, along with supporting concepts and\nauthoritative links:\nThe primary goal is to reduce replication lag in an RDS for MySQL setup while minimizing application code\nchanges and operational overhead. The current MySQL read replicas are experiencing increased lag during\npeak loads.\nOption A, migrating to Amazon Aurora MySQL and using Aurora Replicas with auto-scaling, addresses the\nproblem effectively. Aurora offers significant performance improvements over standard MySQL, especially\nconcerning replication. Aurora's replication is physically based and generally faster and more efficient than\nthe logical replication used by standard MySQL. [https://aws.amazon.com/rds/aurora/features/].\nAurora Replicas share the same underlying storage as the primary instance, reducing replication latency.\nAurora Auto Scaling will automatically adjust the number of replicas based on load, ensuring consistent\nperformance even during peak traffic. This eliminates the need for manual intervention and minimizes\noperational overhead.\nReplacing stored procedures with Aurora MySQL native functions further enhances performance. Native\nfunctions are typically more tightly integrated and optimized for the Aurora environment compared to custom\nstored procedures.\nOption B, using ElastiCache for Redis, introduces a caching layer. While caching can reduce the load on the\ndatabase, it requires significant code changes to implement and maintain the cache invalidation logic. It also\ndoesn't directly address the replication lag issue. Redis does not solve replication lag between the primary\nand replicas, but rather aims to lessen the read load on the database.\nOption C, migrating to MySQL on EC2, adds operational overhead. Managing MySQL on EC2 involves tasks\nlike patching, backups, and scaling. While choosing larger instances might temporarily alleviate the lag, it\ndoes not provide a scalable and automated solution. The operational burden will also increase.\nOption D, migrating to DynamoDB, necessitates substantial application code changes. DynamoDB is a NoSQL\ndatabase with a fundamentally different data model than MySQL. Rewriting the application to work with\nDynamoDB would be a major undertaking. Also, the current problem is replication lag, and DynamoDB doesn't\nsuffer from traditional replication lag issues like MySQL because of its architecture. However, the migration is\nnot justified.\nIn summary, Aurora MySQL, with its faster replication, auto-scaling, and optimized functions, provides the\nmost effective solution for minimizing replication lag with minimal code changes and reduced operational\noverhead.",
    "links": [
      "https://aws.amazon.com/rds/aurora/features/]."
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect must create a disaster recovery (DR) plan for a high-volume software as a service (SaaS)\nplatform. All data for the platform is stored in an Amazon Aurora MySQL DB cluster.\nThe DR plan must replicate data to a secondary AWS Region.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "D",
    "explanation": "The most cost-effective disaster recovery (DR) solution for an Aurora MySQL SaaS platform requiring\nreplication to a secondary region is using Aurora Global Database with a minimum of one DB instance in the\nsecondary region. Here's why:\nAurora Global Database: This feature is specifically designed for DR and global read performance. It provides\nlow latency replication across AWS Regions using dedicated infrastructure.\nhttps://aws.amazon.com/rds/aurora/global-database/\nAutomatic Failover: In case of a primary Region failure, Aurora Global Database enables a fast, managed\nfailover to the secondary Region, minimizing downtime.\nCost Efficiency: While you're paying for storage and compute in the secondary region, having one DB instance\nensures availability while keeping costs down compared to running a fully scaled-out cluster that's idle. This\nallows the DR site to be readily available without incurring the full cost of an actively used environment.\nOther options compared:\nMySQL Binary Log Replication (Option A): Requires more manual configuration and management than Aurora\nGlobal Database. Setting up and maintaining replication, handling failover, and ensuring data consistency are\nmore complex and prone to errors, increasing operational overhead.\nAWS DMS (Option C): While DMS can replicate data, it introduces more overhead than Aurora Global\nDatabase, as it's a separate service. It also may not be the most performant option for a high-volume SaaS\nplatform. Additionally, removing the DB instance entirely negates the DR benefit.\nRemoving DB Instance in Secondary Region (Option B and C): A DR strategy requires at least one instance in\nthe secondary region to provide a failover target. Removing all instances eliminates the immediate failover\ncapability, defeating the purpose of the DR plan.\nTherefore, Aurora Global Database with at least one DB instance in the secondary Region strikes the best\nbalance between cost, performance, and ease of management for a robust DR solution.",
    "links": [
      "https://aws.amazon.com/rds/aurora/global-database/"
    ]
  },
  {
    "question": "CertyIQ\nA company has a custom application with embedded credentials that retrieves information from an Amazon RDS\nMySQL DB instance. Management says the application must be made more secure with the least amount of\nprogramming effort.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Use AWS Key Management Service (AWS KMS) to create keys. Configure the application to load the",
      "B": "Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS",
      "C": "Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS",
      "D": "Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it provides a secure and efficient solution for managing database credentials\nwith minimal code changes. Here's a detailed justification:\nSecrets Manager for credential storage: AWS Secrets Manager is designed specifically for managing\nsensitive information like database credentials. It offers secure storage and retrieval, making it ideal for this\nscenario. Storing credentials directly in the application code is a significant security risk.\nReduced programming effort: The solution leverages Secrets Manager's built-in rotation capabilities,\nminimizing the need for custom code. Options A and B require more custom code to manage the credential\nrotation process.\nRDS integration: Secrets Manager has built-in integration for rotating credentials directly on RDS, simplifying\nthe overall solution.\nAutomated rotation: The key aspect here is the automated credential rotation. This is crucial for security. By\nregularly changing the database credentials, the risk of compromised credentials being used maliciously is\nsubstantially reduced. Secrets Manager offers this capability, making it a suitable choice.\nParameter Store vs. Secrets Manager: While Systems Manager Parameter Store can store secrets, Secrets\nManager is preferred for database credentials due to its built-in rotation features and finer-grained access\ncontrol specifically designed for sensitive information. Parameter Store is best used to store configuration\ndata that is not sensitive.\nKMS considerations: While KMS can encrypt the secrets, it doesn't manage the rotation of those secrets\nwithin RDS, requiring much more coding effort. The application would need to retrieve the encrypted secret,\ndecrypt it using KMS, and then update the RDS database with the new credentials. Secrets Manager\nsimplifies this process.\nIn summary, option C provides a balance between security and ease of implementation by utilizing Secrets\nManager's secure storage and automated rotation capabilities, minimizing the need for custom code and\nmeeting the requirement of \"least amount of programming effort.\"\nSupporting Links:\nAWS Secrets Manager: https://aws.amazon.com/secrets-manager/\nRotate AWS Secrets Manager secrets:\nhttps://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html\nAWS Systems Manager Parameter Store: https://docs.aws.amazon.com/systems-\nmanager/latest/userguide/systems-manager-parameter-store.html",
    "links": [
      "https://aws.amazon.com/secrets-manager/",
      "https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html",
      "https://docs.aws.amazon.com/systems-"
    ]
  },
  {
    "question": "CertyIQ\nA media company hosts its website on AWS. The website applications architecture includes a fleet of Amazon EC2\ninstances behind an Application Load Balancer (ALB) and a database that is hosted on Amazon Aurora. The\ncompanys cybersecurity team reports that the application is vulnerable to SQL injection.\nHow should the company resolve this issue?",
    "options": {
      "A": "Use AWS WAF in front of the ALB. Associate the appropriate web ACLs with AWS WAF.",
      "B": "Associate the appropriate web ACLs with AWS",
      "C": "Subscribe to AWS Shield Advanced to block all SQL injection attempts automatically.",
      "D": "Set up Amazon Inspector to block all SQL injection attempts automatically."
    },
    "answer": "A",
    "explanation": "The correct answer is A: Use AWS WAF in front of the ALB. Associate the appropriate web ACLs with AWS\nWAF.\nHere's a detailed justification:\nSQL injection is a code injection technique used to attack data-driven applications, in which malicious SQL\nstatements are inserted into an entry field for execution. This allows attackers to potentially bypass security\nmeasures and gain unauthorized access to the database.\nAWS WAF (Web Application Firewall) is a web application firewall that helps protect web applications from\ncommon web exploits and bots that may affect availability, compromise security, or consume excessive\nresources. It allows you to control access to your content by defining customizable web security rules. These\nrules can filter out malicious traffic patterns, including those commonly associated with SQL injection\nattacks.\nOption A effectively leverages AWS WAF. By placing AWS WAF in front of the Application Load Balancer\n(ALB), all incoming traffic is inspected against the defined web ACLs (Access Control Lists). These web ACLs\ncontain rules designed to identify and block SQL injection attempts. WAF can inspect HTTP headers, request\nbody, and URI strings for malicious SQL code. By blocking these requests before they reach the application,\nWAF prevents the SQL injection vulnerability from being exploited.\nOption B, creating an ALB listener rule to reply with a fixed response, is not a suitable solution. While it might\nprevent immediate execution of the SQL injection, it doesn't address the underlying vulnerability and could\nlead to application malfunction and denial of service. It's a reactive measure, not a preventative one. It also\nprovides limited protection against evolving attack vectors.\nOption C, subscribing to AWS Shield Advanced, mainly protects against DDoS attacks, not specific\napplication-level vulnerabilities like SQL injection. While Shield Advanced offers visibility and mitigation\nassistance, it's not a direct solution to the SQL injection problem.\nOption D, using Amazon Inspector, is a vulnerability management service that automatically assesses\napplications for security vulnerabilities or deviations from best practices. While Inspector can identify the\npresence of the SQL injection vulnerability, it does not automatically block the attacks in real-time. Inspector\nwould generate findings but wouldn't prevent the attacks. You'd still need a mechanism to block the attacks\nbased on Inspector's findings.\nTherefore, the most appropriate and effective solution is to use AWS WAF with appropriate web ACLs, as this\nprovides real-time protection against SQL injection attacks by inspecting and filtering malicious traffic before\nit reaches the application.\nAuthoritative Links:\nAWS WAF: https://aws.amazon.com/waf/\nOWASP SQL Injection: https://owasp.org/www-community/attacks/SQL_Injection\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
    "links": [
      "https://aws.amazon.com/waf/",
      "https://owasp.org/www-community/attacks/SQL_Injection",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an Amazon S3 data lake that is governed by AWS Lake Formation. The company wants to create a\nvisualization in Amazon QuickSight by joining the data in the data lake with operational data that is stored in an\nAmazon Aurora MySQL database. The company wants to enforce column-level authorization so that the companys\nmarketing team can access only a subset of columns in the database.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Use Amazon EMR to ingest the data directly from the database to the QuickSight SPICE engine. Include only",
      "B": "Use AWS Glue Studio to ingest the data from the database to the S3 data lake. Attach an IAM policy to the",
      "C": "Use AWS Glue Elastic Views to create a materialized view for the database in Amazon S3. Create an S3",
      "D": "Use a Lake Formation blueprint to ingest the data from the database to the S3 data lake. Use Lake Formation"
    },
    "answer": "D",
    "explanation": "The correct answer is D because it leverages Lake Formation's built-in capabilities for data governance and\naccess control, minimizing operational overhead.\nHere's a detailed justification:\nRequirement: Join data from S3 data lake and Aurora MySQL for visualization in QuickSight with column-level\nauthorization for the marketing team.\nWhy D is the best solution:\nLake Formation Blueprint for Ingestion: Lake Formation blueprints automate the process of ingesting data\nfrom various sources, including relational databases like Aurora MySQL, into the S3 data lake. This simplifies\nthe data loading process compared to manual ETL jobs using EMR or Glue Studio. ^1^\nLake Formation for Column-Level Access Control: Lake Formation allows you to define fine-grained access\ncontrols, including column-level permissions. This enables the marketing team to access only the columns\nthey need in both the data lake and the ingested Aurora data. ^2^\nAthena as the Data Source: Athena integrates seamlessly with Lake Formation, enabling it to enforce the\ndefined access controls. When users query the data through Athena, Lake Formation automatically filters the\ndata based on their permissions. Athena is a serverless query service that provides a straightforward way to\nanalyze data in S3. ^3^\nWhy other options are less optimal:\nA (EMR to QuickSight SPICE): This approach bypasses the data lake and Lake Formation. It's less scalable,\nand governance is managed within the SPICE dataset, increasing operational overhead. Additionally, SPICE\nhas limitations in terms of data volume and complexity compared to querying directly from the data lake via\nAthena.\nB (Glue Studio to S3, IAM Policies): While Glue Studio can ingest data, using IAM policies for column-level\naccess control is complex and error-prone. It becomes difficult to manage permissions across different users\nand data sources, especially at scale. This approach also misses the advantage of using Lake Formation's\ncentral governance.\nC (Glue Elastic Views, S3 Bucket Policy): Glue Elastic Views are generally for real-time data propagation,\nwhich isn't the primary requirement here. Using S3 bucket policies for column-level access control is not\npossible, as bucket policies only control access at the object level, not within the object. This approach also\ndoesn't leverage the governance benefits of Lake Formation.\nIn summary, leveraging Lake Formation's built-in capabilities for data ingestion, governance, and access\ncontrol provides the most efficient and scalable solution for this scenario.",
    "links": []
  },
  {
    "question": "CertyIQ\nA transaction processing company has weekly scripted batch jobs that run on Amazon EC2 instances. The EC2\ninstances are in an Auto Scaling group. The number of transactions can vary, but the baseline CPU utilization that\nis noted on each run is at least 60%. The company needs to provision the capacity 30 minutes before the jobs run.\nCurrently, engineers complete this task by manually modifying the Auto Scaling group parameters. The company\ndoes not have the resources to analyze the required capacity trends for the Auto Scaling group counts. The\ncompany needs an automated way to modify the Auto Scaling groups desired capacity.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Create a dynamic scaling policy for the Auto Scaling group. Configure the policy to scale based on the CPU",
      "B": "Create a scheduled scaling policy for the Auto Scaling group. Set the appropriate desired capacity, minimum",
      "C": "Create a predictive scaling policy for the Auto Scaling group. Configure the policy to scale based on forecast.",
      "D": "Create an Amazon EventBridge event to invoke an AWS Lambda function when the CPU utilization metric"
    },
    "answer": "C",
    "explanation": "The correct answer is C: Create a predictive scaling policy for the Auto Scaling group. Configure the policy\nto scale based on forecast. Set the scaling metric to CPU utilization. Set the target value for the metric to\n60%. In the policy, set the instances to pre-launch 30 minutes before the jobs run.\nHere's a detailed justification:\nThe core requirement is to automatically adjust the Auto Scaling group's desired capacity 30 minutes before\nthe batch jobs start, based on anticipated load. Given that the company lacks the resources for detailed\ncapacity trend analysis, predictive scaling offers the most suitable and automated solution.\nPredictive Scaling: Amazon EC2 Auto Scaling's predictive scaling analyzes historical CPU utilization trends\n(which are consistently above 60%). It forecasts future capacity needs and proactively adjusts the Auto\nScaling group. This addresses the need for capacity 30 minutes ahead of time, as specified in the problem. By\nusing forecast data, the solution automatically determines the desired capacity without manual calculations.\nThis approach reduces operational overhead.\nCPU Utilization Metric: The problem states that the baseline CPU utilization during the batch jobs is at least\n60%. Predictive scaling can leverage CPU utilization as a metric to learn and forecast the required capacity.\nPre-launch Instances: The ability to pre-launch instances 30 minutes before the jobs run is a crucial feature\nof the predictive scaling policy that directly addresses the requirement.\nLet's analyze why the other options are less ideal:\nA (Dynamic Scaling): Dynamic scaling responds to current CPU utilization. While it can maintain a target CPU\nof 60%, it won't provision capacity before the load increases. This introduces a delay, which is not acceptable\nas the jobs require the capacity before they start.\nB (Scheduled Scaling): Scheduled scaling relies on fixed schedules. While you can set a weekly schedule to\nincrease capacity 30 minutes before the jobs run, it doesn't account for variations in transaction volume. It\nrequires a good understanding of the required capacity, and therefore would require manual intervention and\ntrend analysis, which the company wishes to avoid. Furthermore, the problem states they do not have\nresources for capacity analysis.\nD (EventBridge & Lambda): Using EventBridge and Lambda introduces unnecessary complexity. It would\nrequire setting up complex rules based on real-time CPU utilization metrics, and then programming Lambda\nto adjust the Auto Scaling group. This increases operational overhead compared to the built-in predictive\nscaling functionality. It suffers from the same delay issue as Option A - it reacts to the 60% threshold being\ncrossed rather than proactively scaling.\nAuthoritative Links:\nAmazon EC2 Auto Scaling Predictive Scaling: https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-\nauto-scaling-predictive-scaling.html\nDynamic Scaling vs Predictive Scaling: https://aws.amazon.com/blogs/compute/introducing-predictive-\nscaling-for-amazon-ec2-auto-scaling/",
    "links": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-",
      "https://aws.amazon.com/blogs/compute/introducing-predictive-"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is designing a companys disaster recovery (DR) architecture. The company has a MySQL\ndatabase that runs on an Amazon EC2 instance in a private subnet with scheduled backup. The DR design needs to\ninclude multiple AWS Regions.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the DR Region.",
      "B": "Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Turn on read replication for the",
      "C": "Migrating the MySQL database to Amazon Aurora Global Database offers the lowest",
      "D": "Store the scheduled backup of the MySQL database in an Amazon S3 bucket that is configured for S3 Cross-"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Migrating the MySQL database to Amazon Aurora Global Database offers the lowest\noperational overhead while meeting the disaster recovery requirements of multi-Region support and minimal\nmanagement. Aurora Global Database is designed for DR scenarios, providing fast recovery with minimal data\nloss.\nHere's why:\nAurora Global Database: This feature allows you to replicate your Aurora MySQL database to multiple AWS\nRegions. The primary region handles writes, and the secondary region(s) are read-only replicas with low\nlatency replication. In the event of a primary region failure, a secondary region can be promoted to become\nthe new primary, minimizing downtime and data loss. https://aws.amazon.com/rds/aurora/features/global-\ndatabase/\nMulti-Region DR with minimal overhead: Aurora Global Database automates much of the replication and\nfailover process, significantly reducing the operational burden compared to manual replication setups.\nHere's why the other options are less suitable:\nOption A (EC2 replication): Setting up MySQL replication on EC2 instances requires manual configuration,\nmonitoring, and maintenance of replication. Failover requires significant manual intervention, increasing\noperational overhead. This method also doesn't offer as robust DR capabilities as Aurora Global Database.\nOption B (RDS Multi-AZ with read replicas): RDS Multi-AZ provides high availability within a single region.\nWhile read replicas can exist in different Availability Zones, they aren't designed for cross-region DR with\nautomated failover. Manual intervention is required for DR failover, and RPO/RTO are higher than with Aurora\nGlobal Database. https://aws.amazon.com/rds/mysql/features/\nOption D (S3 CRR): While S3 Cross-Region Replication can replicate backups to another region, restoring\nfrom backups is a slow and manual process. This significantly increases the Recovery Time Objective (RTO)\nand Recovery Point Objective (RPO) compared to Aurora Global Database, which is designed for rapid failover.\nRestoring from backups also involves additional steps like provisioning a new EC2 instance, installing MySQL,\nand restoring the backup, adding to the operational overhead. https://aws.amazon.com/s3/features/\nIn summary, Aurora Global Database provides the most efficient and automated solution for multi-region DR\nfor a MySQL database, aligning with the requirement of \"least operational overhead.\"",
    "links": [
      "https://aws.amazon.com/rds/aurora/features/global-",
      "https://aws.amazon.com/rds/mysql/features/",
      "https://aws.amazon.com/s3/features/"
    ]
  },
  {
    "question": "CertyIQ\nA company has a Java application that uses Amazon Simple Queue Service (Amazon SQS) to parse messages. The\napplication cannot parse messages that are larger than 256 KB in size. The company wants to implement a\nsolution to give the application the ability to parse messages as large as 50 M",
    "options": {
      "B": "Amazon SQS Extended Client Library: A pre-built solution to extend SQS's functionality to support larger",
      "A": "Use the Amazon SQS Extended Client Library for Java to host messages that are larger than 256 KB in",
      "C": "Change the limit in Amazon SQS to handle messages that are larger than 256 KB.",
      "D": "Store messages that are larger than 256 KB in Amazon Elastic File System (Amazon EFS). Configure Amazon"
    },
    "answer": "A",
    "explanation": "The correct answer is A: \"Use the Amazon SQS Extended Client Library for Java to host messages that are\nlarger than 256 KB in Amazon S3.\"\nHere's why:\nProblem Statement: The application's limitation is parsing SQS messages larger than 256KB. The goal is to\nhandle up to 50MB with minimal code changes.\nWhy Option A is Best: The Amazon SQS Extended Client Library for Java is designed specifically for handling\nlarge messages exceeding the SQS size limit. It stores the large message payload in Amazon S3 and stores a\nreference to the S3 object in the SQS message. This approach allows SQS to act as a pointer to the actual\nmessage content. Because the library handles the complexity of interacting with S3, the code changes\nrequired in the application will be minimal.\nWhy Other Options Are Not Optimal:\nB (Amazon EventBridge): While EventBridge is useful, it's designed for event-driven architectures.\nSubstituting SQS with EventBridge would necessitate a significant rewrite of the existing application,\ndeviating from the \"fewest changes\" requirement. Furthermore, EventBridge has a smaller message size limit\nthan SQS with the Extended Client Library.\nC (Changing SQS Limit): SQS has a hard limit of 256KB for message size. You cannot simply change this limit.\nD (Amazon EFS): Storing large messages in EFS and referencing them from SQS would require significant\ncustom coding for managing files in EFS, handling file paths in SQS messages, ensuring proper access control\nand data consistency, and managing cleanup of old files. This is more complex and less efficient than the\nprovided Extended Client Library, and EFS isn't the appropriate service for message storage as it adds\nunnecessary complexity.\nKey Concepts:\nAmazon SQS Size Limit: SQS has a message size limit of 256KB.\nAmazon SQS Extended Client Library: A pre-built solution to extend SQS's functionality to support larger\nmessages by using Amazon S3 as storage.\nLoose Coupling: The solution maintains loose coupling between the message queue (SQS) and the actual\nmessage content (stored in S3).\nAuthoritative Links:\nAmazon SQS Extended Client Library for Java\nAmazon SQS Message Size and Quotas\nIn summary, the SQS Extended Client Library provides a straightforward and efficient way to handle large\nmessages with the fewest code modifications. It leverages S3 for storage and keeps the application's\ndependency on SQS while enabling the processing of messages up to 50MB.",
    "links": []
  },
  {
    "question": "CertyIQ\nA company wants to restrict access to the content of one of its main web applications and to protect the content\nby using authorization techniques available on AWS. The company wants to implement a serverless architecture\nand an authentication solution for fewer than 100 users. The solution needs to integrate with the main web\napplication and serve web content globally. The solution must also scale as the company's user base grows while\nproviding the lowest login latency possible.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "A": "Use Amazon Cognito for authentication. Use [email protected] for authorization. Use Amazon CloudFront to",
      "B": "Use AWS Directory Service for Microsoft Active Directory for authentication. Use AWS Lambda for",
      "C": "Use Amazon Cognito for authentication. Use AWS Lambda for authorization. Use Amazon S3 Transfer",
      "D": "Use AWS Directory Service for Microsoft Active Directory for authentication. Use [email protected] for"
    },
    "answer": "A",
    "explanation": "The best solution is A because it utilizes cost-effective and scalable AWS services suitable for a small user\nbase and global content delivery. Amazon Cognito is a managed service specifically designed for user\nauthentication, providing features like user registration, sign-in, and access control, and it's a cost-effective\noption for fewer than 100 users. [email protected], a serverless authorization service that seamlessly\nintegrates with Cognito, allows fine-grained access control to web application resources. CloudFront, a global\ncontent delivery network (CDN), is ideal for distributing web content with low latency worldwide.\nOption B is less optimal due to the cost associated with AWS Directory Service for Microsoft Active Directory,\nwhich is more suitable for larger organizations with existing Active Directory infrastructure. Furthermore,\nusing Lambda for authorization adds unnecessary complexity compared to the purpose-built [email\nprotected]. An Application Load Balancer is generally used for distributing traffic across multiple instances,\nwhich is not necessary when serving static content globally through CloudFront.\nOption C is less efficient because Amazon S3 Transfer Acceleration is for accelerating uploads to S3, not for\nserving web content globally with low latency, which is CloudFront's primary function.\nOption D combines the costly AWS Directory Service with Elastic Beanstalk, which is a platform-as-a-service\n(PaaS) that manages the underlying infrastructure. Elastic Beanstalk introduces complexities that are\nunnecessary for a serverless architecture focused on serving static web content globally. [email protected] is\neffective for authorization, but the underlying authentication and delivery are less optimal.\nTherefore, Cognito provides the most cost-effective authentication, [email protected] provides scalable and\nserverless authorization, and CloudFront offers global content delivery with low latency, making option A the\nbest choice.\nSupporting Links:\nAmazon Cognito: https://aws.amazon.com/cognito/\n[email protected]: [https://aws.amazon.com/blogs/security/[email protected]-fine-grained-authorization-\nservice-serverless-applications/](https://aws.amazon.com/blogs/security/[email protected]-fine-grained-\nauthorization-service-serverless-applications/)\nAmazon CloudFront: https://aws.amazon.com/cloudfront/",
    "links": [
      "https://aws.amazon.com/cognito/",
      "https://aws.amazon.com/blogs/security/[email",
      "https://aws.amazon.com/blogs/security/[email",
      "https://aws.amazon.com/cloudfront/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an aging network-attached storage (NAS) array in its data center. The NAS array presents SMB\nshares and NFS shares to client workstations. The company does not want to purchase a new NAS array. The\ncompany also does not want to incur the cost of renewing the NAS arrays support contract. Some of the data is\naccessed frequently, but much of the data is inactive.\nA solutions architect needs to implement a solution that migrates the data to Amazon S3, uses S3 Lifecycle\npolicies, and maintains the same look and feel for the client workstations. The solutions architect has identified\nAWS Storage Gateway as part of the solution.\nWhich type of storage gateway should the solutions architect provision to meet these requirements?",
    "options": {
      "A": "Volume Gateway",
      "B": "This fulfills the requirement of maintaining the same look and feel for the client",
      "C": "Amazon FSx File Gateway",
      "D": "Amazon S3 File Gateway."
    },
    "answer": "D",
    "explanation": "The correct answer is D. Amazon S3 File Gateway.\nHere's a detailed justification:\nThe primary requirements are to migrate data to Amazon S3 while maintaining the same look and feel\n(SMB/NFS shares) for client workstations and implementing S3 Lifecycle policies for tiered storage. The key\nhere is preserving the file share access method.\nAmazon S3 File Gateway allows on-premises applications to access data in Amazon S3 through standard file\nprotocols like NFS and SMB. This fulfills the requirement of maintaining the same look and feel for the client\nworkstations because they continue to interact with file shares. It effectively presents S3 as a network file\nshare. Moreover, data written to the File Gateway is stored as objects in S3, which allows leveraging S3\nLifecycle policies for managing data tiers (frequently accessed vs. inactive data).\nVolume Gateway presents block-based storage to on-premises applications, which isn't suitable for directly\nreplacing a file-based NAS. It replicates on-premises block storage volumes to AWS, but it does not provide\nfile share access (SMB/NFS).\nTape Gateway is designed for backing up data to virtual tapes stored in AWS, primarily for archival purposes,\nnot for active file storage and access. It does not expose fileshares to workstations.\nAmazon FSx File Gateway is used for accessing fully managed file systems in AWS, like Amazon FSx for\nWindows File Server or Amazon FSx for Lustre, and doesn't directly help in migrating and exposing existing\nNAS data in S3 through SMB/NFS shares.\nTherefore, Amazon S3 File Gateway is the best solution because it allows the company to migrate their file\ndata to S3, preserve existing SMB/NFS access, and manage storage costs with S3 Lifecycle policies.\nAuthoritative Links:\nAWS Storage Gateway: https://aws.amazon.com/storagegateway/\nFile Gateway: https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html\nS3 Lifecycle Policies: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-\nexamples.html",
    "links": [
      "https://aws.amazon.com/storagegateway/",
      "https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-"
    ]
  },
  {
    "question": "CertyIQ\nA company has an application that is running on Amazon EC2 instances. A solutions architect has standardized the\ncompany on a particular instance family and various instance sizes based on the current needs of the company.\nThe company wants to maximize cost savings for the application over the next 3 years. The company needs to be\nable to change the instance family and sizes in the next 6 months based on application popularity and usage.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "A": "Compute Savings Plan",
      "B": "EC2 Instance Savings Plan",
      "C": "Zonal Reserved Instances",
      "D": "Standard Reserved Instances"
    },
    "answer": "A",
    "explanation": "The correct answer is A, Compute Savings Plan. Here's why:\nCompute Savings Plans offer the most flexibility and cost savings for this scenario because they\nautomatically apply to EC2 instance usage regardless of instance family, size, operating system, or tenancy\nwithin a specified compute amount per hour. This is crucial because the company anticipates changing\ninstance families and sizes within the next 6 months due to application popularity. EC2 Instance Savings\nPlans, while providing savings on specific instance families within a region, lack the needed flexibility. They\nare tied to a particular instance family and size, making them unsuitable if those parameters need to change.\nReserved Instances (RIs), both Zonal and Standard, offer significant cost savings compared to On-Demand\nInstances. However, Standard RIs are less flexible in terms of AZ placement compared to Zonal RIs. While RIs\noffer savings, they require committing to a specific instance type and availability zone, which contradicts the\nrequirement of changing instance families. Further, modifying RIs after purchase can be complex and may\nincur additional costs. Compute Savings Plans provide more cost optimization in the long run when instance\ntypes are subject to change.\nSince the company anticipates changes, Compute Savings Plans offer the best balance between cost savings\nand flexibility. This allows them to optimize costs over the 3-year term while adapting to evolving application\nneeds without incurring penalties or wasted investments.\nFurther reading:\nAWS Savings Plans: https://aws.amazon.com/savingsplans/\nAWS Reserved Instances: https://aws.amazon.com/ec2/pricing/reserved-instances/",
    "links": [
      "https://aws.amazon.com/savingsplans/",
      "https://aws.amazon.com/ec2/pricing/reserved-instances/"
    ]
  },
  {
    "question": "CertyIQ\nA company collects data from a large number of participants who use wearable devices. The company stores the\ndata in an Amazon DynamoDB table and uses applications to analyze the data. The data workload is constant and\npredictable. The company wants to stay at or below its forecasted budget for DynamoD",
    "options": {
      "B": "Use provisioned mode. Specify the read capacity units (RCUs) and write capacity units (WCUs).",
      "A": "Use provisioned mode and DynamoDB Standard-Infrequent Access (DynamoDB Standard-IA). Reserve",
      "C": "Use on-demand mode. Set the read capacity units (RCUs) and write capacity units (WCUs) high enough to",
      "D": "Use on-demand mode. Specify the read capacity units (RCUs) and write capacity units (WCUs) with reserved"
    },
    "answer": "B",
    "explanation": "The correct answer is B: Use provisioned mode. Specify the read capacity units (RCUs) and write capacity\nunits (WCUs).\nHere's why:\nThe key requirements are cost-effectiveness and a predictable workload. Provisioned mode is ideal for\npredictable workloads because you can specify the read and write capacity units (RCUs and WCUs) needed to\nhandle the anticipated traffic. This allows you to closely control and optimize costs.\nOption A is incorrect because DynamoDB Standard-IA is optimized for infrequently accessed data, which is\nnot specified in the use case. Standard-IA storage is more expensive for frequently accessed data, and the\nproblem states that the data is being analyzed. While reserving capacity within provisioned mode is good,\nsuggesting Standard-IA storage when there's no indication of infrequent access makes this a less ideal\nchoice.\nOption C is incorrect. On-demand mode is best suited for unpredictable workloads with varying traffic\npatterns. While it eliminates the need to manage capacity, it is generally more expensive than provisioned\nmode for predictable workloads because you pay for each read and write request. Setting RCUs and WCUs in\non-demand mode contradicts the very nature of on-demand capacity.\nOption D is incorrect because you cannot \"specify\" RCUs and WCUs along with \"reserved capacity\" in on-\ndemand mode. On-demand mode automatically scales capacity, and you don't have control over setting\nspecific RCUs/WCUs in that mode. Furthermore, suggesting reserved capacity in on-demand mode mixes\nprovisioned and on-demand concepts, which is not directly supported. On-demand pricing covers the scaling.\nTherefore, provisioned mode with specified RCUs and WCUs allows the company to precisely allocate the\nresources needed for the constant and predictable data workload, ensuring the most cost-effective solution\nthat stays within the forecasted budget.Here are some authoritative links for further research:\nDynamoDB Pricing: https://aws.amazon.com/dynamodb/pricing/ - Review the pricing models for provisioned\nand on-demand capacity to understand cost implications.\nChoosing Between On-Demand and Provisioned Capacity:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\n- AWS documentation outlining the differences and use cases for each capacity mode.\nDynamoDB Standard and Standard-IA: https://aws.amazon.com/dynamodb/pricing/storage/ - Detailed\nexplanation of DynamoDB storage types and their appropriate use cases.",
    "links": [
      "https://aws.amazon.com/dynamodb/pricing/",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html",
      "https://aws.amazon.com/dynamodb/pricing/storage/"
    ]
  },
  {
    "question": "CertyIQ\nA company stores confidential data in an Amazon Aurora PostgreSQL database in the ap-southeast-3 Region. The\ndatabase is encrypted with an AWS Key Management Service (AWS KMS) customer managed key. The company\nwas recently acquired and must securely share a backup of the database with the acquiring companys AWS\naccount in ap-southeast-3.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Create a database snapshot. Copy the snapshot to a new unencrypted snapshot. Share the new snapshot",
      "B": "Here's a detailed justification:",
      "C": "Create a database snapshot that uses a different AWS managed KMS key. Add the acquiring companys",
      "D": "Create a database snapshot. Download the database snapshot. Upload the database snapshot to an Amazon"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Here's a detailed justification:\nThe core requirement is to securely share an encrypted Aurora PostgreSQL database backup (snapshot)\nacross AWS accounts while maintaining encryption using KMS. Option B directly addresses this.\n1. Creating a Database Snapshot: The first step is to create a snapshot of the Aurora PostgreSQL\ndatabase. This is the standard method for backing up an Aurora database.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html\n2. Adding the Acquiring Company's AWS Account to the KMS Key Policy: The database is encrypted\nwith a KMS customer-managed key (CMK). To allow the acquiring company to restore and use the\nsnapshot, their AWS account needs permission to use the CMK. This is accomplished by adding the\nacquiring company's AWS account ID as a principal to the CMK's key policy. This grants the acquiring\ncompany's account the necessary permissions (e.g., kms:Decrypt, kms:GenerateDataKey) to decrypt\nthe snapshot and perform restore operations.\nhttps://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html\n3. Sharing the Snapshot: Once the acquiring company's account has the necessary KMS permissions,\nthe snapshot can be shared with their AWS account. The receiving account can then create a new\nAurora cluster from the shared snapshot.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html\nWhy other options are incorrect:\nA: Creating an unencrypted snapshot defeats the purpose of securely sharing the data. The requirement\nspecifies securely sharing the data, implying encryption should be maintained.\nC: Using an AWS-managed KMS key would be simpler initially, but it wouldn't satisfy the requirement of using\nthe existing customer-managed key, and you will need to give permissions to use the key anyway. Key aliases\ndo not control permissions; key policies do.\nD: Downloading and uploading the snapshot via S3 exposes the database backup to potential security risks if\nthe S3 bucket is misconfigured. The process would also be more complex and time-consuming compared to\nsharing the snapshot directly. While S3 can be secure, this method introduces unnecessary complexity and a\nlarger attack surface compared to the direct snapshot sharing method with KMS key policy modification.\nFurthermore, you still need KMS access in the new account to re-encrypt if not already done so.",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html",
      "https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html"
    ]
  },
  {
    "question": "CertyIQ\nA company uses a 100 GB Amazon RDS for Microsoft SQL Server Single-AZ DB instance in the us-east-1 Region to\nstore customer transactions. The company needs high availability and automatic recovery for the DB instance.\nThe company must also run reports on the RDS database several times a year. The report process causes\ntransactions to take longer than usual to post to the customers accounts. The company needs a solution that will\nimprove the performance of the report process.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": {
      "A": "Modify the DB instance from a Single-AZ DB instance to a Multi-AZ deployment. This addresses the high",
      "B": "Take a snapshot of the current DB instance. Restore the snapshot to a new RDS deployment in another",
      "C": "Create a read replica of the DB instance in a different Availability Zone. Point all requests for reports to",
      "D": "Migrate the database to RDS Custom. RDS Custom provides more control over the operating system and"
    },
    "answer": "A",
    "explanation": "Let's analyze why options A and C are the correct choices for achieving high availability, automatic recovery,\nand improved reporting performance for the company's Amazon RDS for Microsoft SQL Server database.\nA. Modify the DB instance from a Single-AZ DB instance to a Multi-AZ deployment. This addresses the high\navailability and automatic recovery requirements. A Multi-AZ deployment automatically provisions a standby\nreplica of the database in a different Availability Zone. In case of an infrastructure failure in the primary AZ,\nRDS automatically fails over to the standby replica, minimizing downtime and ensuring continuous operation.\nSingle-AZ instances do not offer automatic failover, while Multi-AZ does. This configuration ensures the\ndatabase remains accessible even during planned maintenance or unexpected outages.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\nC. Create a read replica of the DB instance in a different Availability Zone. Point all requests for reports to\nthe read replica. This addresses the reporting performance issue without impacting the performance of the\nprimary database handling customer transactions. Read replicas allow you to offload read-only workloads,\nsuch as report generation, to a separate database instance. By directing the report process to the read\nreplica, the primary database remains dedicated to transaction processing, preventing performance\ndegradation. This separation of concerns significantly improves the overall performance of both transaction\nposting and report generation. Read replicas can be in a different AZ than the primary.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.ReadReplicas.html\nLet's examine why the other options are not ideal:\nB. Take a snapshot of the current DB instance. Restore the snapshot to a new RDS deployment in another\nAvailability Zone. While this creates a copy of the database, it's a one-time operation and doesn't provide\ncontinuous, near real-time data for reporting. It doesn't address the high availability requirement for the\nprimary database. The restored instance would be stale and out of sync.\nD. Migrate the database to RDS Custom. RDS Custom provides more control over the operating system and\ndatabase environment, but it also increases operational overhead and complexity. It is unnecessary for the\nstated requirements of high availability, automatic recovery, and improved reporting performance, adding\nunneeded management overhead.\nE. Use RDS Proxy to limit reporting requests to the maintenance window. RDS Proxy primarily manages\ndatabase connections and improves application scalability. While it can help with managing connection\nresources, it doesn't directly address the performance impact of report generation on the primary database or\nthe need for high availability. It would delay reporting, not improve its performance.",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.ReadReplicas.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is moving its data management application to AWS. The company wants to transition to an event-driven\narchitecture. The architecture needs to be more distributed and to use serverless concepts while performing the\ndifferent aspects of the workflow. The company also wants to minimize operational overhead.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Build out the workflow in AWS Glue. Use AWS Glue to invoke AWS Lambda functions to process the",
      "B": "Build out the workflow in AWS Step Functions. Deploy the application on Amazon EC2 instances. Use Step",
      "C": "Build out the workflow in Amazon EventBridge. Use EventBridge to invoke AWS Lambda functions on a",
      "D": "Build out the workflow in AWS Step Functions. Use Step Functions to create a state machine. Use the state"
    },
    "answer": "D",
    "explanation": "The correct answer is D because it leverages AWS Step Functions and Lambda, which are key serverless\ncomponents ideal for building event-driven architectures with minimal operational overhead.\nHere's why:\nStep Functions: AWS Step Functions allows you to define workflows as state machines. This is crucial for\nmanaging the data management application's workflow, breaking it down into discrete, manageable steps.\nThe visual workflow makes it easier to understand, monitor, and maintain the entire process.\n(https://aws.amazon.com/step-functions/)\nLambda: AWS Lambda provides serverless compute capabilities. By using Lambda functions for each step in\nthe workflow, you eliminate the need to manage servers or containers, reducing operational overhead\nsignificantly. Lambda functions execute only when triggered, further optimizing resource utilization and cost.\n(https://aws.amazon.com/lambda/)\nEvent-Driven Architecture: Step Functions facilitates event-driven behavior by triggering different Lambda\nfunctions based on the outcome of previous steps. This asynchronous execution ensures that the application\nis highly responsive and scalable.\nWhy the other options are less suitable:\nA (AWS Glue): While AWS Glue can perform ETL tasks, it's primarily designed for data integration and\npreparation, not for orchestrating a general-purpose event-driven workflow. It also can have an overhead on\nsmaller tasks.\nB (Step Functions + EC2): Deploying the application on EC2 instances introduces the operational overhead of\nmanaging servers, which contradicts the requirement of minimizing operational overhead and adopting\nserverless concepts.\nC (EventBridge + Lambda on a Schedule): Amazon EventBridge is excellent for routing events between\nservices, but scheduling Lambda functions directly doesn't provide the sophisticated workflow orchestration\ncapabilities offered by Step Functions. A schedule lacks the necessary state management and error handling.\nIn summary, Step Functions provides the workflow orchestration, Lambda provides the serverless compute,\nfulfilling the need for a distributed, serverless, and event-driven architecture with minimal operational\noverhead.",
    "links": [
      "https://aws.amazon.com/step-functions/)",
      "https://aws.amazon.com/lambda/)"
    ]
  },
  {
    "question": "CertyIQ\nA company is designing the network for an online multi-player game. The game uses the UDP networking protocol\nand will be deployed in eight AWS Regions. The network architecture needs to minimize latency and packet loss to\ngive end users a high-quality gaming experience.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Transit Gateway with Inter-Region Peering: While transit gateways simplify network management, inter-",
      "B": "Set up AWS Global Accelerator with UDP listeners and endpoint groups in each Region.",
      "C": "Also, UDP support on its own does not address packet loss reduction or intelligent",
      "D": "VPC Peering Mesh: A full mesh of VPC peering connections between eight regions results in a complex"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B (Set up AWS Global Accelerator with UDP listeners and\nendpoint groups in each Region) is the best solution for minimizing latency and packet loss in a multi-region,\nUDP-based online game, and why the other options are less suitable:\nWhy Option B is Correct: AWS Global Accelerator\nAWS Global Accelerator is specifically designed to improve the availability and performance of applications\nfor a global user base. It leverages the AWS global network infrastructure to route traffic to the optimal\nendpoint based on user location, network health, and endpoint health. It uses Anycast static IPs that serve as\na fixed entry point to applications, which minimizes DNS resolution time and offers quick failover to healthy\nendpoints.\nUDP Support: Global Accelerator supports UDP, which is crucial because the game uses the UDP protocol.\nUDP is preferred for online gaming because it's connectionless, leading to lower latency compared to TCP.\nReduced Latency: Global Accelerator intelligently routes traffic over the AWS global network, minimizing\nlatency.\nImproved Reliability: Global Accelerator constantly monitors the health of your application endpoints and\nonly directs traffic to healthy endpoints, thus reducing packet loss and creating a smoother gaming\nexperience. Its failover mechanism automatically reroutes traffic to the nearest available endpoint if one fails.\nRegional Deployment: By setting up endpoint groups in each region, you ensure players are routed to the\nnearest available server, further reducing latency and optimizing their gameplay experience.\nWhy Other Options are Incorrect:\nA. Transit Gateway with Inter-Region Peering: While transit gateways simplify network management, inter-\nregion peering through transit gateways does not inherently optimize latency or packet loss for real-time\napplications like online games. Transit Gateway adds an additional hop. It's also not optimized to specifically\nhandle UDP traffic with the low latency requirements of gaming applications.\nC. Amazon CloudFront with UDP: CloudFront primarily focuses on caching content closer to users for\nefficient content delivery. Although CloudFront supports UDP for some use cases, it's not optimized for\ndynamic, bi-directional, real-time UDP traffic required in online gaming. It's better suited for video streaming\nand file downloads. The core focus of CloudFront is content delivery, not low-latency communication.\nD. VPC Peering Mesh: A full mesh of VPC peering connections between eight regions results in a complex\nand difficult-to-manage network. While VPC peering establishes connectivity, it doesn't provide intelligent\nrouting or optimization for latency and packet loss across regions. It also requires manual management of\nrouting tables in each VPC. Also, UDP support on its own does not address packet loss reduction or intelligent\nrouting for latency.\nAuthoritative Links:\nAWS Global Accelerator: https://aws.amazon.com/global-accelerator/\nAWS Transit Gateway: https://aws.amazon.com/transit-gateway/\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nVPC Peering: https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html",
    "links": [
      "https://aws.amazon.com/global-accelerator/",
      "https://aws.amazon.com/transit-gateway/",
      "https://aws.amazon.com/cloudfront/",
      "https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts a three-tier web application on Amazon EC2 instances in a single Availability Zone. The web\napplication uses a self-managed MySQL database that is hosted on an EC2 instance to store data in an Amazon\nElastic Block Store (Amazon EBS) volume. The MySQL database currently uses a 1 TB Provisioned IOPS SSD (io2)\nEBS volume. The company expects traffic of 1,000 IOPS for both reads and writes at peak traffic.\nThe company wants to minimize any disruptions, stabilize performance, and reduce costs while retaining the\ncapacity for double the IOPS. The company wants to move the database tier to a fully managed solution that is\nhighly available and fault tolerant.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "B",
    "explanation": "The best solution is to use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General\nPurpose SSD (gp2) EBS volume.\nHere's why:\nRequirement fulfillment: This solution satisfies the core requirements of high availability, fault tolerance, and\ncost reduction while moving to a fully managed service.\nFully managed solution: Amazon RDS is a fully managed database service, eliminating the operational\noverhead of managing the database server.\nHigh availability and fault tolerance: A Multi-AZ deployment in RDS provides redundancy. If the primary\ninstance fails, RDS automatically fails over to the standby instance in another Availability Zone, minimizing\ndowntime.\nPerformance: While io2 EBS volume provides consistent performance, the requirement is for 1000 IOPS (with\na peak of 2000 IOPS). The gp2 volume often is enough for this IOPS level. Besides, a gp2 EBS volume with a\nlarge enough size (exceeding 334GB) is capable of delivering performance beyond 1000 IOPS. Additionally,\nRDS instances can also be sized up to meet additional performance requirements.\nCost optimization: gp2 volumes are generally more cost-effective than io2 volumes. This helps to reduce\ncosts without sacrificing performance for the specified workload.\nScalability: RDS allows easy scaling of the database instance and storage as needed.\nWhy other options are less ideal:\nOption A (io2 Block Express EBS): io2 Block Express offers higher IOPS and throughput, but it's more\nexpensive than gp2. The company expects traffic of only 1,000 IOPS (and up to 2000 IOPS), which can be\naccommodated by gp2. This would be an unnecessarily expensive option.\nOption C (S3 Intelligent-Tiering): Amazon S3 is for object storage, not for hosting a relational database.\nOption D (EC2 instances in active-passive mode): This would require significant manual effort for setup,\nfailover management, and database administration. It also involves higher costs associated with EC2 instance\nmanagement.\nAuthoritative Links:\nAmazon RDS Multi-AZ Deployments:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\nAmazon EBS Volume Types: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-\ntypes.html\nAmazon RDS Pricing: https://aws.amazon.com/rds/pricing/",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-",
      "https://aws.amazon.com/rds/pricing/"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts a serverless application on AWS. The application uses Amazon API Gateway, AWS Lambda, and\nan Amazon RDS for PostgreSQL database. The company notices an increase in application errors that result from\ndatabase connection timeouts during times of peak traffic or unpredictable traffic. The company needs a solution\nthat reduces the application failures with the least amount of change to the code.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Reduce the Lambda concurrency rate.",
      "B": "Enable RDS Proxy on the RDS DB instance.",
      "C": "Resize the RDS DB instance class to accept more connections.",
      "D": "Migrate the database to Amazon DynamoDB with on-demand scaling."
    },
    "answer": "B",
    "explanation": "The correct answer is B. Enable RDS Proxy on the RDS DB instance.\nHere's a detailed justification:\nDatabase connection timeouts in serverless applications, like the one described, are often caused by\nconnection exhaustion. Lambda functions, especially during periods of high concurrency, rapidly establish\ndatabase connections. RDS instances have a limit to the number of concurrent connections they can handle.\nWhen Lambda functions try to open new connections beyond this limit, connection timeouts occur.\nRDS Proxy sits between the Lambda functions and the RDS database. It pools and shares database\nconnections, effectively reducing the number of direct connections made from Lambda to RDS. This is a key\naspect of solving connection exhaustion. By multiplexing connections, RDS Proxy allows more Lambda\ninvocations to access the database without overwhelming it.\nOption A (Reduce the Lambda concurrency rate) would directly reduce the application's ability to serve\nrequests, degrading performance and user experience. It addresses the symptom but not the root cause. It is\nalso counter to the implied requirement of handling peak traffic.\nOption C (Resize the RDS DB instance class) might increase the maximum number of connections the\ndatabase can handle. However, it is a costly solution that requires significant downtime and doesn't address\nthe fundamental problem of inefficient connection management, especially for spiky workloads. It is also not\nthe \"least amount of change to the code\" as requested in the prompt, as it requires infrastructure changes.\nOption D (Migrate the database to Amazon DynamoDB) is a significantly larger change involving considerable\ncode refactoring and a completely different data model. DynamoDB is a NoSQL database and likely\nincompatible with the existing application's data schema and query patterns that are tailored for a relational\ndatabase (PostgreSQL). DynamoDB also has a completely different set of considerations for cost optimisation\nwhich will likely lead to a higher operational burden.\nRDS Proxy is designed to solve the exact problem of connection exhaustion in serverless applications using\nrelational databases with minimal code changes. It optimizes connection management and provides\nconnection pooling, leading to improved application stability during peak traffic. It is the most direct and\nefficient solution, requiring the least amount of code change.\nSupporting links:\nUsing Amazon RDS Proxy with AWS Lambda: Provides a detailed explanation of using RDS Proxy with Lambda\nto improve application performance.\nAmazon RDS Proxy: Official documentation for RDS Proxy, including features and benefits.\nBest Practices for Working with AWS Lambda Functions: Covers general best practices for using Lambda,\nincluding database connection management.",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is migrating an old application to AWS. The application runs a batch job every hour and is CPU\nintensive. The batch job takes 15 minutes on average with an on-premises server. The server has 64 virtual CPU\n(vCPU) and 512 GiB of memory.\nWhich solution will run the batch job within 15 minutes with the LEAST operational overhead?",
    "options": {},
    "answer": "D",
    "explanation": "The optimal solution for migrating the CPU-intensive batch job with minimal operational overhead is using\nAWS Batch on Amazon EC2. Here's why:\nAWS Batch is specifically designed for running batch computing workloads. It handles the complexities of\nprovisioning and managing compute resources, allowing the company to focus on the batch job itself. AWS\nBatch automatically scales the compute resources based on the job's requirements, ensuring it can complete\nwithin the desired 15-minute timeframe. https://aws.amazon.com/batch/\nAmazon EC2 offers a wide variety of instance types that can be optimized for CPU-intensive workloads.\nSelecting an instance type that meets the vCPU and memory requirements of the original server (or even\nsurpasses them for faster execution) ensures the batch job's performance.\nOperational Overhead: AWS Batch significantly reduces operational overhead because it manages the\nunderlying infrastructure. The company doesn't have to manually provision, configure, or manage EC2\ninstances. AWS Batch takes care of scheduling jobs, launching instances, and monitoring their progress.\nComparison with other options:\nAWS Lambda: While Lambda is serverless, it has limitations on execution time (currently 15 minutes\nmaximum), memory, and CPU resources. A 15-minute CPU-intensive job likely exceeds these limitations.\nAmazon ECS with Fargate: Fargate eliminates the need to manage servers, but it's primarily designed for\ncontainerized applications, adding complexity if the original application is not already containerized. Also,\nCPU and memory options on Fargate might require extensive configuration to meet the original server's\nspecifications.\nAmazon Lightsail with Auto Scaling: Lightsail is a simpler alternative to EC2, but it provides less flexibility\nand control over instance types and scaling policies than EC2 and AWS Batch. It may not be the most efficient\nway to manage a demanding batch job. Plus, the 512 GB memory requirement isn't easily achieved in Lightsail\ninstances.\nIn summary, AWS Batch on Amazon EC2 provides the right balance of performance, scalability, and reduced\noperational overhead for migrating the batch job to AWS. The ability to customize EC2 instances ensures the\napplication has the necessary resources to complete within the specified timeframe, while AWS Batch\nsimplifies management.",
    "links": [
      "https://aws.amazon.com/batch/"
    ]
  },
  {
    "question": "CertyIQ\nA company stores its data objects in Amazon S3 Standard storage. A solutions architect has found that 75% of the\ndata is rarely accessed after 30 days. The company needs all the data to remain immediately accessible with the\nsame high availability and resiliency, but the company wants to minimize storage costs.\nWhich storage solution will meet these requirements?",
    "options": {
      "A": "Move the data objects to S3 Glacier Deep Archive after 30 days.",
      "B": "Move the data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.",
      "C": "Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.",
      "D": "Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately."
    },
    "answer": "B",
    "explanation": "The correct answer is B, moving the data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30\ndays. This choice effectively balances cost optimization with the requirement for immediate data accessibility\nand high availability. S3 Standard-IA is designed for data that is infrequently accessed but requires rapid\nretrieval when needed. This perfectly aligns with the scenario where 75% of the data is rarely accessed after\n30 days, but the company still needs it to be immediately available.\nOption A, S3 Glacier Deep Archive, is unsuitable because it is designed for long-term archival with retrieval\ntimes ranging from hours to days. This violates the requirement for immediate accessibility. Option C, S3 One\nZone-IA, offers lower costs than S3 Standard-IA but compromises data durability and availability as it stores\ndata in a single Availability Zone (AZ). This contradicts the requirement for high availability and resiliency.\nOption D, moving data immediately to S3 One Zone-IA, similarly fails to meet the high availability and\nresiliency requirements.\nS3 Standard-IA offers the same high durability as S3 Standard (99.999999999% across multiple AZs) while\nproviding a lower storage cost. Although S3 Standard-IA has a retrieval fee, given that only 25% of the data is\nfrequently accessed, the cost savings on the 75% of infrequently accessed data will outweigh the retrieval\nfees. By transitioning the data after 30 days, the company minimizes costs while adhering to the specified\naccessibility and availability criteria. S3 Intelligent-Tiering could also be considered, but with a known access\npattern of 30 days, S3 Standard-IA provides a more predictable and potentially more cost-effective solution.\nFurther research can be found on the AWS website:\nS3 Storage Classes: https://aws.amazon.com/s3/storage-classes/\nS3 Standard-IA: https://aws.amazon.com/s3/storage-classes/ia/",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://aws.amazon.com/s3/storage-classes/ia/"
    ]
  },
  {
    "question": "CertyIQ\nA gaming company is moving its public scoreboard from a data center to the AWS Cloud. The company uses\nAmazon EC2 Windows Server instances behind an Application Load Balancer to host its dynamic application. The\ncompany needs a highly available storage solution for the application. The application consists of static files and\ndynamic server-side code.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "options": {
      "A": "Store the static files on Amazon S3. Use Amazon CloudFront to cache objects at the edge.",
      "B": "Store the static files on Amazon S3. Use Amazon ElastiCache to cache objects at the edge.",
      "C": "Store the server-side code on Amazon Elastic File System (Amazon EFS). Mount the EFS volume on each EC2",
      "D": "Here's a breakdown of why:"
    },
    "answer": "A",
    "explanation": "The correct answer is AD. Here's a breakdown of why:\nOption A: Store the static files on Amazon S3. Use Amazon CloudFront to cache objects at the edge.\nAmazon S3 is ideal for storing static files (images, CSS, JavaScript, etc.) due to its scalability, durability, and\ncost-effectiveness. (https://aws.amazon.com/s3/)\nAmazon CloudFront is a Content Delivery Network (CDN) that caches these static files at edge locations\nglobally. This reduces latency and improves performance for users accessing the scoreboard.\n(https://aws.amazon.com/cloudfront/)\nCloudFront improves availability by serving content from cached locations even if the origin server is\ntemporarily unavailable.\nOption D: Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows\nFile Server volume on each EC2 instance to share the files.\nAmazon FSx for Windows File Server is a fully managed Windows file server built on Windows Server.\n(https://aws.amazon.com/fsx/windows/)\nSince the application uses EC2 Windows Server instances, FSx for Windows File Server provides native\nWindows file system compatibility and supports features like SMB protocol.\nIt enables sharing the dynamic server-side code among multiple EC2 instances behind the Application Load\nBalancer, ensuring that all instances have access to the same codebase. This is crucial for consistent\napplication behavior and high availability.\nFSx for Windows File Server is designed for high availability, ensuring that the application's code is always\naccessible.\nLet's address why the other options are less suitable:\nOption B: Store the static files on Amazon S3. Use Amazon ElastiCache to cache objects at the edge.\nWhile ElastiCache is a powerful caching service, it is primarily designed for caching frequently accessed data\nfrom databases or compute-intensive operations. CloudFront is designed for caching static content.\nElastiCache doesn't provide edge locations and distribution capabilities like CloudFront.\nOption C: Store the server-side code on Amazon Elastic File System (Amazon EFS). Mount the EFS volume\non each EC2 instance to share the files.\nEFS is a good option for shared storage, but FSx for Windows File Server is a better fit in this specific scenario\nsince the EC2 instances are running Windows Server. EFS is designed for Linux instances.\nOption E: Store the server-side code on a General Purpose SSD (gp2) Amazon Elastic Block Store (Amazon\nEBS) volume. Mount the EBS volume on each EC2 instance to share the files.\nEBS volumes are block storage devices that are attached to a single EC2 instance. They cannot be directly\nshared between multiple instances. Sharing data would require complex solutions like clustering or\nreplication, making this approach unsuitable for a highly available, shared codebase.",
    "links": [
      "https://aws.amazon.com/s3/)",
      "https://aws.amazon.com/cloudfront/)",
      "https://aws.amazon.com/fsx/windows/)"
    ]
  },
  {
    "question": "CertyIQ\nA social media company runs its application on Amazon EC2 instances behind an Application Load Balancer (ALB).\nThe ALB is the origin for an Amazon CloudFront distribution. The application has more than a billion images stored\nin an Amazon S3 bucket and processes thousands of images each second. The company wants to resize the\nimages dynamically and serve appropriate formats to clients.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Install an external image management library on an EC2 instance. Use the image management library to",
      "B": "Create a CloudFront origin request policy. Use the policy to automatically resize images and to serve the",
      "C": "Use a [email protected] function with an external image management library. Associate the [email protected]",
      "D": "Create a CloudFront response headers policy. Use the policy to automatically resize images and to serve the"
    },
    "answer": "C",
    "explanation": "The correct answer is C, using a [email protected] function with an external image management library. Here's\nwhy:\nOption C offers the least operational overhead because it leverages the power of serverless computing\nthrough [email protected] This allows image processing to occur dynamically and automatically at the edge,\nclose to the users, without managing any EC2 instances or infrastructure. The [email protected] function is\ntriggered by CloudFront requests. The image management library resizes images based on user requests and\nserves the correct format based on the User-Agent header (for example, serving WebP to browsers that\nsupport it).\nOption A requires managing an EC2 instance and its associated patching, scaling, and maintenance. This adds\nconsiderable operational overhead, making it less desirable.\nOption B is incorrect because CloudFront origin request policies primarily manipulate requests before they\nare sent to the origin (in this case, S3 or the ALB). They can modify headers and query strings, but they cannot\nexecute arbitrary code to perform image resizing or format conversion. They are used for controlling caching\nbehavior, not real-time image manipulation.\nOption D is incorrect because CloudFront response header policies control the HTTP headers returned after\nthe response is received from the origin. They do not provide a mechanism for resizing images or performing\nformat conversion. Their main purpose is to add, modify, or remove headers.\n[email protected] functions are ideal for this scenario because they allow you to intercept CloudFront\nrequests and responses at various points in the content delivery process, execute custom code (image\nresizing using a library), and return the processed image to the user. This is the most scalable, cost-effective,\nand operationally efficient solution.\nRelevant links:\n[email protected] functions: https://aws.amazon.com/lambda/edge/\nCloudFront policies: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-\nmanaged-response-headers-policies.html (explains response header policies)\nCloudFront origin request policies:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-managed-origin-request-\npolicies.html",
    "links": [
      "https://aws.amazon.com/lambda/edge/",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-managed-origin-request-"
    ]
  },
  {
    "question": "CertyIQ\nA hospital needs to store patient records in an Amazon S3 bucket. The hospitals compliance team must ensure\nthat all protected health information (PHI) is encrypted in transit and at rest. The compliance team must administer\nthe encryption key for data at rest.\nWhich solution will meet these requirements?",
    "options": {
      "C": "Here's why:"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Here's why:\nEncryption in Transit: The requirement for encryption in transit is met by enforcing the use of HTTPS (TLS) for\nall connections to the S3 bucket. This is achieved by using the aws:SecureTransport condition in the S3 bucket\npolicy. This policy condition checks if the request is made over HTTPS, preventing unencrypted traffic.\nEncryption at Rest: The requirement for encryption at rest is met by configuring default encryption for the S3\nbucket. Server-Side Encryption with KMS keys (SSE-KMS) allows using KMS to manage encryption keys. This\nway, data is encrypted when stored in S3.\nKey Management Control: The compliance team's requirement to administer the encryption key for data at\nrest is met by using SSE-KMS and assigning the compliance team permissions to manage the KMS keys. This\nallows them to control the encryption and decryption process.\nWhy other options are incorrect:\nA: While using ACM for SSL/TLS certificates is valid for HTTPS, associating the certificate directly with S3\nisn't how encryption in transit is enforced on the bucket level. You control it via the bucket policy.\nB: SSE-S3 (Server-Side Encryption with S3 Managed Keys) doesn't give the compliance team control over the\nkeys, as AWS manages them. This violates the requirement.\nD: While using HTTPS for encryption in transit is correct, Macie is primarily a data security and privacy service\nthat uses machine learning to discover, classify, and protect sensitive data in AWS. It doesn't directly handle\nencryption, so the Compliance team would not manage the keys. While Macie is useful, it doesn't provide the\nrequired key management capabilities specified.\nSupporting Documentation:\nProtecting Data Using Server-Side Encryption with KMS keys (SSE-KMS):\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html\nRequiring Encryption of Data in Transit: https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-\nbucket-policies.html#example-bucket-policies-require-encryption\nAWS Key Management Service (KMS): https://aws.amazon.com/kms/\nAmazon Macie: https://aws.amazon.com/macie/",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-",
      "https://aws.amazon.com/kms/",
      "https://aws.amazon.com/macie/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses Amazon API Gateway to run a private gateway with two REST APIs in the same VP",
    "options": {
      "C": "They minimize the need for code changes and are the most direct solution to the problem.",
      "B": "Use an interface endpoint.",
      "A": "Add an X-API-Key header in the HTTP header for authorization: Adding an API key is for authorization, it",
      "D": "Add an Amazon Simple Queue Service (Amazon SQS) queue between the two REST APIs: This introduces"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Use an interface endpoint.\nHere's a detailed justification:\nThe problem describes a scenario where two private REST APIs within the same VPC, managed by Amazon\nAPI Gateway, are communicating over the internet instead of staying within the VPC network. The goal is to\nensure that BuyStock communicates with CheckFunds through the VPC, minimizing code changes.\nWhy Interface Endpoints are Ideal: Interface endpoints, powered by AWS PrivateLink, create private network\nconnections between your VPC and supported AWS services without exposing traffic to the public internet. In\nthis case, an interface endpoint can be created for the API Gateway service within the VPC. This endpoint\nprovides a private IP address within the VPC that the BuyStock API can use to access the CheckFunds API via\nAPI Gateway, ensuring all communication stays within the VPC. This approach requires minimal or no code\nchanges, typically only needing to update the endpoint URL.\nWhy other options are not optimal:\nA. Add an X-API-Key header in the HTTP header for authorization: Adding an API key is for authorization, it\ndoesn't change the routing of network traffic and won't force communication through the VPC.\nC. Use a gateway endpoint: Gateway endpoints only support Amazon S3 and DynamoDB. They can't be used\nwith API Gateway.\nD. Add an Amazon Simple Queue Service (Amazon SQS) queue between the two REST APIs: This introduces\nasynchronous communication via a queue. While functional, it requires significant architectural and code\nchanges to both APIs to publish and consume messages, violating the \"fewest changes to the code\"\nrequirement. It also changes the communication paradigm from request/response to asynchronous\nmessaging, which may not be desirable.\nIn essence, Interface endpoints are designed specifically for securely connecting to AWS services privately\nfrom your VPC. They minimize the need for code changes and are the most direct solution to the problem.\nAuthoritative Links:\nAWS PrivateLink: https://aws.amazon.com/privatelink/\nUsing API Gateway with VPC Endpoints:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-apis.html",
    "links": [
      "https://aws.amazon.com/privatelink/",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-apis.html"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts a multiplayer gaming application on AWS. The company wants the application to read data with\nsub-millisecond latency and run one-time queries on historical data.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Use Amazon RDS for data that is frequently accessed. Run a periodic custom script to export the data to an",
      "B": "Store the data directly in an Amazon S3 bucket. Implement an S3 Lifecycle policy to move older data to S3",
      "C": "Here's why:",
      "D": "Use Amazon DynamoDB for data that is frequently accessed. Turn on streaming to Amazon Kinesis Data"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Here's why:\nSub-millisecond latency for frequent access: DynamoDB, a NoSQL database, is designed for low-latency\naccess. DynamoDB Accelerator (DAX) further enhances this by providing an in-memory cache, enabling sub-\nmillisecond response times for frequently accessed data. This directly addresses the application's\nrequirement for fast reads.(https://aws.amazon.com/dynamodb/dax/)\nOne-time queries on historical data: DynamoDB table export allows you to export data to S3 in a cost-\neffective and scalable manner. Athena, a serverless query service, can then be used to run SQL queries on the\ndata stored in S3. This satisfies the need for ad-hoc querying of historical information without requiring\npersistent infrastructure.(https://aws.amazon.com/athena/,\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBExport.html)\nLeast operational overhead: DynamoDB is a managed service, reducing the burden of database\nadministration. DAX is also managed, simplifying caching. Athena is serverless, eliminating the need to\nmanage servers for querying. The DynamoDB export feature is straightforward to configure and automate.\nLet's analyze why the other options are less suitable:\nA: RDS (a relational database) might not provide the same level of sub-millisecond latency as DynamoDB,\nespecially for large-scale gaming applications. Creating and maintaining a custom script for exporting data\nadds operational overhead.\nB: Storing all data in S3 directly would not meet the low-latency requirement for frequently accessed data.\nS3 is object storage and not suitable for real-time reads needed by a gaming application. While Athena can\nquery S3 data, it is not ideal for accessing frequently used game data.\nD: Kinesis Data Streams and Firehose would introduce unnecessary complexity and latency. Streaming the\nentire DynamoDB data stream to S3 just for querying historical data is an overkill and more complex than\nusing DynamoDB export and Athena.",
    "links": [
      "https://aws.amazon.com/dynamodb/dax/)",
      "https://aws.amazon.com/athena/,",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBExport.html)"
    ]
  },
  {
    "question": "CertyIQ\nA company uses a payment processing system that requires messages for a particular payment ID to be received\nin the same order that they were sent. Otherwise, the payments might be processed incorrectly.\nWhich actions should a solutions architect take to meet this requirement? (Choose two.)",
    "options": {
      "A": "Write the messages to an Amazon DynamoDB table with the payment ID as the partition key.",
      "B": "Write the messages to an Amazon Kinesis data stream with the payment ID as the partition key.",
      "C": "Write the messages to an Amazon ElastiCache for Memcached cluster with the payment ID as the key.",
      "D": "Amazon SQS FIFO (First-In-First-Out) queues are specifically designed to guarantee that messages are"
    },
    "answer": "B",
    "explanation": "The correct answer is BE. Here's why:\nOption B: Write the messages to an Amazon Kinesis data stream with the payment ID as the partition key.\nKinesis Data Streams inherently guarantees ordering of records within a shard. When you use the payment ID\nas the partition key, all messages related to the same payment ID will be written to the same shard. This\nensures that they are processed in the order they were sent, fulfilling the requirement of sequential\nprocessing for each payment. Kinesis is designed for real-time data streaming and processing, making it a\nsuitable choice for managing payment messages.\nhttps://docs.aws.amazon.com/kinesis/latest/dev/kinesis-data-streams.html\nOption E: Write the messages to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the\nmessage group to use the payment ID.\nAmazon SQS FIFO (First-In-First-Out) queues are specifically designed to guarantee that messages are\nprocessed in the exact order they were sent. By setting the message group ID to the payment ID, you instruct\nSQS to maintain the order of messages within that group. This ensures that all messages related to a\nparticular payment ID are received and processed in the correct sequence, meeting the ordering requirement.\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\nWhy other options are incorrect:\nOption A: DynamoDB does not guarantee ordering of items within a partition key. While you can retrieve all\nitems for a specific payment ID, the order in which they are returned is not guaranteed to be the same as the\norder in which they were written.\nOption C: ElastiCache (Memcached) is primarily used for caching and is not designed for reliable message\nqueuing or guaranteed ordering.\nOption D: Standard SQS queues do not guarantee message ordering. While message attributes can be used,\nthey do not enforce the sequential processing needed in this scenario. Using standard queues could lead to\nincorrect payment processing.",
    "links": [
      "https://docs.aws.amazon.com/kinesis/latest/dev/kinesis-data-streams.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is building a game system that needs to send unique events to separate leaderboard, matchmaking,\nand authentication services concurrently. The company needs an AWS event-driven system that guarantees the\norder of the events.\nWhich solution will meet these requirements?",
    "options": {
      "B": "Amazon Simple Notification Service (Amazon SNS) FIFO topics. Here's why:"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Amazon Simple Notification Service (Amazon SNS) FIFO topics. Here's why:\nThe key requirements are sending events to multiple services concurrently and guaranteeing the order of\nevents.\nAmazon SNS FIFO Topics: SNS FIFO (First-In-First-Out) topics are specifically designed to maintain the exact\norder of messages published to them. They also support message fanout to multiple subscribers\n(leaderboard, matchmaking, and authentication services in this case), fulfilling the concurrency requirement.\nMessages are grouped into message groups based on a message group ID, and messages within a group are\ndelivered in order.\nAmazon EventBridge: EventBridge allows for event routing based on rules, but it primarily focuses on\ndecoupling event producers from consumers. While EventBridge supports ordering through event replay, it's\nnot designed for guaranteed ordering in concurrent fan-out scenarios like SNS FIFO topics.\nAmazon SNS Standard Topics: SNS standard topics provide high throughput and best-effort ordering, but do\nnot guarantee the order of messages. This violates the order requirement.\nAmazon SQS FIFO Queues: SQS FIFO queues guarantee message order, but they are point-to-point (one\nsender to one receiver). To send the same events to three services concurrently, you would need three\nseparate SQS FIFO queues, one for each service. This would not meet the need for a single event triggering all\nthree. While you could implement a fan-out mechanism on top of SQS using Lambda or similar, this is more\ncomplex and less efficient than using SNS FIFO topics, which handles fan-out natively with ordering.\nIn summary, SNS FIFO topics provide a native solution for concurrently distributing ordered events to multiple\nsubscribers, making it the most efficient and appropriate choice for this scenario.\nAuthoritative Links:\nAmazon SNS FIFO topics: https://docs.aws.amazon.com/sns/latest/dg/sns-fifo-topics.html\nAmazon EventBridge: https://aws.amazon.com/eventbridge/\nAmazon SQS FIFO queues:\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html",
    "links": [
      "https://docs.aws.amazon.com/sns/latest/dg/sns-fifo-topics.html",
      "https://aws.amazon.com/eventbridge/",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html"
    ]
  },
  {
    "question": "CertyIQ\nA hospital is designing a new application that gathers symptoms from patients. The hospital has decided to use\nAmazon Simple Queue Service (Amazon SQS) and Amazon Simple Notification Service (Amazon SNS) in the\narchitecture.\nA solutions architect is reviewing the infrastructure design. Data must be encrypted at rest and in transit. Only\nauthorized personnel of the hospital should be able to access the data.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "options": {
      "A": "Turn on server-side encryption on the SQS components. Update the default key policy to restrict key usage",
      "B": "Turn on server-side encryption on the SNS components by using an AWS Key Management Service (AWS",
      "C": "Turn on encryption on the SNS components. Update the default key policy to restrict key usage to a set of",
      "D": "Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS"
    },
    "answer": "B",
    "explanation": "The question requires ensuring data in transit and at rest encryption, and restricting access to only authorized\npersonnel for SQS and SNS components.\nOption B is correct because it properly utilizes AWS KMS for server-side encryption on SNS topics. Using a\ncustomer managed key in KMS allows for granular control over the key's policy, which can be restricted to\nauthorized hospital personnel (principals). This ensures only authorized users and services can encrypt and\ndecrypt messages published to and consumed from the SNS topic.\nOption D is correct because it employs a similar strategy for SQS queues. AWS KMS is used for server-side\nencryption. Applying a custom key policy to restrict usage to specific hospital personnel controls who can\ndecrypt messages in the queue. Setting a condition in the queue policy to allow only encrypted connections\nover TLS (Transport Layer Security) enforces encryption in transit between clients and SQS, fulfilling the\nrequirement for encryption during transport.\nOption A is incorrect because while it enables server-side encryption on SQS, it relies on updating the default\nkey policy. AWS KMS Customer managed keys are preferable as they offer more fine-grained access\ncontrol.Option C is incorrect. It incorrectly states that the SNS encryption policy should be updated, rather\nthan using KMS. Also, setting a condition to allow only encrypted connections over TLS won't be enough, a\ncustom KMS key policy will still be needed to restrict access.Option E is incorrect because while it correctly\nuses KMS for SQS encryption, it mentions updating the IAM policy to restrict key usage instead of the KMS\nkey policy. The KMS key policy, not an IAM policy, is what controls who can use the key for\nencryption/decryption.\nTherefore, options B and D are the most appropriate to meet the specified security and access control\nrequirements.\nRelevant links:\nSNS Encryption:\nSQS Encryption:\nKMS Key Policies:",
    "links": []
  },
  {
    "question": "CertyIQ\nA company runs a web application that is backed by Amazon RDS. A new database administrator caused data loss\nby accidentally editing information in a database table. To help recover from this type of incident, the company\nwants the ability to restore the database to its state from 5 minutes before any change within the last 30 days.\nWhich feature should the solutions architect include in the design to meet this requirement?",
    "options": {
      "C": "Automated backups. Here's a detailed justification:"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Automated backups. Here's a detailed justification:\nAutomated backups in Amazon RDS create point-in-time recovery (PITR) points for your database. These\nbackups enable you to restore your database instance to a specific point in time within the configured\nretention period, which can be up to 35 days. This directly addresses the requirement to restore the database\nto its state from 5 minutes before any change within the last 30 days. RDS uses the database's native backup\nmechanisms, allowing for consistent backups and restores. The transaction logs are also backed up.\nRead replicas (A) are used to offload read traffic from the primary database, improving performance. While\nread replicas can be promoted to become a standalone database in case of a primary database failure, they\ndon't provide granular point-in-time recovery. The data on a read replica replicates from the primary database,\nincluding the unintended changes made by the database administrator.\nManual snapshots (B) provide a full backup of the database at a specific point in time, but they require\nmanual initiation. While helpful, they do not automatically provide the 5-minute recovery granularity needed\nas they require on demand intervention. To achieve this, you'd need continuous manual snapshots, which is\nimpractical and resource-intensive.\nMulti-AZ deployments (D) provide high availability by synchronously replicating data to a standby instance in\na different Availability Zone. If the primary instance fails, the standby instance automatically takes over.\nHowever, Multi-AZ does not prevent or recover from data corruption or accidental data changes; it simply\nensures the database remains available. The change will replicate to both AZs.\nTherefore, automated backups are the only feature that enables the necessary point-in-time recovery to\nmeet the company's recovery objective.\nFor further information, refer to the AWS documentation on RDS automated backups:\nAmazon RDS Backups",
    "links": []
  },
  {
    "question": "CertyIQ\nA companys web application consists of an Amazon API Gateway API in front of an AWS Lambda function and an\nAmazon DynamoDB database. The Lambda function handles the business logic, and the DynamoDB table hosts the\ndata. The application uses Amazon Cognito user pools to identify the individual users of the application. A solutions\narchitect needs to update the application so that only users who have a subscription can access premium content.\nWhich solution will meet this requirement with the LEAST operational overhead?",
    "options": {
      "A": "Enable API caching and throttling on the API Gateway API: Caching and throttling are primarily for",
      "B": "Set up AWS WAF on the API Gateway API. Create a rule to filter users who have a subscription: AWS",
      "C": "Apply fine-grained IAM permissions to the premium content in the DynamoDB table: While fine-grained",
      "D": "Implement API usage plans and API keys to limit the access of users who do not"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Implement API usage plans and API keys to limit the access of users who do not\nhave a subscription.\nHere's a detailed justification:\nOption D offers the most straightforward and efficient method to control access to premium content based on\nsubscription status directly within API Gateway, minimizing operational overhead. API Gateway usage plans\nallow you to define quotas and throttling for API usage. API keys can be associated with these usage plans. By\nassociating subscribed users with a usage plan that permits access to premium content and non-subscribed\nusers with a plan that restricts it, you can effectively manage access. This requires the Lambda function to\ndetermine user subscription status and assign the correct API key.\nHere's why the other options are less suitable:\nA. Enable API caching and throttling on the API Gateway API: Caching and throttling are primarily for\nperformance optimization and preventing abuse, not for authentication or authorization based on subscription\nstatus. They don't inherently differentiate between subscribed and non-subscribed users.\nB. Set up AWS WAF on the API Gateway API. Create a rule to filter users who have a subscription: AWS\nWAF is designed for protecting web applications from common web exploits and bots. While it can filter\nrequests based on IP addresses or other request characteristics, it's not designed for handling user\nauthentication or subscription-based access control. Implementing subscription-based access control in WAF\nwould be complex and require custom rules that examine request headers or cookies, adding unnecessary\noverhead.\nC. Apply fine-grained IAM permissions to the premium content in the DynamoDB table: While fine-grained\nIAM permissions are useful for controlling access to DynamoDB data, they are not the most efficient method\nin this case. It would require the Lambda function to assume different IAM roles or generate temporary\nsecurity credentials based on the user's subscription status, adding complexity to the Lambda function and\nrequiring more complex management of IAM roles and policies. Access control logic should ideally be\nhandled before the database layer when possible. Furthermore, managing IAM permissions at a granular level\nfor each user subscription would become cumbersome as the user base scales.\nUsing API Gateway's built-in features for access control is more native and less operationally intensive.\nSupporting Documentation:\nAPI Gateway Usage Plans: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-\nusage-plans.html\nAPI Gateway API Keys: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-usage-\nplans-api-keys.html",
    "links": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-usage-"
    ]
  },
  {
    "question": "CertyIQ\nA company is using Amazon Route 53 latency-based routing to route requests to its UDP-based application for\nusers around the world. The application is hosted on redundant servers in the company's on-premises data centers\nin the United States, Asia, and Europe. The companys compliance requirements state that the application must be\nhosted on premises. The company wants to improve the performance and availability of the application.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Here's a detailed justification:",
      "B": "4. On-premises Integration: The NLBs are configured to address the on-premises endpoints, fulfilling",
      "C": "Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises",
      "D": "Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-premises"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's a detailed justification:\nThe problem requires improving performance and availability of a UDP-based application hosted on-premises\nacross multiple regions, while complying with hosting requirements. Route 53 latency-based routing is\nalready in place.\nOption A is the most suitable solution because it leverages AWS Global Accelerator in conjunction with\nNetwork Load Balancers (NLBs). NLBs are ideal for UDP traffic.\nHere's why it works:\n1. NLBs for UDP: NLBs are designed to handle UDP traffic efficiently. They can forward UDP traffic\ndirectly to on-premises servers without modification. Application Load Balancers (ALBs), used in\noptions B and D, do not support UDP.\n2. AWS Global Accelerator: Global Accelerator provides static IP addresses that serve as entry points\nfor the application. It intelligently routes traffic to the nearest healthy endpoint (the NLB) based on\nnetwork conditions and geographic proximity. This minimizes latency and improves performance.\n3. Regional Redundancy: Using NLBs in multiple AWS regions (US, Asia, and Europe) provides\nredundancy and increases availability. If one NLB becomes unavailable, Global Accelerator\nautomatically reroutes traffic to another healthy NLB.\n4. On-premises Integration: The NLBs are configured to address the on-premises endpoints, fulfilling\nthe requirement to host the application on-premises.\n5. Simplified DNS Management: Providing access via a CNAME record pointing to the Global\nAccelerator DNS name simplifies DNS management and allows clients to connect to the optimal\nendpoint.\nOption B is incorrect because ALBs do not support UDP traffic.\nOptions C and D are incorrect because while CloudFront can improve performance for caching static content,\nit is not suitable for UDP-based applications requiring real-time, bidirectional communication. Also CloudFront\nis mainly used for caching web content at edge locations, not for direct routing of UDP traffic to on-premises\nservers. CloudFront works best with HTTP/HTTPS. Also, CloudFront does not inherently provide the same\nlevel of global traffic management and failover capabilities as AWS Global Accelerator for UDP workloads.\nIn summary, using NLBs with Global Accelerator is the most efficient and reliable solution for improving the\nperformance and availability of a UDP-based application hosted on-premises across multiple regions.\nSupporting Documentation:\nAWS Global Accelerator: https://aws.amazon.com/global-accelerator/\nNetwork Load Balancer: https://aws.amazon.com/elasticloadbalancing/network-load-balancer/",
    "links": [
      "https://aws.amazon.com/global-accelerator/",
      "https://aws.amazon.com/elasticloadbalancing/network-load-balancer/"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect wants all new users to have specific complexity requirements and mandatory rotation periods\nfor IAM user passwords.\nWhat should the solutions architect do to accomplish this?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A: Set an overall password policy for the entire AWS account. This is the most efficient\nand scalable method to enforce consistent password complexity and rotation requirements across all IAM\nusers within an AWS account.\nIAM (Identity and Access Management) provides a global password policy feature at the account level. This\nallows you to define requirements like minimum password length, required character types (uppercase,\nlowercase, numbers, symbols), password reuse prevention, and maximum password age (rotation period).\nOnce the password policy is set at the account level, all IAM users are automatically subject to those\nrequirements.\nOption B is incorrect because setting password policies individually for each user is not a scalable or\nmaintainable approach, especially in an environment with many users. It would require significant manual\neffort and could lead to inconsistencies.\nOption C is incorrect because while third-party identity providers can be integrated with AWS, using them\nsolely for password policy enforcement is an overkill. AWS native IAM features handle this efficiently.\nEmploying a third-party vendor just for password rules would add unnecessary complexity and cost.\nOption D is incorrect because while CloudWatch Events can monitor IAM events, it is not the intended\nmechanism for setting password policies. CloudWatch Events are best suited for reacting to state changes or\ntriggering automated actions based on events, not for enforcing global security policies like password\ncomplexity. Additionally, directly setting passwords for users programmatically violates the security principle\nof least privilege.\nTherefore, leveraging the built-in IAM password policy feature offers the most straightforward and\nmaintainable solution for establishing mandatory password complexity and rotation for all IAM users in the\nAWS account.\nRelevant\ndocumentation:https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_password_policy.htmlhttps://aws.amazon.com/iam/features/",
    "links": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_password_policy.htmlhttps://aws.amazon.com/iam/features/"
    ]
  },
  {
    "question": "CertyIQ\nA company has migrated an application to Amazon EC2 Linux instances. One of these EC2 instances runs several 1-\nhour tasks on a schedule. These tasks were written by different teams and have no common programming\nlanguage. The company is concerned about performance and scalability while these tasks run on a single instance.\nA solutions architect needs to implement a solution to resolve these concerns.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A: Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon\nEventBridge (Amazon CloudWatch Events).\nHere's why:\nAWS Batch is designed for running batch computing workloads on AWS. It handles the complexities of\nprovisioning and managing compute resources, allowing you to focus on defining your tasks. It dynamically\nprovisions the optimal quantity and type of compute resources (e.g., EC2 instances) based on the resource\nrequirements of the jobs submitted. This dynamic scaling improves performance and scalability. The tasks,\nbeing written in different languages, benefit from Batch's ability to execute various types of applications\nwithout requiring code modifications.\nAmazon EventBridge (formerly CloudWatch Events) provides a serverless event bus service that makes it easy\nto connect applications with data from a variety of sources. EventBridge can be used to schedule tasks in\nAWS Batch. This simplifies the scheduling aspect without needing to manage a separate scheduler on the\nEC2 instance.\nOption B (AWS App Runner) is less suitable. While App Runner is simple for deploying containerized web\napplications and APIs, it's not optimized for batch processing or scheduled tasks. Converting to a container\nadds an extra layer of complexity with limited benefit for these specific task requirements.\nOption C (AWS Lambda) might seem plausible, but it's generally not ideal. While Lambda can handle\nscheduled tasks, there's a limitation on execution time (15 minutes) and potentially memory limitations\ndepending on the tasks' requirements. Refactoring these tasks into Lambda functions would likely require\nsignificant code modifications, increasing operational overhead and negating the requirement that teams do\nnot have a common programming language. The 1-hour task duration also makes Lambda less suitable.\nOption D (Auto Scaling group with an AMI) introduces significant operational overhead. You'd need to manage\nthe AMI, configure scaling policies, and ensure the EC2 instances within the Auto Scaling group have the\nnecessary dependencies and configurations. This is less efficient than using AWS Batch, which abstracts\naway much of the infrastructure management. The tasks would still compete for resources on the instance.\nTherefore, AWS Batch with EventBridge scheduling provides the best balance of performance, scalability, and\nminimal operational overhead.\nRelevant Documentation:\nAWS Batch: https://aws.amazon.com/batch/\nAmazon EventBridge: https://aws.amazon.com/eventbridge/",
    "links": [
      "https://aws.amazon.com/batch/",
      "https://aws.amazon.com/eventbridge/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a public three-tier web application in a VP",
    "options": {
      "C": "Provision a NAT gateway in a public subnet. Modify each private subnet's route table",
      "A": "Provision a NAT instance in a public subnet. Modify each private subnet's route table with a default route",
      "B": "Provision a NAT instance in a private subnet. Modify each private subnet's route table with a default route",
      "D": "Provision a NAT gateway in a private subnet. Modify each private subnet's route table with a default route"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Provision a NAT gateway in a public subnet. Modify each private subnet's route table\nwith a default route that points to the NAT gateway.\nHere's a detailed justification:\nThe requirement is to allow EC2 instances in private subnets to access the internet (specifically, a license\nserver) in a managed and operationally efficient way. NAT (Network Address Translation) is the service that\nallows instances in private subnets to connect to the internet while hiding their private IP addresses.\nOptions A and B involve NAT instances. While NAT instances can provide NAT functionality, they require\nmanual patching, scaling, and are more complex to manage. The question explicitly states the need to\nminimize operational maintenance, making NAT instances a less desirable choice. NAT instances also\nrepresent a single point of failure if not configured for high availability.\nOptions C and D use NAT gateways, which are managed AWS services. This addresses the \"managed solution\"\nrequirement, reducing operational overhead. NAT gateways are highly available by default and scale\nautomatically.\nThe difference between options C and D lies in where the NAT gateway is placed: a public subnet or a private\nsubnet. A NAT gateway must reside in a public subnet. This is because the NAT gateway needs a public IP\naddress to forward traffic to the internet. If a NAT gateway were in a private subnet, it would need to route\nthrough another NAT device or an internet gateway to reach the internet, which defeats the purpose of a\nmanaged solution.\nTherefore, option C is the correct answer because it uses the managed NAT gateway service located in a\npublic subnet, minimizing operational maintenance and enabling internet access for instances in private\nsubnets via route table modifications.\nFurther Research:\nAWS NAT Gateway: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\nNAT Instances vs. NAT Gateways: https://aws.amazon.com/premiumsupport/knowledge-center/nat-\ngateway-instance-differences/",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/nat-"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to host a digital media\nstreaming application. The EKS cluster will use a managed node group that is backed by Amazon Elastic Block\nStore (Amazon EBS) volumes for storage. The company must encrypt all data at rest by using a customer managed\nkey that is stored in AWS Key Management Service (AWS KMS).\nWhich combination of actions will meet this requirement with the LEAST operational overhead? (Choose two.)",
    "options": {
      "A": "Use a Kubernetes plugin that uses the customer managed key to perform data encryption. While",
      "B": "After creation of the EKS cluster, locate the EBS volumes. Enable encryption by using the customer",
      "C": "Enable EBS encryption by default in the AWS Region where the EKS cluster will be created. Select the",
      "D": "Create the EKS cluster. Create an IAM role that has a policy that grants permission to the customer"
    },
    "answer": "C",
    "explanation": "The correct answer is CD. Here's a detailed justification:\nC. Enable EBS encryption by default in the AWS Region where the EKS cluster will be created. Select the\ncustomer managed key as the default key.\nThis is the most efficient method. By enabling EBS encryption by default at the AWS Region level, all new EBS\nvolumes created in that Region, including those automatically provisioned for the EKS managed node group,\nwill be encrypted using the specified customer-managed KMS key. This eliminates the need to manually\nencrypt each volume individually, reducing operational overhead significantly. AWS handles the encryption\nautomatically during EBS volume creation. This approach ensures that encryption is applied consistently and\nwithout manual intervention for future volumes as well.\nD. Create the EKS cluster. Create an IAM role that has a policy that grants permission to the customer\nmanaged key. Associate the role with the EKS cluster.\nEKS cluster already has its IAM role, and this role requires KMS decrypt/encrypt permissions to use KMS keys\nfor encrypting resources. When EBS encryption by default is enabled, EKS cluster needs to have the ability to\nuse the CMK for encryption/decryption operations during volume creation and management by Kubernetes on\nthe worker nodes. Associating an IAM role with the EKS cluster that grants permissions to the KMS key is\nessential for allowing the cluster to encrypt and decrypt EBS volumes using the customer-managed key\nspecified. Without these permissions, the encryption process will fail.\nWhy other options are incorrect:\nA. Use a Kubernetes plugin that uses the customer managed key to perform data encryption. While\npossible, using a Kubernetes plugin adds complexity and operational overhead. It involves deploying and\nmanaging the plugin, configuring it correctly, and ensuring it integrates seamlessly with EBS volume creation.\nThe default encryption feature of EBS provides a simpler solution.\nB. After creation of the EKS cluster, locate the EBS volumes. Enable encryption by using the customer\nmanaged key. This approach requires manual intervention after the cluster is created, leading to higher\noperational overhead. It also necessitates discovering the EBS volumes and applying encryption, which is\ntime-consuming and prone to errors. It also might not be possible to encrypt root volumes without recreating\nthe EC2 instances behind the worker nodes.\nE. Store the customer managed key as a Kubernetes secret in the EKS cluster. Use the customer managed\nkey to encrypt the EBS volumes. This is a security risk. Storing KMS keys directly as Kubernetes secrets\nexposes them and violates security best practices. KMS keys should never be stored within the cluster itself.\nThey should be managed and accessed through IAM roles with appropriate permissions.\nSupporting documentation:\nAWS KMS documentation\nAmazon EBS encryption\nEKS IAM role requirement",
    "links": []
  },
  {
    "question": "CertyIQ\nA company wants to migrate an Oracle database to AWS. The database consists of a single table that contains\nmillions of geographic information systems (GIS) images that are high resolution and are identified by a\ngeographic code.\nWhen a natural disaster occurs, tens of thousands of images get updated every few minutes. Each geographic\ncode has a single image or row that is associated with it. The company wants a solution that is highly available and\nscalable during such events.\nWhich solution meets these requirements MOST cost-effectively?",
    "options": {
      "A": "Store the images and geographic codes in a database table. Use Oracle running on an Amazon RDS Multi-AZ",
      "B": "Option D: This option combines the best aspects of the other approaches. Storing images in S3 provides cost-",
      "C": "Store the images and geographic codes in an Amazon DynamoDB table. Configure DynamoDB Accelerator",
      "D": "Store the images in Amazon S3 buckets. Store geographic codes and image S3 URLs in a database table."
    },
    "answer": "D",
    "explanation": "The correct answer is D, not B. Here's why:\nLet's analyze each option based on the requirements of high availability, scalability, and cost-effectiveness\nfor managing GIS images identified by geographic codes, with frequent updates during natural disasters.\nOption A: Using Oracle on RDS Multi-AZ for both image storage and geographic codes is highly inefficient and\nexpensive. Storing binary image data directly in a relational database is generally discouraged due to\nperformance overhead and storage limitations. RDS, while highly available, isn't as horizontally scalable as\nother solutions for this specific use case.\nOption B: Storing images in S3 and using DynamoDB to map geographic codes to S3 URLs is a good starting\npoint, leveraging the scalability and cost-effectiveness of both services. However, DynamoDB is a NoSQL\ndatabase suitable for frequent reads and writes, but may not scale in write capacity for tens of thousands of\nupdates every few minutes.\nOption C: Storing both images and geographic codes in DynamoDB is not ideal for storing large binary files\nlike images. While DynamoDB is scalable, storing images directly in the database becomes expensive and less\nperformant. DAX is a good caching solution, but it won't address the fundamental issue of storing large binary\ndata within DynamoDB.\nOption D: This option combines the best aspects of the other approaches. Storing images in S3 provides cost-\neffective, scalable, and durable storage for the large image files. Storing geographic codes and\ncorresponding S3 URLs in Oracle on RDS Multi-AZ is also not ideal (as it can be costly), but it will allow\nrelational queries that DynamoDB cannot perform. For example, finding all images of a certain type, size, or\ndate, which may become necessary in the future.\nTherefore, Option B is the most cost-effective and ideal solution.",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has an application that collects data from IoT sensors on automobiles. The data is streamed and stored\nin Amazon S3 through Amazon Kinesis Data Firehose. The data produces trillions of S3 objects each year. Each\nmorning, the company uses the data from the previous 30 days to retrain a suite of machine learning (ML) models.\nFour times each year, the company uses the data from the previous 12 months to perform analysis and train other\nML models. The data must be available with minimal delay for up to 1 year. After 1 year, the data must be retained\nfor archival purposes.\nWhich storage solution meets these requirements MOST cost-effectively?",
    "options": {
      "A": "Use the S3 Intelligent-Tiering storage class. Create an S3 Lifecycle policy to transition objects to S3 Glacier",
      "B": "Use the S3 Intelligent-Tiering storage class. Configure S3 Intelligent-Tiering to automatically move objects",
      "C": "Use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create an S3 Lifecycle policy to",
      "D": "Use the S3 Standard storage class. Create an S3 Lifecycle policy to transition objects to S3 Standard-"
    },
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the most cost-effective solution, along with supporting\nconcepts and links:\nThe core requirement is to minimize storage costs while providing timely access to data for specific periods,\nfollowed by long-term archival. The data is actively used for 30 days, less frequently for up to a year, and then\narchived.\nOption D utilizes S3 Standard initially because the data is accessed frequently during the first 30 days for\ndaily model retraining. S3 Standard offers the lowest access latency, ensuring rapid data retrieval for this\ncritical process. After 30 days, the data is transitioned to S3 Standard-IA via an S3 Lifecycle policy. This\nsignificantly reduces storage costs because S3 Standard-IA is designed for infrequently accessed data (but\nstill requires quick retrieval for the quarterly analysis). Finally, after a year, the data is moved to S3 Glacier\nDeep Archive, the cheapest storage option for long-term archival. This aligns perfectly with the archival\nrequirement and minimizes storage costs for data that is rarely accessed.\nOther options are less optimal:\nOption A: S3 Intelligent-Tiering automatically moves data between Frequent, Infrequent, and Archive Access\ntiers based on access patterns. While convenient, the transition between tiers incurs costs that can be higher\nthan directly transitioning to S3 Standard-IA via a lifecycle policy after a fixed period. Intelligent-Tiering also\ndoesn't directly transition to Glacier Deep Archive.\nOption B: Similar to Option A, S3 Intelligent-Tiering doesn't directly offer the lowest-cost archival option and\ncan be more expensive than a lifecycle policy. It would move to Archive Access (not Glacier Deep Archive),\nand there's no inherent transition to Deep Archive.\nOption C: While S3 Standard-IA is cheaper than S3 Standard upfront, using S3 Standard for the initial 30\ndays is more performant for frequently accessed data, which is a primary need for daily model retraining. This\ninitial period of frequent access justifies the slightly higher cost of S3 Standard.\nIn summary, option D is the most cost-effective because it uses the appropriate storage class based on access\nfrequency and lifespan of the data. Lifecycle policies make the transitions automated and efficient, which\noptimize both performance and cost.\nSupporting Concepts:\nS3 Storage Classes: Understand the cost and access characteristics of S3 Standard, S3 Standard-IA, and S3\nGlacier Deep Archive.\nS3 Lifecycle Policies: Learn how to automate transitions between storage classes to optimize costs.\nAuthoritative Links:\nAmazon S3 Storage Classes: https://aws.amazon.com/s3/storage-classes/\nManaging Your Storage Lifecycle: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-\nconfiguration-concept.html",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-"
    ]
  },
  {
    "question": "CertyIQ\nA company is running several business applications in three separate VPCs within the us-east-1 Region. The\napplications must be able to communicate between VPCs. The applications also must be able to consistently send\nhundreds of gigabytes of data each day to a latency-sensitive application that runs in a single on-premises data\ncenter.\nA solutions architect needs to design a network connectivity solution that maximizes cost-effectiveness.\nWhich solution meets these requirements?",
    "options": {
      "A": "Configure three AWS Site-to-Site VPN connections from the data center to AWS. Establish connectivity by",
      "C": "Option C is costly, and it complicates routing. Setting up three Direct Connect connections would be",
      "D": "Set up one AWS Direct Connect connection from the data center to AWS. Create a transit gateway, and"
    },
    "answer": "D",
    "explanation": "The correct answer is D because it provides the most cost-effective and scalable solution for the given\nrequirements. The scenario requires inter-VPC communication and high-bandwidth, low-latency connectivity\nto an on-premises data center.\nOption D leverages AWS Direct Connect for the high-bandwidth, low-latency connection between the data\ncenter and AWS. Direct Connect is a dedicated network connection that bypasses the public internet,\nresulting in more consistent network performance compared to VPN connections. A single Direct Connect\nconnection is more cost-effective than multiple connections, especially for large data transfers.\nThe Transit Gateway simplifies the network architecture by acting as a central hub. Instead of configuring\nmultiple peering connections between VPCs, each VPC is connected to the Transit Gateway. The Transit\nGateway then handles the routing between the VPCs and the Direct Connect connection. This simplifies\nmanagement and reduces operational overhead.\nOptions A and B are less optimal because Site-to-Site VPN connections over the public internet are not\ndesigned for consistently transferring hundreds of gigabytes of data daily with low latency. VPN connections\nare also less reliable and have variable performance. Option B adds the overhead of managing virtual network\nappliances in each VPC.\nOption C is costly, and it complicates routing. Setting up three Direct Connect connections would be\nexpensive, and managing three separate connections to each VPC is complex. Also, Direct Connect gateway\ncan be shared between the VPCs and the Direct Connect Connection, hence Direct Connect Gateway is a must\nin Direct connect architecture.\nIn summary, option D provides the optimal balance of cost, performance, and manageability by using a single\nDirect Connect connection for high-bandwidth, low-latency access and a Transit Gateway for simplified inter-\nVPC communication.\nRelevant AWS documentation:\nAWS Direct Connect: https://aws.amazon.com/directconnect/\nAWS Transit Gateway: https://aws.amazon.com/transit-gateway/",
    "links": [
      "https://aws.amazon.com/directconnect/",
      "https://aws.amazon.com/transit-gateway/"
    ]
  },
  {
    "question": "CertyIQ\nAn ecommerce company is building a distributed application that involves several serverless functions and AWS\nservices to complete order-processing tasks. These tasks require manual approvals as part of the workflow. A\nsolutions architect needs to design an architecture for the order-processing application. The solution must be able\nto combine multiple AWS Lambda functions into responsive serverless applications. The solution also must\norchestrate data and services that run on Amazon EC2 instances, containers, or on-premises servers.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Use AWS Step Functions to build the application."
    },
    "answer": "A",
    "explanation": "The correct answer is A. Use AWS Step Functions to build the application.\nHere's a detailed justification:\nAWS Step Functions is a serverless orchestration service that allows you to coordinate multiple AWS services\ninto serverless workflows. It provides a visual console to design and manage workflows, making it easy to\ncombine Lambda functions and other AWS services into complex, responsive serverless applications.\nCrucially, Step Functions can orchestrate not only Lambda functions but also data and services running on\nEC2 instances, containers, or on-premises servers via API Gateway integrations or other appropriate services.\nThis aligns directly with the requirement to integrate diverse components regardless of their deployment\nenvironment.\nStep Functions offers built-in error handling, retries, and state management, reducing the operational\noverhead associated with managing these aspects manually. It also supports human-in-the-loop approval\nprocesses using activities, aligning with the manual approval requirement. It uses state machines, which\nclearly define the sequence of steps and the flow of data between them, increasing clarity and\nmaintainability.\nOption B, AWS Glue, is primarily an ETL (Extract, Transform, Load) service for data integration and is not\ndesigned for orchestrating general-purpose application workflows involving Lambda functions and manual\napprovals.\nOption C, Amazon SQS, is a message queue service that enables asynchronous communication between\ndecoupled components. While SQS can be part of a solution, it doesn't provide orchestration capabilities like\nStep Functions. Building a complex, stateful workflow with SQS alone would require significant custom\ncoding and management of state, increasing operational overhead.\nOption D, AWS Lambda functions and Amazon EventBridge events, could orchestrate events, but it is not\ndesigned to manage more complex workflows that require features like state management, built-in retries,\nand human approval steps. It quickly leads to complex event chains and increased operational overhead for\ntracking and error handling. It's better suited for event-driven architectures than orchestrated workflows with\nspecific sequencing and state.\nTherefore, Step Functions provides the best combination of features for orchestrating Lambda functions,\nintegrating with various AWS services and on-premises resources, and supporting manual approvals with the\nleast operational overhead.\nSupporting links:\nAWS Step Functions: https://aws.amazon.com/step-functions/\nOrchestrating Microservices with AWS Step Functions:\nhttps://aws.amazon.com/blogs/compute/orchestrating-microservices-with-aws-step-functions/",
    "links": [
      "https://aws.amazon.com/step-functions/",
      "https://aws.amazon.com/blogs/compute/orchestrating-microservices-with-aws-step-functions/"
    ]
  },
  {
    "question": "CertyIQ\nA company has launched an Amazon RDS for MySQL DB instance. Most of the connections to the database come\nfrom serverless applications. Application traffic to the database changes significantly at random intervals. At\ntimes of high demand, users report that their applications experience database connection rejection errors.\nWhich solution will resolve this issue with the LEAST operational overhead?",
    "options": {
      "A": "Create a proxy in RDS Proxy. Configure the users applications to use the DB instance through RDS Proxy.",
      "B": "Deploy Amazon ElastiCache for Memcached between the users applications and the DB instance.",
      "C": "Migrate the DB instance to a different instance class that has higher I/O capacity. Configure the users",
      "D": "Configure Multi-AZ for the DB instance. Configure the users applications to switch between the DB"
    },
    "answer": "A",
    "explanation": "The correct answer is A: Create a proxy in RDS Proxy. Configure the users applications to use the DB instance\nthrough RDS Proxy.\nHere's why:\nThe problem describes connection rejection errors due to high demand and unpredictable traffic patterns\nfrom serverless applications to an RDS for MySQL database. Serverless applications, by their nature,\nfrequently scale up and down, leading to a burst of database connections which can overwhelm the\ndatabase's connection limits.\nRDS Proxy is designed to handle exactly this scenario. It acts as a connection pooler, sitting between the\napplications and the RDS instance. RDS Proxy maintains a pool of database connections and efficiently\nmultiplexes connections from numerous application instances, reusing existing connections as possible and\npreventing the database from being overwhelmed. It can significantly reduce the overhead of establishing and\ntearing down database connections, which is common with serverless functions.\nOption B, deploying Amazon ElastiCache for Memcached, addresses caching and data retrieval performance,\nnot connection management. While caching can reduce database load, it doesn't directly solve the connection\nlimit issue.\nOption C, migrating to a larger instance class, might increase the database's connection limit, but this is a\nvertical scaling approach. It increases costs and might not fully address the rapid fluctuations in traffic. It's\nalso a relatively disruptive operation compared to implementing RDS Proxy. It doesn't solve the fundamental\nproblem of connection exhaustion caused by numerous, short-lived connections.\nOption D, configuring Multi-AZ, provides high availability and failover capabilities, but it does not manage\ndatabase connections effectively. While Multi-AZ ensures the database remains available, it doesn't prevent\nconnection rejection errors when connection limits are exceeded. The applications would still be generating\nnumerous connection requests.\nRDS Proxy provides a cost-effective and minimally disruptive solution by effectively managing and pooling\ndatabase connections, mitigating the connection rejection errors experienced by the serverless applications.\nIts primary benefit is connection multiplexing, which is the ideal solution in this specific context.\nAuthoritative Links:\nAWS RDS Proxy: https://aws.amazon.com/rds/proxy/\nUsing RDS Proxy with Lambda: https://aws.amazon.com/blogs/database/using-amazon-rds-proxy-with-aws-\nlambda/",
    "links": [
      "https://aws.amazon.com/rds/proxy/",
      "https://aws.amazon.com/blogs/database/using-amazon-rds-proxy-with-aws-"
    ]
  },
  {
    "question": "CertyIQ\nA company recently deployed a new auditing system to centralize information about operating system versions,\npatching, and installed software for Amazon EC2 instances. A solutions architect must ensure all instances\nprovisioned through EC2 Auto Scaling groups successfully send reports to the auditing system as soon as they are\nlaunched and terminated.\nWhich solution achieves these goals MOST efficiently?",
    "options": {
      "A": "Use a scheduled AWS Lambda function and run a script remotely on all EC2 instances to send data to the",
      "B": "Use EC2 Auto Scaling lifecycle hooks to run a custom script to send data to the audit system when instances",
      "C": "Use an EC2 Auto Scaling launch configuration to run a custom script through user data to send data to the",
      "D": "Run a custom script on the instance operating system to send data to the audit system. Configure the script"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the most efficient solution for ensuring EC2 instances within\nAuto Scaling groups report to the auditing system upon launch and termination:\nOption B, utilizing EC2 Auto Scaling lifecycle hooks, is the most efficient because it directly integrates with\nthe Auto Scaling group's lifecycle events. Lifecycle hooks allow you to pause instances during launch or\ntermination, enabling you to perform custom actions before the instance proceeds to the next phase. In this\ncase, the custom action would be running a script to send data to the auditing system. This ensures that the\nreporting occurs reliably and automatically as part of the Auto Scaling process. This method provides clear\ncontrol and ensures reports are sent before the instance serves traffic (on launch) or before resources are\ndeallocated (on termination).\nOption A, using a scheduled Lambda function, is less efficient. It requires the Lambda function to continuously\nscan for new and terminated instances, which adds overhead and increases complexity. Remotely executing\nscripts also introduces potential security concerns and operational challenges related to authentication and\nauthorization. It's not tightly coupled with the Auto Scaling lifecycle.\nOption C, relying on user data within the launch configuration, is suitable for initial configuration during\nlaunch but doesn't address the termination scenario. Furthermore, user data scripts can sometimes fail or be\ndelayed, making it less reliable for critical reporting. It also doesn't handle the termination process, meaning\nyou would need another solution for that phase of the Auto Scaling process.\nOption D is incorrect because it places the responsibility of invoking the script on the instance operating\nsystem and relies on the Auto Scaling group to trigger the script, this isn't a default function of Auto Scaling\ngroups, lifecycle hooks are needed to trigger the actions at launch and terminate.\nIn summary, EC2 Auto Scaling lifecycle hooks (Option B) provide the most direct, reliable, and efficient means\nof executing a custom script for reporting to the auditing system during both instance launch and termination\nevents within an Auto Scaling group. It ensures integration with the autoscaling process and provides\nguaranteed execution.\nSupporting links:\nEC2 Auto Scaling Lifecycle Hooks",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is developing a real-time multiplayer game that uses UDP for communications between the client and\nservers in an Auto Scaling group. Spikes in demand are anticipated during the day, so the game server platform\nmust adapt accordingly. Developers want to store gamer scores and other non-relational data in a database\nsolution that will scale without intervention.\nWhich solution should a solutions architect recommend?",
    "options": {},
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the best solution:\nThe scenario requires a solution that effectively handles UDP traffic distribution to a dynamically scaling\ngame server fleet and a database solution suitable for non-relational data that scales automatically.\nNetwork Load Balancer (NLB): The NLB is the optimal choice for traffic distribution. UDP is a connectionless\nprotocol. NLBs are designed to handle UDP traffic efficiently. Unlike Application Load Balancers (ALB), which\noperate at the application layer (HTTP/HTTPS), NLBs operate at the transport layer (TCP/UDP) and can\nforward UDP packets without modification. This is crucial for real-time gaming where low latency and direct\npacket forwarding are paramount.\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\nAmazon DynamoDB on-demand: DynamoDB is a NoSQL database perfectly suited for storing game scores\nand other non-relational data. The on-demand capacity mode is ideal because it automatically scales capacity\nbased on the application's traffic patterns. This eliminates the need for manual provisioning or capacity\nplanning, directly addressing the requirement for a database solution that scales without intervention. It is\nvery cost effective for variable workloads.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\nWhy other options are less suitable:\nOption A (Route 53 and Aurora Serverless): Route 53 is primarily a DNS service and not suitable for real-time\ntraffic distribution in this scenario. It is great for geo-based routing but not real time load balancing. Aurora\nServerless is good for relational data, not specifically designed for non-relational requirements like the\nprompt specifies.\nOption C (NLB and Aurora Global Database): While NLB is correct for UDP traffic, Aurora Global Database is\nan overkill. Its designed for disaster recovery across regions for relational data and provides read replicas for\nglobal low-latency reads. Overcomplicated, high cost, and relational DB makes this a poor solution.\nOption D (ALB and DynamoDB Global Tables): ALBs are designed for HTTP/HTTPS traffic and cannot handle\nUDP. While DynamoDB global tables are a viable solution for multi-region replication, the on-demand capacity\nmode is more cost-effective and simpler for automatically scaling with the described workload within a single\nregion. ALB makes this entire solution wrong.\nIn summary, NLB provides the necessary low-latency, UDP-aware traffic distribution, and DynamoDB on-\ndemand delivers the automatically scalable NoSQL database, meeting all the requirements efficiently and\ncost-effectively.",
    "links": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts a frontend application that uses an Amazon API Gateway API backend that is integrated with\nAWS Lambda. When the API receives requests, the Lambda function loads many libraries. Then the Lambda\nfunction connects to an Amazon RDS database, processes the data, and returns the data to the frontend\napplication. The company wants to ensure that response latency is as low as possible for all its users with the\nfewest number of changes to the company's operations.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The best solution is to configure provisioned concurrency for the Lambda function. This is because Lambda\nfunctions, particularly those that load many libraries, suffer from \"cold starts.\" A cold start happens when a\nLambda function is invoked for the first time or after a period of inactivity. During a cold start, the Lambda\nservice needs to initialize the execution environment, download the code, and initialize the dependencies,\nleading to increased latency.\nProvisioned concurrency alleviates this problem by pre-initializing a specified number of execution\nenvironments for the Lambda function. These environments are kept warm and ready to serve requests\nimmediately, eliminating the cold start latency. This directly addresses the company's desire for low response\nlatency.\nOption A, bypassing the API and connecting the frontend directly to the database, is a security risk and poor\narchitectural practice. It exposes the database directly and bypasses the API's security measures,\nauthentication, and authorization, which makes the application vulnerable to attacks. Option C, caching\nresults in S3, is beneficial if the data is largely static or frequently requested but doesn't address the cold\nstart issue with the Lambda function itself. Furthermore, implementing a caching strategy introduces\ncomplexity to manage cache invalidation. Option D, increasing the database size, won't significantly reduce\nLambda function's latency. The database size primarily affects query performance and storage capacity.\nProvisioned concurrency offers the most direct and efficient solution with minimal operational changes, as it\nfocuses on improving the Lambda function's performance without altering the overall application architecture\nor introducing caching complexities.\nFor further research:\nAWS Lambda Provisioned Concurrency: https://docs.aws.amazon.com/lambda/latest/dg/configuration-\nconcurrency.html\nUnderstanding AWS Lambda Cold Starts: https://aws.amazon.com/blogs/compute/understanding-aws-\nlambda-cold-starts/",
    "links": [
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-",
      "https://aws.amazon.com/blogs/compute/understanding-aws-"
    ]
  },
  {
    "question": "CertyIQ\nA company is migrating its on-premises workload to the AWS Cloud. The company already uses several Amazon\nEC2 instances and Amazon RDS DB instances. The company wants a solution that automatically starts and stops\nthe EC2 instances and DB instances outside of business hours. The solution must minimize cost and infrastructure\nmaintenance.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Scale the EC2 instances by using elastic resize. Scale the DB instances to zero outside of business hours.",
      "B": "Explore AWS Marketplace for partner solutions that will automatically start and stop the EC2 instances and",
      "C": "Launch another EC2 instance. Configure a crontab schedule to run shell scripts that will start and stop the",
      "D": "Create an AWS Lambda function that will start and stop the EC2 instances and DB instances. Configure"
    },
    "answer": "D",
    "explanation": "The correct answer is D because it offers a cost-effective and infrastructure-minimized solution for\nautomatically starting and stopping EC2 instances and RDS DB instances outside of business hours. AWS\nLambda is a serverless compute service, meaning you only pay for the compute time you consume. This is\nsignificantly cheaper than running a dedicated EC2 instance. Amazon EventBridge (formerly CloudWatch\nEvents) is a serverless event bus service that enables you to schedule events to trigger actions, such as\ninvoking a Lambda function.\nOption D leverages these serverless services to automate the start/stop process without requiring the\ncompany to manage any underlying infrastructure. A Lambda function can be written to execute the\nnecessary AWS CLI or SDK commands to start and stop both EC2 instances and RDS DB instances.\nEventBridge can be configured with a cron expression to trigger the Lambda function at specified times,\nthereby implementing the desired on/off schedule. This eliminates the need for manual intervention and\nminimizes operational overhead.\nOther options are less suitable:\nA: Elastic resize is not a native term; most likely implying Elastic Load Balancing Auto Scaling. Auto Scaling\ngroups manage capacity, not start/stop functionality directly according to a precise schedule. Scaling RDS DB\ninstances to zero is not a standard or supported operation.\nB: AWS Marketplace solutions might exist, but they introduce third-party dependencies and potential costs.\nThe requirement is to minimize cost and infrastructure maintenance. This adds complexity.\nC: Launching another EC2 instance to run a cron schedule adds infrastructure management overhead and\ncost that is not necessary since the solution can be implemented serverlessly.\nTherefore, utilizing Lambda with EventBridge fulfills the requirements of cost minimization, reduced\ninfrastructure maintenance, and automation of instance start/stop procedures based on a defined schedule.\nSupporting Links:\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon EventBridge: https://aws.amazon.com/eventbridge/\nStopping and Starting Your RDS Instance:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html\nStopping and Starting Your EC2 Instance:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html",
    "links": [
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/eventbridge/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts a three-tier web application that includes a PostgreSQL database. The database stores the\nmetadata from documents. The company searches the metadata for key terms to retrieve documents that the\ncompany reviews in a report each month. The documents are stored in Amazon S3. The documents are usually\nwritten only once, but they are updated frequently.\nThe reporting process takes a few hours with the use of relational queries. The reporting process must not prevent\nany document modifications or the addition of new documents. A solutions architect needs to implement a solution\nto speed up the reporting process.\nWhich solution will meet these requirements with the LEAST amount of change to the application code?",
    "options": {
      "A": "Set up a new Amazon DocumentDB (with MongoDB compatibility) cluster that includes a read replica. Scale",
      "B": "Migrating the data to DocumentDB would necessitate significant application code changes, which contradicts",
      "C": "Set up a new Amazon RDS for PostgreSQL Multi-AZ DB instance. Configure the reporting module to query",
      "D": "Set up a new Amazon DynamoDB table to store the documents. Use a fixed write capacity to support new"
    },
    "answer": "B",
    "explanation": "The correct answer is B: Set up a new Amazon Aurora PostgreSQL DB cluster that includes an Aurora Replica.\nIssue queries to the Aurora Replica to generate the reports.\nHere's why: The key requirement is to speed up the reporting process without impacting the primary\ndatabase's performance related to document modifications and additions. The existing setup uses\nPostgreSQL.\nOption B leverages Amazon Aurora PostgreSQL, which is compatible with PostgreSQL. Aurora provides\nsignificantly better performance than standard PostgreSQL, which directly addresses the speed requirement.\nThe critical aspect of this solution is the use of an Aurora Replica. Aurora Replicas are read-only endpoints\nthat share the same underlying storage as the primary instance. This means data is replicated asynchronously\nwith minimal latency, enabling near real-time reporting. The reporting process can query the Aurora Replica\nwithout affecting the performance of the primary instance that handles document modifications and\nadditions. Furthermore, Aurora's architecture allows for easy scaling of read replicas to handle report\ngeneration loads. This approach minimizes code changes since Aurora PostgreSQL is compatible with the\nexisting PostgreSQL database.\nOption A is incorrect because Amazon DocumentDB is a NoSQL database compatible with MongoDB.\nMigrating the data to DocumentDB would necessitate significant application code changes, which contradicts\nthe requirement of minimizing such changes. Additionally, while DocumentDB offers read replicas, it's an\nunnecessary database technology shift when a compatible solution exists within the relational database\ncontext.\nOption C is incorrect because while RDS for PostgreSQL Multi-AZ provides high availability, it's not optimized\nfor read-heavy workloads like reporting. The secondary RDS node in a Multi-AZ setup is primarily for failover\nand might not be optimized for reporting queries. Using it for reporting could still potentially impact the\nperformance of the primary node if there's significant load.\nOption D is incorrect because DynamoDB is a NoSQL database, and migrating the existing PostgreSQL data\nand relational queries to DynamoDB would require a complete application rewrite. This is a substantial change\nthat is not in line with the given requirements. While DynamoDB can scale reads, the conversion effort\noutweighs the benefits compared to leveraging Aurora's PostgreSQL compatibility and Aurora Replica\nfunctionality.\nIn conclusion, Aurora PostgreSQL with an Aurora Replica provides the optimal balance of minimal application\nchanges, improved performance, and isolation of the reporting workload from the primary database.\nRelevant Links:\nAmazon Aurora: https://aws.amazon.com/rds/aurora/\nAurora Replicas:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Replication.html\nAmazon RDS: https://aws.amazon.com/rds/\nAmazon DocumentDB: https://aws.amazon.com/documentdb/\nAmazon DynamoDB: https://aws.amazon.com/dynamodb/",
    "links": [
      "https://aws.amazon.com/rds/aurora/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Replication.html",
      "https://aws.amazon.com/rds/",
      "https://aws.amazon.com/documentdb/",
      "https://aws.amazon.com/dynamodb/"
    ]
  },
  {
    "question": "CertyIQ\nA company has a three-tier application on AWS that ingests sensor data from its users devices. The traffic flows\nthrough a Network Load Balancer (NLB), then to Amazon EC2 instances for the web tier, and finally to EC2\ninstances for the application tier. The application tier makes calls to a database.\nWhat should a solutions architect do to improve the security of the data in transit?",
    "options": {
      "B": "D.Encrypt the Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instances by using AWS Key",
      "A": "Configure a TLS listener. Deploy the server certificate on the NLB."
    },
    "answer": "A",
    "explanation": "The correct answer is A. Configure a TLS listener. Deploy the server certificate on the NLB.\nHere's a detailed justification:\nThe primary goal is to improve the security of data in transit. TLS (Transport Layer Security) encrypts data as\nit travels across the network, protecting it from eavesdropping and tampering. Configuring a TLS listener on\nthe NLB ensures that all traffic between the user's devices and the NLB is encrypted. Deploying the server\ncertificate on the NLB allows the NLB to decrypt the traffic and re-encrypt it before forwarding it to the web\ntier. This establishes a secure connection from the client to the load balancer.\nOption B is incorrect because AWS Shield Advanced primarily protects against DDoS attacks, not general\ndata-in-transit encryption. While AWS WAF (Web Application Firewall) helps protect against web exploits, it\ndoesn't address the fundamental need for encryption across the network.\nOption C is incorrect because while an Application Load Balancer (ALB) can terminate TLS, the core\nrequirement is TLS termination at the entry point, which the NLB can handle just as effectively. Switching to\nan ALB only to add WAF isn't the most direct solution for securing data in transit; TLS configuration on the\nNLB itself directly addresses this concern. Besides, an NLB is often chosen specifically for its high throughput\nand low latency, which might be critical for ingesting real-time sensor data, and switching to an ALB might\nintroduce performance trade-offs.\nOption D is incorrect because encrypting the EBS volumes only protects data at rest. While data-at-rest\nencryption is important, it does not address the immediate need to secure the data while it's being\ntransmitted between the users' devices, the NLB, the EC2 instances, and the database.\nIn summary, configuring a TLS listener on the NLB with a server certificate is the most direct and effective\nway to secure the sensor data in transit in this three-tier architecture. It addresses the specific problem stated\nin the question without introducing unnecessary complexity or altering the fundamental architecture if the\nNLB is performant for the application's demands.\nHere are authoritative links for further research:\nAWS Load Balancing Documentation:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-listeners.html\nTLS Termination on Elastic Load Balancers:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#https-\nlisteners (While this link refers to ALBs, the principle of TLS termination applies to NLBs as well).\nAWS Key Management Service (KMS): https://aws.amazon.com/kms/\nAWS Shield: https://aws.amazon.com/shield/\nAWS WAF: https://aws.amazon.com/waf/",
    "links": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-listeners.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#https-",
      "https://aws.amazon.com/kms/",
      "https://aws.amazon.com/shield/",
      "https://aws.amazon.com/waf/"
    ]
  },
  {
    "question": "CertyIQ\nA company is planning to migrate a commercial off-the-shelf application from its on-premises data center to AWS.\nThe software has a software licensing model using sockets and cores with predictable capacity and uptime\nrequirements. The company wants to use its existing licenses, which were purchased earlier this year.\nWhich Amazon EC2 pricing option is the MOST cost-effective?",
    "options": {
      "A": "Dedicated Reserved Hosts",
      "B": "Dedicated On-Demand Hosts",
      "C": "Dedicated Reserved Instances",
      "D": "Dedicated On-Demand Instances"
    },
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A, Dedicated Hosts with Reserved Instances, is the most cost-\neffective solution in the scenario described:\nThe key requirement is utilizing existing software licenses based on sockets and cores. Dedicated Hosts\nprovide physical servers dedicated for your use, enabling you to bring your existing server-bound software\nlicenses to AWS. This avoids the need to purchase new licenses, which can be a significant cost savings when\nmigrating commercial off-the-shelf (COTS) applications.\nOn-Demand options (B and D) for Dedicated Hosts are expensive for predictable capacity and uptime. Since\nthe company has predictable capacity and uptime requirements, a long-term commitment will significantly\nreduce costs.\nReserved Instances (C) offer a discount compared to On-Demand Instances in exchange for a one- or three-\nyear commitment. However, Reserved Instances do not by themselves ensure you can use your existing\nlicenses tied to specific hardware. Reserved Instances still run on shared tenancy hardware managed by AWS.\nDedicated Hosts with Reserved Instances (A) combine the advantages of both. You get the physical server\nisolation to use your existing software licenses, and you receive a substantial cost reduction through the\nreservation. Reserving a Dedicated Host provides the most significant cost savings over time when compared\nto paying on-demand rates for a dedicated host.\nTherefore, Dedicated Reserved Hosts offer the most cost-effective solution by letting the company leverage\nexisting licenses (socket/core-based) on dedicated hardware while benefiting from the cost savings of\nreserved pricing. This aligns with the business requirements for predictable capacity, uptime, and license\nutilization. Dedicated Hosts allow for granular control over instance placement, useful for licensing\nconstraints, whereas Dedicated Instances are for compliance reasons.\nAuthoritative Links:\nAmazon EC2 Dedicated Hosts: https://aws.amazon.com/ec2/dedicated-hosts/\nAmazon EC2 Pricing: https://aws.amazon.com/ec2/pricing/\nReserved Instances: https://aws.amazon.com/ec2/pricing/reserved-instances/",
    "links": [
      "https://aws.amazon.com/ec2/dedicated-hosts/",
      "https://aws.amazon.com/ec2/pricing/",
      "https://aws.amazon.com/ec2/pricing/reserved-instances/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an application on Amazon EC2 Linux instances across multiple Availability Zones. The application\nneeds a storage layer that is highly available and Portable Operating System Interface (POSIX)-compliant. The\nstorage layer must provide maximum data durability and must be shareable across the EC2 instances. The data in\nthe storage layer will be accessed frequently for the first 30 days and will be accessed infrequently after that\ntime.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C because it addresses all the stated requirements most cost-effectively. The\napplication requires a highly available and POSIX-compliant storage layer shareable across EC2 instances.\nAmazon Elastic File System (EFS) is a network file system specifically designed for use with AWS compute\nservices like EC2. EFS provides a POSIX-compliant file system interface, allowing applications to interact with\nthe storage layer as if it were a local file system. This satisfies the POSIX compliance and shareability\nrequirements. Using the EFS Standard storage class ensures high availability because EFS replicates data\nacross multiple Availability Zones.\nThe data access pattern also dictates the choice of storage class. Because the data is frequently accessed for\nthe first 30 days and infrequently after that, leveraging EFS lifecycle management is the most cost-effective\nsolution. EFS lifecycle management allows automatic transitioning of files from the EFS Standard storage\nclass to the EFS Standard-Infrequent Access (EFS Standard-IA) storage class based on a defined policy. This\nreduces storage costs for infrequently accessed data without requiring any application changes.\nOption A is incorrect because S3 is an object storage service, not a file system, and does not natively support\nPOSIX compliance. Option B is incorrect for the same reason as A, S3 isn't POSIX compliant. Option D is\nincorrect because while EFS One Zone is a cost-effective option, it's not highly available, as it only stores data\nwithin a single Availability Zone, violating the availability requirement. EFS One Zone is suitable for workloads\nwhere durability is less important than cost.\nTherefore, EFS Standard with lifecycle management to EFS Standard-IA is the optimal solution balancing\ncost, availability, POSIX compliance, and data access patterns.\nRelevant links for further research:\nAmazon EFS: https://aws.amazon.com/efs/\nEFS Storage Classes: https://docs.aws.amazon.com/efs/latest/ug/storage-classes.html\nEFS Lifecycle Management: https://docs.aws.amazon.com/efs/latest/ug/lifecycle-management-efs.html",
    "links": [
      "https://aws.amazon.com/efs/",
      "https://docs.aws.amazon.com/efs/latest/ug/storage-classes.html",
      "https://docs.aws.amazon.com/efs/latest/ug/lifecycle-management-efs.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is creating a new VPC design. There are two public subnets for the load balancer, two private\nsubnets for web servers, and two private subnets for MySQL. The web servers use only HTTPS. The solutions\narchitect has already created a security group for the load balancer allowing port 443 from 0.0.0.0/0. Company\npolicy requires that each resource has the least access required to still be able to perform its tasks.\nWhich additional configuration strategy should the solutions architect use to meet these requirements?",
    "options": {
      "A": "Create a security group for the web servers and allow port 443 from 0.0.0.0/0. Create a security group for",
      "B": "Create a network ACL for the web servers and allow port 443 from 0.0.0.0/0. Create a network ACL for the",
      "C": "Create a security group for the web servers and allow port 443 from the load balancer. Create a security",
      "D": "Create a network ACL for the web servers and allow port 443 from the load balancer. Create a network ACL"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it adheres to the principle of least privilege using security groups, which\noperate at the instance level, providing granular control.\nOption C suggests creating a security group for the web servers that allows inbound traffic on port 443 only\nfrom the load balancer's security group. This ensures that only the load balancer can send HTTPS traffic to\nthe web servers, restricting access from other sources. Subsequently, it proposes a security group for the\nMySQL servers, permitting inbound traffic on port 3306 (MySQL's default port) solely from the web servers'\nsecurity group. This setup allows the web servers to communicate with the database while preventing any\nother resources from directly accessing the MySQL servers.\nOption A allows port 443 from 0.0.0.0/0 to the web servers, which violates the least privilege principle by\nopening access to the entire internet.Options B and D use Network ACLs (NACLs). NACLs operate at the\nsubnet level and are stateless, meaning you need to configure both inbound and outbound rules, making\nsecurity group management for inter-instance communication easier to administer.\nTherefore, option C is the only choice that properly leverages security groups to restrict access to the\nminimum necessary, fulfilling the company's policy requirements.\nAWS Security GroupsAWS Network ACLs",
    "links": []
  },
  {
    "question": "CertyIQ\nAn ecommerce company is running a multi-tier application on AWS. The front-end and backend tiers both run on\nAmazon EC2, and the database runs on Amazon RDS for MySQL. The backend tier communicates with the RDS\ninstance. There are frequent calls to return identical datasets from the database that are causing performance\nslowdowns.\nWhich action should be taken to improve the performance of the backend?",
    "options": {
      "A": "Implement Amazon SNS to store the database calls.",
      "B": "Implement Amazon ElastiCache to cache the large datasets.",
      "C": "Implement an RDS for MySQL read replica to cache database calls.",
      "D": "Implement Amazon Kinesis Data Firehose to stream the calls to the database."
    },
    "answer": "B",
    "explanation": "The most effective solution to improve the backend performance involves caching frequently requested,\nidentical datasets.\nOption B, implementing Amazon ElastiCache, is the correct approach. ElastiCache is a fully managed, in-\nmemory data store and caching service. By caching frequently accessed data from the RDS for MySQL\ndatabase in ElastiCache, the backend application can retrieve the data much faster than querying the\ndatabase each time. This reduces the load on the RDS instance and significantly improves the response time\nfor the backend. ElastiCache supports Memcached and Redis, both suitable for caching datasets. Memcached\nprovides a distributed memory object caching system, while Redis offers more advanced data structures and\nfeatures.\nWhy other options are incorrect:\nOption A (Amazon SNS): Amazon SNS (Simple Notification Service) is a messaging service used for pub/sub\narchitectures. It's not designed for caching data.\nOption C (RDS Read Replica): While read replicas can offload read traffic from the primary RDS instance,\nthey still query the database. This doesn't eliminate the latency associated with database queries for identical\ndatasets. Caching avoids database queries altogether for commonly requested information, offering better\nperformance gains in this specific scenario. Read replicas are more beneficial for scaling read operations\nrather than caching identical data.\nOption D (Amazon Kinesis Data Firehose): Amazon Kinesis Data Firehose is a service for streaming data to\ndata lakes, data stores, and analytics services. It is not suitable for caching data or improving the backend\napplication's read performance in the context described. Firehose is mainly used for ingestion of streaming\ndata for analytics and long term storage.\nIn conclusion, ElastiCache provides a dedicated caching layer that directly addresses the issue of\nperformance slowdowns caused by frequent, redundant database calls. It significantly reduces database load\nand improves response times for the backend application by serving frequently accessed data directly from\nmemory.\nAuthoritative Links:\nAmazon ElastiCache: https://aws.amazon.com/elasticache/\nCaching Strategies: https://aws.amazon.com/caching/",
    "links": [
      "https://aws.amazon.com/elasticache/",
      "https://aws.amazon.com/caching/"
    ]
  },
  {
    "question": "CertyIQ\nA new employee has joined a company as a deployment engineer. The deployment engineer will be using AWS\nCloudFormation templates to create multiple AWS resources. A solutions architect wants the deployment\nengineer to perform job activities while following the principle of least privilege.\nWhich combination of actions should the solutions architect take to accomplish this goal? (Choose two.)",
    "options": {
      "D": "Therefore, D and E are the most appropriate actions to follow the principle of least privilege."
    },
    "answer": "D",
    "explanation": "The principle of least privilege dictates granting only the permissions required to perform a specific task.\nOptions A, B, and C are incorrect because they violate this principle.\nOption A is wrong because using the root user grants unrestricted access to all AWS services and resources,\nposing a significant security risk. Root user credentials should be used extremely sparingly, only for tasks\nrequiring it, such as changing the account settings.\nOption B and C are wrong because PowerUser or AdministratorAccess policies grant broad permissions that\nare not necessary for CloudFormation deployments. This level of access could be misused, creating security\nvulnerabilities.\nOption D, creating an IAM user in a group with an IAM policy allowing only CloudFormation actions, aligns with\nthe least privilege principle. It restricts the deployment engineer to CloudFormation-specific tasks, reducing\nthe risk of accidental or malicious actions on other AWS resources. However, even this can be refined further.\nOption E, creating an IAM role with specific permissions for the CloudFormation stack and launching stacks\nusing that role, represents an even more granular application of the least privilege principle. This approach\ngrants the exact permissions needed for that specific stack, limiting any potential blast radius if the\ncredentials are compromised. This provides better security over a blanket CloudFormation only policy as\nsuggested in option D.\nTherefore, D and E are the most appropriate actions to follow the principle of least privilege.\nRelevant links:\nIAM Best Practices: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html\nIAM Roles: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\nCloudFormation Security Considerations:\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html",
    "links": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is deploying a two-tier web application in a VP",
    "options": {
      "C": "It introduces more administrative overhead than simply adjusting the security group rules.",
      "D": "Add an inbound rule to the security group of the database tiers RDS instance to",
      "A": "Add an explicit rule to the private subnets network ACL to allow traffic from the web tiers EC2",
      "B": "Add a route in the VPC route table to allow traffic between the web tiers EC2 instances and the"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Add an inbound rule to the security group of the database tiers RDS instance to\nallow traffic from the web tiers security group.\nHere's why:\nThe problem is that the web tier cannot connect to the RDS database, despite the database being up and\nrunning. The question states that network ACLs and route tables are in their default states. Default network\nACLs allow all inbound and outbound traffic. The default route table allows all traffic within the VPC. This\nmeans the problem likely lies with the security groups, which act as virtual firewalls at the instance level.\nRDS DB instances use security groups to control network access. By default, a security group denies all\ninbound traffic. Therefore, the RDS instance's security group is likely blocking connections from the web tier.\nTo fix this, we need to add an inbound rule to the RDS instance's security group that allows traffic from the\nweb tier's security group. This allows instances in the web tier's security group to connect to the RDS instance\non the appropriate port (typically 3306 for MySQL). Specifying the source as the web tier's security group is\nbest practice, as it allows the RDS instance to only accept traffic from authorized sources within the VPC\nwithout specifying IP addresses.\nWhy the other options are incorrect:\nA. Add an explicit rule to the private subnets network ACL to allow traffic from the web tiers EC2\ninstances: Network ACLs, by default, allow all traffic. Adding explicit rules is unnecessary in this scenario and\nwon't solve the issue, as the restriction is likely at the security group level.\nB. Add a route in the VPC route table to allow traffic between the web tiers EC2 instances and the\ndatabase tier: The default route table already allows all traffic within the VPC, so adding another route won't\nchange anything. The issue isn't routing; it's access control.\nC. Deploy the web tier's EC2 instances and the database tiers RDS instance into two separate VPCs, and\nconfigure VPC peering: This is a complex solution that is not necessary to resolve the connection issue within\nthe existing VPC. It introduces more administrative overhead than simply adjusting the security group rules.\nVPC peering is used to connect separate VPCs, which isn't the problem at hand.\nAuthoritative Links:\nAmazon VPC Security Groups: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\nAmazon RDS Security Groups:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithSecurityGroups.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithSecurityGroups.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has a large dataset for its online advertising business stored in an Amazon RDS for MySQL DB instance\nin a single Availability Zone. The company wants business reporting queries to run without impacting the write\noperations to the production DB instance.\nWhich solution meets these requirements?",
    "options": {
      "A": "Deploy RDS read replicas to process the business reporting queries.",
      "B": "Scale out the DB instance horizontally by placing it behind an Elastic Load Balancer.",
      "C": "Scale up the DB instance to a larger instance type to handle write operations and queries.",
      "D": "Deploy the DB instance in multiple Availability Zones to process the business reporting queries."
    },
    "answer": "A",
    "explanation": "The correct answer is A: Deploy RDS read replicas to process the business reporting queries. This solution\neffectively isolates the business reporting workload from the production database, preventing any\nperformance impact on write operations.\nRDS Read Replicas provide a mechanism for asynchronous replication of data from a source RDS database\ninstance (in this case, MySQL) to one or more read-only copies. These replicas reside in separate DB instances\nand can handle read-heavy workloads without affecting the primary instance's performance. Business\nreporting typically involves complex and resource-intensive queries that can consume significant database\nresources. By directing these queries to the read replicas, the primary instance remains dedicated to handling\nwrite operations and transactional workloads, ensuring optimal performance for the online advertising\nbusiness.\nOption B is incorrect. Placing an RDS instance behind an Elastic Load Balancer does not scale the database\nhorizontally in the traditional sense of sharing the data. ELB primarily handles distributing connection\nrequests. While it could distribute read queries, it won't prevent the primary instance from being burdened by\nthe reporting queries. It's mostly useful for read scale when multiple instances host the same data and the\ninstances are identical in terms of data.\nOption C is also incorrect. Scaling up the DB instance (vertical scaling) might improve performance\ntemporarily, but it doesn't isolate the reporting workload. The reporting queries will still compete for\nresources with the write operations, eventually leading to performance bottlenecks as the dataset grows and\nquery complexity increases. Moreover, there is a limit to how much you can vertically scale.\nOption D is also incorrect. Deploying the DB instance in multiple Availability Zones (Multi-AZ) primarily\nprovides high availability and failover capabilities. While it creates a standby instance, this standby is for\nfailover purposes and isn't designed for actively handling read queries during normal operations. Using it for\nread operations is discouraged and inefficient since the primary instance is still handling both reads and\nwrites.In summary, RDS Read Replicas are the ideal solution for offloading read-only workloads like business\nreporting from the primary RDS instance, ensuring that write performance remains unaffected and\nmaximizing overall database performance and availability.\nFurther Resources:\nAmazon RDS Read Replicas\nWorking with MySQL Read Replicas",
    "links": []
  },
  {
    "question": "CertyIQ\nA company hosts a three-tier ecommerce application on a fleet of Amazon EC2 instances. The instances run in an\nAuto Scaling group behind an Application Load Balancer (ALB). All ecommerce data is stored in an Amazon RDS\nfor MariaDB Multi-AZ DB instance.\nThe company wants to optimize customer session management during transactions. The application must store\nsession data durably.\nWhich solutions will meet these requirements? (Choose two.)",
    "options": {
      "A": "Turn on the sticky sessions feature (session affinity) on the ALB.",
      "B": "Use an Amazon DynamoDB table to store customer session information.",
      "C": "Deploy an Amazon Cognito user pool to manage user session information.",
      "D": "Deploy an Amazon ElastiCache for Redis cluster to store customer session information."
    },
    "answer": "A",
    "explanation": "The requirement is to optimize customer session management for an e-commerce application running on EC2\ninstances behind an ALB, with durable storage of session data.\nOption A, turning on sticky sessions (session affinity) on the ALB, addresses session management directly.\nSticky sessions ensure that a user's requests are consistently routed to the same EC2 instance within the\nAuto Scaling group. This is crucial for maintaining session state and avoiding data loss during transactions.\nALB sticky sessions use cookies to track which server the client should be routed to.\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/sticky-sessions.html\nOption D, deploying an Amazon ElastiCache for Redis cluster, provides durable and in-memory storage for\ncustomer session information. Redis, being an in-memory data store, offers fast read and write operations,\nsignificantly improving session retrieval speed. The question specified the session data must be stored\ndurably, and Redis can be configured with persistence to disk (RDB or AOF), ensuring that session data\nsurvives instance failures. Redis Cluster automatically partitions your data among multiple Redis nodes. It\nimproves availability and performance for large datasets. https://aws.amazon.com/elasticache/redis/\nOption B is incorrect because while DynamoDB provides durable storage, it is not the optimal choice for\nsession management due to higher latency compared to an in-memory solution like ElastiCache Redis,\nespecially for frequently accessed session data.\nOption C, using Amazon Cognito, primarily handles authentication and authorization (identity management),\nwhich is a different aspect of the application than session data persistence. Cognito could be used for user\nauthentication but doesn't satisfy the durability requirement for session data within the context of active\ntransactions.\nOption E, using AWS Systems Manager Application Manager, focuses on managing application operations and\ninfrastructure, rather than directly handling session data. It provides centralized views of applications and\ninsights into their operational status. This does not address the session management or durability\nrequirements.",
    "links": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/sticky-sessions.html",
      "https://aws.amazon.com/elasticache/redis/"
    ]
  },
  {
    "question": "CertyIQ\nA company needs a backup strategy for its three-tier stateless web application. The web application runs on\nAmazon EC2 instances in an Auto Scaling group with a dynamic scaling policy that is configured to respond to\nscaling events. The database tier runs on Amazon RDS for PostgreSQL. The web application does not require\ntemporary local storage on the EC2 instances. The companys recovery point objective (RPO) is 2 hours.\nThe backup strategy must maximize scalability and optimize resource utilization for this environment.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances and database",
      "B": "Configure a snapshot lifecycle policy to take Amazon Elastic Block Store (Amazon EBS) snapshots. Enable",
      "C": "Retain the latest Amazon Machine Images (AMIs) of the web and application tiers. Enable automated",
      "D": "Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances every 2 hours."
    },
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the best solution and why the others are less suitable:\nWhy Option C is Correct:\nStateless Web Application & AMIs: The EC2 instances are stateless, meaning they don't store persistent\ndata. Creating and retaining AMIs (Amazon Machine Images) captures the configuration and software setup of\nthe EC2 instances. When scaling events occur, the Auto Scaling group can launch new instances using the\nlatest AMI, ensuring consistency and rapid deployment. Since there's no application data to back up on the\ninstances themselves, EBS snapshots are redundant and unnecessary for the web tier.\nRDS Automated Backups & Point-in-Time Recovery: Amazon RDS's automated backup feature creates\nregular snapshots of the database and transaction logs. Enabling automated backups and using point-in-time\nrecovery (PITR) allows you to restore the database to any point in time within the backup retention period,\nmeeting the 2-hour RPO. PITR uses both the snapshots and transaction logs for granular recovery.\nScalability and Resource Optimization: AMIs are efficient for deploying stateless applications because they\ncapture the pre-configured instance state. RDS automated backups are designed for database durability\nwithout requiring manual snapshot management. This approach minimizes manual intervention, optimizes\nresource utilization, and scales effectively with the application.\nWhy Other Options Are Incorrect:\nOption A & D: EBS Snapshots for Stateless Instances: Taking EBS snapshots of the EC2 instances in a\nstateless web application is unnecessary. Since the instances are stateless, snapshots offer no benefit for\napplication data recovery. This wastes storage resources and management overhead.\nOption B: Snapshot lifecycle for EBS & RDS Automated Backups: It is important to note that option B does\nnot address the EC2 instances.\nReliance on EBS snapshot for RDS: While automated backups are correct for RDS, the other options also\nrecommend EBS snapshots for the stateless EC2 instances, making them redundant and less optimal.\nIn summary, Option C provides the most efficient and scalable solution for backing up a stateless web\napplication on EC2 and a PostgreSQL database on RDS. It leverages AMIs for the stateless web tier and RDS\nautomated backups with PITR for the database tier, meeting the required RPO while optimizing resource\nutilization.\nSupporting Links:\nAmazon Machine Images (AMIs): https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\nAmazon RDS Automated Backups:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html\nAmazon RDS Point-in-Time Recovery:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIT.html",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIT.html"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to deploy a new public web application on AWS. The application includes a web server tier that\nuses Amazon EC2 instances. The application also includes a database tier that uses an Amazon RDS for MySQL DB\ninstance.\nThe application must be secure and accessible for global customers that have dynamic IP addresses.\nHow should a solutions architect configure the security groups to meet these requirements?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A because it implements the principle of least privilege while ensuring global\naccessibility.\nHere's a detailed justification:\nWeb Server Security Group: The web servers need to be accessible from the internet for global customers.\nSince the customers have dynamic IP addresses, specifying individual IP addresses (as suggested in options B\nand C) is impractical and unmanageable. Opening port 443 (HTTPS) to 0.0.0.0/0 allows traffic from any IP\naddress to reach the web servers securely, enabling global access. This is acceptable because the web\nservers are designed to handle public-facing traffic.\nDatabase Security Group: Exposing the database directly to the internet (as suggested in option D) is a\nsignificant security risk. Instead, the database should only accept connections from the web servers. This is\nachieved by configuring the DB instance's security group to allow inbound traffic on port 3306 (MySQL's\ndefault port) only from the security group of the web servers. This means only EC2 instances associated with\nthe web server's security group can connect to the database. This approach isolates the database and\nprotects it from unauthorized access from the internet.\nWhy other options are incorrect: Option B and C are incorrect because specifying individual customer IP\naddresses is not feasible with dynamic IP addresses. Option D is wrong because it opens the database to the\nentire internet, which is a major security vulnerability.\nIn summary, option A balances accessibility and security. It allows global customers to access the web\napplication while restricting database access to only the necessary components (the web servers). This\nconfiguration follows AWS best practices for securing multi-tier applications.\nRelevant AWS Documentation:\nSecurity Groups: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\nRDS Security:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html"
    ]
  },
  {
    "question": "CertyIQ\nA payment processing company records all voice communication with its customers and stores the audio files in an\nAmazon S3 bucket. The company needs to capture the text from the audio files. The company must remove from\nthe text any personally identifiable information (PII) that belongs to customers.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Process the audio files by using Amazon Kinesis Video Streams. Use an AWS Lambda function to scan for",
      "B": "When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start an Amazon",
      "C": "Configure an Amazon Transcribe transcription job with PII redaction turned on. When an audio file is",
      "D": "Create an Amazon Connect contact flow that ingests the audio files with transcription turned on. Embed an"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it directly addresses the requirements of transcribing audio files and\nredacting PII, utilizing services specifically designed for these tasks.\nOption C utilizes Amazon Transcribe, a service designed for speech-to-text conversion. Transcribe's built-in\nPII redaction feature automatically identifies and removes sensitive information from the transcribed text,\nsatisfying the PII removal requirement efficiently. Configuring a Lambda function to trigger the transcription\njob upon S3 upload provides an automated, event-driven workflow. Storing the output in a separate S3 bucket\nensures separation of the original audio and the processed text, further enhancing security and organization.\nThis approach minimizes custom code and leverages AWS managed services for core functionality, adhering\nto best practices for scalability and maintainability.\nOption A is less ideal because Kinesis Video Streams is primarily for real-time video and audio ingestion, not\nfor batch processing of existing audio files in S3. Scanning for PII patterns within a Lambda function would\nrequire custom development and maintenance of PII detection logic, which is less efficient than using\nTranscribe's built-in feature.\nOption B uses Amazon Textract, which is primarily designed for extracting text from documents and images,\nnot audio. While Textract can process images of spectrograms generated from audio, this is an indirect and\ninefficient approach compared to using a dedicated transcription service like Transcribe.\nOption D suggests Amazon Connect, which is a contact center as a service. While Connect offers transcription\ncapabilities, it's typically used for real-time call center interactions. Using Connect for batch processing S3\naudio files is an overkill and not its intended use case. Scanning for PII using Lambda within a contact flow is\nalso less efficient and scalable than utilizing Transcribe's PII redaction feature.\nTherefore, option C offers the most direct, efficient, and cost-effective solution by leveraging Amazon\nTranscribe's built-in PII redaction capabilities and automating the process with Lambda triggers.\nRelevant Links:\nAmazon Transcribe: https://aws.amazon.com/transcribe/\nAmazon Transcribe PII Redaction: https://docs.aws.amazon.com/transcribe/latest/dg/pii-redaction.html\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon S3: https://aws.amazon.com/s3/",
    "links": [
      "https://aws.amazon.com/transcribe/",
      "https://docs.aws.amazon.com/transcribe/latest/dg/pii-redaction.html",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/s3/"
    ]
  },
  {
    "question": "CertyIQ\nA company is running a multi-tier ecommerce web application in the AWS Cloud. The application runs on Amazon\nEC2 instances with an Amazon RDS for MySQL Multi-AZ DB instance. Amazon RDS is configured with the latest\ngeneration DB instance with 2,000 GB of storage in a General Purpose SSD (gp3) Amazon Elastic Block Store\n(Amazon EBS) volume. The database performance affects the application during periods of high demand.\nA database administrator analyzes the logs in Amazon CloudWatch Logs and discovers that the application\nperformance always degrades when the number of read and write IOPS is higher than 20,000.\nWhat should a solutions architect do to improve the application performance?",
    "options": {
      "B": "Increase the number of IOPS on the gp3 volume."
    },
    "answer": "D",
    "explanation": "The correct answer is B. Increase the number of IOPS on the gp3 volume.\nHere's why and why the other options are incorrect:\nThe problem states that performance degrades when IOPS exceed 20,000. The current gp3 volume is the\nbottleneck. gp3 volumes offer a baseline performance and allow you to provision additional IOPS if needed.\nThe root cause is that the provisioned IOPS are insufficient for the application's peak demand. Therefore,\nincreasing the provisioned IOPS is the most direct and efficient way to address the performance issue.\nOption A: Replace the volume with a magnetic volume. Magnetic volumes are the lowest performance EBS\nvolume type. Switching to magnetic volumes would drastically decrease performance and is the opposite of\nwhat the scenario requires.\nOption C: Replace the volume with a Provisioned IOPS SSD (io2) volume. While io2 volumes offer higher\nIOPS performance than gp3, it's not the most cost-effective initial approach. The best approach is to maximize\nIOPS within the gp3 volume before upgrading the volume. In addition, the newer io2 Block Express volume is\nsignificantly higher in performance and would cost significantly more.\nOption D: Replace the 2,000 GB gp3 volume with two 1,000 GB gp3 volumes. Splitting the volume doesn't\ninherently increase the total IOPS available to the database. The limit for IOPS depends on the volume size (up\nto 16,000 IOPS per GB). This might improve performance in some specific IO patterns, but the problem\nstatement clearly indicates the issue is exceeding the total IOPS limit. Also, it does not give the needed IOPS\nfor the EC2 instance.\nJustification for increasing IOPS on the gp3 volume:\n1. The problem identifies that the bottleneck is the RDS database when read/write IOPS exceed 20,000.\n2. The application is already using gp3 volumes, a general-purpose SSD volume type designed to\nbalance price and performance.\n3. gp3 volumes allow provisioning specific IOPS, meaning you can increase the IOPS as needed for a\ngiven volume size.\n4. The gp3 volume type has a maximum IOPS to size ratio. Increasing the volume size and then\nincreasing IOPS is a viable solution.\n5. Increasing IOPS directly addresses the stated problem: the database's inability to handle high IOPS\ndemands.\n6. It's the most cost-effective solution for the current information.\nAuthoritative Links:\nAmazon EBS volume types: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\ngp3 volumes: https://aws.amazon.com/ebs/gp3/",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html",
      "https://aws.amazon.com/ebs/gp3/"
    ]
  },
  {
    "question": "CertyIQ\nAn IAM user made several configuration changes to AWS resources in their company's account during a\nproduction deployment last week. A solutions architect learned that a couple of security group rules are not\nconfigured as desired. The solutions architect wants to confirm which IAM user was responsible for making\nchanges.\nWhich service should the solutions architect use to find the desired information?",
    "options": {
      "A": "Amazon GuardDuty: GuardDuty is a threat detection service that continuously monitors your AWS",
      "B": "Amazon Inspector: Inspector is a vulnerability management service that automatically assesses AWS",
      "C": "AWS CloudTrail.",
      "D": "AWS Config: Config enables you to assess, audit, and evaluate the configurations of your AWS resources."
    },
    "answer": "C",
    "explanation": "The correct answer is C. AWS CloudTrail.\nAWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of\nyour AWS account. CloudTrail logs API calls made to AWS services, including the identity of the caller (IAM\nuser, role, or AWS account), the time of the call, the source IP address, the request parameters, and the\nresponse elements returned by the AWS service. This allows you to track changes made to your AWS\nresources and identify the IAM user responsible for specific actions. In this scenario, the solutions architect\nneeds to identify the IAM user who modified the security group rules. CloudTrail provides the necessary audit\nlogs to pinpoint the user and the exact configuration changes they made.\nHere's why the other options are incorrect:\nA. Amazon GuardDuty: GuardDuty is a threat detection service that continuously monitors your AWS\naccounts and workloads for malicious activity and unauthorized behavior. It doesn't provide detailed logs of\nconfiguration changes made by IAM users.\nB. Amazon Inspector: Inspector is a vulnerability management service that automatically assesses AWS\nworkloads for software vulnerabilities and unintended network exposure. It helps improve the security and\ncompliance of applications deployed on AWS but does not track IAM user actions.\nD. AWS Config: Config enables you to assess, audit, and evaluate the configurations of your AWS resources.\nWhile Config can detect that a security group rule has drifted from its desired state, it is CloudTrail that\nprovides the comprehensive audit trail of who made the specific change. Config works well with Cloudtrail\ndata to provide a detailed audit history.\nIn summary, CloudTrail is the ideal service for identifying the IAM user responsible for making configuration\nchanges to AWS resources due to its API call logging capabilities.\nFurther research:\nAWS CloudTrail: https://aws.amazon.com/cloudtrail/",
    "links": [
      "https://aws.amazon.com/cloudtrail/"
    ]
  },
  {
    "question": "CertyIQ\nA company has implemented a self-managed DNS service on AWS. The solution consists of the following:\n Amazon EC2 instances in different AWS Regions\n Endpoints of a standard accelerator in AWS Global Accelerator\nThe company wants to protect the solution against DDoS attacks.\nWhat should a solutions architect do to meet this requirement?",
    "options": {
      "A": "Subscribe to AWS Shield Advanced. Add the accelerator as a resource to protect.",
      "B": "Subscribe to AWS Shield Advanced. Add the EC2 instances as resources to protect.",
      "C": "Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the accelerator.",
      "D": "Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the EC2"
    },
    "answer": "A",
    "explanation": "The correct answer is A: Subscribe to AWS Shield Advanced. Add the accelerator as a resource to protect.\nHere's why:\nDDoS Protection with AWS Shield: AWS Shield is a managed DDoS protection service that safeguards\napplications running on AWS. It comes in two tiers: Standard and Advanced.\nAWS Shield Standard: This tier provides automatic, always-on protection against common, frequently\noccurring network and transport layer DDoS attacks. It's included at no extra cost for all AWS customers. It\nprotects your AWS resources.\nAWS Shield Advanced: This tier provides enhanced DDoS protection for applications running on EC2, Elastic\nLoad Balancing (ELB), CloudFront, Global Accelerator, and Route 53. Key benefits include:\n24/7 DDoS response team support: Access to AWS DDoS experts who can help mitigate sophisticated\nattacks.\nVisibility into DDoS attacks: Detailed monitoring and reporting to understand attack patterns.\nCost protection: Credits for usage spikes caused by DDoS attacks.\nGlobal Accelerator and DDoS Protection: Global Accelerator uses a static IP address that acts as a single\npoint of contact for your applications, distributed across multiple AWS Regions. This makes it a prime target\nfor DDoS attacks, but also a strategic point to implement protection. Shield Advanced can protect Global\nAccelerator endpoints.\nWhy other options are incorrect:\nOption B (Protecting EC2 instances directly with Shield Advanced): While possible, it's less effective in this\nscenario. The Global Accelerator provides a centralized entry point. Protecting the accelerator directly shields\nthe underlying EC2 instances from the initial onslaught of DDoS traffic.\nOptions C and D (Using AWS WAF): AWS WAF is a web application firewall that helps protect your web\napplications from common web exploits. It can mitigate some DDoS attacks, especially those at the\napplication layer (Layer 7). However, it doesn't provide the comprehensive, network-layer protection of AWS\nShield Advanced. More specifically, AWS WAF is better suited for filtering out malicious requests based on\nHTTP headers, cookies, or other application-level data and is insufficient for volumetric DDoS attacks. While\nassociating WAF with the accelerator might offer some defense, the best practice is to use Shield Advanced,\nwhich integrates with WAF to provide a layered defense. Furthermore, WAF rate-based rules can help with\nsome types of denial of service attacks, they are not a replacement for Shield Advanced.\nIn Summary: Since the company wants to protect against DDoS attacks and is already using Global\nAccelerator, subscribing to AWS Shield Advanced and protecting the accelerator is the most effective and\ncomprehensive solution. This provides network and transport layer protection while simplifying the\nconfiguration.\nAuthoritative Links:\nAWS Shield: https://aws.amazon.com/shield/\nAWS Global Accelerator: https://aws.amazon.com/global-accelerator/\nAWS WAF: https://aws.amazon.com/waf/",
    "links": [
      "https://aws.amazon.com/shield/",
      "https://aws.amazon.com/global-accelerator/",
      "https://aws.amazon.com/waf/"
    ]
  },
  {
    "question": "CertyIQ\nAn ecommerce company needs to run a scheduled daily job to aggregate and filter sales records for analytics. The\ncompany stores the sales records in an Amazon S3 bucket. Each object can be up to 10 GB in size. Based on the\nnumber of sales events, the job can take up to an hour to complete. The CPU and memory usage of the job are\nconstant and are known in advance.\nA solutions architect needs to minimize the amount of operational effort that is needed for the job to run.\nWhich solution meets these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the most suitable solution, along with why the other options\nare less ideal:\nOption C: Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch\ntype. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the\njob.\nJustification: This option provides the best balance of managed services, scalability, and operational\nsimplicity.\nECS with Fargate: Fargate is a serverless compute engine for containers. This eliminates the need to manage\nunderlying EC2 instances, handling patching, scaling, and maintenance for you. This significantly reduces\noperational overhead. You specify the CPU and memory requirements for your container (as these are known\nin advance), and Fargate provisions the resources.\nContainerization: Using a container (e.g., a Docker image) allows you to package your application and its\ndependencies consistently. This eliminates the \"it works on my machine\" problem and ensures the application\nruns identically across different environments.\nAmazon EventBridge: EventBridge provides a serverless event bus that allows you to schedule the job to run\ndaily. It can directly trigger an ECS task execution based on a defined schedule. This creates a fully\nautomated process.\nScalability: ECS with Fargate can easily scale based on the resource requirements of the job. You define\nresource limits, and Fargate handles the scaling of underlying resources.\nSuitable Duration: The job duration is up to one hour, which is generally well within the limits of ECS tasks on\nFargate.\nCost-Effective: You only pay for the resources consumed during the task execution. This can be more cost-\neffective than having a dedicated EC2 instance running.\nOperational Efficiency: No EC2 instance management, container orchestration through ECS, scheduled\nexecutions by EventBridge all result in minimized operational effort.\nWhy other options are less ideal:\nOption A: Create an AWS Lambda function that has an Amazon EventBridge notification. Schedule the\nEventBridge event to run once a day.\nLimitation: Lambda has a maximum execution time limit of 15 minutes. The job can take up to an hour to\ncomplete, making Lambda unsuitable.\nMemory Limitation: Lambda functions also have memory limits. While the memory usage is known, complex\naggregation and filtering of large sales records (up to 10 GB per object) might exceed these limits,\nparticularly if multiple objects are processed in parallel.\nOption B: Create an AWS Lambda function. Create an Amazon API Gateway HTTP API, and integrate the\nAPI with the function. Create an Amazon EventBridge scheduled event that calls the API and invokes the\nfunction.\nLimitation: Similar to Option A, Lambda's execution time limit of 15 minutes renders this solution ineffective\nfor a job that can take up to an hour. Adding API Gateway introduces unnecessary complexity.\nOption D: Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch\ntype and an Auto Scaling group with at least one EC2 instance. Create an Amazon EventBridge scheduled\nevent that launches an ECS task on the cluster to run the job.\nIncreased Operational Overhead: This option requires managing EC2 instances. This includes patching,\nscaling, and maintenance. While Auto Scaling helps, it still introduces more operational overhead compared to\nFargate.\nCost: Running EC2 instances even when the job is not executing can be less cost-effective than Fargate,\nwhich only charges you for the resources used during the job's execution.\nAuthoritative Links:\nAWS ECS: https://aws.amazon.com/ecs/\nAWS Fargate: https://aws.amazon.com/fargate/\nAmazon EventBridge: https://aws.amazon.com/eventbridge/\nAWS Lambda Limits: https://docs.aws.amazon.com/lambda/latest/dg/services-limits.html\nIn summary, Option C, leveraging ECS with Fargate and EventBridge, offers the most suitable solution by\nproviding a managed, scalable, cost-effective, and operationally efficient approach to running the scheduled\ndaily job.",
    "links": [
      "https://aws.amazon.com/ecs/",
      "https://aws.amazon.com/fargate/",
      "https://aws.amazon.com/eventbridge/",
      "https://docs.aws.amazon.com/lambda/latest/dg/services-limits.html"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to transfer 600 TB of data from its on-premises network-attached storage (NAS) system to the\nAWS Cloud. The data transfer must be complete within 2 weeks. The data is sensitive and must be encrypted in\ntransit. The companys internet connection can support an upload speed of 100 Mbps.\nWhich solution meets these requirements MOST cost-effectively?",
    "options": {},
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the most cost-effective solution, along with supporting\nconcepts and links:\nOption C, utilizing AWS Snowball Edge Storage Optimized devices, is the most cost-effective because it\naddresses the time constraint and security requirements without incurring excessive costs. Let's break it\ndown:\n1. Data Volume and Time Constraint: Transferring 600 TB of data in 2 weeks (14 days) with a 100 Mbps\nconnection is infeasible. 100 Mbps equates to roughly 12.5 MBps. To transfer 600 TB (600,000 GB)\nwould take approximately (600,000 GB * 1024 MB/GB) / 12.5 MBps = 49,152,000 seconds, or about\n569 days. This far exceeds the 14-day requirement.\n2. Snowball Edge for Large Data Transfer: AWS Snowball Edge is designed for moving large amounts\nof data into and out of AWS. It provides physical storage devices you can ship to your location, load\nwith data, and then ship back to AWS for upload to S3. This circumvents the limitations of the\ninternet connection. https://aws.amazon.com/snowball/\n3. Security: Snowball Edge devices encrypt data both in transit and at rest, meeting the sensitive data\nrequirement. AWS provides tamper-evident packaging and secure data erasure services.\n4. Cost-Effectiveness: While Snowball Edge has upfront costs, it is more cost-effective than the other\noptions:\nOption A (S3 Multi-part Upload): Unrealistic due to the internet bandwidth limitation, rendering the\nlarge upload impossible within the timeframe. It is also vulnerable to errors and disconnections over\nsuch a long time.\nOption B (VPN): Still limited by the 100 Mbps internet connection. While a VPN provides secure\ntransport, the data transfer will still be prohibitively slow.\nOption D (Direct Connect): Direct Connect provides a dedicated network connection to AWS.\nHowever, setting up a 10 Gbps Direct Connect connection is significantly more expensive than using\nSnowball Edge, especially if the large data transfer is a one-time event. The costs involve installation\nfees, monthly port fees, and data transfer charges. A VPN on top of Direct Connect, while secure,\nadds further complexity and costs.\n5. Cost Breakdown (Illustrative): Snowball Edge Storage Optimized pricing varies by region and\ncontract. It is typically charged on a per-day usage basis plus data transfer costs into S3. This would\nbe considerably cheaper than Direct Connect with monthly port fees and VPN setup.\nIn summary, AWS Snowball Edge offers the optimal balance of speed, security, and cost-effectiveness for a\none-time, large-scale data migration when limited by internet bandwidth. It bypasses the limitations of the\ninternet connection and securely transports the data to AWS within the required timeframe, making it the\nmost suitable choice.",
    "links": [
      "https://aws.amazon.com/snowball/"
    ]
  },
  {
    "question": "CertyIQ\nA financial company hosts a web application on AWS. The application uses an Amazon API Gateway Regional API\nendpoint to give users the ability to retrieve current stock prices. The companys security team has noticed an\nincrease in the number of API requests. The security team is concerned that HTTP flood attacks might take the\napplication offline.\nA solutions architect must design a solution to protect the application from this type of attack.\nWhich solution meets these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Create an Amazon CloudFront distribution in front of the API Gateway Regional API endpoint with a",
      "B": "Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web",
      "C": "Use Amazon CloudWatch metrics to monitor the Count metric and alert the security team when the",
      "D": "Create an Amazon CloudFront distribution with [email protected] in front of the API Gateway Regional API"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web\nACL with the API Gateway stage.\nHere's a detailed justification:\nThe primary goal is to protect the API Gateway application from HTTP flood attacks (DDoS) with minimal\noperational overhead. AWS WAF is specifically designed for this purpose. A rate-based rule in WAF allows\nyou to specify the maximum number of requests allowed from a single IP address within a defined period (e.g.,\n5 minutes). If an IP exceeds this threshold, WAF can block or take other defined actions (like CAPTCHA)\nagainst the offending IP.\nAssociating the WAF web ACL directly with the API Gateway stage offers the most direct and efficient\nprotection. WAF integrates seamlessly with API Gateway and requires minimal configuration and\nmaintenance after the initial setup. The protection is applied at the regional level, safeguarding the\napplication from attacks originating from within the AWS region.\nOption A (CloudFront with a maximum TTL) primarily addresses caching and content delivery. While it can\nhelp reduce load on the API Gateway, it doesn't directly mitigate HTTP flood attacks. The high TTL could also\nlead to stale stock prices.\nOption C (CloudWatch metrics and alerts) only provides monitoring and notification, which doesn't\nautomatically prevent the attacks. Manual intervention is needed to respond to alerts, resulting in higher\noperational overhead and potentially a delayed response to ongoing attacks.\nOption D (CloudFront with [email protected] and Lambda) is more complex and introduces operational\noverhead. [email protected] adds latency. While it can provide custom IP blocking, it requires managing and\nmaintaining a Lambda function, which is more complex than using AWS WAF's built-in rate-limiting feature.\nAlso, it is better to use AWS WAF's built-in rate limiting rules.\nTherefore, AWS WAF provides the most effective and least operationally intensive solution for mitigating\nHTTP flood attacks against an API Gateway endpoint. It automatically blocks traffic exceeding defined rates,\npreventing the application from being overwhelmed.\nRelevant links:\nAWS WAF: https://aws.amazon.com/waf/\nAWS WAF Rate-Based Rule: https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-\ntype-rate-based.html\nAPI Gateway and WAF: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-\naccess-aws-waf.html",
    "links": [
      "https://aws.amazon.com/waf/",
      "https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-"
    ]
  },
  {
    "question": "CertyIQ\nA meteorological startup company has a custom web application to sell weather data to its users online. The\ncompany uses Amazon DynamoDB to store its data and wants to build a new service that sends an alert to the\nmanagers of four internal teams every time a new weather event is recorded. The company does not want this new\nservice to affect the performance of the current application.\nWhat should a solutions architect do to meet these requirements with the LEAST amount of operational overhead?",
    "options": {
      "A": "Use DynamoDB transactions to write new event data to the table. Configure the transactions to notify",
      "B": "Have the current application publish a message to four Amazon Simple Notification Service (Amazon SNS)",
      "C": "Enable Amazon DynamoDB Streams on the table. Use triggers to write to a single Amazon Simple",
      "D": "Add a custom attribute to each record to flag new items. Write a cron job that scans the table every minute"
    },
    "answer": "C",
    "explanation": "Option C is the best solution because it leverages DynamoDB Streams, which provides a near real-time,\nordered stream of item-level modifications in a DynamoDB table. This is a managed service that minimizes\noperational overhead. By enabling DynamoDB Streams and configuring a trigger (e.g., an AWS Lambda\nfunction), the solution can asynchronously process new weather event records without impacting the\nperformance of the existing web application. The Lambda function can then publish to a single SNS topic,\nsimplifying management by having all teams subscribe to one topic, rather than multiple topics.\nOption A is not optimal because using DynamoDB transactions might impact the performance of the current\napplication, especially under heavy write loads, as transactions are more resource-intensive than regular\nwrites. Configuring transactions to notify internal teams is also not a built-in feature, requiring significant\ncustom coding and management.\nOption B isn't as efficient. Directly publishing to four SNS topics from the current application couples the\napplication tightly with the notification service. This increases complexity within the application and\npotentially affects its performance. Managing four separate SNS topics adds unnecessary overhead.\nOption D is the least efficient. Scanning a DynamoDB table every minute is highly inefficient and expensive,\nespecially as the table grows. This approach is also disruptive to DynamoDB's performance and scalability.\nUsing SQS is unnecessary when SNS can directly notify the teams. Moreover, managing a cron job adds\noperational overhead.\nTherefore, option C offers the best balance of minimal operational overhead, asynchronous processing to\navoid performance impact, and a manageable notification strategy using a single SNS topic.\nRelevant links:\nAmazon DynamoDB Streams\nAmazon SNS\nAWS Lambda Triggers",
    "links": []
  },
  {
    "question": "CertyIQ\nA company wants to use the AWS Cloud to make an existing application highly available and resilient. The current\nversion of the application resides in the company's data center. The application recently experienced data loss\nafter a database server crashed because of an unexpected power outage.\nThe company needs a solution that avoids any single points of failure. The solution must give the application the\nability to scale to meet user demand.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A because it directly addresses the requirements for high availability, resilience, and\nscalability while avoiding single points of failure and preventing data loss.\nHere's why:\nApplication Servers in Auto Scaling Group across multiple AZs: Deploying application servers across\nmultiple Availability Zones (AZs) ensures that if one AZ fails, the application remains available in other AZs.\nAn Auto Scaling group dynamically adjusts the number of EC2 instances based on demand, ensuring\nscalability and resilience. https://aws.amazon.com/autoscaling/\nAmazon RDS DB Instance in Multi-AZ configuration: Using Amazon RDS with a Multi-AZ configuration\nreplicates the database to a standby instance in a different AZ. In case of a failure of the primary database\ninstance, RDS automatically fails over to the standby, minimizing downtime and preventing data loss. This\naddresses the company's concern regarding database server crashes and data loss due to power outages.\nhttps://aws.amazon.com/rds/features/multi-az/\nLet's analyze why the other options are not optimal:\nB: Using a single AZ creates a single point of failure. EC2 Auto Recovery only restarts an instance on different\nhardware within the same AZ; it doesn't provide cross-AZ redundancy. Deploying the database on an EC2\ninstance without proper replication management is also a risk.\nC: While using Auto Scaling across multiple AZs is good, relying on manually promoting a read replica to\nreplace the primary database instance introduces manual intervention and longer recovery times, which are\nnot ideal for high availability and resilience. The read replica being in a single AZ also presents a single point\nof failure for the replicated database.\nD: Using EBS Multi-Attach is not recommended for most relational database workloads as it can lead to data\ncorruption if not managed very carefully. Setting up database replication on EC2 instances manually can be\ncomplex and error-prone compared to using RDS Multi-AZ, which provides managed failover and replication.\nIn summary, option A provides a fully managed, highly available, and scalable solution using AWS services\ndesigned for these purposes, meeting all the stated requirements.",
    "links": [
      "https://aws.amazon.com/autoscaling/",
      "https://aws.amazon.com/rds/features/multi-az/"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to ingest and handle large amounts of streaming data that its application generates. The\napplication runs on Amazon EC2 instances and sends data to Amazon Kinesis Data Streams, which is configured\nwith default settings. Every other day, the application consumes the data and writes the data to an Amazon S3\nbucket for business intelligence (BI) processing. The company observes that Amazon S3 is not receiving all the\ndata that the application sends to Kinesis Data Streams.\nWhat should a solutions architect do to resolve this issue?",
    "options": {
      "A": "Update the Kinesis Data Streams default settings by modifying the data retention period.",
      "B": "Update the application to use the Kinesis Producer Library (KPL) to send the data to Kinesis Data Streams.",
      "C": "Update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data",
      "D": "Turn on S3 Versioning within the S3 bucket to preserve every version of every object that is ingested in the"
    },
    "answer": "A",
    "explanation": "The problem indicates that data is being lost between Kinesis Data Streams and S3. With default settings,\nKinesis Data Streams retains data for 24 hours. Since the data is only consumed and written to S3 every other\nday, it's highly probable that the data older than 24 hours is being discarded by Kinesis before it can be\nprocessed and stored in S3. Increasing the retention period allows Kinesis Data Streams to store the data for\na longer duration, ensuring that the BI process can access and write it to S3 even when run every other day.\nOption A directly addresses this data retention issue by modifying the Kinesis Data Streams default settings.\nIncreasing the retention period to at least 48 hours would ensure that the data is available when the\napplication consumes and writes it to S3.\nOption B, using KPL, optimizes data ingestion into Kinesis Data Streams by aggregating records and\nimproving throughput, but it doesn't directly solve the data loss problem caused by insufficient retention.\nOption C, increasing the number of shards, addresses throughput and scaling issues in Kinesis Data Streams\nbut won't prevent data loss if the data retention period is too short. Shards influence the parallelism of the\ndata stream processing, but do not impact data retention.\nOption D, enabling S3 Versioning, helps maintain a history of objects in S3 and prevent accidental data loss\nwithin S3, but does not retrieve data already lost between Kinesis Data Streams and S3. It is useful after data\nhas been ingested and to recover from overwrite or deletion.\nTherefore, modifying the data retention period in Kinesis Data Streams is the most direct and effective\nsolution to prevent data loss in this scenario.\nRelevant Link:\nAmazon Kinesis Data Streams Data Retention",
    "links": []
  },
  {
    "question": "CertyIQ\nA developer has an application that uses an AWS Lambda function to upload files to Amazon S3 and needs the\nrequired permissions to perform the task. The developer already has an IAM user with valid IAM credentials\nrequired for Amazon S3.\nWhat should a solutions architect do to grant the permissions?",
    "options": {
      "B": "It's poor practice to embed IAM user credentials directly in Lambda code."
    },
    "answer": "D",
    "explanation": "The correct approach to granting a Lambda function the necessary permissions to interact with other AWS\nservices, like S3, is to use an IAM execution role. Here's why:\nIAM execution roles provide a secure and best-practice method for granting permissions to AWS resources\nlike Lambda functions. An IAM role is an IAM entity that defines a set of permissions for making AWS service\nrequests. Attaching an IAM role to the Lambda function gives the function temporary credentials to assume\nthe role and perform actions permitted by the role's policies. This is superior to using hardcoded IAM\ncredentials for several reasons.\nOption A, adding permissions to the Lambda function's resource policy, is typically used for granting\npermissions to other AWS accounts or services to invoke the Lambda function itself, not for the Lambda\nfunction to access other services.\nOption B, creating a signed request using existing IAM credentials within the Lambda function, is strongly\ndiscouraged due to security risks. Embedding IAM credentials directly in code exposes them, potentially\nleading to unauthorized access if the code is compromised. Managing credential rotation also becomes\ndifficult and cumbersome. Hardcoding AWS credentials in an application is considered a severe security\nvulnerability.\nOption C, creating a new IAM user and using its credentials in the Lambda function, suffers from the same\nsecurity issues as option B. It's poor practice to embed IAM user credentials directly in Lambda code.\nOption D, creating an IAM execution role and attaching it to the Lambda function, is the recommended\napproach because it provides a secure, manageable, and best-practice method for granting the Lambda\nfunction the necessary permissions. AWS manages the temporary credentials associated with the role,\nautomatically rotating them and removing the need for manual credential management in the code. This\naligns with the principle of least privilege and promotes secure application design. The role-based approach is\nfar more secure and easier to manage. The function assumes the role dynamically when it is executed.\nHere are some relevant resources for more information:\nIAM Roles for AWS Lambda: https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-\nrole.html\nSecurity best practices in IAM: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html\nAWS IAM documentation: https://docs.aws.amazon.com/IAM/index.html",
    "links": [
      "https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html",
      "https://docs.aws.amazon.com/IAM/index.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has deployed a serverless application that invokes an AWS Lambda function when new documents are\nuploaded to an Amazon S3 bucket. The application uses the Lambda function to process the documents. After a\nrecent marketing campaign, the company noticed that the application did not process many of the documents.\nWhat should a solutions architect do to improve the architecture of this application?",
    "options": {
      "A": "Set the Lambda function's runtime timeout value to 15 minutes.",
      "B": "Configure an S3 bucket replication policy. Stage the documents in the S3 bucket for later processing.",
      "C": "Deploy an additional Lambda function. Load balance the processing of the documents across the two",
      "D": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Send the requests"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Create an Amazon Simple Queue Service (Amazon SQS) queue. Send the requests\nto the queue. Configure the queue as an event source for Lambda.\nHere's a detailed justification:\nThe issue described indicates an event-driven architecture struggling to handle a surge in document uploads\nto S3, overwhelming the Lambda function. Direct invocation of Lambda by S3 events is susceptible to event\nloss and throttling, especially during peak loads.\nOption D introduces Amazon SQS as an intermediary buffer, effectively decoupling the S3 events from the\nLambda function invocations. When an S3 object is created, instead of directly invoking the Lambda function,\na message is sent to the SQS queue. The Lambda function is then triggered by the SQS queue. This approach\noffers several benefits:\n1. Buffering and Scalability: SQS acts as a buffer, absorbing the surge in requests. SQS is designed to\nhandle high volumes of messages reliably. It scales automatically, preventing event loss during peak\nloads.\n2. Reliability: SQS ensures guaranteed message delivery at least once. If the Lambda function fails to\nprocess a message, it remains in the queue and can be retried. This prevents data loss and ensures all\ndocuments are eventually processed.\n3. Asynchronous Processing: Decoupling through SQS allows the S3 bucket to continue accepting\nuploads without waiting for the Lambda function to process each one immediately. This improves the\napplication's responsiveness.\n4. Error Handling: If the Lambda function consistently fails to process a specific document, SQS can be\nconfigured with a dead-letter queue (DLQ) to store the problematic messages for later investigation\nand resolution.\nOption A (increasing Lambda timeout) might help for individual documents taking longer to process, but it\ndoesn't address the fundamental issue of potential event loss or the overwhelming of Lambda during a surge.\nOption B (S3 replication) doesn't address the processing issue at all. Replication is for data redundancy and\ndisaster recovery, not for handling event processing loads.\nOption C (adding another Lambda function with load balancing) is a valid approach for scaling Lambda, but\ndirectly load balancing from S3 events is complex. SQS offers a more robust and manageable way to achieve\nhorizontal scaling and resilience. SQS's built-in mechanisms for retry and DLQ make it a better solution in this\nscenario. While increased concurrency in Lambda can help (via Reserved Concurrency or Provisioned\nConcurrency), SQS as the intermediary provides the crucial buffer for bursts of S3 events.\nIn summary, using SQS provides a reliable and scalable solution for processing S3 events by buffering\nrequests, ensuring delivery, and enabling asynchronous processing, which are critical for handling surges in\ndocument uploads and preventing event loss.\nAuthoritative Links:\nAWS SQS: https://aws.amazon.com/sqs/\nUsing AWS Lambda with Amazon SQS: https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\nServerless Architectures with AWS Lambda: https://aws.amazon.com/serverless/architectures/",
    "links": [
      "https://aws.amazon.com/sqs/",
      "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html",
      "https://aws.amazon.com/serverless/architectures/"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is designing the architecture for a software demonstration environment. The environment will\nrun on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The system will\nexperience significant increases in traffic during working hours but is not required to operate on weekends.\nWhich combination of actions should the solutions architect take to ensure that the system can scale to meet\ndemand? (Choose two.)",
    "options": {
      "A": "Use AWS Auto Scaling to adjust the ALB capacity based on request rate. The ALB automatically scales to",
      "B": "Use AWS Auto Scaling to scale the capacity of the VPC internet gateway. Internet Gateways are",
      "C": "Launch the EC2 instances in multiple AWS Regions to distribute the load across Regions. While this is",
      "D": "Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU utilization."
    },
    "answer": "D",
    "explanation": "Here's a detailed justification for why options D and E are the correct choices, and why the others are not:\nD. Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU utilization.\nJustification: Target tracking scaling policies automatically adjust the number of EC2 instances in the Auto\nScaling group to maintain a specified target value for a chosen metric, such as CPU utilization. During working\nhours when traffic increases, CPU utilization on the existing instances will rise. The scaling policy responds by\nlaunching more instances to keep the CPU utilization near the target value, thereby ensuring application\nperformance remains consistent despite increasing load. This dynamic scaling is essential for meeting\nfluctuating demand within working hours.\nWhy it works: This approach provides a reactive, automated scaling mechanism. It eliminates the need for\nmanual intervention or constant monitoring. If the instances' CPU utilization increases due to higher request\nloads, the auto-scaling group launches new instances to distribute that load and reduce the CPU per instance,\nkeeping the performance level consistent.\nRelevant Concept: Auto Scaling Groups, Target Tracking Scaling.\nAuthoritative Link: https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-target-\ntracking.html\nE. Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to\nzero for weekends. Revert to the default values at the start of the week.\nJustification: Since the system is not required to operate on weekends, scheduled scaling is a perfect fit. By\nsetting the minimum, maximum, and desired capacity to zero for weekends, all instances will be terminated,\neliminating unnecessary costs. At the start of the week, the scheduled scaling will revert the capacity\nsettings to the appropriate values, ensuring the system is ready to handle the weekday traffic. This allows for\nefficient cost management by only running resources when needed.\nWhy it works: Scheduled scaling allows you to proactively manage the capacity of your Auto Scaling group\nbased on predictable patterns like workdays versus weekends. This ensures that you don't pay for compute\ncapacity when it's not needed.\nRelevant Concept: Auto Scaling Groups, Scheduled Scaling.\nAuthoritative Link: https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\nWhy the other options are incorrect:\nA. Use AWS Auto Scaling to adjust the ALB capacity based on request rate. The ALB automatically scales to\nhandle varying request rates. Auto Scaling is used for the EC2 instances behind the ALB, not the ALB itself.\nB. Use AWS Auto Scaling to scale the capacity of the VPC internet gateway. Internet Gateways are\nhorizontally scaled and redundant, so they don't need to be scaled using Auto Scaling.\nC. Launch the EC2 instances in multiple AWS Regions to distribute the load across Regions. While this is\nuseful for high availability and disaster recovery, it's overkill for a demonstration environment that only needs\nto be shut down on weekends. Moreover, it is not cost-effective. Distributing across regions increases the\ncomplexity and cost significantly more than simply stopping instances when they are not needed. The\nproblem statement focuses on scaling to meet demand within working hours and avoiding costs on weekends,\nmaking a multi-region deployment unnecessary for the given constraints.",
    "links": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-target-",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is designing a two-tiered architecture that includes a public subnet and a database subnet.\nThe web servers in the public subnet must be open to the internet on port 443. The Amazon RDS for MySQL DB\ninstance in the database subnet must be accessible only to the web servers on port 3306.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "options": {
      "D": "Here's why:"
    },
    "answer": "C",
    "explanation": "The correct answer is CD. Here's why:\nC: Create a security group for the web servers in the public subnet. Add a rule to allow traffic from\n0.0.0.0/0 on port 443.\nThis step is crucial for allowing internet traffic to reach the web servers on port 443 (HTTPS). Security groups\nact as virtual firewalls at the instance level. Allowing 0.0.0.0/0 (all IP addresses) on port 443 enables external\nusers to connect to the web servers via HTTPS. Without this rule, users wouldn't be able to access the web\napplication.\nD: Create a security group for the DB instance. Add a rule to allow traffic from the web servers security\ngroup on port 3306.\nThis step ensures that only the web servers can access the database on port 3306 (MySQL's default port). By\nreferencing the web servers' security group as the source in the DB instance's security group rule, you're\nspecifically granting access only to instances associated with that security group. This is a more secure\napproach than using CIDR blocks, as it automatically adjusts if the web server's IP address changes due to\nscaling or instance replacement. This principle of least privilege restricts access and hardens security.\nWhy other options are incorrect:\nA: Create a network ACL for the public subnet. Add a rule to deny outbound traffic to 0.0.0.0/0 on port\n3306. Network ACLs operate at the subnet level and are stateless. While you could theoretically use them,\nsecurity groups provide a more granular and easier-to-manage solution for this scenario. Denying outbound\ntraffic from the public subnet is not the solution. The database traffic flows from the public subnet (web\nservers) to the database subnet (RDS instance). Blocking outbound traffic from the public subnet wouldn't\nprevent the access. Security Groups should be configured on the DB instance.\nB: Create a security group for the DB instance. Add a rule to allow traffic from the public subnet CIDR block\non port 3306. While this allows communication, it's less secure than referencing the web servers' security\ngroup. If the web server's IP changes, the security group rule based on CIDR would need manual updating. The\nsecurity group referencing provides a dynamic association between the web servers and the DB instance.\nE: Create a security group for the DB instance. Add a rule to deny all traffic except traffic from the web\nservers security group on port 3306. While the intended outcome is correct (restrict access), security group\nrules operate as \"allow\" rules. There are no explicit \"deny\" rules in security groups (except the implicit deny\nfor everything not explicitly allowed).\nAuthoritative Links:\nAmazon VPC Security Groups\nAmazon VPC Network ACLs",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is implementing a shared storage solution for a gaming application that is hosted in the AWS Cloud.\nThe company needs the ability to use Lustre clients to access data. The solution must be fully managed.\nWhich solution meets these requirements?",
    "options": {
      "B": "It does not natively support the Lustre protocol."
    },
    "answer": "D",
    "explanation": "The correct answer is D because Amazon FSx for Lustre provides a fully managed, high-performance file\nsystem optimized for compute-intensive workloads, including gaming applications. This service inherently\nsupports Lustre clients, fulfilling the requirement to access data using them.\nOption A is incorrect because AWS DataSync is primarily a data transfer service, not a file system. It doesn't\nprovide a mountable file system directly.\nOption B is incorrect because AWS Storage Gateway file gateway translates between cloud storage (like S3)\nand on-premises file protocols like NFS or SMB. It does not natively support the Lustre protocol.\nOption C is incorrect because Amazon EFS is a fully managed NFS file system suitable for a wide range of\nworkloads, but it does not support the Lustre protocol.\nTherefore, only Amazon FSx for Lustre meets the requirements of a fully managed solution with native Lustre\nclient support, making it the most suitable choice for the gaming application's shared storage needs. FSx for\nLustre's architecture is specifically designed for high-performance computing and can deliver the low latency\nand high throughput required by demanding gaming applications.\nReferences:\nAmazon FSx for Lustre: https://aws.amazon.com/fsx/lustre/\nAWS DataSync: https://aws.amazon.com/datasync/\nAWS Storage Gateway: https://aws.amazon.com/storagegateway/\nAmazon EFS: https://aws.amazon.com/efs/",
    "links": [
      "https://aws.amazon.com/fsx/lustre/",
      "https://aws.amazon.com/datasync/",
      "https://aws.amazon.com/storagegateway/",
      "https://aws.amazon.com/efs/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an application that receives data from thousands of geographically dispersed remote devices that\nuse UDP. The application processes the data immediately and sends a message back to the device if necessary. No\ndata is stored.\nThe company needs a solution that minimizes latency for the data transmission from the devices. The solution also\nmust provide rapid failover to another AWS Region.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Configure an Amazon Route 53 failover routing policy. Create a Network Load Balancer (NLB) in each of the",
      "B": "Process the data in",
      "C": "Use AWS Global Accelerator. Create an Application Load Balancer (ALB) in each of the two Regions as an",
      "D": "Configure an Amazon Route 53 failover routing policy. Create an Application Load Balancer (ALB) in each of"
    },
    "answer": "B",
    "explanation": "The correct answer is B because it leverages AWS Global Accelerator for low latency and failover, and\nNetwork Load Balancers (NLBs) for UDP traffic.\nHere's a detailed justification:\nLow Latency with AWS Global Accelerator: AWS Global Accelerator uses the AWS global network to direct\ntraffic to the optimal endpoint based on location, network conditions, and health. This significantly reduces\nlatency for geographically dispersed devices, as traffic is routed through the nearest AWS edge location,\nimproving the UDP data transmission speed. https://aws.amazon.com/global-accelerator/\nUDP Support with Network Load Balancer (NLB): NLBs are designed to handle UDP traffic, which is crucial\nfor the application's data reception. Application Load Balancers (ALBs) only support HTTP/HTTPS and cannot\nhandle UDP. https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\nRapid Failover: Global Accelerator provides rapid failover capabilities. If one AWS Region becomes\nunavailable, Global Accelerator automatically redirects traffic to the healthy NLB in the other Region,\nensuring business continuity.\nECS with Fargate for Processing: ECS with Fargate provides a scalable and managed container orchestration\nservice where the data processing logic can be hosted. This allows the application to process the incoming\ndata immediately, as required.\nEliminating other options reasoning:\nA & D: Route 53 failover, although capable of failover, relies on DNS propagation, which is slower than the\nnear-instantaneous failover of Global Accelerator. Furthermore, option D uses ALB, which cannot handle UDP\ntraffic.\nC: Option C uses ALB, which cannot handle UDP traffic and therefore is not the optimal choice.",
    "links": [
      "https://aws.amazon.com/global-accelerator/",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect must migrate a Windows Internet Information Services (IIS) web application to AWS. The\napplication currently relies on a file share hosted in the user's on-premises network-attached storage (NAS). The\nsolutions architect has proposed migrating the IIS web servers to Amazon EC2 instances in multiple Availability\nZones that are connected to the storage solution, and configuring an Elastic Load Balancer attached to the\ninstances.\nWhich replacement to the on-premises file share is MOST resilient and durable?",
    "options": {
      "A": "Migrate the file share to Amazon RDS.",
      "B": "Migrate the file share to AWS Storage Gateway.",
      "C": "Migrate the file share to Amazon FSx for Windows File Server.",
      "D": "Migrate the file share to Amazon Elastic File System (Amazon EFS)."
    },
    "answer": "C",
    "explanation": "The most resilient and durable replacement for the on-premises file share in this scenario is Amazon FSx for\nWindows File Server (Option C).\nHere's why:\nWindows Compatibility: The application is a Windows IIS web application that currently relies on a file share.\nFSx for Windows File Server provides native support for the SMB protocol and NTFS file system, ensuring\nseamless compatibility without requiring application code changes. This compatibility minimizes migration\neffort and potential issues.\nIntegration with EC2 and Active Directory: FSx for Windows File Server seamlessly integrates with EC2\ninstances and existing Active Directory environments. This simplifies user authentication and authorization,\naccess control, and overall management.\nHigh Availability and Durability: FSx for Windows File Server offers multi-AZ deployment options, providing\nhigh availability and data redundancy across multiple Availability Zones. This helps ensure the application\nremains available even in the event of an AZ failure. It uses Windows Server failover clustering for high\navailability.\nScalability and Performance: FSx for Windows File Server offers scalable storage capacity and performance\noptions, allowing the application to handle increasing workloads without performance bottlenecks. It's\ndesigned to support enterprise-grade workloads.\nRDS (Option A): Amazon RDS is a relational database service and is not designed to serve as a general-\npurpose file share. Migrating a file share to a database would be highly complex and inefficient.\nAWS Storage Gateway (Option B): Storage Gateway connects on-premises applications to cloud storage.\nWhile it can provide access to AWS storage, it still requires an on-premises component, negating the goal of\nmigrating the file share to AWS. It introduces added latency and management overhead.\nAmazon EFS (Option D): Amazon EFS is a network file system designed primarily for Linux-based\napplications. While it can be used with Windows EC2 instances, it requires additional configuration and\ndoesn't natively support the SMB protocol or Active Directory integration as seamlessly as FSx for Windows\nFile Server.\nIn summary, FSx for Windows File Server is the best option because it offers native Windows compatibility,\nhigh availability, durability, scalability, and seamless integration with EC2 and Active Directory, making it the\nmost suitable replacement for the on-premises file share.\nAuthoritative Links:\nAmazon FSx for Windows File Server: https://aws.amazon.com/fsx/windows/",
    "links": [
      "https://aws.amazon.com/fsx/windows/"
    ]
  },
  {
    "question": "CertyIQ\nA company is deploying a new application on Amazon EC2 instances. The application writes data to Amazon Elastic\nBlock Store (Amazon EBS) volumes. The company needs to ensure that all data that is written to the EBS volumes\nis encrypted at rest.\nWhich solution will meet this requirement?",
    "options": {
      "B": "Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2\ninstances.\nHere's why this is the optimal solution and why the other options are incorrect:\nWhy Option B is Correct: EBS encryption is configured at the volume level. When you create an EBS volume\nand specify encryption, all data at rest on the volume, all data moving between the EC2 instance and the\nvolume, and all snapshots created from the volume are encrypted. This fulfills the requirement of encrypting\nall data written to EBS volumes at rest.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\nWhy Option A is Incorrect: IAM roles are used to grant permissions to EC2 instances to access other AWS\nservices. While you can control access to KMS keys used for EBS encryption via IAM, simply attaching an IAM\nrole that \"specifies EBS encryption\" doesn't inherently encrypt existing or newly created EBS volumes. IAM\ncontrols who can use encryption, not if encryption is enforced by default on EBS volumes.\nWhy Option C is Incorrect: EC2 instance tags are metadata applied to EC2 instances for organization and\nautomation. They do not enforce EBS encryption. Tags are used for cost allocation, automation scripts, etc.,\nbut they don't have a direct impact on encryption settings for associated resources.\nWhy Option D is Incorrect: While an AWS KMS key policy can enforce encryption, it must be paired with EBS\nvolume creation that uses the KMS key. This is only effective when creating new EBS volumes with the\nenforcement key; existing unencrypted EBS volumes would remain unencrypted. It also wouldn't ensure that\nnewly created EBS volumes are always encrypted by default, unless coupled with other mechanisms. Option\nB provides direct encryption at the volume creation step.\nTherefore, creating encrypted EBS volumes directly at the time of their creation is the most straightforward\nand reliable method to ensure all data written to those volumes is encrypted at rest, satisfying the company's\nrequirement. It ensures every created EBS volume is encrypted, by default, during the volume creation\nprocess.",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has a web application with sporadic usage patterns. There is heavy usage at the beginning of each\nmonth, moderate usage at the start of each week, and unpredictable usage during the week. The application\nconsists of a web server and a MySQL database server running inside the data center. The company would like to\nmove the application to the AWS Cloud, and needs to select a cost-effective database platform that will not\nrequire database modifications.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C, MySQL-compatible Amazon Aurora Serverless. Here's a detailed justification:\nAurora Serverless v2 is ideal for applications with sporadic and unpredictable workloads. It automatically\nscales the database capacity up or down based on the application's needs, eliminating the need to provision\nand manage database servers. This on-demand scaling aligns perfectly with the company's heavy monthly\nusage, moderate weekly usage, and unpredictable usage during the week. It provides cost optimization by\nonly charging for the compute and storage resources consumed.\nAmazon RDS for MySQL (option B) would require the company to provision a specific instance size, which\nmight lead to over-provisioning during periods of low usage, resulting in unnecessary costs. While RDS can\nscale, it doesn't automatically scale to zero like Serverless does during periods of inactivity.\nAmazon DynamoDB (option A) is a NoSQL database and would require significant application modifications to\nbe compatible, which contradicts the requirement of not needing database modifications. DynamoDB's data\nmodel and query language are fundamentally different from MySQL.\nMySQL deployed on Amazon EC2 in an Auto Scaling group (option D) would involve significantly more\noperational overhead for managing the database, including patching, backups, and scaling, compared to\nAurora Serverless. Also, while Auto Scaling addresses the compute scaling for the web tier, it doesn't\ninherently solve the database scaling problem. It requires more configurations and scripts for the database\nlayer.\nAurora Serverlesss MySQL compatibility allows for a smooth migration without any code changes. Also,\nAurora offers improved performance and availability over standard MySQL.\nIn summary, Aurora Serverless v2 offers the best combination of cost-effectiveness, automatic scaling,\nminimal management overhead, and MySQL compatibility, making it the most suitable choice for the given\nscenario.\nRelevant links:\nAmazon Aurora Serverless v2\nAmazon RDS for MySQL\nAmazon DynamoDB",
    "links": []
  },
  {
    "question": "CertyIQ\nAn image-hosting company stores its objects in Amazon S3 buckets. The company wants to avoid accidental\nexposure of the objects in the S3 buckets to the public. All S3 objects in the entire AWS account need to remain\nprivate.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Use Amazon GuardDuty to monitor S3 bucket policies. Create an automatic remediation action rule that uses",
      "B": "Use AWS Trusted Advisor to find publicly accessible S3 buckets. Configure email notifications in Trusted",
      "C": "Use AWS Resource Access Manager to find publicly accessible S3 buckets. Use Amazon Simple Notification",
      "D": "Use the S3 Block Public Access feature on the account level. Use AWS Organizations to create a service"
    },
    "answer": "D",
    "explanation": "The correct answer is D because it provides a comprehensive and proactive approach to preventing accidental\npublic exposure of S3 objects at the account level, with a mechanism to ensure the configuration isn't altered.\nHere's a detailed justification:\nS3 Block Public Access (BPA) at the Account Level: S3 BPA is designed specifically to prevent public access\nto S3 buckets and objects. Applying it at the account level ensures that all new and existing buckets within\nthe account adhere to the block public access settings. This is the primary preventative measure. It blocks\npublic access through bucket policies, ACLs, or both.\nAWS Organizations and Service Control Policies (SCPs): AWS Organizations enables you to centrally\nmanage multiple AWS accounts. SCPs are policies that you can attach to the root, organizational units (OUs),\nor individual accounts within your organization. By creating an SCP that prevents IAM users from disabling the\nS3 BPA setting, you establish a guardrail. This ensures that even IAM users with otherwise sufficient\npermissions cannot inadvertently (or maliciously) bypass the account-level security measure. This ensures\ncompliance across all accounts managed within the AWS Organizations.\nWhy other options are less suitable:\nOption A (GuardDuty): GuardDuty is a threat detection service. While it can identify publicly accessible\nbuckets, it's a reactive measure. The objects might be publicly accessible for a period before GuardDuty\ndetects it.\nOption B (Trusted Advisor): Similar to GuardDuty, Trusted Advisor is more of an advisory tool. It provides\nrecommendations but doesn't actively prevent public access. It also requires manual intervention, increasing\nthe risk of delayed remediation.\nOption C (Resource Access Manager and SNS): AWS RAM shares AWS resources with other AWS accounts\nor within your organization. It's not directly related to preventing public access to S3 buckets. While SNS\ncould trigger a Lambda function, this setup is more complex and less effective than directly using S3 BPA and\nSCPs.\nIn summary, option D provides a robust, proactive, and centrally managed solution that directly addresses the\nproblem of accidental public exposure of S3 objects across an entire AWS account. It employs both\npreventive controls (S3 BPA) and enforcement mechanisms (SCPs) to ensure consistent security.\nRelevant links for further research:\nS3 Block Public Access: https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-\npublic-access.html\nAWS Organizations SCPs:\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-",
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html"
    ]
  },
  {
    "question": "CertyIQ\nAn ecommerce company is experiencing an increase in user traffic. The companys store is deployed on Amazon\nEC2 instances as a two-tier web application consisting of a web tier and a separate database tier. As traffic\nincreases, the company notices that the architecture is causing significant delays in sending timely marketing and\norder confirmation email to users. The company wants to reduce the time it spends resolving complex email\ndelivery issues and minimize operational overhead.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Create a separate application tier using EC2 instances dedicated to email processing.",
      "B": "Configure the web instance to send email through Amazon Simple Email Service",
      "C": "Configure the web instance to send email through Amazon Simple Notification Service (Amazon SNS).",
      "D": "Create a separate application tier using EC2 instances dedicated to email processing. Place the instances in"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Configure the web instance to send email through Amazon Simple Email Service\n(Amazon SES).\nHere's a detailed justification:\nThe primary problem is the delay in sending emails (marketing and order confirmations) due to increasing\ntraffic impacting the web application performance. Option B proposes using Amazon SES, which is a fully\nmanaged email sending service. This offloads the email sending responsibility from the web application\nservers. By using SES, the web application can quickly hand off email requests to AWS without blocking\nthreads or consuming resources needed to serve web traffic. This reduces the application's workload and\nimproves responsiveness. SES is designed for high deliverability and scalability, minimizing the operational\noverhead of managing email infrastructure, including dealing with bounce handling, spam filtering, and\nreputation management.\nOption A and D involve creating a separate EC2-based application tier for email processing. While it might\nseem like a solution, this introduces operational complexity and overhead, including managing and scaling\nthese instances. It also doesn't address the underlying email delivery issues that a managed service like SES\nis designed to handle. It adds more servers to manage instead of removing workload from current servers.\nOption C suggests using Amazon SNS. While SNS is a messaging service, it is not designed for sending\ntransactional emails like order confirmations or marketing emails. SNS is primarily used for push notifications,\napplication-to-application (A2A) and application-to-person (A2P) messaging. Using SNS for this purpose\nwould be an inappropriate and inefficient approach.\nTherefore, using Amazon SES is the most efficient and scalable solution for handling email sending, reducing\noperational overhead, and improving the overall performance of the e-commerce application. It aligns\nperfectly with the company's requirements.\nAuthoritative links:\nAmazon SES: https://aws.amazon.com/ses/\nAmazon SNS: https://aws.amazon.com/sns/",
    "links": [
      "https://aws.amazon.com/ses/",
      "https://aws.amazon.com/sns/"
    ]
  },
  {
    "question": "CertyIQ\nA company has a business system that generates hundreds of reports each day. The business system saves the\nreports to a network share in CSV format. The company needs to store this data in the AWS Cloud in near-real time\nfor analysis.\nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": {
      "A": "Use AWS DataSync to transfer the files to Amazon S3. Create a scheduled task that runs at the end of each",
      "B": "Create an Amazon S3 File Gateway. Update the business system to use a new network share from the S3 File",
      "C": "Use AWS DataSync to transfer the files to Amazon S3. Create an application that uses the DataSync API in",
      "D": "Deploy an AWS Transfer for SFTP endpoint. Create a script that checks for new files on the network share"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the best solution, along with supporting explanations and\nlinks:\nThe core requirement is near-real-time data storage in S3 with minimal administrative overhead. Option B,\nleveraging Amazon S3 File Gateway, achieves this most effectively. S3 File Gateway provides a local cache\nfor frequently accessed data, ensuring fast access for the business system. It integrates seamlessly with the\nexisting network share setup, minimizing disruption. The business system only needs to be reconfigured to\nwrite to the new S3 File Gateway network share, a straightforward change.\nThe gateway automatically and securely transfers data to S3, eliminating the need for complex scripting or\nscheduling. Data is transferred asynchronously, minimizing impact on the business system's performance. The\nautomatic transfer mechanism reduces administrative overhead, as the file transfer process is handled by the\ngateway. This solution avoids building custom data transfer pipelines or managing periodic data transfer jobs,\nthereby streamlining the entire process.\nOption A requires scheduling tasks with AWS DataSync which increases the complexity and overhead. While\nDataSync is a good option for migrating large amounts of data, in this case we need to consider a continuous\ndata transfer with near-real time constraints. It is not optimal to schedule the task to only run at the end of\neach day.\nOption C also involves DataSync but introduces the added complexity of managing an application interacting\nwith the DataSync API, further increasing administrative burden. This approach demands more development\nand maintenance effort.\nOption D, using AWS Transfer for SFTP, requires managing an SFTP server and writing a script to monitor the\nnetwork share, increasing complexity and overhead. SFTP is primarily intended for secure file transfer by\nhumans, not automated system processes. This is not suitable for an automated system that generates\nhundreds of reports daily.\nTherefore, S3 File Gateway directly addresses the problem by integrating with the existing network share,\nproviding automatic, near-real-time data transfer to S3 with minimal administrative effort.\nSupporting Links:\nAmazon S3 File Gateway: https://aws.amazon.com/storagegateway/file/\nAWS DataSync: https://aws.amazon.com/datasync/\nAWS Transfer Family: https://aws.amazon.com/aws-transfer-family/",
    "links": [
      "https://aws.amazon.com/storagegateway/file/",
      "https://aws.amazon.com/datasync/",
      "https://aws.amazon.com/aws-transfer-family/"
    ]
  },
  {
    "question": "CertyIQ\nA company is storing petabytes of data in Amazon S3 Standard. The data is stored in multiple S3 buckets and is\naccessed with varying frequency. The company does not know access patterns for all the data. The company\nneeds to implement a solution for each S3 bucket to optimize the cost of S3 usage.\nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": {
      "A": "Create an S3 Lifecycle configuration with a rule to transition the objects in the S3",
      "B": "Use the S3 storage class analysis tool to determine the correct tier for each object in the S3 bucket. Move",
      "C": "Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Glacier",
      "D": "Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 One Zone-"
    },
    "answer": "A",
    "explanation": "The best solution is A. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3\nbucket to S3 Intelligent-Tiering. Here's why:\nS3 Intelligent-Tiering: This storage class is designed to automatically optimize storage costs by moving data\nbetween frequent, infrequent, and archive access tiers based on changing access patterns. It incurs a small\nmonthly monitoring and automation fee per object, but it eliminates the operational overhead of manually\nidentifying and moving data.\nUnknown Access Patterns: The problem explicitly states the company doesn't know the access patterns for\nall data. Intelligent-Tiering excels in this scenario because it automatically adapts to unknown and changing\naccess patterns, optimizing costs without requiring manual analysis.\nOperational Efficiency: S3 Lifecycle configurations automate the process of transitioning objects between\nstorage classes. Using Intelligent-Tiering with a Lifecycle configuration provides the most hands-off and\nautomated approach, minimizing operational burden.\nS3 Storage Class Analysis (Option B): While useful, this requires manual analysis and subsequent action to\nmove objects. This is less operationally efficient than Intelligent-Tiering, which is designed to automate this\nprocess. Moreover, the analysis is a one-time snapshot and wouldn't adapt to changing access patterns over\ntime without repeated manual analysis.\nS3 Glacier Instant Retrieval (Option C): Glacier is designed for archival data with infrequent access. It's not\nthe most suitable choice when access frequency is unknown, as frequent retrieval incurs higher costs.\nS3 One Zone-Infrequent Access (S3 One Zone-IA) (Option D): While cost-effective for infrequent access, it's\nnot ideal when access patterns are unknown. Intelligent-Tiering can also move data to infrequent access tiers,\nand offers the flexibility to return data to frequent access if needed, which One Zone-IA does not.\nFurthermore, One Zone-IA has lower availability than Intelligent-Tiering.\nIn essence, Intelligent-Tiering is the most efficient and appropriate choice because it automates cost\noptimization based on access patterns without requiring upfront knowledge of those\npatterns.https://aws.amazon.com/s3/storage-classes/intelligent-\ntiering/https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-examples.html",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/intelligent-",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-examples.html"
    ]
  },
  {
    "question": "CertyIQ\nA rapidly growing global ecommerce company is hosting its web application on AWS. The web application includes\nstatic content and dynamic content. The website stores online transaction processing (OLTP) data in an Amazon\nRDS database The websites users are experiencing slow page loads.\nWhich combination of actions should a solutions architect take to resolve this issue? (Choose two.)",
    "options": {
      "B": "Set up an Amazon CloudFront distribution: CloudFront is a content delivery network (CDN) service that",
      "D": "Create a read replica for the RDS DB instance: Creating a read replica allows read-only traffic to be"
    },
    "answer": "B",
    "explanation": "The best combination of actions to resolve slow page loads for the ecommerce web application is to use\nAmazon CloudFront and create a read replica for the RDS DB instance.\nB. Set up an Amazon CloudFront distribution: CloudFront is a content delivery network (CDN) service that\ncaches static content (images, CSS, JavaScript, etc.) closer to users globally. By caching content at edge\nlocations, CloudFront reduces latency and improves page load times. This is especially beneficial for a global\necommerce company with users distributed worldwide. (https://aws.amazon.com/cloudfront/)\nD. Create a read replica for the RDS DB instance: Creating a read replica allows read-only traffic to be\noffloaded from the primary RDS database. This reduces the load on the primary database, allowing it to focus\non write operations (transactions). The web application can be configured to read data from the read replica,\nimproving response times for data-intensive read operations and contributing to faster page loads. OLTP\ndatabases benefit from read replicas as read queries often overwhelm the primary DB instance.\n(https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html)\nOption A (Amazon Redshift) is not the best choice because Redshift is designed for data warehousing and\nanalytics, not for serving real-time OLTP data for a web application. Option C (Host the dynamic web content\nin Amazon S3) is incorrect because S3 is designed for static content. Dynamic content needs a compute\nenvironment to execute. Option E (Configure a Multi-AZ deployment for the RDS DB instance) improves the\navailability and durability of the database, but it doesn't directly address the performance issue of slow page\nloads. Multi-AZ is for failover, not performance scaling.",
    "links": [
      "https://aws.amazon.com/cloudfront/)",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html)"
    ]
  },
  {
    "question": "CertyIQ\nA company uses Amazon EC2 instances and AWS Lambda functions to run its application. The company has VPCs\nwith public subnets and private subnets in its AWS account. The EC2 instances run in a private subnet in one of the\nVPCs. The Lambda functions need direct network access to the EC2 instances for the application to work.\nThe application will run for at least 1 year. The company expects the number of Lambda functions that the\napplication uses to increase during that time. The company wants to maximize its savings on all application\nresources and to keep network latency between the services low.\nWhich solution will meet these requirements?",
    "options": {
      "C": "Placing Lambda in a public subnet (option B) is less secure and not necessary given the option"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Here's why:\nThe requirements include cost optimization, low latency, application running for at least 1 year with increasing\nLambda function count, and direct network access from Lambda to EC2 in a private subnet.\nSavings Plan Selection: Compute Savings Plans offer flexibility by applying discounts across EC2, Lambda,\nand Fargate, unlike EC2 Instance Savings Plans which only apply to EC2. Given the anticipated growth in\nLambda usage, a Compute Savings Plan ensures optimal savings across all compute resources.\nhttps://aws.amazon.com/savingsplans/\nLambda Optimization: Optimizing Lambda functions' duration, memory, invocations, and data transfer directly\nimpacts cost. Reducing execution time and memory allocation minimizes resource consumption, and\noptimizing data transfer reduces network charges.\nVPC Connection: Connecting Lambda functions to the private subnet provides direct, low-latency network\naccess to the EC2 instances without traversing the public internet. It also ensures secure communication\nwithin the VPC. Placing Lambda in a public subnet (option B) is less secure and not necessary given the option\nfor private subnet connectivity. Keeping Lambda in the Lambda service VPC (option D) would require more\ncomplex networking (like VPC peering or PrivateLink) to reach the EC2 instances, increasing latency and cost.\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html\nTherefore, Option C provides the most comprehensive solution for cost savings, low latency, scalability, and\nsecure network access between Lambda functions and EC2 instances within the private subnet.",
    "links": [
      "https://aws.amazon.com/savingsplans/",
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect needs to allow team members to access Amazon S3 buckets in two different AWS accounts:\na development account and a production account. The team currently has access to S3 buckets in the\ndevelopment account by using unique IAM users that are assigned to an IAM group that has appropriate\npermissions in the account.\nThe solutions architect has created an IAM role in the production account. The role has a policy that grants access\nto an S3 bucket in the production account.\nWhich solution will meet these requirements while complying with the principle of least privilege?",
    "options": {
      "A": "Attach the Administrator Access policy to the development account users.",
      "B": "Add the development account as a principal in the trust policy of the role in the",
      "C": "Turn off the S3 Block Public Access feature on the S3 bucket in the production account.",
      "D": "Create a user in the production account with unique credentials for each team member."
    },
    "answer": "B",
    "explanation": "The correct answer is B. Add the development account as a principal in the trust policy of the role in the\nproduction account.\nHere's a detailed justification:\nThe scenario requires granting users in the development account access to S3 resources in the production\naccount while adhering to the principle of least privilege. This means giving only the necessary permissions\nand avoiding overly broad access.\nOption A, attaching the AdministratorAccess policy to the development account users, violates the principle of\nleast privilege. It grants excessive permissions to the development account users, far beyond what's needed\nto access the specific S3 bucket in the production account. This opens up security risks.\nOption B is the correct solution because it utilizes IAM roles for cross-account access, a secure and\nrecommended practice. The IAM role is created in the production account and has a policy defining what\nactions (e.g., s3:GetObject, s3:PutObject) are allowed on the S3 bucket. Crucially, the role's trust policy is\nmodified to allow the development account to assume the role. This is done by specifying the development\naccount's AWS account ID as a principal in the trust policy.\nUsers in the development account can then assume this role, temporarily gaining the permissions associated\nwith it in the production account. To assume the role, the IAM users in the development account need explicit\npermissions to do so (using sts:AssumeRole in their IAM policy). This approach is secure and follows least\nprivilege, as users only gain production access when they explicitly assume the role, and the role itself only\ngrants specific S3 access.\nOption C, turning off S3 Block Public Access, is incorrect and dangerous. It makes the S3 bucket publicly\naccessible, which is a severe security risk and has nothing to do with granting cross-account access to\nspecific users.\nOption D, creating a user in the production account for each team member, creates unnecessary\nadministrative overhead and doesn't scale well. Managing separate user credentials for each team member in\nmultiple accounts is complex and prone to errors. Also, it doesn't leverage the benefits of IAM roles for\ntemporary access.\nIn summary, using IAM roles with a trust policy that specifies the development account as a principal allows\nauthorized users in the development account to assume the role and access the S3 bucket in the production\naccount in a controlled and secure manner, adhering to the principle of least privilege.\nFurther reading:\nAWS IAM Roles: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\nIAM Roles for Cross-Account Access: https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-\naccount-with-roles.html\nSTS AssumeRole: https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html",
    "links": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-",
      "https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html"
    ]
  },
  {
    "question": "CertyIQ\nA company uses AWS Organizations with all features enabled and runs multiple Amazon EC2 workloads in the ap-\nsoutheast-2 Region. The company has a service control policy (SCP) that prevents any resources from being\ncreated in any other Region. A security policy requires the company to encrypt all data at rest.\nAn audit discovers that employees have created Amazon Elastic Block Store (Amazon EBS) volumes for EC2\ninstances without encrypting the volumes. The company wants any new EC2 instances that any IAM user or root\nuser launches in ap-southeast-2 to use encrypted EBS volumes. The company wants a solution that will have\nminimal effect on employees who create EBS volumes.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is CE. Here's a detailed justification:\nC: Create an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the\nec2:CreateVolume action when the ec2:Encrypted condition equals false.\nThis is the most effective way to enforce encryption at the organizational level using AWS Organizations.\nSCPs act as guardrails, limiting the actions that can be performed by accounts within an OU. By creating an\nSCP that denies the ec2:CreateVolume action when the ec2:Encrypted condition is false, you ensure that no\nunencrypted EBS volumes can be created within the scope of the OU (in this case, the entire organization\nbecause it is attached to the root OU). This enforces the security policy for all new EBS volumes created.\nE: In the Organizations management account, specify the Default EBS volume encryption setting.\nSpecifying default EBS encryption at the organizational level simplifies encryption configuration. If an IAM\nuser/role launches an EC2 instance and doesn't explicitly specify EBS encryption, then the \"Default EBS\nvolume encryption\" setting takes precedence. This minimizes the impact on users since they can skip\nexplicitly specifying encryption during volume creation, yet the organization remains compliant with the\npolicy. This setting also applies to root users. This adds a layer of encryption regardless of what is done in the\nEC2 console.\nWhy the other options are incorrect:\nA: In the Amazon EC2 console, select the EBS encryption account attribute and define a default encryption\nkey. While you can set a default encryption key in the EC2 console, this only applies at the account level, not\nthe organizational level. Also, this is just a default configuration, and does not enforce that EBS volumes must\nbe encrypted, which the question requires.\nB: Create an IAM permission boundary. Attach the permission boundary to the root organizational unit (OU).\nDefine the boundary to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.\nIAM permission boundaries set the maximum permissions that an IAM principal can have. They do not deny\nexplicit actions like SCPs do. They also aren't attached to OUs.\nD: Update the IAM policies for each account to deny the ec2:CreateVolume action when the ec2:Encrypted\ncondition equals false. Updating IAM policies across multiple accounts is complex, time-consuming, and\nerror-prone. It's much less manageable than using SCPs at the organizational level. It is also not as effective\nas using the default volume encryption.\nSupporting Documentation:\nService Control Policies (SCPs):\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html\nDefault EBS Encryption by Default:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\nIAM Permission Boundaries:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\nIn conclusion, using an SCP and default volume encryption ensures that all newly created EBS volumes are\nencrypted without placing a significant burden on employees, satisfying the requirements stated in the\nquestion.",
    "links": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to use an Amazon RDS for PostgreSQL DB cluster to simplify time-consuming database\nadministrative tasks for production database workloads. The company wants to ensure that its database is highly\navailable and will provide automatic failover support in most scenarios in less than 40 seconds. The company\nwants to offload reads off of the primary instance and keep costs as low as possible.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D: Use an Amazon RDS Multi-AZ DB cluster deployment. Point the read workload to\nthe reader endpoint.\nHere's a detailed justification:\nHigh Availability and Fast Failover: Multi-AZ DB clusters in RDS provide significantly faster failover times\ncompared to Multi-AZ DB instances. While both offer high availability, clusters are designed to meet the 40-\nsecond failover requirement, while Multi-AZ instances are not explicitly guaranteed to that level of speed.\nMulti-AZ DB instances failover to a standby in same availability zone for better availability.\nRead Scaling: RDS clusters, specifically Aurora clusters but also RDS for PostgreSQL clusters in this context,\nhave a reader endpoint. This endpoint automatically distributes read traffic across multiple read replicas\nwithin the cluster. This offloads read workload from the primary instance as requested by the customer. Multi-\nAZ DB instances do not have reader endpoints and is not best practice to direct read queries to the standby\ninstance.\nCost Optimization: Using the reader endpoint to direct read traffic eliminates the need to manually manage\nconnections to individual read replicas. This simplifies architecture and reduces the operational overhead\nassociated with managing multiple read replicas. Moreover, using the cluster's reader endpoint effectively\nbalances the load across available read replicas, optimizing resource utilization and cost.\nMulti-AZ DB Instance limitations: Options A and C suggest using Multi-AZ DB instances. While they provide\nhigh availability through a standby instance, they don't inherently provide read scalability or fast failover\nguarantees like a cluster. The standby in a Multi-AZ instance is not intended for direct read traffic.\nTwo Read Replicas: Option B suggests two read replicas. While it enhances read capacity, it is not the most\ncost efficient option. The prompt does not require read replicas and is not needed to meet high availability or\nfailover requirements.\nReader Endpoint vs. Instance Endpoint: The reader endpoint is the key feature of a DB cluster that directly\naddresses the requirement to offload reads and automatically balance the load. Options that do not utilize a\nreader endpoint fail to address the question.\nIn summary, an RDS Multi-AZ DB cluster with a reader endpoint delivers the required high availability with\nfast failover, offloads reads efficiently, and simplifies management, making it the optimal solution.\nSupporting Documentation:\nAmazon RDS High Availability: Overview of HA capabilities for RDS.\nWorking with Amazon RDS Read Replicas: Information on using read replicas to offload read traffic.\nAmazon RDS for PostgreSQL: General information about RDS for PostgreSQL.",
    "links": []
  },
  {
    "question": "CertyIQ\nA company runs a highly available SFTP service. The SFTP service uses two Amazon EC2 Linux instances that run\nwith elastic IP addresses to accept traffic from trusted IP sources on the internet. The SFTP service is backed by\nshared storage that is attached to the instances. User accounts are created and managed as Linux users in the\nSFTP servers.\nThe company wants a serverless option that provides high IOPS performance and highly configurable security. The\ncompany also wants to maintain control over user permissions.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the most appropriate solution, along with explanations of\nwhy the other options are less suitable:\nWhy Option B is Correct:\nOption B leverages AWS Transfer Family with Amazon EFS to meet the stated requirements. AWS Transfer\nFamily is the AWS service designed to provide serverless, secure, and scalable file transfer capabilities. Using\nEFS satisfies the need for high IOPS performance shared storage. Specifically:\nAmazon EFS: Provides shared storage with high IOPS. EFS is a scalable, fully managed, NFS file system that\ncan be shared between multiple EC2 instances or accessed by AWS Transfer Family. This addresses the\ncompany's need for high IOPS and shared storage.\nAWS Transfer Family SFTP Service: Offers a serverless SFTP solution, fulfilling the company's desire to\nmove away from managing EC2 instances.\nElastic IP Addresses & VPC Endpoint with Internet Facing Access: Using an internet-facing VPC endpoint\nenables the SFTP service to be accessed from trusted IP addresses over the internet, as the company\ncurrently does with their EC2 instances. The elastic IP addresses are applied at the endpoint level, allowing\nfor predictable addressing for the trusted sources to connect.\nSecurity Group: Attaching a security group to the endpoint that allows traffic only from trusted IP addresses\nprovides granular security control, mirroring the current security setup with the EC2 instances.\nUser Access Control: AWS Transfer Family allows for integration with identity providers to manage user\nauthentication and permissions. This ensures that the company retains control over user permissions.\nWhy Other Options are Incorrect:\nOption A (EBS): EBS volumes are block storage, not designed for direct sharing between services like AWS\nTransfer Family. It is usually attached to compute instances. EBS is not a suitable storage option for this\nservice as it cannot be directly attached as a file system for AWS Transfer Family. Additionally, attaching an\nEBS volume directly to the SFTP endpoint is not a supported configuration.\nOption C and D (S3 with Public/Private Endpoint): While S3 can be used with AWS Transfer Family, it doesn't\ninherently provide the high IOPS performance that EFS does. Transferring data to/from S3 through SFTP can\nlead to higher costs and increased latency due to limitations in the underlying SFTP protocol. While the\nrequirement can technically be met with Amazon S3, Amazon EFS aligns more directly with the high IOPS\nrequirement described in the prompt. Additionally, if choosing S3, a private endpoint (Option D) is better for\nsecurity, but Option B provides a more balanced solution.\nAuthoritative Links:\nAWS Transfer Family: https://aws.amazon.com/aws-transfer-family/\nAmazon EFS: https://aws.amazon.com/efs/\nIn summary, Option B provides a serverless, secure, and high-performance SFTP solution that leverages AWS\nTransfer Family with EFS, allowing the company to meet all requirements outlined in the scenario.",
    "links": [
      "https://aws.amazon.com/aws-transfer-family/",
      "https://aws.amazon.com/efs/"
    ]
  },
  {
    "question": "CertyIQ\nA company is developing a new machine learning (ML) model solution on AWS. The models are developed as\nindependent microservices that fetch approximately 1 GB of model data from Amazon S3 at startup and load the\ndata into memory. Users access the models through an asynchronous API. Users can send a request or a batch of\nrequests and specify where the results should be sent.\nThe company provides models to hundreds of users. The usage patterns for the models are irregular. Some models\ncould be unused for days or weeks. Other models could receive batches of thousands of requests at a time.\nWhich design should a solutions architect recommend to meet these requirements?",
    "options": {
      "B": "B.Direct the requests from the API to an Application Load Balancer (ALB). Deploy the models as Amazon Elastic"
    },
    "answer": "D",
    "explanation": "Option D provides the most scalable and cost-effective solution for the given requirements, especially\nconsidering the irregular usage patterns and the need to load 1 GB of data into memory.\nHere's why:\nAsynchronous Processing with SQS: Using Amazon SQS decouples the API from the model processing. This\nallows the API to quickly accept requests and queue them for processing, preventing the API from being\noverwhelmed by sudden spikes in traffic. https://aws.amazon.com/sqs/\nContainerization with ECS: Deploying the models as Amazon ECS services allows for efficient resource\nutilization and scalability. Containers provide a consistent environment for the models to run, regardless of\nthe underlying infrastructure. https://aws.amazon.com/ecs/\nAuto Scaling based on Queue Size: Auto Scaling ECS based on the SQS queue size ensures that the number\nof model instances scales up or down dynamically based on the incoming request volume. This optimizes\ncosts by only using the necessary resources. When the queue is empty (models unused), ECS can scale down\nclose to zero (depending on chosen configuration).\nEfficient Memory Management: While the models need to load 1 GB of data, ECS allows you to specify the\nnecessary memory and CPU resources for each container. This is crucial for efficiently handling large model\ndata sets.\nWhy other options are less suitable:\nOption A (Lambda invoked by NLB): Lambda functions have execution time limits and memory constraints.\nWhile Lambda can be a good fit for some ML workloads, loading 1 GB of model data into memory at each\ninvocation would likely exceed Lambda's limitations and lead to timeouts and poor performance. Also, cold\nstarts could introduce latency and Lambda might not be as cost efficient for long lived processes.\nOption B (ECS with App Mesh): App Mesh is a service mesh that provides fine-grained control over traffic\nrouting and observability. While useful for complex microservice architectures, it adds unnecessary\ncomplexity and cost for this use case.\nOption C (Lambda invoked by SQS with Auto Scaling): While Lambda can be invoked by SQS, increasing\nvCPUs for Lambda functions is not a supported feature. Lambda functions have fixed vCPU allocation based\non the memory configured. Additionally, the memory limitations of Lambda would still be a concern with the\nlarge model size.",
    "links": [
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/ecs/"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect wants to use the following JSON text as an identity-based policy to grant specific\npermissions:\nWhich IAM principals can the solutions architect attach this policy to? (Choose two.)",
    "options": {},
    "answer": "A",
    "explanation": "identity-based policy used for role and group",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is running a custom application on Amazon EC2 On-Demand Instances. The application has frontend\nnodes that need to run 24 hours a day, 7 days a week and backend nodes that need to run only for a short time\nbased on workload. The number of backend nodes varies during the day.\nThe company needs to scale out and scale in more instances based on workload.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "B": "Use Reserved Instances for the frontend nodes. Use Spot Instances for the",
      "C": "It uses Spot Instances for the front-end nodes,"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Use Reserved Instances for the frontend nodes. Use Spot Instances for the\nbackend nodes.\nHere's a detailed justification:\nThe problem states that the frontend nodes must run 24/7. Reserved Instances (RIs) are the most cost-\neffective option for consistently running EC2 instances over a long period (1 or 3 years). By purchasing RIs for\nthe frontend nodes, the company can significantly reduce the hourly cost compared to On-Demand Instances.\nRIs offer a substantial discount in exchange for a commitment to a specific instance type and region.\nThe backend nodes only need to run based on workload and the number of instances varies. Spot Instances\noffer EC2 capacity at significantly reduced prices (up to 90% off On-Demand prices). Spot Instances are ideal\nfor fault-tolerant, stateless workloads that can be interrupted. Since the backend nodes scale based on\nworkload and the number of backend nodes varies during the day, Spot Instances will automatically fit into\nthe cost-effectiveness requirement.\nOption A is incorrect because AWS Fargate is a serverless compute engine for containers. While Fargate can\nbe suitable for certain backend workloads, it's generally more expensive than EC2 Spot Instances for tasks\nthat can tolerate interruptions. The question emphasizes cost-effectiveness and implies the application is\nalready designed to run on EC2. Fargate is a better choice when the application is containerized.\nOption C is incorrect because Spot Instances are unsuitable for the frontend nodes that must run 24/7. Spot\nInstances can be terminated if the Spot price exceeds your bid or if capacity becomes unavailable. The\nfrontend requires constant uptime.\nOption D is incorrect for the same reasons as options A and C. It uses Spot Instances for the front-end nodes,\nwhich need constant uptime. It also suggests Fargate for the backend nodes, which is not the most cost-\neffective solution compared to Spot Instances for this specific EC2 scaling scenario.\nTherefore, using Reserved Instances for the always-on frontend and Spot Instances for the flexible, scalable\nbackend provides the most cost-effective solution.\nAuthoritative Links:\nAmazon EC2 Reserved Instances: https://aws.amazon.com/ec2/pricing/reserved-instances/\nAmazon EC2 Spot Instances: https://aws.amazon.com/ec2/spot/\nAWS Fargate: https://aws.amazon.com/fargate/",
    "links": [
      "https://aws.amazon.com/ec2/pricing/reserved-instances/",
      "https://aws.amazon.com/ec2/spot/",
      "https://aws.amazon.com/fargate/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses high block storage capacity to runs its workloads on premises. The company's daily peak input\nand output transactions per second are not more than 15,000 IOPS. The company wants to migrate the workloads\nto Amazon EC2 and to provision disk performance independent of storage capacity.\nWhich Amazon Elastic Block Store (Amazon EBS) volume type will meet these requirements MOST cost-\neffectively?",
    "options": {},
    "answer": "C",
    "explanation": "Here's a detailed justification for why the correct answer is C (GP3 volume type):\nThe key requirements are high block storage capacity, a maximum of 15,000 IOPS, and the need for\nperformance independent of storage capacity, all while being cost-effective. Let's analyze each option:\nGP2: GP2 (General Purpose SSD) volumes offer a balance of price and performance. However, GP2's IOPS\nperformance is tied to the volume size. To achieve 15,000 IOPS with GP2, you would need to provision a very\nlarge volume, which would be more expensive than the other options and is not ideal when performance needs\nto be provisioned independent of capacity. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-\nvolume-types.html\nio1/io2: io1 and io2 (Provisioned IOPS SSD) volumes are designed for high-performance, I/O intensive\nworkloads. They allow you to specify the exact IOPS you need. While io1/io2 could meet the performance\nrequirement, they are typically more expensive than GP3, especially for a relatively low IOPS requirement like\n15,000. These are optimal when you need very high, guaranteed IOPS.\nGP3: GP3 (General Purpose SSD) volumes offer a balance between cost and performance. Crucially, GP3\nallows you to provision IOPS and throughput independent of storage capacity. GP3 volumes provide a\nbaseline performance and allow users to provision additional IOPS and throughput as needed, up to a certain\nlimit. For 15,000 IOPS, GP3 is a cost-effective option that avoids over-provisioning storage just to meet IOPS\nrequirements. GP3 also gives you flexibility on throughput. https://aws.amazon.com/blogs/aws/new-amazon-\nebs-gp3-volume-provides-up-to-20-lower-cost-and-higher-performance/\nTherefore, GP3 is the most cost-effective solution for a company needing high block storage capacity and\n15,000 IOPS, while also demanding performance independent of capacity. It provides a better price-\nperformance ratio compared to io1/io2 for this IOPS range and avoids the capacity-tied limitations of GP2.",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-",
      "https://aws.amazon.com/blogs/aws/new-amazon-"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to store data from its healthcare application. The applications data frequently changes. A new\nregulation requires audit access at all levels of the stored data.\nThe company hosts the application on an on-premises infrastructure that is running out of storage capacity. A\nsolutions architect must securely migrate the existing data to AWS while satisfying the new regulation.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A because it provides a mechanism for both data migration and audit logging of data\naccess events, fulfilling all the requirements.\nHere's a detailed justification:\nThe company needs to migrate data to AWS, ensure data is securely stored, and enable audit access. Amazon\nS3 serves as the central storage location because it's a scalable, durable, and secure object storage service\nsuitable for healthcare application data.\nAWS DataSync efficiently and securely transfers large datasets from on-premises storage to Amazon S3,\nhandling data transfer and encryption during transit, making it ideal for migrating the existing data.\nThe new regulation requires audit access at all levels of stored data. AWS CloudTrail logs API calls made to\nAWS services, providing a detailed audit trail of who accessed what data and when. By configuring CloudTrail\nto log data events for the S3 bucket where the healthcare application data is stored, the company gains\ncomprehensive visibility into all data access activities. Data events specifically track object-level operations\nsuch as GET, PUT, DELETE, and HEAD, providing the necessary level of audit access.\nOption B is incorrect because AWS Snowcone is primarily used for data collection, processing, and migration\nin edge locations or environments where network connectivity is limited, which is not the primary concern in\nthis scenario, where the existing data is hosted on an on-premises infrastructure. Further, logging\nmanagement events will not satisfy the detailed audit requirement for data access at all levels.\nOption C is incorrect because Amazon S3 Transfer Acceleration only speeds up transfers to S3 via the public\ninternet. While it improves transfer speed, it doesn't address the core requirement of audit logging. Similar to\nOption B, logging data events is what is needed, not just accelerating the transfer.\nOption D is incorrect. AWS Storage Gateway connects on-premises software appliances to cloud-based\nstorage, it is primarily designed to create seamless hybrid storage solutions rather than a one-time migration\nof the existing data. More importantly, auditing management events is insufficient for the required data\naccess audit trail. The focus on data events is critical to meeting the audit requirements.\nIn summary, using AWS DataSync for migration and AWS CloudTrail configured for data events on the S3\nbucket addresses both the data migration and detailed data access auditing requirements effectively.\nAuthoritative Links:\nAWS DataSync: https://aws.amazon.com/datasync/\nAmazon S3: https://aws.amazon.com/s3/\nAWS CloudTrail: https://aws.amazon.com/cloudtrail/",
    "links": [
      "https://aws.amazon.com/datasync/",
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/cloudtrail/"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is implementing a complex Java application with a MySQL database. The Java application\nmust be deployed on Apache Tomcat and must be highly available.\nWhat should the solutions architect do to meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B: Deploy the application by using AWS Elastic Beanstalk. Configure a load-balanced\nenvironment and a rolling deployment policy.\nHere's a detailed justification:\nElastic Beanstalk is a PaaS (Platform as a Service) that simplifies the deployment and management of web\napplications and services. It handles the underlying infrastructure, allowing the architect to focus on the\napplication code. It inherently supports Java applications with Tomcat, making it a suitable choice for\ndeploying the application.\nTo meet the high availability requirement, Elastic Beanstalk provides the capability to create a load-balanced\nenvironment. This automatically distributes traffic across multiple EC2 instances running the application,\nensuring that if one instance fails, others can handle the load. The load balancer (usually an Application Load\nBalancer or Classic Load Balancer) monitors the health of the instances and routes traffic accordingly.\nA rolling deployment policy minimizes downtime during application updates. Instead of taking down all\ninstances at once, a rolling deployment updates the instances in batches, ensuring that some instances are\nalways available to serve traffic. This reduces the impact of deployments on users and increases overall\navailability. Rolling deployments with health checks are crucial for maintaining service availability during\nupdates.\nOption A is incorrect because Lambda is better suited for event-driven, serverless applications, not for\ncomplex, long-running Java applications like this one running on Tomcat. Furthermore, while API Gateway can\nexpose Lambda functions, it doesn't directly address the need for a Tomcat environment.\nOption C is incorrect because ElastiCache is a caching service, not a replacement for a MySQL database.\nMigrating the database to ElastiCache would fundamentally change the application's architecture and likely\nbreak its functionality as ElastiCache does not offer the relational features needed.\nOption D is incorrect because manually managing EC2 instances, AMIs, and Auto Scaling groups adds\nconsiderable operational overhead. While it can achieve the desired result, Elastic Beanstalk provides a higher\nlevel of abstraction and automation, simplifying the process and reducing management complexity. Manually\nmanaging AMIs and scaling policies is less efficient than using Elastic Beanstalk's managed environment. This\napproach increases operational overhead.\nTherefore, Elastic Beanstalk with a load-balanced environment and a rolling deployment policy is the most\nefficient and straightforward way to deploy the Java application on Tomcat while meeting the high availability\nrequirements.\nSupporting links:\nAWS Elastic Beanstalk: https://aws.amazon.com/elasticbeanstalk/\nElastic Beanstalk Rolling Deployments: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-\nfeatures.rolling-updates.html",
    "links": [
      "https://aws.amazon.com/elasticbeanstalk/",
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-"
    ]
  },
  {
    "question": "CertyIQ\nA serverless application uses Amazon API Gateway, AWS Lambda, and Amazon DynamoD",
    "options": {
      "B": "Therefore, using an IAM role specifically designed for the Lambda function, as outlined in option B, aligns with"
    },
    "answer": "B",
    "explanation": "The correct answer is B because it leverages IAM roles, the recommended and most secure way to grant\npermissions to AWS services. IAM roles avoid the need to store and manage long-term credentials (like\naccess keys) within the Lambda function or its environment, mitigating the risk of credential exposure.\nOption B creates an IAM role, which is an identity that Lambda can assume. This role explicitly trusts Lambda\n(as specified in the trust policy of the role) to use it. The role is then granted permissions to read and write to\nDynamoDB via an attached policy. Finally, the Lambda function is configured to use this role as its execution\nrole. This means when Lambda executes the function, it automatically assumes the permissions defined in the\nrole's policy. This process is seamless and requires no manual credential management.\nOption A is less secure because it involves creating an IAM user and storing its access keys in the Lambda\nenvironment variables. This is a poor practice as it increases the risk of credential compromise. If the\nenvironment variables are exposed (e.g., through a configuration leak), attackers can use the credentials to\naccess DynamoDB.\nOption C, while using Systems Manager Parameter Store to store the access keys, still relies on managing\nlong-term credentials. Although storing secrets in Parameter Store is better than storing them directly in the\nenvironment variables, the need to manage and retrieve these credentials introduces complexity and\npotential security risks.\nOption D is incorrect because IAM roles are used to grant permissions to services (like Lambda) to access\nother AWS resources, not the other way around. DynamoDB wouldn't assume a role to allow Lambda to\naccess it. Lambda assumes the role to access DynamoDB.\nTherefore, using an IAM role specifically designed for the Lambda function, as outlined in option B, aligns with\nAWS security best practices by adhering to the principle of least privilege and eliminating the need to\nmanage long-term credentials directly.\nSupporting documentation:\nIAM Roles: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\nLambda Execution Role: https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html\nSecurity best practices in IAM: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html",
    "links": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html"
    ]
  },
  {
    "question": "CertyIQ\nThe following IAM policy is attached to an IAM group. This is the only policy applied to the group.\nWhat are the effective IAM permissions of this policy for group members?",
    "options": {},
    "answer": "D",
    "explanation": "Group members are allowed the ec2:StopInstances and ec2:TerminateInstances permissions for the us-east-1\nRegion only when logged in with multi-factor authentication (MFA). Group members are permitted any other\nAmazon EC2 action within the us-east-1 Region.",
    "links": []
  },
  {
    "question": "CertyIQ\nA manufacturing company has machine sensors that upload .csv files to an Amazon S3 bucket. These .csv files\nmust be converted into images and must be made available as soon as possible for the automatic generation of\ngraphical reports.\nThe images become irrelevant after 1 month, but the .csv files must be kept to train machine learning (ML) models\ntwice a year. The ML trainings and audits are planned weeks in advance.\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": {
      "B": "Design an AWS Lambda function that converts the .csv files into images and stores the images in the S3",
      "C": "Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3"
    },
    "answer": "B",
    "explanation": "The question requires converting .csv files uploaded to S3 into images for rapid report generation, maintaining\nthe .csv files for ML training, and managing storage costs. The selected solution, BC, addresses these\nrequirements effectively.\nB. Design an AWS Lambda function that converts the .csv files into images and stores the images in the S3\nbucket. Invoke the Lambda function when a .csv file is uploaded. This is the most efficient way to\nimmediately convert the .csv files into images. Using S3 event notifications and a Lambda function, the\nconversion happens automatically upon upload. Lambda is cost-effective for event-driven tasks as you only\npay for the compute time used during the conversion. An EC2 instance (option A) would be constantly running,\nincurring costs even when idle, making Lambda more economical for this task. This ensures the images are\navailable for report generation as quickly as possible.\nC. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3\nStandard to S3 Glacier 1 day after they are uploaded. Expire the image files after 30 days. This part handles\nthe lifecycle management of both the .csv and image files to optimize storage costs. Since the images are\nonly needed for a month, expiring them after 30 days prevents unnecessary storage charges. Transitioning\nthe .csv files to S3 Glacier reduces the storage costs for the .csv files, as they are only used twice a year. S3\nGlacier is a low-cost storage option suitable for infrequently accessed data. S3 Standard-IA (option E) would\nbe more expensive than Glacier for data accessed only twice a year. S3 One Zone-IA (option D) loses data if\nthe availability zone is destroyed, violating best practices. Storing images in RRS (option E) is deprecated and\nnot recommended.\nTherefore, Lambda is the ideal service for immediate file conversion and S3 lifecycle policies efficiently\nmanage storage costs, fulfilling both speed and cost-effectiveness requirements.\nRelevant Links:\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon S3 Lifecycle: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-management-\noverview.html\nAmazon S3 Storage Classes: https://aws.amazon.com/s3/storage-classes/",
    "links": [
      "https://aws.amazon.com/lambda/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-management-",
      "https://aws.amazon.com/s3/storage-classes/"
    ]
  },
  {
    "question": "CertyIQ\nA company has developed a new video game as a web application. The application is in a three-tier architecture in\na VPC with Amazon RDS for MySQL in the database layer. Several players will compete concurrently online. The\ngames developers want to display a top-10 scoreboard in near-real time and offer the ability to stop and restore\nthe game while preserving the current scores.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "B": "Set up an Amazon ElastiCache for Redis cluster to compute and cache the scores",
      "A": "Amazon ElastiCache for Memcached: While Memcached is also an in-memory cache, it primarily focuses",
      "C": "Amazon CloudFront: CloudFront is a CDN (Content Delivery Network) that caches static content at edge",
      "D": "Amazon RDS for MySQL Read Replica: While a read replica offloads read traffic from the primary"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Set up an Amazon ElastiCache for Redis cluster to compute and cache the scores\nfor the web application to display.\nHere's a detailed justification:\nThe requirements include near-real-time scoreboard updates and the ability to save/restore game state\nincluding scores. Redis is an in-memory data store that excels in these scenarios due to its speed and\nversatile data structures.\n1. Near Real-Time Updates: Redis provides extremely low latency read and write operations, crucial for\ndisplaying scores in near real-time. Unlike disk-based databases (like RDS MySQL), in-memory\noperations are significantly faster.\n2. Score Computation and Storage: Redis offers sorted sets, a data structure perfectly suited for\nmaintaining a ranked leaderboard. The score for each player can be added or updated, and Redis\nautomatically maintains the order. Retrieving the top 10 scores is an efficient operation.\n3. Save/Restore Game State: Redis supports persistence. Data can be periodically saved to disk (RDB\nsnapshots) or logged to an append-only file (AOF), enabling the restoration of the entire dataset\n(including scores) in case of failure or for the stop/restore feature.\n4. ElastiCache for Redis: AWS ElastiCache simplifies the management of a Redis cluster, handling\ntasks like scaling, patching, and monitoring. It provides a managed service, reducing the operational\noverhead for the game developers.\nWhy other options are not ideal:\nA. Amazon ElastiCache for Memcached: While Memcached is also an in-memory cache, it primarily focuses\non simple key-value caching and does not offer the sorted set data structure or built-in persistence\ncapabilities required for the scoreboard and game state preservation.\nC. Amazon CloudFront: CloudFront is a CDN (Content Delivery Network) that caches static content at edge\nlocations. While it's useful for caching static parts of the web application, it is not suitable for caching\ndynamically changing data like the scoreboard, as CloudFront cache invalidation may not be quick enough for\nnear-real-time updates. Furthermore, it cannot handle the score computation or game state persistence.\nD. Amazon RDS for MySQL Read Replica: While a read replica offloads read traffic from the primary\ndatabase, it is still a disk-based database. Computing the scoreboard on the read replica and serving read\ntraffic would introduce latency and not meet the near-real-time requirement. Also, reading top scores from a\nrelational database is much slower than using a Redis sorted set. Moreover, a read replica doesn't offer the in-\nmemory performance or data structures optimized for real-time ranking and quick restore.\nAuthoritative Links:\nAmazon ElastiCache for Redis: https://aws.amazon.com/elasticache/redis/\nRedis Sorted Sets: https://redis.io/docs/data-types/sorted-sets/\nRedis Persistence: https://redis.io/docs/data-persistence/",
    "links": [
      "https://aws.amazon.com/elasticache/redis/",
      "https://redis.io/docs/data-types/sorted-sets/",
      "https://redis.io/docs/data-persistence/"
    ]
  },
  {
    "question": "CertyIQ\nAn ecommerce company wants to use machine learning (ML) algorithms to build and train models. The company\nwill use the models to visualize complex scenarios and to detect trends in customer data. The architecture team\nwants to integrate its ML models with a reporting platform to analyze the augmented data and use the data\ndirectly in its business intelligence dashboards.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "C": "Amazon QuickSight: QuickSight is a serverless business intelligence service that allows you to easily create"
    },
    "answer": "B",
    "explanation": "The correct answer is B because it leverages managed AWS services specifically designed for machine\nlearning and business intelligence, minimizing operational overhead.\nHere's a detailed justification:\nAmazon SageMaker: SageMaker is a fully managed machine learning service that provides a comprehensive\nenvironment for building, training, and deploying ML models. It eliminates the need for managing underlying\ninfrastructure, handling dependencies, and configuring training clusters. This significantly reduces\noperational overhead compared to options A and C.\nAmazon QuickSight: QuickSight is a serverless business intelligence service that allows you to easily create\nand share interactive dashboards and reports. Its direct integration with AWS data sources, including\nSageMaker-generated data, simplifies the process of visualizing and analyzing ML-augmented data. Unlike\nOpenSearch Service (in options A and C), QuickSight is designed specifically for business intelligence and\nreporting, offering features like calculated fields and interactive visualizations, reducing the need for custom\ncoding and configurations. QuickSight also scales automatically, minimizing operational overhead.\nOption A is less suitable because AWS Glue ML Transforms are primarily for data cleaning and\ntransformation, not comprehensive model building and training. While OpenSearch Service can visualize data,\nit's primarily a search and analytics engine and requires more configuration and management for BI use cases\ncompared to QuickSight.\nOption C requires managing a custom AMI, which increases operational overhead related to security patching,\nsoftware updates, and compatibility issues. Furthermore, using OpenSearch Service for visualization, as\nmentioned before, is not as optimized as using QuickSight.\nOption D is incorrect because QuickSight's calculated fields are limited in their machine learning capabilities.\nThey cannot be used to build and train complex ML models as required by the problem statement.\nIn summary, Option B using SageMaker for ML model development and QuickSight for visualization provides a\nfully managed, integrated, and scalable solution with the least operational overhead for the given\nrequirements.\nSupporting Links:\nAmazon SageMaker: https://aws.amazon.com/sagemaker/\nAmazon QuickSight: https://aws.amazon.com/quicksight/\nAWS Glue: https://aws.amazon.com/glue/\nAmazon OpenSearch Service: https://aws.amazon.com/opensearch-service/",
    "links": [
      "https://aws.amazon.com/sagemaker/",
      "https://aws.amazon.com/quicksight/",
      "https://aws.amazon.com/glue/",
      "https://aws.amazon.com/opensearch-service/"
    ]
  },
  {
    "question": "CertyIQ\nA company is running its production and nonproduction environment workloads in multiple AWS accounts. The\naccounts are in an organization in AWS Organizations. The company needs to design a solution that will prevent\nthe modification of cost usage tags.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C: Create a service control policy (SCP) to prevent tag modification except by\nauthorized principals. Here's a detailed justification:\nSCPs are a powerful feature of AWS Organizations that allow you to centrally control permissions for all\naccounts within your organization. They act as guardrails, setting the maximum permissions available to\nmember accounts. Critically, SCPs affect all IAM users and roles within the affected accounts, even the root\nuser. This centralized enforcement is precisely what's needed to prevent unauthorized tag modification across\nmultiple accounts.\nOption C's \"except by authorized principals\" is also key. SCPs can include conditions to allow specific IAM\nroles or users to bypass the restriction. This enables authorized users (like those in the Cost Management\nteam) to modify tags as needed for legitimate purposes, while still preventing accidental or malicious\nmodification by others. This conditional control makes SCPs ideal for this scenario.\nOption A (AWS Config rule) can detect non-compliant tag modifications after they happen, but it can't prevent\nthem directly. Config is for auditing, not enforcement. Option B (CloudTrail) also logs activity, so it's valuable\nfor auditing but not prevention. Option D (CloudWatch logs) similarly focuses on monitoring and alerting\nbased on logs; it doesn't inherently block actions. Only SCPs offer the preventive control necessary.\nIn summary, SCPs provide a centralized and enforceable mechanism to restrict actions (like tag modification)\nacross multiple AWS accounts within an organization, while allowing exceptions for authorized users. This\nmakes it the best choice for the specified requirements.\nFurther reading:\nAWS Organizations documentation:\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html\nService control policies (SCPs):\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html",
    "links": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html",
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts its application in the AWS Cloud. The application runs on Amazon EC2 instances behind an\nElastic Load Balancer in an Auto Scaling group and with an Amazon DynamoDB table. The company wants to\nensure the application can be made available in anotherAWS Region with minimal downtime.\nWhat should a solutions architect do to meet these requirements with the LEAST amount of downtime?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A because it provides the most streamlined approach to minimizing downtime during a\nfailover to a disaster recovery region.\nHere's a breakdown of why:\nAuto Scaling Group and Load Balancer in DR Region: Creating these resources before a disaster event allows\nfor a faster switchover. The EC2 instances are ready (or can be pre-warmed).\nDynamoDB Global Table: DynamoDB global tables provide automatic, multi-region, multi-active replication.\nThis ensures that the data is available in the disaster recovery region and that data consistency is maintained.\nThis removes the need to handle data replication manually, which can be time-consuming and error-prone.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables.html\nDNS Failover: Using Route 53 DNS failover allows for quick redirection of traffic to the disaster recovery\nregion by simply changing the DNS record when needed. This is a standard and efficient method for handling\nregional failures. https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html\nWhy other options are less ideal:\nOption B: Creating resources using CloudFormation only when needed introduces delay. This is not an ideal\nsolution for minimizing downtime, as the application is unavailable while the resources are provisioned.\nFurthermore, it does not consider the need to have the database layer pre-synced.\nOption C: This option is missing the Auto Scaling Group. If EC2 instances are not created using ASG, it might\nbe difficult to scale them up, especially during/after a DR event.\nOption D: This option is overly complicated. It uses CloudWatch alarm and Lambda function for Route 53 DNS,\nwhich adds unnecessary complexity and increases the potential for failure. Route 53 supports native failover\nmechanisms and is the best choice.\nIn conclusion, option A provides the best combination of pre-provisioned resources, data replication via\nDynamoDB global tables, and efficient failover using Route 53 to minimize downtime during a regional\ndisaster.",
    "links": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables.html",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to migrate a MySQL database from its on-premises data center to AWS within 2 weeks. The\ndatabase is 20 TB in size. The company wants to complete the migration with minimal downtime.\nWhich solution will migrate the database MOST cost-effectively?",
    "options": {},
    "answer": "A",
    "explanation": "The most cost-effective solution for migrating a 20 TB MySQL database to AWS within 2 weeks with minimal\ndowntime is option A, using AWS Snowball Edge Storage Optimized and AWS DMS with AWS SCT.\nHere's why:\nData Size and Time Constraint: Migrating 20 TB over the internet, even with a dedicated 1 GB AWS Direct\nConnect connection (Option D), is likely to take longer than 2 weeks and cause more significant downtime.\nAWS Snowball Edge devices are designed for large-scale data transfers when network bandwidth is a\nconstraint. https://aws.amazon.com/snowball/\nCost-Effectiveness: AWS Snowball Edge Storage Optimized devices are more cost-effective for this amount\nof data compared to AWS Snowmobile (Option B). AWS Snowmobile is intended for exabyte-scale data\ntransfers.\nMinimal Downtime via Replication: AWS Database Migration Service (DMS) allows for continuous replication\nof changes to the database during the migration process. The AWS Schema Conversion Tool (SCT) helps\nconvert the source database schema to the target AWS database (if needed) reducing manual intervention\nand potential errors. This keeps downtime minimal. https://aws.amazon.com/dms/\nhttps://aws.amazon.com/sct/\nCompute Optimized Snowball: AWS Snowball Edge Compute Optimized (Option C) is not needed since the\nprimary requirement is data transfer and not intensive computation at the edge. Storage Optimized is\nsufficient and more cost-effective.\nProcess: The process involves using DMS/SCT to set up the initial database schema and begin replicating\ndata to the Snowball Edge while it's on-premises. Once the initial data is transferred to the Snowball, it's\nshipped to AWS. After AWS receives the Snowball and uploads the data to AWS, the continuous replication\nvia DMS ensures that only the changes that occurred during shipping need to be applied to the target\ndatabase, minimizing the final cutover downtime.\nDirect Connect Limitations: Though Direct Connect provides dedicated network connectivity, transferring 20\nTB of data within two weeks while maintaining minimal downtime can still be challenging and potentially\ncostly due to bandwidth usage charges.",
    "links": [
      "https://aws.amazon.com/snowball/",
      "https://aws.amazon.com/dms/",
      "https://aws.amazon.com/sct/"
    ]
  },
  {
    "question": "CertyIQ\nA company moved its on-premises PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. The\ncompany successfully launched a new product. The workload on the database has increased. The company wants\nto accommodate the larger workload without adding infrastructure.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "A",
    "explanation": "The question asks for the most cost-effective solution to handle an increased database workload in Amazon\nRDS for PostgreSQL without adding infrastructure. Option A, buying reserved DB instances for the total\nworkload and making the RDS instance larger, is the most cost-effective.\nHere's why:\nReserved Instances (RIs): RIs offer significant cost savings (up to 75%) compared to on-demand instances for\npredictable workloads. Buying RIs for the expected total workload guarantees these savings.\nVertical Scaling (Increasing Instance Size): RDS allows you to scale vertically (increase the instance size) to\nprovide more CPU, memory, and storage. This addresses the increased workload without requiring code\nchanges or creating new databases. It leverages existing infrastructure.\nCost-Effectiveness: While RIs require upfront commitment, the substantial discount on the hourly rate over\nthe long term makes them far cheaper than continuously running larger on-demand instances.\nAlternative B (Multi-AZ): Multi-AZ provides high availability and failover, which is important, but does not\ndirectly address the performance scaling issue required by increased workload. It's mainly for disaster\nrecovery, not performance scaling. While Multi-AZ provides a standby replica, it is typically for failover and\nnot for read-heavy operations.\nAlternative C (Adding another RDS Instance): This implies creating a read replica (or some other form of\nsharding/database partitioning), which adds infrastructure and complexity. While read replicas can help with\nread scaling, they are not necessary if increasing instance size is sufficient and not the most cost effective.\nThe question says 'without adding infrastructure'.\nAlternative D (On-Demand): Using on-demand instances is the most expensive option for a continuous\nworkload. It offers no cost savings compared to RIs and is generally used for unpredictable or short-term\nworkloads.\nTherefore, purchasing Reserved Instances to cover the projected increased workload, coupled with scaling\nthe existing RDS instance, offers the most cost-effective path to accommodating the larger load without the\ncomplexity of new infrastructure or the higher expense of on-demand resources.\nHere are authoritative links for further research:\nAmazon RDS Pricing: https://aws.amazon.com/rds/pricing/\nAmazon RDS Reserved Instances: https://aws.amazon.com/rds/reserved-instances/\nScaling Amazon RDS Instances:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ScalingUp.html",
    "links": [
      "https://aws.amazon.com/rds/pricing/",
      "https://aws.amazon.com/rds/reserved-instances/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ScalingUp.html"
    ]
  },
  {
    "question": "CertyIQ\nA company operates an ecommerce website on Amazon EC2 instances behind an Application Load Balancer (ALB)\nin an Auto Scaling group. The site is experiencing performance issues related to a high request rate from\nillegitimate external systems with changing IP addresses. The security team is worried about potential DDoS\nattacks against the website. The company must block the illegitimate incoming requests in a way that has a\nminimal impact on legitimate users.\nWhat should a solutions architect recommend?",
    "options": {
      "B": "Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.",
      "A": "Deploy Amazon Inspector and associate it with the ALB. Amazon Inspector is a vulnerability management",
      "C": "Deploy rules to the network ACLs associated with the ALB to block the incoming traffic. Network ACLs",
      "D": "Deploy Amazon GuardDuty and enable rate-limiting protection when configuring GuardDuty. Amazon"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.\nHere's a detailed justification:\nThe scenario requires blocking illegitimate requests targeting an ecommerce website, mitigating potential\nDDoS attacks, and minimizing impact on legitimate users. AWS WAF (Web Application Firewall) is specifically\ndesigned to protect web applications from common web exploits and bots that could affect availability,\ncompromise security, or consume excessive resources.\nAWS WAF's Rate-Based Rules: WAF allows you to define rules based on the rate of requests coming from a\nspecific IP address. By configuring a rate-limiting rule, you can automatically block or count requests\nexceeding a defined threshold within a specified time period. This directly addresses the problem of high\nrequest rates from illegitimate systems with changing IP addresses. Legitimate users are less likely to trigger\nthese rate limits.\nAssociation with ALB: AWS WAF can be directly associated with an Application Load Balancer (ALB), making\nit an ideal solution for filtering traffic before it reaches the EC2 instances. This integration provides a point of\ncontrol at the application layer (Layer 7).\nMinimal Impact on Legitimate Users: WAF's rate-based rules and other filtering mechanisms are designed to\nminimize the impact on legitimate users. Instead of a blanket block, only requests exceeding the defined rate\nare blocked. You can also configure WAF rules to challenge suspicious requests (e.g., with CAPTCHA) instead\nof outright blocking them.\nLet's examine why the other options are less suitable:\nA. Deploy Amazon Inspector and associate it with the ALB. Amazon Inspector is a vulnerability management\nservice that primarily focuses on identifying security vulnerabilities within your EC2 instances and network\nconfigurations. It is not designed to block incoming traffic based on request rates.\nC. Deploy rules to the network ACLs associated with the ALB to block the incoming traffic. Network ACLs\noperate at the subnet level and are primarily used for network-level filtering (Layer 3 and 4). While they can\nblock traffic based on IP addresses, they are not suitable for dynamic rate limiting or complex web application\nfiltering like AWS WAF. Updating ACLs requires more operational overhead and can be difficult when IP\naddresses are changing rapidly.\nD. Deploy Amazon GuardDuty and enable rate-limiting protection when configuring GuardDuty. Amazon\nGuardDuty is a threat detection service that analyzes AWS CloudTrail, VPC Flow Logs, and DNS logs to\nidentify malicious activity. While GuardDuty can detect potential DDoS attacks, it does not provide the direct\ntraffic filtering and rate-limiting capabilities required to mitigate the issue in real-time. It mainly sends alerts\nto the security team.\nTherefore, AWS WAF with rate-limiting rules is the most effective solution for blocking illegitimate incoming\nrequests from changing IP addresses while minimizing the impact on legitimate users, addressing the\nrequirement to protect the ecommerce website from potential DDoS attacks.\nAuthoritative Links:\nAWS WAF: https://aws.amazon.com/waf/\nAWS WAF Rate-Based Rules: https://docs.aws.amazon.com/waf/latest/developerguide/waf-rate-based-\nrule.html\nAmazon Inspector: https://aws.amazon.com/inspector/\nAmazon GuardDuty: https://aws.amazon.com/guardduty/",
    "links": [
      "https://aws.amazon.com/waf/",
      "https://docs.aws.amazon.com/waf/latest/developerguide/waf-rate-based-",
      "https://aws.amazon.com/inspector/",
      "https://aws.amazon.com/guardduty/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to share accounting data with an external auditor. The data is stored in an Amazon RDS DB\ninstance that resides in a private subnet. The auditor has its own AWS account and requires its own copy of the\ndatabase.\nWhat is the MOST secure way for the company to share the database with the auditor?",
    "options": {},
    "answer": "D",
    "explanation": "The most secure way to share the database with the external auditor is to create an encrypted snapshot and\nshare it with the auditor's AWS account along with access to the KMS key. This approach ensures data\nconfidentiality and integrity while minimizing the attack surface.\nOption D is superior because it avoids exposing a live, replicated database instance directly to the auditor's\naccount (as in Option A). Sharing a live replica introduces potential security vulnerabilities and risks\nunauthorized access or modifications. Encrypting the snapshot at rest ensures that the data remains\nprotected even if the snapshot itself is compromised. Sharing the snapshot rather than the live data allows\nthe auditor to work on a point-in-time copy, without impacting the company's production database. Sharing a\nsnapshot is better than exporting to text files (as in B) which can be cumbersome, difficult to manage, and\nincrease the risk of exposure. Additionally, storing in an S3 bucket and granting bucket access to the auditor\nis more complex and presents additional risk when compared to sharing a snapshot. Sharing the snapshot is\npreferred over sharing user keys (as in C) which is insecure.\nSharing an encrypted snapshot through AWS provides proper access controls and accountability, while\nsharing user keys is considered a security anti-pattern. It also allows for proper key rotation policies and\nauditing of key usage. By leveraging KMS, the company maintains control over the encryption key and can\nrevoke access at any time. The auditor, in their own AWS account, can then restore the snapshot into a new\ndatabase instance in their own environment, isolated from the company's infrastructure.\nFor further information, you can refer to the following AWS documentation:\nSharing AWS KMS keys: https://docs.aws.amazon.com/kms/latest/developerguide/share-keys.html\nSharing snapshots: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-share-snapshot.html",
    "links": [
      "https://docs.aws.amazon.com/kms/latest/developerguide/share-keys.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-share-snapshot.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect configured a VPC that has a small range of IP addresses. The number of Amazon EC2\ninstances that are in the VPC is increasing, and there is an insufficient number of IP addresses for future\nworkloads.\nWhich solution resolves this issue with the LEAST operational overhead?",
    "options": {
      "C": "This offers the least"
    },
    "answer": "A",
    "explanation": "The correct answer is A: Add an additional IPv4 CIDR block to the existing VPC. This offers the least\noperational overhead because it directly addresses the IP address exhaustion issue within the existing\ninfrastructure without requiring the creation and management of new VPCs or complex networking\nconfigurations.\nHere's a detailed justification:\nIP Address Exhaustion: The core problem is the lack of available IP addresses within the current VPC's CIDR\nblock.\nDirect Solution: Adding an IPv4 CIDR block expands the address space of the existing VPC, providing more IP\naddresses for EC2 instances. This avoids the need to create new network infrastructure.\nSubnet Creation: Additional subnets can be created using the new CIDR block, allowing for logical grouping\nand organization of resources.\nOperational Overhead: Option A involves modifying the existing VPC rather than creating entirely new ones,\nminimizing the complexity of routing configurations, security groups, and network ACLs. It also keeps all\nresources within a single VPC, simplifying management and monitoring.\nPeering, Transit Gateway, and VPNs: Options B, C, and D all involve creating additional VPCs and connecting\nthem using peering connections, Transit Gateway, or Site-to-Site VPN. These are valid solutions for network\nconnectivity and scalability but introduce significant operational overhead compared to simply expanding the\nexisting VPC's address space. They require managing inter-VPC routing, security considerations, and\npotential bandwidth limitations.\nLeast Effort: Adding a CIDR block is the most straightforward way to increase the number of IP addresses in\nan existing VPC, requiring minimal configuration changes.\nIn summary, adding an additional IPv4 CIDR block to the existing VPC addresses the IP address exhaustion\nissue directly with the least amount of configuration and management overhead compared to creating and\nconnecting multiple VPCs.\nFor further research, refer to the AWS documentation on VPCs:\nYour VPC and subnets\nAssociate additional IPv4 CIDR blocks with your VPC",
    "links": []
  },
  {
    "question": "CertyIQ\nA company used an Amazon RDS for MySQL DB instance during application testing. Before terminating the DB\ninstance at the end of the test cycle, a solutions architect created two backups. The solutions architect created\nthe first backup by using the mysqldump utility to create a database dump. The solutions architect created the\nsecond backup by enabling the final DB snapshot option on RDS termination.\nThe company is now planning for a new test cycle and wants to create a new DB instance from the most recent\nbackup. The company has chosen a MySQL-compatible edition ofAmazon Aurora to host the DB instance.\nWhich solutions will create the new DB instance? (Choose two.)",
    "options": {
      "C": "Option A: Import the RDS snapshot directly into Aurora. RDS snapshots are the native backup mechanism"
    },
    "answer": "A",
    "explanation": "The correct solutions for creating a new MySQL-compatible Amazon Aurora DB instance from the backups\nare A and C.\nOption A: Import the RDS snapshot directly into Aurora. RDS snapshots are the native backup mechanism\nprovided by AWS for RDS databases. Aurora is designed to be compatible with RDS snapshots of MySQL and\nPostgreSQL engines. Therefore, you can directly restore an RDS snapshot of a MySQL instance into an Aurora\nMySQL instance using the AWS Management Console, AWS CLI, or SDKs. This is the most straightforward\nand efficient method if you have a readily available RDS\nsnapshot.https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/AuroraMySQL.Managing.html\nOption C: Upload the database dump to Amazon S3. Then import the database dump into Aurora. The\nmysqldump utility creates a logical backup of your database. This backup can be imported into a new database\ninstance. The mysqldump file needs to be uploaded to Amazon S3 for Aurora to access it. Aurora can then\nimport the data from S3. This method provides flexibility, especially if you need to perform some data\ntransformations before importing into\nAurora.https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/AuroraMySQL.Migrating.ExtMySQL.html\nWhy the other options are incorrect:\nB: Upload the RDS snapshot to Amazon S3. Then import the RDS snapshot into Aurora. While you can\nexport RDS snapshots to S3, this process is more complex and primarily used for cross-account or cross-\nregion migrations. You can directly restore from an RDS snapshot without uploading to S3 first.\nD: Use AWS Database Migration Service (AWS DMS) to import the RDS snapshot into Aurora. While DMS is\nuseful for heterogeneous database migrations or for migrating with minimal downtime, it is not needed to\nimport RDS snapshots of MySQL into Aurora. Native restoration from snapshot is simpler and faster.\nE: Upload the database dump to Amazon S3. Then use AWS Database Migration Service (AWS DMS) to\nimport the database dump into Aurora. DMS is not needed for restoring mysqldump files to Aurora. Aurora\nhas built-in functionality to restore from mysqldump files located in S3. DMS would add unnecessary\ncomplexity and cost.",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/AuroraMySQL.Managing.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/AuroraMySQL.Migrating.ExtMySQL.html"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts a multi-tier web application on Amazon Linux Amazon EC2 instances behind an Application Load\nBalancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company observes\nthat the Auto Scaling group launches more On-Demand Instances when the application's end users access high\nvolumes of static web content. The company wants to optimize cost.\nWhat should a solutions architect do to redesign the application MOST cost-effectively?",
    "options": {},
    "answer": "C",
    "explanation": "The most cost-effective solution is to use Amazon CloudFront to serve static web content from an Amazon S3\nbucket (Option C). Here's why:\nCost Optimization: The question specifically highlights the need for cost optimization. Serving static content\ndirectly from EC2 instances behind an Application Load Balancer is expensive due to the compute resources\nconsumed and the associated networking costs. CloudFront and S3 are designed for efficient and cost-\neffective static content delivery.\nCloudFront Caching: CloudFront is a Content Delivery Network (CDN) that caches content at edge locations\nglobally. This means that when a user requests static content, CloudFront can often serve it from a nearby\nedge location, reducing latency and offloading traffic from the EC2 instances. This reduces the need for the\nAuto Scaling group to scale up when static content is frequently accessed.\nS3 Cost Efficiency: Amazon S3 is a highly scalable and cost-effective object storage service. Storing static\ncontent in S3 is significantly cheaper than storing it on EC2 instance storage.\nReduced EC2 Load: By offloading static content delivery to CloudFront and S3, the EC2 instances behind the\nApplication Load Balancer only need to handle dynamic content and application logic. This reduces the overall\nload on the EC2 instances and allows the Auto Scaling group to scale based on the actual application\nworkload, rather than static content requests.\nAlternatives Considered:\nOption A (Reserved Instances): While Reserved Instances can offer cost savings compared to On-Demand\nInstances, they don't address the underlying problem of using compute instances to serve static content. It is\nstill less efficient than using a CDN.\nOption B (Spot Instances): Spot Instances can be cheaper than On-Demand Instances, but they are subject to\ninterruption. Serving essential web content from Spot Instances can lead to application instability if the Spot\nInstances are terminated, moreover it does not address the inefficiency of serving static content from EC2\ninstances.\nOption D (Lambda and API Gateway): Using Lambda and API Gateway for static content delivery is not\ngenerally recommended. While it's possible, it's more complex and potentially more expensive than using\nCloudFront and S3. Lambda is better suited for dynamic content and serverless functions, and API Gateway\nadds unnecessary overhead for simply serving static files.\nTherefore, offloading the static contents to S3 and distributing it through CloudFront offers optimal cost\nsavings, enhanced performance, and simplified architecture compared to the other options.\nRelevant Links:\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nAmazon S3: https://aws.amazon.com/s3/",
    "links": [
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/s3/"
    ]
  },
  {
    "question": "CertyIQ\nA company stores several petabytes of data across multiple AWS accounts. The company uses AWS Lake\nFormation to manage its data lake. The company's data science team wants to securely share selective data from\nits accounts with the company's engineering team for analytical purposes.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D because it provides a centralized and scalable approach to cross-account data\nsharing with minimal operational overhead using Lake Formation. Lake Formation's tag-based access control\nallows you to categorize data based on tags and then grant permissions based on these tags. This simplifies\nmanagement, especially when dealing with petabytes of data across multiple accounts. By tagging the\nspecific data needed by the engineering team and granting the engineering team's accounts cross-account\npermissions based on these tags, you achieve selective data sharing without needing to copy or move data.\nOption A, copying data to a common account, introduces data duplication, increased storage costs, and the\nneed for a separate access management system. Managing data synchronization and ensuring data\nconsistency become added burdens.\nOption B, granting permissions directly to individual users, is not scalable and creates a management\nnightmare as the engineering team grows or data access requirements change. Modifying permissions for\neach user across multiple accounts is operationally expensive.\nOption C, using AWS Data Exchange, is more suitable for sharing data with external parties. It incurs higher\ncosts than native Lake Formation features and is not optimized for internal data sharing within an\norganization. The overhead associated with data exchange, even privately, is significantly greater.\nTag-based access control allows you to centrally define who has access to what data based on the applied\ntags. This significantly reduces the overhead associated with managing permissions for individual users or\nroles across multiple accounts. Because Lake Formation manages the underlying access controls, the\noperation is optimized for the AWS environment.\nFor further research, refer to the AWS documentation on Lake Formation's tag-based access control:\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/tag-based-access-control.html and cross-account\naccess: https://docs.aws.amazon.com/lake-formation/latest/dg/cross-account-permissions.html.",
    "links": [
      "https://docs.aws.amazon.com/lake-formation/latest/dg/tag-based-access-control.html",
      "https://docs.aws.amazon.com/lake-formation/latest/dg/cross-account-permissions.html."
    ]
  },
  {
    "question": "CertyIQ\nA company wants to host a scalable web application on AWS. The application will be accessed by users from\ndifferent geographic regions of the world. Application users will be able to download and upload unique data up to\ngigabytes in size. The development team wants a cost-effective solution to minimize upload and download latency\nand maximize performance.\nWhat should a solutions architect do to accomplish this?",
    "options": {
      "A": "Use Amazon S3 with Transfer Acceleration to host the application.",
      "B": "Use Amazon S3 with CacheControl headers to host the application. While CacheControl headers are",
      "C": "Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application. EC2 with Auto",
      "D": "Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application. ElastiCache is an in-"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Use Amazon S3 with Transfer Acceleration to host the application.\nHere's why:\nS3 for Object Storage: Amazon S3 (Simple Storage Service) is designed for scalable, durable, and cost-\neffective object storage. It's ideal for storing and serving large files like those being uploaded and\ndownloaded by the application's users. https://aws.amazon.com/s3/\nTransfer Acceleration for Speed: S3 Transfer Acceleration leverages the globally distributed AWS edge\nlocations (same infrastructure as CloudFront) to accelerate data transfers to and from S3. When a user\nuploads or downloads data, the connection is routed to the nearest edge location, which then uses optimized\nnetwork paths to transfer the data to or from the S3 bucket. This significantly reduces latency for users in\ndifferent geographic regions. https://aws.amazon.com/s3/transfer-acceleration/\nCost-Effectiveness: While Transfer Acceleration incurs some additional cost, it is generally a cost-effective\nway to improve transfer speeds, especially for geographically dispersed users and large files. The increased\nperformance can outweigh the added expense.\nLet's analyze why the other options are less suitable:\nB. Use Amazon S3 with CacheControl headers to host the application. While CacheControl headers are\nuseful for controlling how browsers and CDNs cache content, they don't inherently accelerate uploads. They\nprimarily optimize download speeds for content that's already stored in S3 and cached.\nC. Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application. EC2 with Auto\nScaling would be suitable for the application's compute layer (e.g., web servers, application servers).\nCloudFront would be excellent for caching and distributing static content, but it doesn't directly accelerate\nuploads. Also, using EC2 to store multi-gigabyte user data is not the most cost-effective approach. S3 is\nbetter suited for large object storage.\nD. Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application. ElastiCache is an in-\nmemory data caching service. It is great for accelerating reads of frequently accessed data, but it doesn't\nhelp with large file uploads or the global distribution of content in the way that Transfer Acceleration does.\nLike option C, using EC2 as the primary storage for multi-gigabyte user data is not ideal.",
    "links": [
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/s3/transfer-acceleration/"
    ]
  },
  {
    "question": "CertyIQ\nA company has hired a solutions architect to design a reliable architecture for its application. The application\nconsists of one Amazon RDS DB instance and two manually provisioned Amazon EC2 instances that run web\nservers. The EC2 instances are located in a single Availability Zone.\nAn employee recently deleted the DB instance, and the application was unavailable for 24 hours as a result. The\ncompany is concerned with the overall reliability of its environment.\nWhat should the solutions architect do to maximize reliability of the application's infrastructure?",
    "options": {
      "B": "CloudWatch alarms would only notify about a problem after it already occurred."
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the best solution to maximize reliability:\nThe Problem: The original architecture has several single points of failure: a single RDS instance and EC2\ninstances within a single Availability Zone. A manual deletion of the RDS instance caused a significant outage,\nhighlighting the need for redundancy and automated recovery mechanisms.\nWhy Option B is the Best Solution:\nMulti-AZ RDS: Upgrading the RDS instance to Multi-AZ creates a synchronous standby replica in a different\nAvailability Zone. If the primary instance fails, the service automatically fails over to the standby, minimizing\ndowntime. https://aws.amazon.com/rds/features/multi-az/\nDeletion Protection: Enabling deletion protection on the RDS instance prevents accidental deletion,\naddressing the root cause of the initial outage.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.Overview.html\nApplication Load Balancer (ALB): An ALB distributes incoming traffic across multiple EC2 instances,\nimproving availability and scalability. It also performs health checks and automatically routes traffic away\nfrom unhealthy instances. https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\nEC2 Auto Scaling Group (ASG) across multiple AZs: Placing the EC2 instances in an ASG spanning multiple\nAvailability Zones ensures that if one AZ experiences an outage, the application remains available in other\nAZs. The ASG automatically replaces unhealthy instances, maintaining the desired capacity.\nhttps://aws.amazon.com/autoscaling/\nWhy Other Options are Less Suitable:\nOption A: Deleting one EC2 instance reduces capacity and does not address Availability Zone failures.\nTermination protection only protects a single instance from termination, and while multi-AZ RDS helps, this\noption doesn't scale or balance load.\nOption C: Introducing API Gateway and Lambda adds complexity and potential points of failure without\nnecessarily improving the core reliability of the web server tier. Writing to two DB instances manually through\nLambda can introduce data consistency issues and is not a standard high-availability configuration for RDS.\nOption D: While using an ASG across multiple AZs and multi-AZ RDS are good ideas, using Spot Instances can\nlead to interruptions if the Spot price increases, potentially reducing reliability. Also, while CloudWatch\nalarms are helpful for monitoring, they don't automatically provide fault tolerance like a Multi-AZ setup and\nan ALB. CloudWatch alarms would only notify about a problem after it already occurred.\nIn summary, option B provides a comprehensive approach to maximize the application's reliability by\naddressing both data and compute layer redundancy, automating recovery, and preventing accidental data\nloss, making it the optimal solution.",
    "links": [
      "https://aws.amazon.com/rds/features/multi-az/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.Overview.html",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
      "https://aws.amazon.com/autoscaling/"
    ]
  },
  {
    "question": "CertyIQ\nA company is storing 700 terabytes of data on a large network-attached storage (NAS) system in its corporate\ndata center. The company has a hybrid environment with a 10 Gbps AWS Direct Connect connection.\nAfter an audit from a regulator, the company has 90 days to move the data to the cloud. The company needs to\nmove the data efficiently and without disruption. The company still needs to be able to access and update the data\nduring the transfer window.\nWhich solution will meet these requirements?",
    "options": {
      "B": "S3 Integration: The data can be directly transferred into an Amazon S3 bucket, making it readily available for"
    },
    "answer": "A",
    "explanation": "The most suitable solution is A: Using AWS DataSync to transfer the data to Amazon S3. Here's why:\nEfficient Transfer: AWS DataSync is specifically designed for online data transfer between on-premises\nstorage and AWS services. It uses a purpose-built protocol and parallel data streams to accelerate the\ntransfer process, maximizing the 10 Gbps Direct Connect link.\nNo Disruption: DataSync supports incremental transfers. Only the changed data is copied after the initial\ntransfer, minimizing the impact on ongoing operations and allowing continuous access and updates during the\n90-day window.\nLarge Dataset Handling: DataSync is designed to handle large datasets, making it a good fit for 700 TB.\nS3 Integration: The data can be directly transferred into an Amazon S3 bucket, making it readily available for\ncloud-based applications and analysis.\nLet's examine why other options are less suitable:\nB (Snowball Edge): While Snowball Edge is useful for transferring large datasets, it involves physical\nshipping, which is not the most efficient approach given the Direct Connect link and the 90-day deadline. More\nimportantly, Option B includes mounting the S3 bucket on-premises, which is not how S3 works; it's not a\nmountable filesystem.\nC (rsync): rsync can transfer data, but it lacks the advanced features of DataSync, like built-in data\nverification, encryption, bandwidth management, and optimized transfer protocols. It would likely be slower\nand require more manual configuration.\nD (Tapes): Transferring data via tapes is an outdated approach. It is extremely slow and impractical\nconsidering the availability of a 10 Gbps Direct Connect link.\nTherefore, DataSync provides the optimal balance of speed, efficiency, and minimal disruption for transferring\nthe 700 TB dataset within the given timeframe.\nSupporting Documentation:\nAWS DataSync: https://aws.amazon.com/datasync/",
    "links": [
      "https://aws.amazon.com/datasync/"
    ]
  },
  {
    "question": "CertyIQ\nA company stores data in PDF format in an Amazon S3 bucket. The company must follow a legal requirement to\nretain all new and existing data in Amazon S3 for 7 years.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the best solution for meeting the company's data retention\nrequirement with the least operational overhead:\nThe core requirement is to retain all data (new and existing) in Amazon S3 for 7 years while adhering to legal\nmandates. This necessitates immutability, preventing accidental or intentional deletion or modification during\nthe retention period.\nS3 Object Lock: This is the ideal feature for implementing immutability. It prevents objects from being\ndeleted or overwritten for a specified retention period or until a specific date.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html\nCompliance Retention Mode: This mode offers the strongest protection, as it prevents even users with\nadministrative privileges from deleting or shortening the retention period. Governance mode allows privileged\nusers to modify the retention settings, which could violate the legal requirement.\n7-Year Retention Period: Setting the retention period to 7 years aligns directly with the legal retention\nrequirement.\nAddressing Existing Data: Since the retention requirement applies to both new and existing data, all objects\nin the S3 bucket must be protected by Object Lock. Simply enabling Object Lock won't retroactively protect\nexisting objects.\nS3 Batch Operations: This feature provides a highly efficient way to apply Object Lock configurations to the\nexisting data in the S3 bucket. Instead of manually recopying objects (as in options B and C), S3 Batch\nOperations automates the process, minimizing operational overhead. S3 Batch Operations allows you to\nperform large-scale actions on S3 objects. https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-\nops-intro.html\nLet's examine why the other options are less suitable:\nOption A: S3 Versioning is useful for recovering from accidental deletions or overwrites, but it does not\nprevent deletion if proper permissions are present. Configuring MFA Delete adds an extra layer of security,\nbut an authorized user with MFA can still delete objects. S3 Lifecycle policies would eventually delete all data\nafter 7 years, but it doesn't guarantee immutability during those 7 years. Thus, Option A fails the immutability\nrequirement.\nOptions B and C: While they correctly use S3 Object Lock and the 7-year retention period, they suggest\nrecopying all existing objects to bring them into compliance. This is a manual and operationally expensive\napproach compared to using S3 Batch Operations. Furthermore, using governance mode (Option B) does not\nfully protect from deletion if a privileged user decides to override the settings.",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-"
    ]
  },
  {
    "question": "CertyIQ\nA company has a stateless web application that runs on AWS Lambda functions that are invoked by Amazon API\nGateway. The company wants to deploy the application across multiple AWS Regions to provide Regional failover\ncapabilities.\nWhat should a solutions architect do to route traffic to multiple Regions?",
    "options": {
      "B": "Create an Amazon CloudFront distribution with an origin for each Region. Use"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Create an Amazon CloudFront distribution with an origin for each Region. Use\nCloudFront health checks to route traffic.\nHere's why:\nRegional Failover Requirement: The prompt emphasizes deploying across multiple AWS Regions for Regional\nfailover. This means that if one Region experiences an outage, traffic should automatically be routed to a\nhealthy Region.\nCloudFront for Global Content Delivery and Routing: Amazon CloudFront is a content delivery network (CDN)\nthat caches content closer to users. Crucially, it also offers origin failover capabilities based on health checks.\nOrigin Groups and Health Checks: By creating a CloudFront distribution with an origin group consisting of the\nAPI Gateway endpoints in each Region, you can leverage CloudFront's health checks. If CloudFront detects\nthat an origin (a specific Region's API Gateway) is unhealthy, it automatically routes traffic to a healthy origin\nin another Region. This provides seamless failover.\nStateless Application Compatibility: The web application is stateless, which means requests can be served\nfrom any Region without causing data consistency issues. This perfectly aligns with CloudFront's ability to\nroute requests to different origins based on health.\nWhy other options are incorrect:\nA (Route 53 active-active): While Route 53 can do active-active failover, it's primarily DNS-based. The failure\ndetection and switchover are slower than CloudFront. Also, it would introduce complexity in managing DNS\nrecords and may not be as effective for API-driven failover.\nC (Transit Gateway): Transit Gateway is for connecting VPCs and on-premises networks. It's overkill for\nrouting traffic to API Gateway endpoints for regional failover. It also doesn't offer the built-in health check\nand failover capabilities suitable for this scenario.\nD (Application Load Balancer): An ALB is regional. While you can create an ALB, the single ALB wouldn't\nprovide multi-region failover. ALB's are designed to distribute traffic within the same region, not across\nmultiple regions. The \"target group\" should be in one region.\nIn summary, CloudFront provides the most efficient and reliable solution by natively supporting origin failover\nwith health checks, aligning perfectly with the need for Regional failover and the stateless nature of the web\napplication.\nRelevant Links:\nAmazon CloudFront Origin Failover\nAmazon CloudFront Health Checks",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has two VPCs named Management and Production. The Management VPC uses VPNs through a\ncustomer gateway to connect to a single device in the data center. The Production VPC uses a virtual private\ngateway with two attached AWS Direct Connect connections. The Management and Production VPCs both use a\nsingle VPC peering connection to allow communication between the applications.\nWhat should a solutions architect do to mitigate any single point of failure in this architecture?",
    "options": {
      "C": "Answer: C"
    },
    "answer": "C",
    "explanation": "The most critical single point of failure in the described architecture lies in the Management VPC's\nconnectivity to the data center via a single VPN connection and customer gateway. If this single VPN or\ngateway fails, the Management VPC loses its connection to the data center, impacting critical functions\npotentially dependent on the data center resources.\nOption C, adding a second set of VPNs to the Management VPC from a second customer gateway device,\naddresses this single point of failure directly. By establishing a redundant VPN connection through a separate\ncustomer gateway, the solution ensures that if one VPN or customer gateway fails, the other will maintain\nconnectivity, preventing disruption to the Management VPC's operations. This establishes redundancy and\nhigh availability for the data center connection.\nOption A doesn't address the single point of failure in the Management VPC's data center connection. It only\nadds connectivity between the Management and Production VPCs, not to the data center.\nOption B is incorrect. A second virtual private gateway attached to the Management VPC doesn't solve the\nsingle point of failure, as both gateways would still rely on the single VPN and customer gateway device for\ndata center connectivity.\nOption D, adding a second VPC peering connection, only increases the bandwidth and resilience of\ncommunication between the two VPCs but does not address the vulnerability of the single VPN connection\nfrom the Management VPC to the data center. VPC peering is irrelevant to the data center connectivity\nproblem.\nIn summary, option C provides the necessary redundancy at the customer gateway level, mitigating the risk of\nlosing connectivity between the Management VPC and the on-premises data center. This aligns with best\npractices for high availability in hybrid cloud environments.\nFurther reading on VPN gateway redundancy and high availability can be found in the AWS documentation:\nAWS VPN Connections\nAWS Site-to-Site VPN",
    "links": []
  },
  {
    "question": "CertyIQ\nA company runs its application on an Oracle database. The company plans to quickly migrate to AWS because of\nlimited resources for the database, backup administration, and data center maintenance. The application uses\nthird-party database features that require privileged access.\nWhich solution will help the company migrate the database to AWS MOST cost-effectively?",
    "options": {},
    "answer": "B",
    "explanation": "The most cost-effective solution for migrating the Oracle database to AWS while maintaining third-party\ndatabase feature support and addressing resource constraints is Amazon RDS Custom for Oracle.\nHere's a detailed justification:\nRDS Custom for Oracle: This service offers managed database infrastructure similar to standard RDS, but\nwith the added capability to customize the underlying operating system and database settings. This is critical\nfor supporting third-party database features requiring privileged access that are not available in standard\nRDS. https://aws.amazon.com/rds/custom/\nCost-Effectiveness: RDS Custom provides a balance between managed services and control. Migrating to\nRDS Custom avoids the overhead of fully self-managing an Oracle database on EC2, reducing administrative\nburden and operational costs associated with patching, backups, and infrastructure management.\nPrivileged Access: The key requirement is that the third-party features require privileged access, which is not\navailable in the normal RDS. RDS Custom provides OS and DB access.\nOption A (RDS for Oracle): Standard RDS for Oracle does not allow privileged access or customization\nneeded for third-party features, thus not meeting the requirements. It would require the third-party\napplications to be re-written.\nOption C (EC2 AMI for Oracle): While offering full control, running Oracle on EC2 necessitates significant\nadministrative overhead for database management, patching, backups, and scaling. This contradicts the need\nfor reduced administrative burden and might not be the most cost-effective approach, considering the other\nmanaged service offerings.\nOption D (RDS for PostgreSQL): Rewriting the application code to remove Oracle APEX dependency would be\na time-consuming and costly undertaking and is not the question is asking. Migration should be done quickly.\nTherefore, migrating to Amazon RDS Custom for Oracle is the most cost-effective solution that maintains\nthird-party database feature support while benefiting from managed database services, addressing the\ncompany's limited resources.",
    "links": [
      "https://aws.amazon.com/rds/custom/"
    ]
  },
  {
    "question": "CertyIQ\nA company has a three-tier web application that is in a single server. The company wants to migrate the application\nto the AWS Cloud. The company also wants the application to align with the AWS Well-Architected Framework\nand to be consistent with AWS recommended best practices for security, scalability, and resiliency.\nWhich combination of solutions will meet these requirements? (Choose three.)",
    "options": {
      "C": "Create a VPC across two Availability Zones. Refactor the application to host the web tier, application"
    },
    "answer": "C",
    "explanation": "Here's a detailed justification for why options C, E, and F are the most suitable for migrating the three-tier\nweb application to AWS, aligning with the Well-Architected Framework, and ensuring security, scalability, and\nresiliency:\nE. Use Elastic Load Balancers in front of the web tier. Control access by using security groups containing\nreferences to each layer's security groups.\nELBs are essential for distributing incoming traffic across multiple EC2 instances in the web tier, providing\nhigh availability and fault tolerance. This directly addresses scalability and resilience.\nSecurity groups at the web tier should only allow traffic from the ELB, and the ELB acts as the single point of\nentry, minimizing the attack surface.\nSecurity group references enable a layered security approach, controlling traffic flow between tiers and\nenhancing security posture.\nAuthoritative link: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\nF. Use an Amazon RDS database Multi-AZ cluster deployment in private subnets. Allow database access\nonly from application tier security groups.\nMulti-AZ RDS deployment ensures high availability and failover capabilities for the database tier. If the\nprimary database instance fails, RDS automatically fails over to a standby instance in another Availability\nZone.\nPlacing the database in private subnets restricts direct internet access and protects sensitive data.\nSecurity group rules should be configured so that only EC2 instances in the application tier can access the\ndatabase. This minimizes potential access points and mitigates risks.\nAuthoritative link: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\nC. Create a VPC across two Availability Zones. Refactor the application to host the web tier, application\ntier, and database tier. Host each tier on its own private subnet with Auto Scaling groups for the web tier\nand application tier.\nA VPC across two Availability Zones provides infrastructure isolation and fault tolerance. If one Availability\nZone experiences an issue, the application can continue running in the other.\nRefactoring the application into three tiers promotes modularity and easier management. Each tier can be\nscaled and secured independently.\nHosting each tier in its own private subnet improves security by isolating each tier from direct internet access.\nThe web tier typically resides in a public subnet for internet-facing access.\nAuto Scaling groups enable automatic scaling of the web and application tiers based on demand, ensuring\napplication performance and resilience.\nAuthoritative link: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Introduction.html\nWhy the other options are incorrect:\nA: While creating a VPC and using Auto Scaling is helpful, maintaining the existing monolithic architecture on\nEC2 doesn't provide the full benefits of the cloud in terms of scalability and modularity. It doesn't fully\nleverage AWS best practices.\nB: Setting up security groups and using a single RDS instance is a basic improvement, but it doesn't address\nhigh availability and fault tolerance for the database tier, which is a critical aspect of the Well-Architected\nFramework.",
    "links": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Introduction.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is migrating its applications and databases to the AWS Cloud. The company will use Amazon Elastic\nContainer Service (Amazon ECS), AWS Direct Connect, and Amazon RDS.\nWhich activities will be managed by the company's operational team? (Choose three.)",
    "options": {
      "B": "Creation of an Amazon RDS DB instance and configuring the scheduled maintenance window: The",
      "C": "Configuration of additional software components on Amazon ECS for monitoring, patch management,",
      "A": "Management of the Amazon RDS infrastructure layer, operating system, and platforms: AWS manages",
      "D": "Installation of patches for all minor and major database versions for Amazon RDS: AWS handles the"
    },
    "answer": "B",
    "explanation": "The correct answer is BCF. Here's why:\nB. Creation of an Amazon RDS DB instance and configuring the scheduled maintenance window: The\ncompany is responsible for the configuration and operation of their RDS DB instances, including tasks such as\ncreating the instance, setting parameters, and scheduling maintenance windows. Amazon handles the\nunderlying infrastructure, but the configurations are managed by the user.\nhttps://docs.aws.amazon.com/rds/latest/userguide/USER_CreateDBInstance.html\nC. Configuration of additional software components on Amazon ECS for monitoring, patch management,\nlog management, and host intrusion detection: While ECS manages container orchestration, responsibilities\nlike monitoring the containers and host, patching software within the containers, implementing log\nmanagement solutions, and setting up host intrusion detection systems fall under the company's operational\ncontrol. AWS provides tools and services (like CloudWatch, Systems Manager, and security agents), but the\nuser configures and manages them within their ECS environment.\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/monitoring-cloudwatch.html\nF. Encryption of the data that moves in transit through Direct Connect: While Direct Connect provides a\ndedicated network connection, securing data in transit is the customer's responsibility. The company must\nimplement encryption mechanisms (e.g., VPN over Direct Connect, or application-level encryption) to protect\nsensitive data moving between their on-premises environment and AWS. AWS handles the physical\nconnection, but the security protocols are implemented and managed by the customer.\nhttps://aws.amazon.com/directconnect/security/\nHere's why the other options are incorrect:\nA. Management of the Amazon RDS infrastructure layer, operating system, and platforms: AWS manages\nthe underlying infrastructure, operating system, and platform for RDS under the shared responsibility model.\nD. Installation of patches for all minor and major database versions for Amazon RDS: AWS handles the\npatching of the RDS database engine itself. While the customer schedules when these updates are applied\nwithin the maintenance window they configure, the actual patching is performed by AWS.\nE. Ensure the physical security of the Amazon RDS infrastructure in the data center: AWS is responsible for\nthe physical security of the data centers where the RDS infrastructure resides.",
    "links": [
      "https://docs.aws.amazon.com/rds/latest/userguide/USER_CreateDBInstance.html",
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/monitoring-cloudwatch.html",
      "https://aws.amazon.com/directconnect/security/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a Java-based job on an Amazon EC2 instance. The job runs every hour and takes 10 seconds to run.\nThe job runs on a scheduled interval and consumes 1 GB of memory. The CPU utilization of the instance is low\nexcept for short surges during which the job uses the maximum CPU available. The company wants to optimize the\ncosts to run the job.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The best solution is B: Copy the code into an AWS Lambda function with 1 GB of memory and create an\nAmazon EventBridge scheduled rule to run the code each hour. This is because Lambda is a serverless\ncompute service that allows you to run code without provisioning or managing servers.\nHere's why the other options are not as suitable:\nA: While containerizing the job and running it on Fargate is a viable option, it is more complex and likely more\nexpensive than using Lambda for such a short, infrequent task. Fargate is better suited for continuous or long-\nrunning applications. This adds complexity and cost compared to the event-driven approach.\nC: Containerizing the job and installing the container in the existing AMI is overly complex and does not\noptimize costs. The EC2 instance still needs to be running constantly, even though the job only runs for 10\nseconds each hour. It does not fully leverage serverless technologies.\nD: Stopping and restarting the EC2 instance will add latency to the job and is not cost-effective because you\nwill be charged for the EC2 instance even when it is stopped. Also, stopping/starting an EC2 instance takes a\nfew minutes, exceeding the 10-second runtime.\nWhy Lambda and EventBridge are Ideal:\nCost Optimization: Lambda's pricing model is based on the actual compute time your function consumes.\nSince the job only runs for 10 seconds per hour, the cost will be minimal. You are not paying for idle time.\nScalability: Lambda automatically scales to handle the workload. If the job becomes more frequent or\nrequires more resources in the future, Lambda can handle the increased load.\nSimplicity: Lambda functions are easy to deploy and manage. You can easily copy the code into a Lambda\nfunction and configure the required memory and timeout.\nEvent-Driven: Amazon EventBridge is a serverless event bus that allows you to schedule events to trigger\nLambda functions. This provides a simple and reliable way to run the job every hour. It is directly integrated\nwith Lambda, making configuration straightforward.\nSuitable for short, intermittent workloads: Lambda is specifically designed for tasks that run for a short\nduration and are triggered periodically, aligning perfectly with the questions requirements.\nLambda's execution model and cost structure are perfect for this use case.\nAuthoritative Links:\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon EventBridge: https://aws.amazon.com/eventbridge/",
    "links": [
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/eventbridge/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to implement a backup strategy for Amazon EC2 data and multiple Amazon S3 buckets.\nBecause of regulatory requirements, the company must retain backup files for a specific time period. The company\nmust not alter the files for the duration of the retention period.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D because it leverages AWS Backup with a compliance mode vault lock, fulfilling the\nrequirements for data immutability and retention.\nAWS Backup is a centralized backup service that makes it easy to automate and manage backups across\nvarious AWS services, including EC2 and S3. The key requirement is immutable backups with a fixed retention\nperiod.\nAWS Backup Vault Lock is a feature that helps you enforce a write-once-read-many (WORM) model on your\nbackups. This ensures that backups cannot be deleted or altered during a specified retention period, adhering\nto compliance and regulatory needs. There are two modes for Vault Lock: Governance and Compliance.\nGovernance mode allows privileged users to delete the lock before the retention period expires under certain\nconditions. This doesn't fully meet the 'must not alter the files' requirement.\nCompliance mode, however, is more restrictive. Once locked in compliance mode, even privileged users\ncannot reduce the retention period or delete the lock. This ensures immutability and adherence to retention\npolicies.\nTherefore, using AWS Backup with a Compliance mode Vault Lock directly fulfills the company's\nrequirements for immutability and retention, across both EC2 and S3, making option D the most suitable\nsolution. A backup plan defines what resources to back up, when to back up, and how long to retain backups,\nmaking it an integral component of the solution.\nOption A (Governance mode) does not guarantee immutability, failing the 'must not alter the files' criterion.\nOption B (Amazon Data Lifecycle Manager) is primarily for EBS snapshots and doesn't directly address S3\nbucket backups with retention requirements. Option C (S3 File Gateway) is designed for on-premises\napplications to store data in S3 and doesn't provide the enforced immutability provided by Vault Lock.\nHere are some authoritative links:\nAWS Backup: https://aws.amazon.com/backup/\nAWS Backup Vault Lock: https://docs.aws.amazon.com/aws-backup/latest/devguide/vault-lock.html\nAWS Backup Plans: https://docs.aws.amazon.com/aws-backup/latest/devguide/whatis-backup-plans.html",
    "links": [
      "https://aws.amazon.com/backup/",
      "https://docs.aws.amazon.com/aws-backup/latest/devguide/vault-lock.html",
      "https://docs.aws.amazon.com/aws-backup/latest/devguide/whatis-backup-plans.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has resources across multiple AWS Regions and accounts. A newly hired solutions architect discovers\na previous employee did not provide details about the resources inventory. The solutions architect needs to build\nand map the relationship details of the various workloads across all accounts.\nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "options": {},
    "answer": "C",
    "explanation": "The best solution is C, using Workload Discovery on AWS. Here's why:\nWorkload Discovery on AWS (Option C) is specifically designed to automatically discover and map\napplication components and their relationships across AWS accounts and Regions. It generates architecture\ndiagrams, providing a visual representation of the infrastructure. This automation directly addresses the\nproblem of undocumented resources and relationships, minimizing manual effort. It's the most operationally\nefficient way to map existing workloads and build architecture diagrams in a multi-account, multi-region\nenvironment.\nWhy other options are less suitable:\nAWS Systems Manager Inventory (Option A): While Systems Manager Inventory collects metadata about\nmanaged instances, it primarily focuses on software, patches, and configuration details. It does not\nautomatically map relationships between different AWS services or generate architecture diagrams.\nGenerating a \"map view\" from this data is possible but requires significant manual configuration and scripting,\nmaking it less efficient.\nAWS Step Functions (Option B): Step Functions orchestrates workflows, but it doesn't inherently discover\nresources or map relationships. Building architecture diagrams manually defeats the purpose of operational\nefficiency. This approach requires writing custom code to gather workload details and then manually create\ndiagrams, which is time-consuming and error-prone.\nAWS X-Ray (Option D): X-Ray traces requests as they move through an application, providing insights into\nperformance bottlenecks and dependencies within an application. It's not designed to discover and map the\noverall infrastructure landscape or generate architecture diagrams across multiple accounts and Regions. Its\nfocus is on application performance analysis, not infrastructure mapping.\nIn conclusion, Workload Discovery on AWS offers the most automated and efficient solution for discovering\nresources, mapping relationships, and generating architecture diagrams, directly addressing the solutions\narchitect's need to understand undocumented workloads across a complex AWS environment.\nAuthoritative Links:\nWorkload Discovery on AWS: https://aws.amazon.com/solutions/implementations/workload-discovery-on-\naws/\nAWS Systems Manager Inventory: https://docs.aws.amazon.com/systems-\nmanager/latest/userguide/inventory.html\nAWS Step Functions: https://aws.amazon.com/step-functions/\nAWS X-Ray: https://aws.amazon.com/xray/",
    "links": [
      "https://aws.amazon.com/solutions/implementations/workload-discovery-on-",
      "https://docs.aws.amazon.com/systems-",
      "https://aws.amazon.com/step-functions/",
      "https://aws.amazon.com/xray/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses AWS Organizations. The company wants to operate some of its AWS accounts with different\nbudgets. The company wants to receive alerts and automatically prevent provisioning of additional resources on\nAWS accounts when the allocated budget threshold is met during a specific period.\nWhich combination of solutions will meet these requirements? (Choose three.)",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is BDF because they provide a comprehensive approach to budget control and resource\nprovisioning prevention using AWS Budgets, IAM roles, and budget actions.\nB - Use AWS Budgets to create a budget. Set the budget amount under the Billing dashboards of the\nrequired AWS accounts: AWS Budgets is a service designed for cost management and budget tracking. It\nallows setting custom budgets for individual AWS accounts, providing a granular control over spending. The\nBilling dashboards are the logical location to configure these budgets. https://aws.amazon.com/aws-cost-\nmanagement/aws-budgets/\nD - Create an IAM role for AWS Budgets to run budget actions with the required permissions: AWS Budgets\nrequires specific permissions to take actions when a budget threshold is met. Using an IAM role is the\nrecommended security best practice because it allows granting only the necessary permissions to the AWS\nBudgets service to perform actions without using long-term credentials. This ensures least privilege access.\nhttps://docs.aws.amazon.com/cost-management/latest/userguide/budgets-actions.html\nF - Add an alert to notify the company when each account meets its budget threshold. Add a budget action\nthat selects the IAM identity created with the appropriate service control policy (SCP) to prevent\nprovisioning of additional resources: This step involves setting up notifications through AWS Budgets when\nthe threshold is reached. Crucially, it also creates a budget action. SCPs are a powerful feature within AWS\nOrganizations that enable central control over the AWS accounts in an organization. By linking a budget\naction to an IAM identity, and defining an appropriate SCP, provisioning of additional resources can be\nblocked once the budget threshold is reached.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\nA is incorrect because the Cost and Usage Reports section is primarily for generating detailed reports about\nyour AWS costs and usage, and not for directly setting budget amounts.\nC is incorrect because creating an IAM user for AWS Budgets to run budget actions is not a recommended\nsecurity best practice. IAM roles are preferred over IAM users in this scenario. Using a user requires managing\nlong-term credentials which adds unnecessary risk.\nE is incorrect because config rules are not a mechanism to prevent provisioning of additional resources, and\nthey do not directly integrate with budget actions to stop resource provisioning. SCP's, not config rules, are\nused within Organizations to control what services and actions are allowed within member accounts.",
    "links": [
      "https://aws.amazon.com/aws-cost-",
      "https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-actions.html",
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs applications on Amazon EC2 instances in one AWS Region. The company wants to back up the\nEC2 instances to a second Region. The company also wants to provision EC2 resources in the second Region and\nmanage the EC2 instances centrally from one AWS account.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "C": "Create a backup plan by using AWS Backup. Configure cross-Region",
      "A": "Create a disaster recovery (DR) plan that has a similar number of EC2 instances in the second Region.",
      "B": "Create point-in-time Amazon Elastic Block Store (Amazon EBS) snapshots of the EC2 instances. Copy",
      "D": "Deploy a similar number of EC2 instances in the second Region. Use AWS DataSync to transfer the data"
    },
    "answer": "C",
    "explanation": "The most cost-effective solution is C. Create a backup plan by using AWS Backup. Configure cross-Region\nbackup to the second Region for the EC2 instances.\nHere's why:\nAWS Backup: AWS Backup is a fully managed backup service that centralizes and automates data protection\nacross AWS services, including EC2. This addresses the requirement of central management from one AWS\naccount.\nCost-Effectiveness: AWS Backup provides a cost-optimized solution for creating and managing backups. You\nonly pay for the backup storage and the data transferred during backup and restore operations. Compared to\nrunning a separate replication solution or maintaining a full DR environment, this is generally more\neconomical.\nCross-Region Backup: AWS Backup supports cross-Region backup, allowing you to copy your EC2 instance\nbackups (including EBS volumes) to a second AWS Region for disaster recovery purposes. This meets the\nrequirement of backing up to a second Region.\nRecovery Point Objective (RPO) and Recovery Time Objective (RTO): You can configure backup frequency\n(RPO) and restore capabilities to meet your business requirements. The AWS Backup console simplifies the\nrecovery process.\nLet's analyze why the other options are less suitable:\nA. Create a disaster recovery (DR) plan that has a similar number of EC2 instances in the second Region.\nConfigure data replication. This option involves maintaining a hot or warm DR site with provisioned EC2\ninstances, which is the most expensive approach. While it provides a low RTO, it incurs significant costs for\nidle resources.\nB. Create point-in-time Amazon Elastic Block Store (Amazon EBS) snapshots of the EC2 instances. Copy\nthe snapshots to the second Region periodically. While EBS snapshots can be used for backup, this approach\nrequires manual scripting and management of snapshot copies across Regions. It lacks the centralized\nmanagement and automation provided by AWS Backup. It's also more complex to restore and maintain\nconsistency.\nD. Deploy a similar number of EC2 instances in the second Region. Use AWS DataSync to transfer the data\nfrom the source Region to the second Region. Similar to option A, this approach involves running EC2\ninstances continuously in the second Region, making it costly. AWS DataSync is primarily for migrating data,\nnot continuous backup.\nAuthoritative Links:\nAWS Backup: https://aws.amazon.com/backup/\nCross-Region Backup with AWS Backup: https://docs.aws.amazon.com/aws-backup/latest/devguide/cross-\nregion-backup.html",
    "links": [
      "https://aws.amazon.com/backup/",
      "https://docs.aws.amazon.com/aws-backup/latest/devguide/cross-"
    ]
  },
  {
    "question": "CertyIQ\nA company that uses AWS is building an application to transfer data to a product manufacturer. The company has\nits own identity provider (IdP). The company wants the IdP to authenticate application users while the users use the\napplication to transfer data. The company must use Applicability Statement 2 (AS2) protocol.\nWhich solution will meet these requirements?",
    "options": {
      "C": "Here's why:"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Here's why:\nWhy AWS Transfer Family?\nAWS Transfer Family is the correct choice because it is a fully managed service that directly supports the\nAS2 protocol, which is a core requirement of the question. AS2 is a secure and widely adopted protocol for\nEDI (Electronic Data Interchange) over the internet, frequently used for B2B data transfer. AWS Transfer\nFamily simplifies the setup and management of AS2 file transfers.\nWhy AWS Lambda for IdP Authentication?\nAWS Lambda is a serverless compute service that allows you to run code without provisioning or managing\nservers. In this scenario, a Lambda function can be used to integrate with the company's existing identity\nprovider (IdP). The Lambda function would be responsible for authenticating users based on the IdP's\nauthentication mechanism (e.g., SAML, OAuth) before granting them access to transfer data. Lambda\nfunctions provide the flexibility needed to connect to any custom or third-party IdP.\nWhy other options are incorrect:\nA (AWS DataSync): AWS DataSync is designed for moving large amounts of data between on-premises\nstorage and AWS storage services. It does not inherently support AS2 or integration with external IdPs for\nuser-level authentication within the application.\nB (Amazon AppFlow): Amazon AppFlow is a fully managed integration service that enables you to securely\ntransfer data between SaaS applications and AWS services. While AppFlow can handle data transfer, it\ndoesn't natively support AS2 and the need for custom IdP integration is better served by Lambda.\nD (AWS Storage Gateway, Amazon Cognito): AWS Storage Gateway connects on-premises applications to\nAWS cloud storage. While useful for hybrid cloud setups, it is not directly involved in data transfer to\nmanufacturers or the application's user authentication flow. While Amazon Cognito is for Identity and Access\nManagement, using Cognito identity pools in this scenario does not fit because the company already has an\nexisting IdP and wants to leverage that for authentication. Cognito identity pools are typically used for\nscenarios where you want to authenticate users directly against AWS or a social identity provider, rather than\nintegrating with an existing corporate IdP and also doesn't solve the AS2 transfer protocol requirement.\nIn summary, AWS Transfer Family addresses the AS2 protocol requirement directly, and AWS Lambda allows\nflexible integration with the company's existing IdP for user authentication.\nSupporting links:\nAWS Transfer Family: https://aws.amazon.com/transfer/\nAWS Lambda: https://aws.amazon.com/lambda/",
    "links": [
      "https://aws.amazon.com/transfer/",
      "https://aws.amazon.com/lambda/"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is designing a RESTAPI in Amazon API Gateway for a cash payback service. The application\nrequires 1 GB of memory and 2 GB of storage for its computation resources. The application will require that the\ndata is in a relational format.\nWhich additional combination ofAWS services will meet these requirements with the LEAST administrative effort?\n(Choose two.)",
    "options": {
      "B": "AWS Lambda: AWS Lambda is a serverless compute service. It allows you to run code without provisioning",
      "C": "Amazon RDS: Amazon Relational Database Service (RDS) is a managed database service that simplifies",
      "A": "Amazon EC2: While EC2 instances can certainly meet the memory and storage requirements, managing",
      "D": "Amazon DynamoDB: DynamoDB is a NoSQL database service. The problem states that the data needs to be"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why the correct answer is B (AWS Lambda) and C (Amazon RDS), along with\nexplanations of why the other options are less suitable, and authoritative links for further reading.\nThe problem states that a REST API needs to be designed for a cash payback service requiring 1 GB of\nmemory, 2 GB of storage, and a relational database. The goal is to choose the combination of AWS services\nthat meets these requirements with the least administrative overhead.\nB. AWS Lambda: AWS Lambda is a serverless compute service. It allows you to run code without provisioning\nor managing servers. Lambda functions can be configured with up to 10 GB of memory and execution times up\nto 15 minutes. The serverless nature drastically reduces administrative overhead compared to managing EC2\ninstances. Using Lambda for the API's compute element aligns with the principle of minimizing operational\nburden. It is a suitable option for the required memory allocation.\nC. Amazon RDS: Amazon Relational Database Service (RDS) is a managed database service that simplifies\nsetting up, operating, and scaling a relational database in the cloud. RDS supports various database engines\n(MySQL, PostgreSQL, MariaDB, Oracle, SQL Server). Since the requirement specifically mentions a relational\ndatabase format, RDS is the most logical and efficient choice. RDS offers automatic backups, patching, and\nscaling options, which significantly reduces the administrative overhead compared to self-managing a\ndatabase on EC2.\nWhy other options are incorrect:\nA. Amazon EC2: While EC2 instances can certainly meet the memory and storage requirements, managing\nthem involves significantly more administrative overhead. You'd need to provision, configure, patch, and scale\nthe instances yourself. Also, you'd need to setup the relational database yourself on this EC2 instance adding\nmore to the administrative overhead, going against the goal of \"least administrative effort\".\nD. Amazon DynamoDB: DynamoDB is a NoSQL database service. The problem states that the data needs to be\nin a relational format. DynamoDB does not satisfy this fundamental requirement.\nE. Amazon Elastic Kubernetes Service (Amazon EKS): EKS is a container orchestration service used for\nmanaging and deploying containerized applications. While you could use EKS to run a containerized\napplication meeting the resource requirements, it introduces considerably more complexity and\nadministrative overhead compared to Lambda. EKS requires managing the Kubernetes cluster itself, including\nnodes, networking, and scaling. It's overkill for this specific scenario focused on minimal administration.\nIn summary, Lambda handles the compute workload serverlessly, and RDS provides a managed relational\ndatabase. This combination minimizes administrative tasks and aligns with the problem's objectives.\nAuthoritative Links:\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon RDS: https://aws.amazon.com/rds/",
    "links": [
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/rds/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses AWS Organizations to run workloads within multiple AWS accounts. A tagging policy adds\ndepartment tags to AWS resources when the company creates tags.\nAn accounting team needs to determine spending on Amazon EC2 consumption. The accounting team must\ndetermine which departments are responsible for the costs regardless ofAWS account. The accounting team has\naccess to AWS Cost Explorer for all AWS accounts within the organization and needs to access all reports from\nCost Explorer.\nWhich solution meets these requirements in the MOST operationally efficient way?",
    "options": {
      "A": "Here's a detailed justification:",
      "C": "Using an AWS-defined tag when a custom 'department' tag already exists is"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's a detailed justification:\nThe scenario requires a centralized way to track EC2 costs across all accounts in an AWS Organization,\ngrouped by department tag. The solution must be operationally efficient and leverage Cost Explorer, which\nthe accounting team already has access to.\nOption A is correct because it centralizes the configuration and reporting within the Organizations\nmanagement account. Activating a user-defined cost allocation tag named \"department\" in the management\naccount ensures that all accounts in the organization can utilize this tag for cost tracking. User-defined tags\nare suitable since the tagging policy is already adding the 'department' tag. Creating one cost report in Cost\nExplorer, grouping by the \"department\" tag and filtering by EC2, provides a consolidated view of EC2 costs\nper department across all accounts. This approach fulfills the requirement for a single report and centralized\nview.\nOption B is incorrect because AWS-defined cost allocation tags are specific AWS-managed tags, not ones the\nuser defines like the 'department' tag in the company's tagging policy.Option C is incorrect because activating\ncost allocation tags from each member account would be extremely inefficient and would require the\naccounting team to create and manage multiple cost reports, one per account or a combined report requiring\nmore effort to set up. Centralizing the tag activation makes operations significantly easier.Option D suffers\nfrom the same problem as C. Using an AWS-defined tag when a custom 'department' tag already exists is\ninappropriate.\nThe critical element here is that the cost allocation tags must be activated from the management account to\napply to all accounts in the organization, and using the user-defined tag is appropriate since the company\nalready has a tagging policy for the 'department' tag. This allows for centralized reporting through Cost\nExplorer as required.\nSupporting Links:\nAWS Cost Allocation Tags\nAWS Organizations Billing and Cost Management\nAWS Cost Explorer",
    "links": []
  },
  {
    "question": "CertyIQ\nA company wants to securely exchange data between its software as a service (SaaS) application Salesforce\naccount and Amazon S3. The company must encrypt the data at rest by using AWS Key Management Service\n(AWS KMS) customer managed keys (CMKs). The company must also encrypt the data in transit. The company has\nenabled API access for the Salesforce account.",
    "options": {
      "C": "Create Amazon AppFlow flows to transfer the data securely from Salesforce to",
      "A": "D. Create a custom connector for Salesforce: Developing a custom connector is a significant undertaking",
      "B": "AWS Step Functions workflow: Step Functions can orchestrate multiple Lambda functions or other tasks."
    },
    "answer": "C",
    "explanation": "The correct answer is C. Create Amazon AppFlow flows to transfer the data securely from Salesforce to\nAmazon S3.\nHere's a detailed justification:\nAmazon AppFlow is a fully managed integration service that enables secure data transfer between SaaS\napplications like Salesforce and AWS services like S3. AppFlow is designed to address exactly this type of\nintegration scenario. It offers built-in security features and is specifically designed for data transfer between\nSaaS applications and AWS.\nWhy AppFlow is ideal:\nNative Integration: AppFlow natively integrates with Salesforce and S3, simplifying the configuration and\ndata transfer process. It handles the complexities of API authentication and authorization.\nSecurity: AppFlow supports encryption at rest using AWS KMS customer managed keys (CMKs), fulfilling the\nrequirement for data encryption. It also ensures data is encrypted in transit using TLS. You can configure the\nflow to use a CMK for encrypting the data stored during the transfer process.\nManaged Service: Being a fully managed service, AppFlow handles the underlying infrastructure, scaling, and\nmaintenance, reducing operational overhead. You don't need to manage servers or code.\nNo-Code/Low-Code: AppFlow provides a visual interface to create flows without writing custom code,\naccelerating the development process.\nWhy other options are less suitable:\nA. AWS Lambda functions: While Lambda functions can be used for data transfer, it would require writing\ncustom code to handle Salesforce API authentication, data extraction, transformation, and loading into S3.\nFurthermore, you would need to manage encryption, security, and scaling on your own. This is more complex\nthan using AppFlow.\nB. AWS Step Functions workflow: Step Functions can orchestrate multiple Lambda functions or other tasks.\nBut using it directly for data transfer between Salesforce and S3 is overly complex. The underlying data\ntransfer still requires Lambda function development with the same drawbacks as option A.\nD. Create a custom connector for Salesforce: Developing a custom connector is a significant undertaking\nthat requires deep expertise in Salesforce API and AWS services. AppFlow essentially is the pre-built\nconnector that handles this complexity. It's unnecessary to create a connector from scratch.\nIn summary, Amazon AppFlow provides a secure, efficient, and managed solution for transferring data from\nSalesforce to S3 with encryption at rest using KMS CMKs and encryption in transit, perfectly aligning with the\ncompany's requirements.\nAuthoritative Links:\nAmazon AppFlow: https://aws.amazon.com/appflow/\nAmazon AppFlow Security: https://docs.aws.amazon.com/appflow/latest/userguide/security.html\nAWS KMS: https://aws.amazon.com/kms/",
    "links": [
      "https://aws.amazon.com/appflow/",
      "https://docs.aws.amazon.com/appflow/latest/userguide/security.html",
      "https://aws.amazon.com/kms/"
    ]
  },
  {
    "question": "CertyIQ\nA company is developing a mobile gaming app in a single AWS Region. The app runs on multiple Amazon EC2\ninstances in an Auto Scaling group. The company stores the app data in Amazon DynamoD",
    "options": {
      "B": "Update CloudFront to use the ALB as the origin."
    },
    "answer": "B",
    "explanation": "The optimal solution for minimizing latency for a globally distributed mobile gaming app utilizing TCP and\nUDP traffic with EC2 instances, DynamoDB, and an Auto Scaling group is to use AWS Global Accelerator with\na Network Load Balancer (NLB).\nHere's why:\nAWS Global Accelerator: Global Accelerator is specifically designed to direct traffic to optimal endpoints\nacross the AWS global network, reducing latency for users worldwide. It leverages the AWS backbone\nnetwork, known for its speed and reliability. https://aws.amazon.com/global-accelerator/\nNetwork Load Balancer (NLB): NLBs are ideal for handling TCP and UDP traffic at extremely high throughput\nand low latency. Unlike Application Load Balancers (ALBs), NLBs operate at Layer 4 (transport layer), making\nthem more efficient for routing non-HTTP traffic. Gaming applications frequently use UDP for real-time data\ntransmission, making NLB a natural fit.\nALB Incompatibility: Application Load Balancers are designed for HTTP/HTTPS traffic. While they are\nexcellent for web applications, they cannot handle raw TCP or UDP traffic directly without modification,\nmaking them inappropriate for the stated needs.\nCloudFront Limitations: While CloudFront excels at caching static content, it is not designed for real-time\nbidirectional communication like gaming applications, and does not support UDP. CloudFront is better suited\nfor distributing game assets like textures and updates, not for handling the real-time game communication\ntraffic itself.\nDirect Integration: Global Accelerator offers seamless integration with NLBs. It can automatically route\ntraffic to healthy instances behind an NLB in multiple Availability Zones or even Regions.\nLowest Latency: Global Accelerator ensures that users are routed to the nearest healthy endpoint based on\ntheir geographic location and network conditions, resulting in the lowest possible latency. The NLB provides\nthe necessary handling for the TCP and UDP traffic from the application.",
    "links": [
      "https://aws.amazon.com/global-accelerator/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an application that processes customer orders. The company hosts the application on an Amazon\nEC2 instance that saves the orders to an Amazon Aurora database. Occasionally when traffic is high the workload\ndoes not process orders fast enough.\nWhat should a solutions architect do to write the orders reliably to the database as quickly as possible?",
    "options": {
      "A": "Using SNS doesn't offer the benefits of queueing"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the best solution:\nThe problem describes a scenario where an EC2 instance hosting an application struggles to keep up with\nprocessing customer orders and writing them to an Aurora database during peak traffic. The goal is to reliably\nwrite orders to the database as quickly as possible, even under heavy load.\nOption B proposes using Amazon SQS as a buffer for incoming orders. SQS is a fully managed message\nqueuing service that allows decoupling the application from the database. Instead of writing directly to the\ndatabase, the EC2 instance quickly writes orders to the SQS queue. This immediately frees up the EC2\ninstance to handle more incoming requests.\nThe solution further suggests using an Auto Scaling group of EC2 instances behind an Application Load\nBalancer (ALB). These EC2 instances are responsible for reading messages from the SQS queue and\nprocessing them into the Aurora database. The ALB distributes the workload across these instances, ensuring\nhigh availability and scalability. The Auto Scaling group automatically adjusts the number of EC2 instances\nbased on the queue length or other performance metrics, dynamically scaling to meet demand.\nThis approach effectively decouples the order receiving and order processing components. SQS acts as a\nbuffer, preventing the database from being overwhelmed during peak traffic. The Auto Scaling group allows\nfor horizontal scaling of the processing capacity, ensuring that orders are processed and written to the\ndatabase as quickly as possible. The ALB ensures that the processing load is evenly distributed among the\navailable EC2 instances.\nOption A is incorrect because SNS is a publish/subscribe service, better suited for broadcasting notifications\nto multiple subscribers. It doesn't provide the buffering and guaranteed delivery that SQS offers. Directly\nsubscribing the database endpoint to an SNS topic is also generally not recommended due to potential issues\nwith high message volume and database overload.\nOption C is also incorrect for similar reasons as option A. Using SNS doesn't offer the benefits of queueing\nand guaranteed delivery that SQS provides.\nOption D introduces unnecessary complexity and relies on reactive scaling. Waiting for the EC2 instance to\nreach CPU threshold limits before queuing orders adds latency and doesn't proactively address the problem.\nScheduled scaling can be helpful in anticipating traffic patterns, but Auto Scaling based on queue length\noffers a more dynamic and responsive approach. Moreover, it ties the queuing decision to the original EC2\ninstance, defeating the purpose of decoupling.\nTherefore, option B provides the most robust and scalable solution for reliably writing orders to the database\nas quickly as possible under high traffic.\nAuthoritative Links:\nAmazon SQS: https://aws.amazon.com/sqs/\nAmazon EC2 Auto Scaling: https://aws.amazon.com/autoscaling/\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
    "links": [
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/autoscaling/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/"
    ]
  },
  {
    "question": "CertyIQ\nAn IoT company is releasing a mattress that has sensors to collect data about a users sleep. The sensors will send\ndata to an Amazon S3 bucket. The sensors collect approximately 2 MB of data every night for each mattress. The\ncompany must process and summarize the data for each mattress. The results need to be available as soon as\npossible. Data processing will require 1 GB of memory and will finish within 30 seconds.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "C",
    "explanation": "The question requires a cost-effective solution for processing IoT sleep data from mattresses, given\nconstraints on memory, processing time, and data volume. AWS Lambda emerges as the most suitable option\ndue to its serverless nature and cost model.\nLambda functions are billed based on execution time and memory consumed. Given the data processing\nrequirements of 1 GB of memory and a 30-second execution time, a Lambda function can readily handle this\nworkload. The small data size (2MB per mattress per night) is easily accommodated by Lambda.\nAWS Glue (using either Scala or PySpark) and Amazon EMR with Spark are designed for large-scale data\nprocessing. These services involve starting and maintaining clusters, which incurs higher costs even for small\ndatasets. While they offer powerful data transformation capabilities, they are overkill for this scenario. The\noverhead of managing a Glue or EMR cluster, even briefly, significantly increases costs compared to Lambda.\nPython is a perfectly acceptable language for Lambda. The prompt stated that the results need to be\navailable as soon as possible. While there is \"cold start\" latency with Lambda, the ability to automatically\ntrigger processing upon data arrival into S3 and the speed of Python are sufficient in this case.\nTherefore, AWS Lambda provides the most cost-effective solution because it avoids the overhead of cluster\nmanagement, scales automatically, and only charges for actual execution time and memory consumed.\nSupporting Links:\nAWS Lambda Pricing: https://aws.amazon.com/lambda/pricing/\nAWS Glue Pricing: https://aws.amazon.com/glue/pricing/\nAmazon EMR Pricing: https://aws.amazon.com/emr/pricing/",
    "links": [
      "https://aws.amazon.com/lambda/pricing/",
      "https://aws.amazon.com/glue/pricing/",
      "https://aws.amazon.com/emr/pricing/"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts an online shopping application that stores all orders in an Amazon RDS for PostgreSQL Single-\nAZ DB instance. Management wants to eliminate single points of failure and has asked a solutions architect to\nrecommend an approach to minimize database downtime without requiring any changes to the application code.\nWhich solution meets these requirements?",
    "options": {
      "A": "Converting the existing RDS for PostgreSQL Single-AZ instance to a Multi-AZ"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Converting the existing RDS for PostgreSQL Single-AZ instance to a Multi-AZ\ndeployment provides high availability without requiring application code changes. RDS Multi-AZ\nsynchronously replicates data to a standby instance in a different Availability Zone. If the primary instance\nfails, RDS automatically fails over to the standby instance. The connection endpoint remains the same,\neliminating the need for application modifications.\nOption B involves creating a new Multi-AZ deployment and restoring a snapshot. While functional, it\nintroduces a period of downtime during the restoration process, which isn't ideal for minimizing downtime.\nOption C, using a read replica and Route 53, is unsuitable because read replicas are asynchronous and\nprimarily for read scaling, not high availability. Route 53 weighted records are inappropriate for database\nfailover because they don't guarantee data consistency or transactional integrity. Furthermore, the\napplication needs to be able to handle potential read inconsistencies and direct writes to the appropriate\ndatabase.\nOption D, using EC2 Auto Scaling and Route 53, is not directly applicable to RDS managed databases. RDS is\na managed service that abstracts away the underlying infrastructure management. Also, directly managing\nthe database replication across instances in an Auto Scaling group is more complex than leveraging the built-\nin Multi-AZ feature of RDS.\nTherefore, Multi-AZ deployment in RDS provides the simplest and most efficient approach to achieve high\navailability for the database without application code changes.\nRefer to the AWS documentation for more information on RDS Multi-AZ deployments:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is developing an application to support customer demands. The company wants to deploy the\napplication on multiple Amazon EC2 Nitro-based instances within the same Availability Zone. The company also\nwants to give the application the ability to write to multiple block storage volumes in multiple EC2 Nitro-based\ninstances simultaneously to achieve higher application availability.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The requirement is to provide simultaneous write access to multiple EBS volumes from multiple EC2 Nitro-\nbased instances for high availability. Amazon EBS Multi-Attach allows attaching one EBS volume to multiple\nEC2 instances within the same Availability Zone.\nOption C, using io2 EBS volumes with EBS Multi-Attach, is the correct solution because io2 volumes are\nspecifically designed to support EBS Multi-Attach. Other volume types like gp2, gp3, and st1 either don't\nsupport Multi-Attach at all or have limitations when used with it. io2 volumes also offer the highest IOPS\n(Input/Output Operations Per Second) performance, which can be important when multiple instances are\nwriting to the same volume concurrently, as this increases the demand on I/O.\nOptions A, B and D are incorrect. gp2 and gp3 volumes support Multi-Attach, however, it is not advisable for\nhigh-availability applications that require concurrent write access because io2 is optimized for this use case.\nThe st1 volume type does not support the EBS Multi-Attach feature. The need for concurrent writes indicates\na requirement for high-performance shared storage, and io2 volumes are designed to meet such needs by\noffering high IOPS and low latency.\nRefer to the AWS documentation on EBS Multi-Attach and EBS volume types for more information:\nEBS Multi-Attach\nEBS Volume Types",
    "links": []
  },
  {
    "question": "CertyIQ\nA company designed a stateless two-tier application that uses Amazon EC2 in a single Availability Zone and an\nAmazon RDS Multi-AZ DB instance. New company management wants to ensure the application is highly available.\nWhat should a solutions architect do to meet this requirement?",
    "options": {},
    "answer": "A",
    "explanation": "The best way to achieve high availability for a stateless two-tier application running on EC2 with an RDS\nMulti-AZ DB instance is to distribute the EC2 instances across multiple Availability Zones (AZs) and use a load\nbalancer to distribute traffic.\nOption A is the correct answer because it implements this strategy. Multi-AZ EC2 Auto Scaling ensures that\nEC2 instances are launched and maintained in multiple AZs. If an AZ fails, Auto Scaling automatically\nlaunches new instances in a healthy AZ. The Application Load Balancer (ALB) distributes traffic across the\nhealthy EC2 instances in all available AZs. This setup ensures that the application remains available even if\none or more AZs experience an outage.\nOption B, while providing a backup strategy, doesn't provide immediate high availability. Snapshots can be\nused for disaster recovery, but restoring from snapshots takes time, causing application downtime.\nOption C, using Route 53 latency-based routing, improves performance by directing users to the closest\nregion but doesn't inherently guarantee high availability within a single region where the application is\ncurrently deployed.\nOption D, configuring Route 53 rules combined with a Multi-AZ ALB is incomplete. While the Multi-AZ ALB is a\ngood start, it needs the backing EC2 instances also deployed across multiple AZs using something like Auto\nScaling for full high availability within a region. Route 53 is best utilized for global failover between regions.\nTherefore, the combination of Multi-AZ EC2 Auto Scaling and an Application Load Balancer offers the most\nrobust solution for high availability in a single AWS Region.\nRelevant links for further reading:\nAWS Auto Scaling: https://aws.amazon.com/autoscaling/\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\nAWS Regions and Availability Zones: https://aws.amazon.com/about-aws/global-infrastructure/",
    "links": [
      "https://aws.amazon.com/autoscaling/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
      "https://aws.amazon.com/about-aws/global-infrastructure/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses AWS Organizations. A member account has purchased a Compute Savings Plan. Because of\nchanges in the workloads inside the member account, the account no longer receives the full benefit of the\nCompute Savings Plan commitment. The company uses less than 50% of its purchased compute power.",
    "options": {
      "B": "Turn on discount sharing from the Billing Preferences section of the account"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Turn on discount sharing from the Billing Preferences section of the account\nconsole in the company's Organizations management account.\nHere's why:\nAWS Organizations Discount Sharing: AWS Organizations allows for centralized management and billing of\nmultiple AWS accounts. One key benefit is discount sharing, where savings plans (like Compute Savings\nPlans) purchased by one account can be applied across all accounts within the organization. This maximizes\nthe utilization of the savings and reduces overall costs.\nCentralized Management Account: Discount sharing is configured at the management account (formerly\nknown as the master account) level within AWS Organizations, not at the individual member account level.\nThe management account acts as the central point for managing the organization and its billing.\nTurning on Discount Sharing: Within the management account's Billing Preferences, there's a setting to\nenable savings plans and reserved instance sharing. By activating this, the unused portion of the Compute\nSavings Plan's commitment from the member account can be applied to compute usage in other accounts\nwithin the organization.\nWhy other options are incorrect:\nA: Attempting to enable discount sharing within the member account that purchased the savings plan won't\nwork. The setting exists solely in the management account.\nC: While migrating additional workloads could utilize more of the compute power, it might not be feasible or\nstrategically aligned with the company's architecture. Also, it does not address the issue of sharing the saving\nif the loads are in the member account that holds the saving plan. Discount sharing is a more direct and\nefficient solution.\nD: Savings Plans cannot be sold on the Reserved Instance Marketplace. This Marketplace is for selling\nReserved Instances, not Savings Plans.\nIn summary, leveraging the discount sharing feature of AWS Organizations allows the company to maximize\nthe benefits of its Compute Savings Plan by automatically applying unused savings to other accounts within\nthe organization. This is configured in the Organizations management account's billing preferences.\nAuthoritative Links for further research:\nAWS Organizations Billing: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_getting-\nstarted_concepts.html\nSavings Plans Sharing within Organizations: https://docs.aws.amazon.com/savingsplans/latest/userguide/sp-\nsharing.html\nCompute Savings Plans: https://aws.amazon.com/savingsplans/compute-savings-plans/",
    "links": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_getting-",
      "https://docs.aws.amazon.com/savingsplans/latest/userguide/sp-",
      "https://aws.amazon.com/savingsplans/compute-savings-plans/"
    ]
  },
  {
    "question": "CertyIQ\nA company is developing a microservices application that will provide a search catalog for customers. The\ncompany must use REST APIs to present the frontend of the application to users. The REST APIs must access the\nbackend services that the company hosts in containers in private VPC subnets.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B because it aligns perfectly with the requirements of creating a REST API frontend for\naccessing backend microservices hosted in private subnets.\nHere's a breakdown of the justification:\nREST API Requirement: The problem explicitly states that the application must expose REST APIs. Amazon\nAPI Gateway can be configured to create RESTful APIs.\nContainerization in Private Subnets: The backend services are hosted in containers within private VPC\nsubnets. This means they are not directly accessible from the public internet. Amazon ECS is suitable to\nmanage the containers.\nPrivate VPC Link: To securely access the services in the private subnets from API Gateway, a Private VPC Link\nis the appropriate solution. It establishes a private connection between API Gateway and the ECS services,\nwithout exposing them to the internet. This improves security and reduces attack surface.\nSecurity Groups vs. VPC Link: While security groups are important for controlling traffic within the VPC, they\ndon't create a secure, direct connection between API Gateway and resources within the private subnet. VPC\nlinks are designed specifically for this purpose.\nWebSocket APIs: WebSocket APIs are used for real-time, bidirectional communication, which is not a\nrequirement in this scenario. REST APIs are appropriate for a simple request-response model that is suitable\nfor catalog search.\nTherefore, designing a REST API with API Gateway, hosting the application in Amazon ECS in private subnets,\nand creating a private VPC link is the solution that will meet all requirements.\nFurther Reading:\nAmazon API Gateway: https://aws.amazon.com/api-gateway/\nAmazon ECS: https://aws.amazon.com/ecs/\nAPI Gateway Private Integration:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-apis.html",
    "links": [
      "https://aws.amazon.com/api-gateway/",
      "https://aws.amazon.com/ecs/",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-apis.html"
    ]
  },
  {
    "question": "CertyIQ\nA company stores raw collected data in an Amazon S3 bucket. The data is used for several types of analytics on\nbehalf of the company's customers. The type of analytics requested determines the access pattern on the S3\nobjects.\nThe company cannot predict or control the access pattern. The company wants to reduce its S3 costs.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C: Use S3 Lifecycle rules to transition objects from S3 Standard to S3 Intelligent-\nTiering. Here's why:\nThe problem requires optimizing S3 costs for unpredictable access patterns. S3 Intelligent-Tiering is\ndesigned for this exact scenario. It automatically moves data between frequent, infrequent, and archive\naccess tiers based on usage patterns, with no operational overhead and no retrieval fees when accessing the\ndata.\nS3 Lifecycle rules enable automated transitioning of objects between storage classes based on defined\ncriteria. This allows moving objects from S3 Standard (a higher-cost, frequently accessed tier) to S3\nIntelligent-Tiering without manual intervention. The lifecycle rule dictates the transition, ensuring that data is\nplaced in the appropriate tier based on its access pattern.\nOption A, S3 Replication, is primarily for creating copies of data in different regions or buckets for disaster\nrecovery or data sovereignty. It doesn't directly address cost optimization based on access patterns within a\nsingle bucket.\nOption B, transitioning to S3 Standard-IA, is suitable for infrequently accessed data, but if the access pattern\nis unpredictable and the data sometimes needs to be accessed frequently, using Standard-IA will incur\nretrieval charges every time the infrequently accessed data is accessed, making it more expensive. S3\nIntelligent-Tiering is preferable because it automatically adapts to fluctuating access patterns.\nOption D, using S3 Inventory, helps identify infrequently accessed objects, but doesn't automatically\ntransition them. This still requires additional steps to move the data and introduces a manual intervention\ncomponent, while S3 Lifecycle rules automatically does the transition. S3 Inventory works well with S3\nLifecycle, and allows reporting on the success or failure of applying rules.\nIn summary, S3 Intelligent-Tiering paired with Lifecycle rules offers the most effective and automated\nsolution for cost optimization when dealing with unpredictable data access patterns on S3, ensuring that\nfrequently accessed data is readily available while minimizing storage costs for infrequently accessed data.\nAuthoritative Links:\nS3 Intelligent-Tiering: https://aws.amazon.com/s3/storage-classes/intelligent-tiering/\nS3 Lifecycle: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-management-\noverview.html",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/intelligent-tiering/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-management-"
    ]
  },
  {
    "question": "CertyIQ\nA company has applications hosted on Amazon EC2 instances with IPv6 addresses. The applications must initiate\ncommunications with other external applications using the internet. However the companys security policy states\nthat any external service cannot initiate a connection to the EC2 instances.\nWhat should a solutions architect recommend to resolve this issue?",
    "options": {
      "D": "Create an egress-only internet gateway and make it the destination of the subnet's"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Create an egress-only internet gateway and make it the destination of the subnet's\nroute table.\nHere's why:\nThe scenario requires EC2 instances with IPv6 addresses to initiate outbound connections to the internet\nwhile preventing inbound connections from the internet. A standard Internet Gateway (IGW) facilitates both\ninbound and outbound traffic, which violates the security policy. A NAT Gateway is designed for IPv4\naddresses, providing a mechanism for instances in a private subnet to connect to the internet. This is not\nsuitable for instances with IPv6 addresses. A Virtual Private Gateway (VGW) is used to connect a VPC to a\ncorporate data center using VPN or Direct Connect, which is irrelevant to this scenario.\nAn egress-only internet gateway (EIGW) is specifically designed for IPv6. It allows instances in a VPC to\ninitiate outbound connections over IPv6 to the internet but prevents the internet from initiating an IPv6\nconnection with the instances. By associating the EIGW with the subnet's route table as the destination for\nIPv6 traffic (::/0), outbound traffic from the EC2 instances will be routed through the EIGW, while preventing\nunsolicited inbound traffic. This approach aligns perfectly with the company's security policy.\nTherefore, the EIGW fulfills the requirement of enabling outbound-only IPv6 internet access for the EC2\ninstances while adhering to the specified security constraints.\nRelevant Documentation:\nEgress-Only Internet Gateway: https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-\ngateway.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-"
    ]
  },
  {
    "question": "CertyIQ\nA company is creating an application that runs on containers in a VP",
    "options": {
      "C": "Here's a detailed justification:",
      "B": "They are cost-effective because they"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Create a gateway VPC endpoint for Amazon S3. Associate this endpoint with all\nroute tables in the VPC.\nHere's a detailed justification:\nThe company needs to minimize costs and ensure that S3 traffic doesn't traverse the internet. A VPC endpoint\nallows secure and private connectivity between resources within your VPC and supported AWS services,\nincluding S3, without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect\nconnection. This addresses the requirement of avoiding internet traffic.\nGateway endpoints are specifically designed for S3 and DynamoDB. They are cost-effective because they\ndon't incur data transfer charges within the same AWS Region. The question prioritizes cost minimization,\nmaking the gateway endpoint a strong candidate.\nCreating a gateway VPC endpoint and associating it with all route tables in the VPC ensures that all\ncontainerized applications within the VPC automatically route their S3 traffic through the endpoint. This\nachieves the desired private connectivity to S3 for all workloads.\nOption A (S3 Intelligent-Tiering) optimizes storage costs based on access patterns but doesn't address the\nrequirement of keeping traffic within the AWS network. It's about storage tier optimization, not network\nrouting.\nOption B (S3 Transfer Acceleration) is designed to speed up data transfers to S3 using CloudFront's globally\ndistributed edge locations. While it can improve performance, it's not essential for keeping traffic private and\ncan potentially increase costs.\nOption D (Interface endpoint for S3) also provides private connectivity to S3, however, it's based on AWS\nPrivateLink, which can incur hourly and data processing charges. This is less cost-effective than a gateway\nendpoint, which has no hourly or data processing fees for S3 traffic within the same Region. Since the\nproblem emphasizes cost minimization, the gateway endpoint is the better choice. Interface endpoints are\nalso more complex to manage and are not generally the preferred method for S3 access within a VPC unless\nvery specific security requirements necessitate them. Gateway endpoints automatically scale horizontally to\nhandle network throughput.\nTherefore, a gateway VPC endpoint provides a cost-effective and simple solution for ensuring private S3\nconnectivity within the VPC, fulfilling both the cost and security requirements.\nReference Links:\nVPC Endpoints: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\nGateway VPC Endpoints: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html\nInterface VPC Endpoints (AWS PrivateLink): https://docs.aws.amazon.com/vpc/latest/userguide/vpc-\nendpoints-interface.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-"
    ]
  },
  {
    "question": "CertyIQ\nA company has a mobile chat application with a data store based in Amazon DynamoD",
    "options": {
      "B": "DAX, by caching frequently accessed data"
    },
    "answer": "A",
    "explanation": "The correct answer is A: Configure Amazon DynamoDB Accelerator (DAX) for the new messages table and\nupdate the code to use the DAX endpoint. Here's why:\nDynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB. Its\nprimary purpose is to reduce read latency for frequently accessed data, making it ideal for latency-sensitive\napplications like chat applications. DAX sits in front of DynamoDB and caches the results of read requests.\nSubsequent read requests for the same data are served directly from the cache, bypassing DynamoDB and\nresulting in significantly lower latency (often measured in microseconds).\nOption A requires minimal application changes because it involves only updating the application to use the\nDAX endpoint instead of directly connecting to DynamoDB for reads. This minimizes the development effort\nand disruption to existing code. DAX automatically manages cache invalidation and synchronization with\nDynamoDB, ensuring data consistency.\nOption B, adding DynamoDB read replicas, is not as suitable. While read replicas can help offload read traffic,\nthey introduce eventual consistency. This means that reads from the replica might not immediately reflect the\nlatest writes. For a chat application where users expect to see new messages instantly, eventual consistency\nis unacceptable. Furthermore, managing read replicas and ensuring failover can be more complex than using\nDAX.\nOption C, doubling read capacity units, may improve read throughput but doesn't address latency as\neffectively as DAX. Increasing capacity increases the resources available for read operations but still involves\nnetwork latency and the inherent overhead of querying DynamoDB. DAX, by caching frequently accessed data\nin memory, provides a much lower latency. Additionally, simply doubling read capacity might lead to increased\ncosts without necessarily achieving the desired low latency.\nOption D, adding Amazon ElastiCache for Redis, could provide low latency reads but would require significant\napplication changes. The application would need to implement caching logic, manage cache invalidation, and\nhandle synchronization between DynamoDB and Redis. This is a much more complex solution than using DAX.\nIt also introduces additional operational overhead for managing a separate caching cluster. DAX is purpose-\nbuilt for DynamoDB, so it offers better integration and requires significantly less development effort for the\napplication.\nIn summary, DAX offers the optimal balance of low latency, minimal application changes, and ease of\nmanagement for this use case.\nRelevant links:\nAmazon DynamoDB Accelerator (DAX)\nDynamoDB Read Replicas\nAmazon ElastiCache for Redis",
    "links": []
  },
  {
    "question": "CertyIQ\nA company hosts a website on Amazon EC2 instances behind an Application Load Balancer (ALB). The website\nserves static content. Website traffic is increasing, and the company is concerned about a potential increase in\ncost.",
    "options": {
      "B": "Add a rule to the web ACL to cache static files"
    },
    "answer": "A",
    "explanation": "The best solution to reduce costs while handling increasing website traffic for static content served from EC2\ninstances behind an ALB is option A: creating an Amazon CloudFront distribution.\nCloudFront is a content delivery network (CDN) that caches website content at edge locations closer to users.\nThis significantly reduces latency and improves user experience. More importantly, it reduces the load on the\norigin servers (EC2 instances), as CloudFront serves the cached static content directly to users without hitting\nthe EC2 instances. This reduces the compute resources needed to handle web traffic, resulting in decreased\nEC2 instance utilization and thus lowers costs. CloudFront's global network and caching capabilities are\npurpose-built for this scenario.\nOption B, using ElastiCache, is not ideal for caching static content. ElastiCache is primarily used for caching\ndynamic data that requires frequent updates, like database query results or session data. Connecting the ALB\nto ElastiCache would require significant code changes and may not be as effective for caching static files as\nCloudFront. Also, ElastiCache sits within the AWS region, so it does not have the benefit of edge locations.\nOption C, AWS WAF, is for web application firewalls and not optimized for caching static content. While WAF\ncan provide some caching capabilities, its primary focus is protecting against web exploits, not content\ndelivery optimization. Caching within WAF would be less efficient and more costly than using CloudFront.\nOption D, creating a second ALB in another region, primarily addresses reducing data transfer costs through\nregional proximity but it increases the overall cost as there are additional instances, ALB infrastructure, and\nmore resources required to maintain the other region. It doesn't address the issue of caching static assets\ndirectly.\nIn summary, CloudFront is the most cost-effective and efficient solution because it utilizes edge locations to\ncache static content, reduces the load on the origin servers, and improves website performance.\nRelevant Documentation:\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nAmazon ElastiCache: https://aws.amazon.com/elasticache/\nAWS WAF: https://aws.amazon.com/waf/",
    "links": [
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/elasticache/",
      "https://aws.amazon.com/waf/"
    ]
  },
  {
    "question": "CertyIQ\nA company has multiple VPCs across AWS Regions to support and run workloads that are isolated from workloads\nin other Regions. Because of a recent application launch requirement, the companys VPCs must communicate with\nall other VPCs across all Regions.\nWhich solution will meet these requirements with the LEAST amount of administrative effort?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C: Use AWS Transit Gateway to manage VPC communication in a single Region and\nTransit Gateway peering across Regions to manage VPC communications.\nHere's why:\nAWS Transit Gateway (TGW): TGW simplifies network architecture by acting as a central hub to connect\nmultiple VPCs within a Region. Instead of creating numerous point-to-point VPC peering connections, you\nconnect each VPC to the TGW, significantly reducing administrative overhead and\ncomplexity.https://aws.amazon.com/transit-gateway/\nTransit Gateway Peering: To extend connectivity across AWS Regions, you can use Transit Gateway peering.\nThis allows TGWs in different Regions to establish a connection, enabling VPCs in one Region to communicate\nwith VPCs in another Region through the interconnected TGWs. This avoids the need for individual inter-\nregion VPC peering connections.\nLeast Administrative Effort: TGW and TGW peering together dramatically reduce the administrative burden\ncompared to managing a full mesh of VPC peering connections across multiple Regions. The number of\nconnections you need to manage grows linearly with the number of Regions when using TGW peering, as\nopposed to exponentially when using VPC peering.\nWhy other options are incorrect:\nA (VPC Peering): While VPC peering establishes connections between VPCs, managing a full mesh of VPC\npeering connections across multiple Regions becomes extremely complex and administratively intensive as\nthe number of VPCs and Regions increases. It creates a complex web of connections to manage, which can be\nprone to errors.\nB (AWS Direct Connect Gateway): Direct Connect Gateway is used to connect on-premises networks to AWS,\nnot VPCs within AWS. While it can be part of a larger solution to connect on-premises and AWS, it's not\nsuitable for connecting VPCs in different regions directly and adds unnecessary complexity.\nD (AWS PrivateLink): PrivateLink is designed to provide private connectivity to AWS services or services\nhosted by other AWS customers. It's not meant for general VPC-to-VPC communication. Using it for this\npurpose would be an inefficient and complex workaround. Furthermore, PrivateLink focuses on providing a\nservice, not interconnecting multiple VPCs for general communication.https://aws.amazon.com/privatelink/",
    "links": [
      "https://aws.amazon.com/transit-gateway/",
      "https://aws.amazon.com/privatelink/"
    ]
  },
  {
    "question": "CertyIQ\nA company is designing a containerized application that will use Amazon Elastic Container Service (Amazon ECS).\nThe application needs to access a shared file system that is highly durable and can recover data to another AWS\nRegion with a recovery point objective (RPO) of 8 hours. The file system needs to provide a mount target m each\nAvailability Zone within a Region.\nA solutions architect wants to use AWS Backup to manage the replication to another Region.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C: Amazon Elastic File System (Amazon EFS) with the Standard storage class.\nHere's why:\nEFS and Mount Targets: EFS is designed to provide a shared file system that can be mounted concurrently\nfrom multiple EC2 instances or containers across multiple Availability Zones (AZs) within a Region. This\ndirectly addresses the requirement of providing a mount target in each AZ.\nDurability and Availability: EFS stores data redundantly across multiple AZs, offering high durability and\navailability.\nAWS Backup Integration: AWS Backup natively supports EFS, enabling automated backups and cross-region\nreplication for disaster recovery. This supports the requirement of using AWS Backup to manage replication\nto another region and meet the RPO.\nRPO: While the exact RPO is dependent on your backup schedule configuration in AWS Backup, it can be\nconfigured to achieve an 8-hour RPO for EFS.\nFSx for Windows File Server & FSx for NetApp ONTAP Limitations: While FSx for Windows File Server and\nFSx for NetApp ONTAP are suitable file systems for specific workloads, they are not the best fit for this\nscenario because they may have higher costs and complexity for simple file sharing across multiple AZs with\neasy AWS Backup integration. Also, FSx for Windows File Server is typically used for Windows-based\napplications.\nFSx for OpenZFS Limitations: Although FSx for OpenZFS offers high performance, it might be an overkill for\na general purpose containerized application needing just shared storage and durability.\nTherefore, Amazon EFS offers the simplest and most cost-effective solution for providing a highly durable,\nshared file system with multi-AZ access and easy integration with AWS Backup for cross-region replication\nand disaster recovery.\nRelevant Links:\nAmazon EFS: https://aws.amazon.com/efs/\nAWS Backup: https://aws.amazon.com/backup/\nCross-Region Backups with AWS Backup: https://aws.amazon.com/blogs/storage/enabling-cross-region-\nbackup-with-aws-backup/",
    "links": [
      "https://aws.amazon.com/efs/",
      "https://aws.amazon.com/backup/",
      "https://aws.amazon.com/blogs/storage/enabling-cross-region-"
    ]
  },
  {
    "question": "CertyIQ\nA company is expecting rapid growth in the near future. A solutions architect needs to configure existing users\nand grant permissions to new users on AWS. The solutions architect has decided to create IAM groups. The\nsolutions architect will add the new users to IAM groups based on department.\nWhich additional action is the MOST secure way to grant permissions to the new users?",
    "options": {},
    "answer": "C",
    "explanation": "The most secure way to grant permissions to new users added to IAM groups, given the scenario, is to create\nan IAM policy that grants least privilege permission and attach that policy to the IAM groups (Option C).\nHere's why:\nIAM Policies: IAM policies are JSON documents that define permissions. They specify what actions a user,\ngroup, or role can perform on AWS resources. Attaching a policy directly to an IAM group is a straightforward\nand effective way to define the permissions for all users within that group.\nLeast Privilege: Least privilege is a security principle that dictates granting users only the permissions they\nneed to perform their job duties. Creating a specific IAM policy tailored to each group and granting only the\nnecessary permissions minimizes the potential impact of compromised credentials.\nIAM Groups vs. Roles: IAM groups are collections of IAM users. They are a convenient way to manage\npermissions for multiple users who require the same level of access. IAM roles, on the other hand, are\nassumed by entities, not directly assigned to users. While roles are excellent for EC2 instances and other\nservices, they aren't the most direct way to manage access for users within groups.\nWhy other options are less secure:\nService Control Policies (SCPs) (Option A): SCPs are used at the AWS Organizations level to manage\npermissions across multiple AWS accounts. While valuable for central governance, they are not the best\napproach for granting specific permissions to IAM groups within a single account. SCPs define maximum\npermissions; IAM policies still need to be in place to grant the actual access.\nIAM Roles attached to Groups (Option B): You don't attach roles to groups. Roles are assumed by entities\n(like EC2 instances) or assumed by users (via the console or CLI). While a user can assume a role, it adds\nunnecessary complexity and is not the standard way to grant basic permissions to a group of users.\nPermissions Boundaries (Option D): Permissions boundaries define the maximum permissions that an IAM\nentity can have. While using them with Roles is a valid setup, a Role isn't the most fitting solution here.\nFurthermore, attaching a permissions boundary without also defining specific permissions within IAM policies\nwill result in no access.\nIn summary, attaching IAM policies that grant least privilege directly to IAM groups is the most direct,\nmanageable, and secure approach in this scenario. It follows the principle of least privilege while leveraging\nthe convenience of IAM groups for managing user permissions.\nReference:\nIAM Policies\nIAM Groups\nLeast Privilege\nIAM Roles",
    "links": []
  },
  {
    "question": "CertyIQ\nA group requires permissions to list an Amazon S3 bucket and delete objects from that bucket. An administrator\nhas created the following IAM policy to provide access to the bucket and applied that policy to the group. The\ngroup is not able to delete objects in the bucket. The company follows least-privilege access rules.\nWhich statement should a solutions architect add to the policy to correct bucket access?",
    "options": {
      "A": "B.",
      "C": "D."
    },
    "answer": "D",
    "explanation": "D for sure",
    "links": []
  },
  {
    "question": "CertyIQ\nA law firm needs to share information with the public. The information includes hundreds of files that must be\npublicly readable. Modifications or deletions of the files by anyone before a designated future date are prohibited.\nWhich solution will meet these requirements in the MOST secure way?",
    "options": {
      "B": "Let's dissect why it's the most secure solution for the law firm's requirements."
    },
    "answer": "B",
    "explanation": "The correct answer is B. Let's dissect why it's the most secure solution for the law firm's requirements.\nOption B leverages several AWS security features optimally. First, creating a new S3 bucket ensures a\ndedicated space for public-facing data, minimizing the risk of accidental exposure of other sensitive\ninformation. Enabling S3 Versioning is crucial because it preserves every version of an object, providing a\nbuilt-in audit trail and facilitating easy recovery if unintended changes occur. The cornerstone of this solution\nis S3 Object Lock. By using Object Lock with a retention period extending to the designated future date, the\nlaw firm prevents modification or deletion of the files, adhering strictly to the requirement of immutability.\nConfiguring the S3 bucket for static website hosting makes the files readily accessible to the public. Finally,\nsetting an S3 bucket policy to allow read-only access explicitly restricts write operations, further enhancing\nsecurity by limiting potential avenues for unauthorized modifications.\nOption A falls short in terms of true immutability. While read-only IAM permissions restrict who can directly\nwrite to the bucket, they don't protect against accidental or malicious deletion by authorized principals.\nOption C introduces unnecessary complexity and potential latency. Using Lambda functions triggered by\nobject modification/deletion adds an additional layer of processing that can increase response times and\nintroduces a potential point of failure. While the intention is good, the built-in capabilities of S3 Object Lock\nprovide a more direct and efficient solution. Also, storing original versions in a separate private bucket adds to\nmanagement overhead.\nOption D incorrectly assumes that Object Lock can be applied to folders within S3. Object Lock is applied at\nthe object level. This option also requires IAM policy to set read-only permissions, but it's not the most secure\noption since it doesn't prevent accidental deletion if user account compromised.\nTherefore, Option B is the most secure and efficient solution. It leverages built-in features of S3 to enforce\nimmutability, provide versioning for recovery, and grant read-only access, directly addressing all\nrequirements without unnecessary complexity.\nAuthoritative links for further research:\nS3 Object Lock: https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html\nS3 Versioning: https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html\nS3 Bucket Policies: https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-iam-policies.html\nS3 Static Website Hosting: https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-iam-policies.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is making a prototype of the infrastructure for its new website by manually provisioning the necessary\ninfrastructure. This infrastructure includes an Auto Scaling group, an Application Load Balancer and an Amazon\nRDS database. After the configuration has been thoroughly validated, the company wants the capability to\nimmediately deploy the infrastructure for development and production use in two Availability Zones in an\nautomated fashion.\nWhat should a solutions architect recommend to meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The best approach is to define the infrastructure as code using AWS CloudFormation. CloudFormation allows\nyou to create and manage AWS infrastructure as code, using templates written in YAML or JSON. This\napproach ensures consistency, repeatability, and version control.CloudFormation templates describe the\ndesired state of your infrastructure, including resources like Auto Scaling groups, Application Load Balancers,\nand RDS databases. The template serves as a blueprint for creating identical infrastructure in different\nenvironments or regions.Option B directly addresses the requirement to deploy the infrastructure in an\nautomated fashion into two Availability Zones. CloudFormation handles the provisioning of resources across\nmultiple Availability Zones based on the template's configuration.Option A, AWS Systems Manager, is\nprimarily designed for managing existing infrastructure rather than provisioning new infrastructure from\nscratch. While SSM can automate tasks, it's not the ideal choice for defining and deploying entire\ninfrastructure stacks.Option C, AWS Config, is used for assessing, auditing, and evaluating the configurations\nof your AWS resources. It records the configuration history of your resources but doesn't directly provision\ninfrastructure like CloudFormation.Option D, AWS Elastic Beanstalk, is a PaaS (Platform as a Service) offering\ndesigned for deploying and managing web applications. While it can deploy applications, it's not designed for\nmanaging the entire infrastructure stack as comprehensively as CloudFormation, especially custom\nconfigurations involving multiple components.CloudFormation is the most suitable solution because it enables\ninfrastructure as code, ensuring consistency, repeatability, and automated deployment across different\nenvironments and Availability Zones.\nHere are some links for further research:\nAWS CloudFormation: https://aws.amazon.com/cloudformation/\nCloudFormation Documentation: https://docs.aws.amazon.com/cloudformation/\nAWS Systems Manager: https://aws.amazon.com/systems-manager/\nAWS Config: https://aws.amazon.com/config/\nAWS Elastic Beanstalk: https://aws.amazon.com/elasticbeanstalk/",
    "links": [
      "https://aws.amazon.com/cloudformation/",
      "https://docs.aws.amazon.com/cloudformation/",
      "https://aws.amazon.com/systems-manager/",
      "https://aws.amazon.com/config/",
      "https://aws.amazon.com/elasticbeanstalk/"
    ]
  },
  {
    "question": "CertyIQ\nA business application is hosted on Amazon EC2 and uses Amazon S3 for encrypted object storage. The chief\ninformation security officer has directed that no application traffic between the two services should traverse the\npublic internet.\nWhich capability should the solutions architect use to meet the compliance requirements?",
    "options": {},
    "answer": "B",
    "explanation": "Here's a detailed justification for why VPC endpoints are the correct solution:\nThe requirement is to ensure that traffic between an EC2 instance and an S3 bucket does not traverse the\npublic internet.\nVPC Endpoints: VPC endpoints enable you to privately connect your VPC to supported AWS services and VPC\nendpoint services powered by PrivateLink without requiring an internet gateway, NAT device, VPN connection,\nor AWS Direct Connect connection. Endpoints are horizontally scaled, redundant, and highly available VPC\ncomponents that allow communication between instances in your VPC and services without imposing\navailability risks or bandwidth constraints on your network traffic. Specifically, using a Gateway Endpoint for\nS3 allows traffic to remain within the AWS network, bypassing the internet.\nAWS KMS (Option A): AWS KMS is for managing encryption keys. While it is important for securing data at\nrest and in transit, it doesn't directly prevent traffic from going over the public internet. It addresses\nencryption, not network routing.\nPrivate Subnet (Option C): A private subnet, by definition, does not have a route to an internet gateway.\nHowever, instances in a private subnet still require a NAT gateway or NAT instance to access S3 over the\npublic internet unless a VPC endpoint is configured. Simply placing the EC2 instance in a private subnet\ndoesn't solve the core problem.\nVirtual Private Gateway (Option D): A Virtual Private Gateway is used to establish a VPN connection between\nyour VPC and your on-premises network. This is irrelevant to the requirement of keeping traffic within the\nAWS network for communication between EC2 and S3.\nTherefore, the correct answer is VPC endpoints because they provide a private connection to S3, ensuring the\ntraffic stays within the AWS network and does not traverse the public internet, fulfilling the compliance\nrequirement.\nAuthoritative Links:\nVPC Endpoints: https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html\nGateway Endpoints for S3: https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html",
      "https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts a three-tier web application in the AWS Cloud. A Multi-AZAmazon RDS for MySQL server forms\nthe database layer Amazon ElastiCache forms the cache layer. The company wants a caching strategy that adds or\nupdates data in the cache when a customer adds an item to the database. The data in the cache must always\nmatch the data in the database.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The question requires a caching strategy that ensures data consistency between the database and the cache\nlayer for a three-tier web application on AWS. The desired behavior is that when data is added to the database\n(specifically, an item is added), the corresponding cache entry is immediately updated to reflect this change.\nOption B, implementing a write-through caching strategy, is the correct answer. A write-through cache\nupdates both the cache and the database simultaneously. When a customer adds an item (data) to the\ndatabase, the write-through cache mechanism will synchronously update the cache. This ensures that the\ncache always contains the most up-to-date information, matching the data in the database. This satisfies the\nrequirement of data consistency.\nOption A, lazy loading, is incorrect. Lazy loading caches data only when it is first requested. While it saves\nresources on initial data loading, it doesn't guarantee immediate cache updates after database modifications,\nleading to potential staleness.\nOption C, adding TTL (Time-to-Live), while a common caching optimization, is not a strategy itself but rather a\nparameter. Even with TTL, there's no guarantee the cache is updated immediately upon database changes.\nStale data could exist until the TTL expires.\nOption D, AWS AppConfig, is primarily a service for application configuration management, not a caching\nstrategy. While you could use it to control parameters related to caching, it doesn't inherently provide the\nsynchronous update mechanism required.\nTherefore, only the write-through caching strategy provides the guarantee of data consistency by updating\nthe cache immediately when the database is updated.\nFor more information on caching strategies:\nAmazon ElastiCache Strategies: https://aws.amazon.com/elasticache/\nCaching Strategies: https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside (though\nAzure-specific, the caching concepts apply generally)",
    "links": [
      "https://aws.amazon.com/elasticache/",
      "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to migrate 100 GB of historical data from an on-premises location to an Amazon S3 bucket. The\ncompany has a 100 megabits per second (Mbps) internet connection on premises. The company needs to encrypt\nthe data in transit to the S3 bucket. The company will store new data directly in Amazon S3.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "B",
    "explanation": "Here's a detailed justification of why option B is the best solution for migrating 100 GB of on-premises data to\nAmazon S3 with encryption in transit and minimal operational overhead, given the specified constraints:\nOption B: Use AWS DataSync\nDataSync is a fully managed data transfer service that simplifies, automates, and accelerates moving data\nbetween on-premises storage and AWS storage services. Crucially, DataSync encrypts data in transit using\nTLS, meeting the encryption requirement.\nEfficiency and Speed: DataSync utilizes a purpose-built transfer protocol designed to optimize data transfer\nover the internet or AWS Direct Connect. It automatically handles tasks such as data validation,\nchecksumming, and error recovery, reducing operational burden.\nEncryption in Transit: DataSync natively encrypts data during transit, fulfilling the requirement to encrypt\ndata moving to the S3 bucket.\nSimplicity and Automation: DataSync simplifies the migration process through its agent-based architecture.\nYou deploy an agent in your on-premises environment, configure it to connect to your S3 bucket, and then\ndefine a transfer task. It automates the transfer process and provides monitoring capabilities.\nNo Hardware Logistics: Unlike AWS Snowball, DataSync doesn't involve shipping physical devices,\neliminating associated logistical overhead and delays.\nWhy other options are less suitable:\nOption A (s3 sync with AWS CLI): While functional, s3 sync lacks the optimization features of DataSync,\nleading to slower transfer speeds and increased operational overhead. It requires more manual intervention to\nmonitor and restart transfers.\nOption C (AWS Snowball): Snowball is overkill for only 100 GB of data. The turnaround time for shipping a\nSnowball device can be significantly longer than using DataSync over a 100 Mbps connection. It also\nintroduces logistical complexity.\nOption D (IPsec VPN + s3 cp with AWS CLI): Setting up and maintaining an IPsec VPN introduces significant\noperational overhead. While it secures the connection, it doesn't provide the optimized transfer capabilities of\nDataSync. s3 cp shares the same drawbacks as s3 sync in terms of optimization and automation.\nCalculations & Network Limitations:\nTransferring 100GB (800Gb) over a 100Mbps connection would take approximately 22.2 hours at maximum\ntheoretical speed. DataSync's features to optimize this, it becomes the best option.\nConclusion:\nAWS DataSync provides the best balance of speed, security, and simplicity for migrating 100 GB of data to S3\nover a 100 Mbps connection. It eliminates the complexity of VPNs and physical devices while ensuring\nencryption in transit.\nSupporting Documentation:\nAWS DataSync: https://aws.amazon.com/datasync/\nDataSync Encryption: https://docs.aws.amazon.com/datasync/latest/userguide/security.html",
    "links": [
      "https://aws.amazon.com/datasync/",
      "https://docs.aws.amazon.com/datasync/latest/userguide/security.html"
    ]
  },
  {
    "question": "CertyIQ\nA company containerized a Windows job that runs on .NET 6 Framework under a Windows container. The company\nwants to run this job in the AWS Cloud. The job runs every 10 minutes. The jobs runtime varies between 1 minute\nand 3 minutes.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "C": "Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate"
    },
    "answer": "C",
    "explanation": "The most cost-effective solution is C. Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate\nto run the job. Create a scheduled task based on the container image of the job to run every 10 minutes.\nHere's why:\nCost-Effectiveness: Fargate allows you to pay only for the compute resources your container uses, without\nmanaging the underlying infrastructure. For a short-running job executed every 10 minutes, this is more cost-\neffective than provisioning and maintaining EC2 instances.\nScheduled Tasks with ECS: ECS provides built-in scheduling capabilities to run tasks on a defined schedule\nusing services like Amazon CloudWatch Events (now EventBridge), making option C a straightforward solution\nto the problem requirements.\nContainerization Compatibility: ECS Fargate readily supports running containerized applications, including\nWindows containers, fulfilling the requirement that the job is already containerized.\nNo need for external schedulers: Option D requires the use of Windows task scheduler, which is not a native\ncloud scheduler for AWS services and requires an external dependency.\nComparing to Lambda (Option A): AWS Lambda generally has a maximum execution time limit that could be\nan issue if the job's runtime occasionally approaches or exceeds that limit. Furthermore, while Lambda\nsupports container images, it's generally optimized for event-driven, stateless functions, not periodic tasks\nthat require more persistent processing.\nIn contrast:\nAWS Batch (Option B), while suitable for batch workloads, adds unnecessary complexity for a simple\nscheduled task that requires consistent intervals.\nLambda (Option A) AWS Lambda's maximum execution time limit and general optimization for event-driven\nstateless functions makes it less suitable for periodic tasks that require persistent processing.\nWindows task scheduler (Option D) requires external dependencies.\nTherefore, ECS on Fargate with scheduled tasks provides the most straightforward, cost-effective, and\nmanageable solution.\nSupporting Resources:\nAmazon ECS Scheduled Tasks\nAWS Fargate Pricing\nAWS Lambda Function Configuration",
    "links": []
  },
  {
    "question": "CertyIQ\nA company wants to move from many standalone AWS accounts to a consolidated, multi-account architecture. The\ncompany plans to create many new AWS accounts for different business units. The company needs to\nauthenticate access to these AWS accounts by using a centralized corporate directory service.\nWhich combination of actions should a solutions architect recommend to meet these requirements? (Choose two.)",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is AE. Here's why:\nA: Create a new organization in AWS Organizations with all features turned on. Create the new AWS\naccounts in the organization. AWS Organizations is designed to centrally manage and govern multiple AWS\naccounts. Creating an organization provides a hierarchical structure to manage permissions and policies\nacross accounts, which is essential for a consolidated, multi-account architecture. With all features enabled,\nyou can leverage service control policies (SCPs) for centralized governance.\nhttps://aws.amazon.com/organizations/\nE: Set up AWS IAM Identity Center (AWS Single Sign-On) in the organization. Configure IAM Identity\nCenter, and integrate it with the company's corporate directory service. AWS IAM Identity Center\n(successor to AWS SSO) enables you to centrally manage access to multiple AWS accounts and applications.\nIntegrating it with your corporate directory service (like Active Directory) allows users to use their existing\ncredentials to access AWS resources. This centralizes authentication and simplifies user management across\nthe organization. https://aws.amazon.com/iam/identity-center/\nWhy other options are incorrect:\nB: Set up an Amazon Cognito identity pool. Configure AWS IAM Identity Center (AWS Single Sign-On) to\naccept Amazon Cognito authentication. Cognito Identity Pools are primarily used to grant temporary AWS\ncredentials to users of mobile and web applications. While Cognito can integrate with identity providers, it's\nnot the direct and preferred method for integrating a corporate directory service with multiple AWS accounts\nin a multi-account architecture, compared to IAM Identity Center.\nC: Configure a service control policy (SCP) to manage the AWS accounts. Add AWS IAM Identity Center\n(AWS Single Sign-On) to AWS Directory Service. While SCPs are important for governance, you don't add\nIAM Identity Center to AWS Directory Service. Instead, you integrate IAM Identity Center with the directory\nservice, configuring IAM Identity Center to use the directory as its identity source. This integration is managed\nwithin IAM Identity Center, not the directory service itself. You cannot simply \"add\" IAM Identity Center in\nsuch a manner.\nD: Create a new organization in AWS Organizations. Configure the organization's authentication mechanism\nto use AWS Directory Service directly. AWS Organizations itself doesn't have a direct authentication\nmechanism to directly integrate with a directory service. The authentication is facilitated via a service like\nAWS IAM Identity Center. Organizations provides the framework to manage the accounts, and IAM Identity\nCenter handles the authentication across them.",
    "links": [
      "https://aws.amazon.com/organizations/",
      "https://aws.amazon.com/iam/identity-center/"
    ]
  },
  {
    "question": "CertyIQ\nA company is looking for a solution that can store video archives in AWS from old news footage. The company\nneeds to minimize costs and will rarely need to restore these files. When the files are needed, they must be\navailable in a maximum of five minutes.\nWhat is the MOST cost-effective solution?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A because it offers the most cost-effective storage option for infrequently accessed\ndata that needs to be retrieved within five minutes. Amazon S3 Glacier is designed for archiving data at a very\nlow cost.\nHere's a detailed breakdown:\nCost-Effectiveness: S3 Glacier is significantly cheaper than S3 Standard-IA and S3 One Zone-IA for storage.\nThe company's primary requirement is to minimize costs, making Glacier a strong contender.\nRetrieval Time: The key differentiator is the retrieval time requirement of \"a maximum of five minutes.\" While\nGlacier has different retrieval options, \"Expedited retrievals\" allow for access to data within 1-5 minutes.\nWhy not B? Standard retrievals in S3 Glacier can take several hours (3-5 hours), which violates the five-\nminute requirement.\nWhy not C? S3 Standard-IA is more expensive than S3 Glacier for storage. While retrieval times are faster by\ndefault, the cost is not justified for archives rarely needed.\nWhy not D? S3 One Zone-IA is cheaper than S3 Standard-IA but still more expensive than S3 Glacier.\nAdditionally, S3 One Zone-IA stores data in only one Availability Zone, which makes it less durable than S3\nGlacier. Durability is implicitly desired when archiving old, important footage, and using Glacier is a better\npractice for data longevity.\nIn summary, Amazon S3 Glacier with Expedited retrievals perfectly balances the need for low-cost storage\nwith the specific requirement of quick access when needed.\nAuthoritative Links for further research:\nAmazon S3 Glacier: https://aws.amazon.com/glacier/\nS3 Storage Classes: https://aws.amazon.com/s3/storage-classes/\nS3 Glacier Retrieval Options: https://docs.aws.amazon.com/AmazonS3/latest/userguide/retrieving-objects-\nglacier.html",
    "links": [
      "https://aws.amazon.com/glacier/",
      "https://aws.amazon.com/s3/storage-classes/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/retrieving-objects-"
    ]
  },
  {
    "question": "CertyIQ\nA company is building a three-tier application on AWS. The presentation tier will serve a static website The logic\ntier is a containerized application. This application will store data in a relational database. The company wants to\nsimplify deployment and to reduce operational costs.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Here's why:"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's why:\nStatic Website Hosting: Amazon S3 is a cost-effective and scalable solution for hosting static websites. It\noffers high availability and durability without requiring server management. https://aws.amazon.com/s3/\nContainerized Application: AWS Fargate allows you to run containers without managing the underlying EC2\ninstances. It simplifies deployment and reduces operational overhead, perfectly aligning with the requirement\nto reduce operational costs. Amazon Elastic Container Service (ECS) is a fully managed container\norchestration service that supports Fargate. https://aws.amazon.com/fargate/\nRelational Database: Amazon RDS provides managed relational database services, reducing the operational\nburden of database administration. It automates tasks like patching, backups, and scaling.\nhttps://aws.amazon.com/rds/\nLet's look at why the other options are less ideal:\nOption B: Using Amazon CloudFront solely for static content is overkill. While CloudFront is a great CDN for\ncaching and distributing content globally, it primarily adds value on top of a static storage solution like S3.\nFor a basic static website, S3 alone is usually sufficient and more cost-effective. Also, while ECS with EC2 is\nvalid, Fargate is preferable for reducing operational costs.\nOption C: Amazon EKS (Kubernetes) is generally more complex than ECS, especially when deploying it with\nFargate. While EKS with Fargate is a perfectly acceptable option for microservice architectures and\napplications requiring portability, it might be an unnecessary complexity layer for a simple three-tier\napplication. Choosing ECS simplifies setup and management.\nOption D: Using Amazon EC2 Reserved Instances for static content is the least optimal. EC2 instances require\nconstant management, and they are not designed to serve static content, while S3 is. Also, EKS with EC2\nincreases complexity and operational overhead compared to ECS with Fargate.",
    "links": [
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/fargate/",
      "https://aws.amazon.com/rds/"
    ]
  },
  {
    "question": "CertyIQ\nA company seeks a storage solution for its application. The solution must be highly available and scalable. The\nsolution also must function as a file system be mountable by multiple Linux instances in AWS and on premises\nthrough native protocols, and have no minimum size requirements. The company has set up a Site-to-Site VPN for\naccess from its on-premises network to its VP",
    "options": {
      "C": "Which storage solution meets these requirements?"
    },
    "answer": "C",
    "explanation": "The correct answer is C: Amazon Elastic File System (Amazon EFS) with multiple mount targets. Here's why:\nAmazon EFS is designed to provide a scalable, elastic, and highly available network file system. Crucially, EFS\nsupports multiple Linux instances mounting the file system concurrently. This directly addresses the\nrequirement for a file system mountable by multiple Linux instances in AWS and on-premises. EFS seamlessly\nintegrates with a Site-to-Site VPN, enabling access from the on-premises network. EFS is also elastic,\nmeaning its storage capacity automatically grows and shrinks as you add and remove files, thus fulfilling the\n\"no minimum size requirements\" criterion.\nMultiple mount targets in different Availability Zones (AZs) within a region enhance availability. If one AZ\nexperiences an issue, instances in other AZs can still access the file system.\nOption A, Amazon FSx Multi-AZ deployments, although highly available, is designed for Windows file servers\nor Lustre, not general-purpose file systems mountable by Linux instances using standard file protocols across\non-premises and cloud environments.\nOption B, Amazon Elastic Block Store (Amazon EBS) Multi-Attach volumes, allows attaching one EBS volume\nto multiple instances. However, it's restricted to specific Nitro-based instances and, importantly, doesn't offer\ndirect support for accessing the file system from on-premises environments via Site-to-Site VPN. It's also\nblock storage and doesn't behave as a network file system inherently.\nOption D, Amazon Elastic File System (Amazon EFS) with a single mount target and multiple access points,\nwhile utilizing EFS, a single mount target would create a single point of failure and not provide the necessary\nhigh availability across Availability Zones, which is crucial. While access points enhance security and simplify\naccess management, they do not replace the need for multiple mount targets for availability.\nTherefore, only Amazon EFS with multiple mount targets provides the required scalability, availability, file\nsystem functionality, and on-premises access via Site-to-Site VPN, satisfying all the conditions outlined in the\nscenario.\nFurther reading:\nAmazon EFS: https://aws.amazon.com/efs/\nEFS Mount Targets: https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-mount-targets.html\nEFS Access from On-premises: https://docs.aws.amazon.com/efs/latest/ug/accessing-fs-nfs-vpc.html",
    "links": [
      "https://aws.amazon.com/efs/",
      "https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-mount-targets.html",
      "https://docs.aws.amazon.com/efs/latest/ug/accessing-fs-nfs-vpc.html"
    ]
  },
  {
    "question": "CertyIQ\nA 4-year-old media company is using the AWS Organizations all features feature set to organize its AWS accounts.\nAccording to the company's finance team, the billing information on the member accounts must not be accessible\nto anyone, including the root user of the member accounts.\nWhich solution will meet these requirements?",
    "options": {
      "C": "Create a service control policy (SCP) to deny access to the billing information."
    },
    "answer": "C",
    "explanation": "The correct answer is C. Create a service control policy (SCP) to deny access to the billing information.\nAttach the SCP to the root organizational unit (OU).\nHere's why:\nService Control Policies (SCPs) are the ideal mechanism to enforce organization-wide governance in AWS\nOrganizations. They act as guardrails, defining the maximum permissions that member accounts within an OU\ncan have. Even the root user of a member account cannot bypass restrictions imposed by an SCP applied to\nthe OU they belong to. This makes SCPs the only option to reliably restrict access to billing information, even\nfor root users.\nOption A is incorrect because assigning finance team users to an IAM group and attaching an AWS managed\npolicy would grant them access, not restrict it.\nOption B is incorrect because IAM policies within a member account cannot override restrictions enforced by\nSCPs at the organizational level. The root user could potentially modify or detach the policy.\nOption D, switching to the consolidated billing feature set, does not, by itself, restrict access to billing\ninformation within member accounts. It just consolidates billing for the organization into the management\naccount. The consolidated billing setup does not offer control over who views billing data within member\naccounts. The all features set is required to use SCPs, so the idea of converting to the consolidated billing\nfeature set is antithetical to the problem at hand.\nAttaching the SCP to the root OU ensures that the restriction applies to all accounts within the organization\n(unless explicitly overridden in a lower-level OU, which wouldn't meet the requirements). By denying access to\nbilling information through an SCP, the organization ensures compliance with the finance team's requirement\nthat member accounts' billing data remains inaccessible. This is essential for maintaining financial control and\npreventing unauthorized access within the AWS environment.\nFurther reading:\nAWS Organizations documentation on SCPs:\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\nAWS Organizations features: https://aws.amazon.com/organizations/features/",
    "links": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html",
      "https://aws.amazon.com/organizations/features/"
    ]
  },
  {
    "question": "CertyIQ\nAn ecommerce company runs an application in the AWS Cloud that is integrated with an on-premises warehouse\nsolution. The company uses Amazon Simple Notification Service (Amazon SNS) to send order messages to an on-\npremises HTTPS endpoint so the warehouse application can process the orders. The local data center team has\ndetected that some of the order messages were not received.\nA solutions architect needs to retain messages that are not delivered and analyze the messages for up to 14 days.\nWhich solution will meet these requirements with the LEAST development effort?",
    "options": {
      "B": "Additionally, configuring TTL attributes and ensuring"
    },
    "answer": "C",
    "explanation": "The best solution to retain undelivered SNS messages for up to 14 days with the least development effort is to\nuse a dead-letter queue (DLQ) backed by Amazon Simple Queue Service (SQS).\nHere's why:\nDead-Letter Queues (DLQs): SNS dead-letter queues are specifically designed for handling undeliverable\nmessages. When SNS fails to deliver a message to a subscriber after retries, it can be configured to send the\nmessage to a DLQ. This ensures that messages are not lost and can be analyzed later.\nAmazon SQS: SQS is a fully managed message queuing service that allows you to decouple and scale\nmicroservices, distributed systems, and serverless applications. SQS offers message retention periods of up\nto 14 days, which directly fulfills the requirement.\nLeast Development Effort: SNS natively supports configuring DLQs with SQS as the target. This is a\nconfiguration-based solution, requiring minimal code development. You simply configure the SNS\nsubscription with the DLQ ARN.\nLet's analyze why other options are less suitable:\nOption A (Kinesis Data Streams): While Kinesis Data Streams can store data, it is more suited for real-time\ndata processing and analytics. Using it solely for retaining undelivered messages adds unnecessary\ncomplexity. The primary purpose of Kinesis is not message queuing with retention for analysis like SQS.\nOption B (SQS queue between application and SNS): This option doesn't directly address the SNS dead-\nletter queue concept. It introduces an intermediary queue, which could complicate the message flow and\nrequire changes to the application that sends messages to SNS. It's not a direct solution to the problem of\nmessages failing delivery from SNS to the on-premises endpoint.\nOption D (DynamoDB with TTL): DynamoDB with TTL (Time To Live) could technically store the messages, but\nit involves more development effort. You would need to write code to take the messages from the SNS failure\nnotification, format them, and store them in DynamoDB. Additionally, configuring TTL attributes and ensuring\nproper data formatting adds overhead. Storing messages intended for queuing in a database seems\nunnatural.\nIn summary, option C leverages native AWS features (SNS DLQ and SQS) to provide a simple, configuration-\nbased solution for message retention with minimal development effort. SQS queue retention period and SNS\nDLQ configurations are straightforward within the AWS console or using Infrastructure as Code (IaC).\nAuthoritative Links:\nAmazon SNS Dead-Letter Queues: https://docs.aws.amazon.com/sns/latest/dg/sns-attribute-delivery-\npolicy.html\nAmazon SQS Message Retention:\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-message-\nretention-period.html",
    "links": [
      "https://docs.aws.amazon.com/sns/latest/dg/sns-attribute-delivery-",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-message-"
    ]
  },
  {
    "question": "CertyIQ\nA gaming company uses Amazon DynamoDB to store user information such as geographic location, player data,\nand leaderboards. The company needs to configure continuous backups to an Amazon S3 bucket with a minimal\namount of coding. The backups must not affect availability of the application and must not affect the read capacity\nunits (RCUs) that are defined for the table.\nWhich solution meets these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B because it directly leverages built-in DynamoDB features for backups with minimal\ncoding and no impact on application availability or RCUs. Let's break down why the other options are not as\nsuitable:\nOption A (EMR/Hive): Using EMR and Hive is overkill. It requires significant setup, configuration, and coding\nto create the Hive job and manage the cluster. This introduces complexity and operational overhead that can\nbe avoided. https://aws.amazon.com/emr/\nOption C (DynamoDB Streams/Lambda): DynamoDB Streams captures item-level changes. While it can be\nused for data replication, it's not designed for full table backups. Using Lambda to consume the stream and\nexport data to S3 requires custom coding to assemble the full table state and handle potential\ninconsistencies. Also, data stored in S3 might not be consistent with the table state.\nhttps://aws.amazon.com/dynamodb/streams/\nOption D (Lambda/Periodic Export): While Lambda can be used for data export, scheduling it on a regular\nbasis might impact the table's performance and RCU consumption, depending on the frequency and size of\nthe data being exported. This method requires custom coding and careful monitoring to avoid impacting\napplication availability. Also, it only provides point-in-time recovery for the time when you scheduled backups\nand is not truly continuous. https://aws.amazon.com/lambda/\nOption B leverages DynamoDB's native export functionality directly to S3 with continuous backups using\nPoint-in-Time Recovery (PITR).\nExport to S3: DynamoDB allows you to export a table's data directly to an S3 bucket. This functionality is\nmanaged by DynamoDB and avoids impacting the table's RCU.\nPoint-in-Time Recovery (PITR): PITR allows you to restore your DynamoDB table to any point in time during\nthe preceding 35 days. This provides continuous backup capabilities because it continuously maintains\nbackup data. Turning on PITR provides automatic and continuous backups without affecting application\nperformance. It does not consume RCUs.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html and\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/S3Export.html\nMinimal Coding: Turning on PITR and starting an export to S3 requires minimal configuration through the\nAWS Management Console, AWS CLI, or SDK. It does not require writing custom code.\nIn summary, option B offers the simplest, most efficient, and most reliable way to back up DynamoDB data to\nS3 without impacting application availability or RCUs by utilizing the built-in DynamoDB features for export\nand PITR.",
    "links": [
      "https://aws.amazon.com/emr/",
      "https://aws.amazon.com/dynamodb/streams/",
      "https://aws.amazon.com/lambda/",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/S3Export.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is designing an asynchronous application to process credit card data validation requests for a\nbank. The application must be secure and be able to process each request at least once.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A is the most cost-effective solution for processing credit card\ndata validation requests asynchronously, securely, and at least once, using Lambda and SQS:\nKey Requirements and Considerations:\nAsynchronous Processing: Decoupling the request origin from the processing is essential for scalability and\nresponsiveness. SQS enables this.\nSecurity: Protecting sensitive credit card data requires encryption at rest.\nAt-Least-Once Delivery: Guarantees that each request is processed, even in failure scenarios.\nCost-Effectiveness: Minimizing operational expenses is a crucial design goal.\nWhy Option A is the Best Choice:\nOption A leverages SQS standard queues with AWS Lambda event source mapping, offering a balance of\nfunctionality and cost. Let's break down the components:\n1. SQS Standard Queues: Standard queues provide high throughput and best-effort ordering, which is\nsuitable for credit card validation as the order of individual validations isn't as critical as the speed\nand ability to process all requests. Standard queues are generally cheaper than FIFO queues.\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-queue-\ntypes.html\n2. AWS Lambda Event Source Mapping: This feature directly connects the SQS queue to the Lambda\nfunction. Lambda automatically polls the queue for messages and invokes the function whenever\nmessages are available. This eliminates the need for custom polling logic or additional services.\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\n3. SSE-KMS Encryption: Server-Side Encryption with KMS provides robust encryption at rest using\nkeys managed by AWS Key Management Service (KMS). This ensures that sensitive credit card data\nis protected while stored in the SQS queue.\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-\nsse.html\n4. kms:Decrypt Permission: Granting the Lambda execution role the kms:Decrypt permission allows the\nLambda function to decrypt messages encrypted with SSE-KMS. This is essential for the Lambda\nfunction to process the encrypted data.\nhttps://docs.aws.amazon.com/kms/latest/developerguide/services-sqs.html\nWhy Other Options are Less Suitable:\nOptions B and C (FIFO Queues): FIFO (First-In-First-Out) queues guarantee message ordering, but come with\nhigher cost and lower throughput than standard queues. Ordering is not a critical requirement, using FIFO\nadds unnecessary cost. The at-least-once delivery guarantee can still be maintained with standard queues\nusing retry mechanisms and dead-letter queues (DLQs).\nOption B (SSE-SQS): SSE-SQS uses SQS managed encryption keys, which is simpler to set up but offers less\ncontrol and flexibility compared to SSE-KMS. SSE-KMS is preferred for sensitive data because you can\nmanage key rotation, access control, and auditing policies more granularly. Additionally, the permission in the\nlambda execution role to allow KMS Decryption.\nOption D: Option D describes using KMS but grants \"encryption key invocation permission\" to the Lambda\nfunction. Instead, it must be kms:Decrypt permission for the Lambda execution role, as the Lambda role\nperforms decryption.\nCost Considerations:\nStandard queues are generally more cost-effective than FIFO queues due to their higher throughput and less\nstringent ordering requirements. While both SSE-SQS and SSE-KMS incur encryption costs, the additional\ncontrol and auditing capabilities of SSE-KMS are often justified for sensitive data. Lambda event source\nmapping is a cost-effective way to integrate SQS with Lambda because it eliminates the need for constant\npolling or a separate polling service.\nIn summary, Option A provides the necessary security, delivery guarantees, and asynchronous processing\ncapabilities in a cost-optimized manner by leveraging SQS standard queues with SSE-KMS encryption and\nLambda event source mapping with the correct KMS decryption permission.",
    "links": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-queue-",
      "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-",
      "https://docs.aws.amazon.com/kms/latest/developerguide/services-sqs.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has multiple AWS accounts for development work. Some staff consistently use oversized Amazon EC2\ninstances, which causes the company to exceed the yearly budget for the development accounts. The company\nwants to centrally restrict the creation of AWS resources in these accounts.\nWhich solution will meet these requirements with the LEAST development effort?",
    "options": {},
    "answer": "B",
    "explanation": "The most efficient solution to centrally restrict EC2 instance creation in multiple AWS development accounts\ninvolves using AWS Organizations and Service Control Policies (SCPs). AWS Organizations allows you to\norganize multiple AWS accounts into organizational units (OUs). By applying an SCP to an OU, you can restrict\nthe actions that IAM users and roles in the member accounts can perform.\nIn this case, you can create an SCP that denies the creation of EC2 instances beyond a specific instance type\nsize or family. This centralized control mechanism ensures that developers within the development accounts\ncannot launch oversized EC2 instances that exceed the budget. This approach minimizes development effort\nsince SCPs are declarative policies that are easily defined and attached to OUs without requiring custom\ncoding or complex infrastructure. Options A, C, and D require more development and maintenance effort\ncompared to leveraging the native capabilities of AWS Organizations and SCPs for centralized governance.\nWhile Systems Manager templates (Option A) and Service Catalog (Option D) can enforce instance type\nchoices, they don't provide central control across multiple accounts as effectively as Organizations. Option C,\nusing EventBridge and Lambda, is reactive and requires continuous monitoring and updates, making it less\nefficient than SCPs. SCPs are proactively applied, preventing non-compliant instances from being created in\nthe first place.\nAuthoritative links:\nAWS Organizations: https://aws.amazon.com/organizations/\nService Control Policies (SCPs):\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html",
    "links": [
      "https://aws.amazon.com/organizations/",
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to use artificial intelligence (AI) to determine the quality of its customer service calls. The\ncompany currently manages calls in four different languages, including English. The company will offer new\nlanguages in the future. The company does not have the resources to regularly maintain machine learning (ML)\nmodels.\nThe company needs to create written sentiment analysis reports from the customer service call recordings. The\ncustomer service call recording text must be translated into English.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D, E, and F. Here's why:\nD: Use Amazon Transcribe to convert the audio recordings in any language into text. Amazon Transcribe is a\nspeech-to-text service that can handle multiple languages. It's designed to transcribe audio from customer\nservice calls into text, a crucial first step for sentiment analysis. This aligns directly with the requirement of\nconverting call recordings into text, regardless of the language. https://aws.amazon.com/transcribe/\nE: Use Amazon Translate to translate text in any language to English. Since the requirement is to translate\nthe call recording text into English for sentiment analysis, Amazon Translate is the appropriate service. It\noffers automated translation from various source languages into the target language (English). This\naddresses the company's need to handle calls in multiple languages and future expansion into new\nlanguages. https://aws.amazon.com/translate/\nF: Use Amazon Comprehend to create the sentiment analysis reports. Amazon Comprehend is a natural\nlanguage processing (NLP) service that can perform sentiment analysis on text. The translated text from the\ncall recordings can be fed into Comprehend, which will analyze the sentiment (positive, negative, neutral) and\ngenerate the required reports. Comprehend requires no ML model maintenance, satisfying that constraint.\nhttps://aws.amazon.com/comprehend/\nNow, let's explain why the other options are incorrect:\nA: Use Amazon Comprehend to translate the audio recordings into English. Comprehend is a text-based NLP\nservice, not an audio translation service. It can't directly translate audio; it requires text as input.\nB: Use Amazon Lex to create the written sentiment analysis reports. Amazon Lex is a service for building\nconversational interfaces (chatbots). While Lex can integrate with other services to perform sentiment\nanalysis, it's not directly designed for generating sentiment analysis reports from call recordings.\nC: Use Amazon Polly to convert the audio recordings into text. Amazon Polly is a text-to-speech service,\nwhich performs the opposite function of what is needed. It converts text into audio, rather than audio into text.",
    "links": [
      "https://aws.amazon.com/transcribe/",
      "https://aws.amazon.com/translate/",
      "https://aws.amazon.com/comprehend/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses Amazon EC2 instances to host its internal systems. As part of a deployment operation, an\nadministrator tries to use the AWS CLI to terminate an EC2 instance. However, the administrator receives a 403\n(Access Denied) error message.\nThe administrator is using an IAM role that has the following IAM policy attached:\nWhat is the cause of the unsuccessful request?",
    "options": {},
    "answer": "D",
    "explanation": "The request to terminate the EC2 instance does not originate from the CIDR blocks 192.0.2.0/24 or\n203.0.113.0/24.",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is conducting an internal audit. The company wants to ensure that the data in an Amazon S3 bucket\nthat is associated with the companys AWS Lake Formation data lake does not contain sensitive customer or\nemployee data. The company wants to discover personally identifiable information (PII) or financial information,\nincluding passport numbers and credit card numbers.\nWhich solution will meet these requirements?",
    "options": {
      "C": "Configure Amazon Macie"
    },
    "answer": "C",
    "explanation": "The best solution for discovering sensitive data within an S3 bucket linked to AWS Lake Formation,\nspecifically PII and financial information like passport and credit card numbers, is C. Configure Amazon Macie\nto run a data discovery job that uses managed identifiers for the required data types.\nHere's why:\nAmazon Macie is a fully managed data security and data privacy service that uses machine learning and\npattern matching to discover and protect sensitive data stored in Amazon S3. It is designed for this specific\npurpose. https://aws.amazon.com/macie/\nMacie provides pre-defined managed identifiers for common types of PII and financial data, including credit\ncard numbers, passport numbers, social security numbers, and more. This allows for out-of-the-box detection\ncapabilities tailored to the company's requirements.\nMacies data discovery jobs scan the contents of S3 objects and identify instances of sensitive data based on\nthe configured managed identifiers. It can handle various file formats and storage classes.\nMacie integrates with other AWS services, enabling automated remediation actions based on its findings.\nOption A is incorrect because AWS Audit Manager is primarily used for compliance auditing against\npredefined frameworks, not for specific data discovery within an S3 bucket. PCI DSS is relevant if the data is\nrelated to payment card information, but Audit Manager does not directly scan the S3 bucket contents for PII\nlike Macie does.\nOption B is incorrect because Amazon S3 Inventory provides a list of objects in the bucket with metadata\n(size, modification date, etc.). Athena can then be used to query the inventory, but the inventory data does not\ninclude the contents of the files. Thus, it would not be able to detect PII inside files.\nOption D is incorrect because Amazon S3 Select allows querying the contents of S3 objects using SQL. While\ntechnically possible, it would require a significant amount of manual SQL query crafting and maintenance to\nidentify all potential PII patterns, compared to Macies managed approach. It also requires knowledge of the\ndata schemas stored within the S3 bucket.\nLake Formation provides a central repository for data catalog and access controls, but it doesn't offer built-in\nsensitive data discovery. Macie's discovery integrates well with S3 buckets associated with Lake Formation\ndata lakes. Macie directly analyzes the data at rest to identify patterns that match sensitive data, making it\nthe most effective and efficient choice.",
    "links": [
      "https://aws.amazon.com/macie/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses on-premises servers to host its applications. The company is running out of storage capacity. The\napplications use both block storage and NFS storage. The company needs a high-performing solution that\nsupports local caching without re-architecting its existing applications.\nWhich combination of actions should a solutions architect take to meet these requirements? (Choose two.)",
    "options": {},
    "answer": "B",
    "explanation": "The company's requirements include addressing storage capacity issues for both block and NFS storage on-\npremises, achieving high performance with local caching, and minimizing application re-architecting.\nOption B, deploying an AWS Storage Gateway file gateway to replace NFS storage, directly addresses the\nneed for NFS storage expansion. A file gateway allows on-premises applications to access data stored in\nAmazon S3 as NFS file shares. Critically, it provides a local cache, which improves performance by storing\nfrequently accessed data locally, thereby reducing latency. This is a core function of file gateways.\n[https://aws.amazon.com/storagegateway/file-gateway/]\nOption D, deploying an AWS Storage Gateway volume gateway to replace block storage, directly addresses\nthe need for block storage expansion. A volume gateway enables on-premises applications to access cloud-\nbased block storage volumes as iSCSI devices. Like the file gateway, it provides a local cache for frequently\naccessed data. This ensures that applications experience low-latency access for frequently used data while\noffloading infrequently used data to the cloud. The volume gateway comes in two flavors - cached volumes\nwhich stores all data in S3 and copies a cache of frequently accessed data locally. This ensures local low-\nlatency and Stored Volumes - keeps the entire data copy locally, backed up asynchronously to AWS. Since\nthe question specifies the need for high performance that supports local caching, the Cached Volumes is\nimplicitly implied. [https://aws.amazon.com/storagegateway/volume-gateway/]\nOption A is incorrect because directly mounting Amazon S3 as a file system to on-premises servers typically\ndoesn't offer the high-performance local caching required. Although tools exist to achieve this, they often\nintroduce complexity. Option C is inappropriate. AWS Snowball Edge is primarily for large-scale data\nmigrations and edge computing and not generally used for continuous NFS mounts. Option E is incorrect\nbecause while Amazon EFS can be mounted on-premises via AWS Direct Connect or VPN, it does not offer the\nlocal caching mechanism needed for high performance in this scenario. Also it doesn't address block storage\nrequirements.",
    "links": [
      "https://aws.amazon.com/storagegateway/file-gateway/]",
      "https://aws.amazon.com/storagegateway/volume-gateway/]"
    ]
  },
  {
    "question": "CertyIQ\nA company has a service that reads and writes large amounts of data from an Amazon S3 bucket in the same AWS\nRegion. The service is deployed on Amazon EC2 instances within the private subnet of a VP",
    "options": {
      "C": "The service"
    },
    "answer": "C",
    "explanation": "The most cost-effective solution to reduce data output costs for EC2 instances accessing S3 within the same\nAWS Region is to use a VPC gateway endpoint for S3. VPC gateway endpoints allow EC2 instances in private\nsubnets to access S3 directly, bypassing the NAT gateway and the associated data transfer charges.\nOption A is incorrect because while using a dedicated EC2 NAT instance might offer slightly more control, it\ndoesn't eliminate data transfer charges and introduces management overhead.\nOption B is incorrect because placing a NAT instance in the private subnet would not solve the problem of\ndata transfer costs and would violate common best practices.\nOption D is incorrect because provisioning a second NAT gateway would double the NAT gateway costs and\nwouldn't address the data transfer charges between EC2 and S3.\nOption C, provisioning a VPC gateway endpoint, is the best solution. VPC gateway endpoints are free to use\n(you only pay for the S3 storage and requests), and they provide a direct, secure connection to S3 without\ntraversing the public internet. This eliminates data transfer charges associated with using a NAT gateway and\nreduces overall costs. The route table configuration directs S3-bound traffic through the endpoint, ensuring a\ndirect connection.\nHere are some authoritative links for further research:\nVPC Endpoints: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\nNAT Gateways: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html"
    ]
  },
  {
    "question": "CertyIQ\nA company uses Amazon S3 to store high-resolution pictures in an S3 bucket. To minimize application changes,\nthe company stores the pictures as the latest version of an S3 object. The company needs to retain only the two\nmost recent versions of the pictures.\nThe company wants to reduce costs. The company has identified the S3 bucket as a large expense.\nWhich solution will reduce the S3 costs with the LEAST operational overhead?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A: Use S3 Lifecycle to delete expired object versions and retain the two most recent\nversions. Here's why:\nS3 Lifecycle policies are designed specifically to automate the management of objects over their lifetime,\nincluding transitioning them to different storage classes or deleting them after a specified period or based on\nversioning rules. In this scenario, the requirement is to reduce S3 costs by retaining only the two most recent\nversions of the pictures while minimizing operational overhead. S3 Lifecycle rules can be configured to\nautomatically delete noncurrent versions of objects, allowing you to specify the number of recent versions to\nkeep. This directly addresses the requirement.\nOption B (Lambda function) would involve writing and maintaining custom code, which increases operational\noverhead and complexity. While Lambda is powerful, it's overkill for a task that S3 Lifecycle is designed to\nhandle. Option C (S3 Batch Operations) is suitable for performing large-scale batch operations on S3 objects.\nWhile it could technically be used to delete older versions, it involves more setup and is less efficient for\ncontinuous version management compared to Lifecycle policies. Option D (deactivating versioning) would\ncompletely remove the ability to retain multiple versions, which is contrary to the requirement of keeping the\ntwo most recent versions. Therefore, it is not a suitable option.\nS3 Lifecycle policies are built into S3 and require minimal configuration. They automatically and continuously\nmanage object versions, resulting in the least operational overhead. Lifecycle rules can be configured from\nthe S3 Management Console or using the AWS CLI/SDKs, making the process simple and straightforward. By\nusing S3 Lifecycle rules to delete older versions, the company can automatically reduce storage costs\nwithout continuous manual intervention.\nIn conclusion, S3 Lifecycle is the most efficient and cost-effective solution for managing object versions in S3\nwhile minimizing operational overhead.\nAuthoritative Links:\nAmazon S3 Lifecycle documentation\nManaging your storage lifecycle",
    "links": []
  },
  {
    "question": "CertyIQ\nA company needs to minimize the cost of its 1 Gbps AWS Direct Connect connection. The company's average\nconnection utilization is less than 10%. A solutions architect must recommend a solution that will reduce the cost\nwithout compromising security.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D because it directly addresses the company's need to minimize cost while maintaining\nsecurity and connectivity.\nHere's why:\nCost Reduction: The company's average utilization is only 10% of the 1 Gbps connection, indicating significant\nover-provisioning. Reducing the connection speed to 200 Mbps aligns the bandwidth with actual usage,\nleading to lower costs.\nAWS Direct Connect Partner for Hosted Connection: Contacting an AWS Direct Connect Partner is\nnecessary to obtain a hosted connection. AWS directly provides dedicated connections (1 Gbps and above),\nbut for lower speeds, you must work through a partner.\nHosted Connection Advantage: Hosted connections are generally more cost-effective for lower bandwidth\nrequirements compared to dedicated connections. The partner handles the physical infrastructure and shares\nthe cost among multiple customers.\nSecurity Maintenance: Switching to a 200 Mbps hosted connection doesn't inherently compromise security.\nSecurity is managed through Virtual Private Gateways (VGWs), Direct Connect Gateways, and VPC\nconfigurations, which remain independent of the physical connection speed.\nExisting AWS Account: Using an existing AWS account simplifies the setup and integration process. No new\naccounts or complex cross-account configurations are needed.\nWhy other options are incorrect:\nA: Setting up another 1 Gbps connection and sharing it doesn't reduce costs; it duplicates them. Sharing also\nadds complexity and potential security concerns.\nB: Setting up a 200 Mbps connection directly in the AWS Management Console isn't possible; AWS only\nprovides higher speed connections directly, lower speeds are available through partners.\nC: Similar to A, setting up another 1 Gbps connection doesn't solve the over-provisioning issue. Sharing with\nanother AWS account introduces complexity. While AWS Direct Connect Partners provide connections, a 1\nGbps connection isn't needed when the average utilization is 10%.\nIn summary: Answer D provides the optimal balance between cost reduction and security by scaling the\nconnection speed down to match utilization through a cost-effective hosted connection obtained via an AWS\nDirect Connect Partner.\nSupporting Links:\nAWS Direct Connect Pricing: https://aws.amazon.com/directconnect/pricing/ (Illustrates cost differences\nbased on connection speed)\nAWS Direct Connect Partners: https://aws.amazon.com/directconnect/partners/\nAWS Direct Connect FAQs: https://aws.amazon.com/directconnect/faqs/ (Clarifies dedicated vs. hosted\nconnections)",
    "links": [
      "https://aws.amazon.com/directconnect/pricing/",
      "https://aws.amazon.com/directconnect/partners/",
      "https://aws.amazon.com/directconnect/faqs/"
    ]
  },
  {
    "question": "CertyIQ\nA company has multiple Windows file servers on premises. The company wants to migrate and consolidate its files\ninto an Amazon FSx for Windows File Server file system. File permissions must be preserved to ensure that access\nrights do not change.\nWhich solutions will meet these requirements? (Choose two.)",
    "options": {
      "B": "The AWS CLI copy to the"
    },
    "answer": "A",
    "explanation": "The question requires selecting solutions for migrating on-premises Windows file servers to Amazon FSx for\nWindows File Server while preserving file permissions.\nOption A: Deploy AWS DataSync agents on premises. Schedule DataSync tasks to transfer the data to the\nFSx for Windows File Server file system. This is a valid option because AWS DataSync is specifically designed\nfor online data transfer between on-premises storage and AWS storage services, including FSx for Windows\nFile Server. DataSync preserves file metadata and permissions during the transfer, fulfilling the requirement.\nDeploying agents on-premises allows DataSync to access the file servers and transfer the data efficiently.\nOption D: Order an AWS Snowcone device. Connect the device to the on-premises network. Launch AWS\nDataSync agents on the device. Schedule DataSync tasks to transfer the data to the FSx for Windows File\nServer file system. This is also a valid option, especially when dealing with limited network bandwidth or a\nneed to minimize network impact during the migration. Snowcone is a small, ruggedized, and secure edge\ncomputing and data transfer device. By launching DataSync agents directly on Snowcone, the data transfer\nprocess can be accelerated by minimizing the reliance on the network connectivity. DataSync, as in Option A,\nmaintains permissions. The Snowcone would need to be connected to the network.\nWhy other options are incorrect:\nOption B: Copying shares to S3 using the AWS CLI and then using DataSync is not the correct approach\nbecause the native file share structure and NTFS permissions would be lost during the S3 transfer. S3 is an\nobject storage, not a file system. The conversion to an object storage format would discard the metadata\nrequired.\nOption C: Shipping drives to AWS for import into S3 is not the correct approach for the same reason as option\nB. Removing the drives from the file servers and shipping them is also likely to be extremely disruptive and\nimpractical. It won't preserve the file permissions and requires physical handling.\nOption E: This option is also not appropriate for the same reasons as Option B. The AWS CLI copy to the\nSnowball and subsequent transfer to S3 results in the loss of file permissions and NTFS metadata.\nFurthermore, it is an unnecessary step.\nKey Concepts:\nAWS DataSync: An online data transfer service that simplifies, automates, and accelerates the secure\nmovement of data between on-premises storage and AWS storage services.\nAmazon FSx for Windows File Server: A fully managed native Microsoft Windows file system built on\nWindows Server.\nAWS Snowcone: A small, rugged, and secure edge computing and data transfer device.\nFile Permissions Preservation: Maintaining the original access control settings (NTFS permissions) of files\nduring migration.\nAuthoritative Links:\nAWS DataSync: https://aws.amazon.com/datasync/\nAmazon FSx for Windows File Server: https://aws.amazon.com/fsx/windows/\nAWS Snowcone: https://aws.amazon.com/snowcone/",
    "links": [
      "https://aws.amazon.com/datasync/",
      "https://aws.amazon.com/fsx/windows/",
      "https://aws.amazon.com/snowcone/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to ingest customer payment data into the company's data lake in Amazon S3. The company\nreceives payment data every minute on average. The company wants to analyze the payment data in real time.\nThen the company wants to ingest the data into the data lake.\nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": {},
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the most operationally efficient solution for ingesting and\nanalyzing real-time payment data into an S3 data lake:\nOption C: Amazon Kinesis Data Firehose and Amazon Kinesis Data Analytics\nKinesis Data Firehose: Kinesis Data Firehose is designed specifically for loading streaming data into data\nlakes and data stores like S3. It's fully managed, requiring minimal administration. It automatically scales to\nhandle the data volume and provides built-in features for data transformation, compression, and encryption. It\ncan deliver directly to S3 with configurable buffering, which optimizes costs and performance.\nhttps://aws.amazon.com/kinesis/data-firehose/\nKinesis Data Analytics: Kinesis Data Analytics is a powerful service for real-time data stream processing. It\nallows you to write SQL queries or use Apache Flink to analyze streaming data as it arrives. It provides low-\nlatency results and integrates seamlessly with other AWS services. https://aws.amazon.com/kinesis/data-\nanalytics/\nWhy other options are less optimal:\nOption A (Kinesis Data Streams & Lambda): While Kinesis Data Streams can ingest data, using Lambda for\nreal-time analysis for every payment event might be expensive and less efficient than Kinesis Data Analytics.\nLambda functions have execution time limitations, and managing concurrency and scaling can be complex for\ncontinuous, high-volume streams. Lambda doesn't inherently integrate with S3 for data lake population.\nhttps://aws.amazon.com/lambda/ https://aws.amazon.com/kinesis/data-streams/\nOption B (Glue & Kinesis Data Analytics): AWS Glue is primarily for ETL (Extract, Transform, Load) operations\nand data cataloging, not real-time data ingestion. While Kinesis Data Analytics is suitable for analyzing data\nonce ingested, Glue would not satisfy the requirement to have data ingested on a minute-by-minute basis.\nGlue is more suited for batch processing. https://aws.amazon.com/glue/\nOption D (API Gateway & Lambda): API Gateway is designed to manage APIs, not stream high volumes of\ndata directly into a data lake. Using API Gateway to forward every payment to Lambda and then to S3 would\nadd unnecessary overhead and cost. API Gateway is not meant for ingestion of high velocity data.\nhttps://aws.amazon.com/api-gateway/\nIn summary:\nOption C provides the most streamlined and cost-effective approach. Kinesis Data Firehose efficiently\nhandles the ingestion of streaming data into S3, and Kinesis Data Analytics provides a platform for real-time\nprocessing, meeting both real-time analysis and data lake population requirements with minimal operational\noverhead. The managed nature of these services reduces the operational burden on the company.",
    "links": [
      "https://aws.amazon.com/kinesis/data-firehose/",
      "https://aws.amazon.com/kinesis/data-",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/kinesis/data-streams/",
      "https://aws.amazon.com/glue/",
      "https://aws.amazon.com/api-gateway/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a website that uses a content management system (CMS) on Amazon EC2. The CMS runs on a\nsingle EC2 instance and uses an Amazon Aurora MySQL Multi-AZ DB instance for the data tier. Website images are\nstored on an Amazon Elastic Block Store (Amazon EBS) volume that is mounted inside the EC2 instance.\nWhich combination of actions should a solutions architect take to improve the performance and resilience of the\nwebsite? (Choose two.)",
    "options": {
      "C": "Move the website images onto an Amazon Elastic File System (Amazon EFS) file system that is mounted"
    },
    "answer": "C",
    "explanation": "The correct answer is C and E. Here's why:\nC. Move the website images onto an Amazon Elastic File System (Amazon EFS) file system that is mounted\non every EC2 instance.\nPerformance and Scalability: Storing images on a single EBS volume tied to a single EC2 instance creates a\nsingle point of failure and performance bottleneck. EFS provides a shared file system that can be\nsimultaneously accessed by multiple EC2 instances. This allows the website to scale horizontally and improve\nperformance by distributing the image serving load across multiple servers.\nResilience: If the original EC2 instance fails, the images are still available because they are stored on EFS,\nwhich is designed for high availability and durability. EFS replicates data across multiple Availability Zones.\nShared Storage: EFS is designed for scenarios where multiple instances need to access the same data.\nhttps://aws.amazon.com/efs/\nE. Create an Amazon Machine Image (AMI) from the existing EC2 instance. Use the AMI to provision new\ninstances behind an Application Load Balancer as part of an Auto Scaling group. Configure the Auto\nScaling group to maintain a minimum of two instances. Configure an Amazon CloudFront distribution for\nthe website.\nResilience: An Auto Scaling group with a minimum of two instances ensures that the website remains\navailable even if one instance fails. The Application Load Balancer distributes traffic across healthy instances.\nPerformance and Scalability: The Application Load Balancer distributes incoming traffic across multiple EC2\ninstances, improving response times and handling increased load. Auto Scaling allows the website to\nautomatically scale up or down based on traffic demands.\nContent Delivery: CloudFront caches website content, including images, at edge locations around the world.\nThis reduces latency for users and offloads traffic from the origin server (EC2 instances).\nhttps://aws.amazon.com/cloudfront/\nAMI for Consistent Deployment: Creating an AMI from the existing instance ensures that new instances are\nprovisioned with the same configuration and software as the original instance, reducing the risk of\ninconsistencies. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\nWhy other options are less suitable:\nA: Mounting an S3 bucket directly on EC2 instances is generally not recommended for frequently accessed\nfiles like website images due to performance limitations. S3 is object storage, not a file system.\nB: NFS shares are complex to manage and can become single points of failure. EFS is a managed service and\nmuch simpler.\nD: While creating an AMI and using an ALB with an ASG is good, configuring an accelerator does not address\ncontent delivery to end users as well as a CDN.",
    "links": [
      "https://aws.amazon.com/efs/",
      "https://aws.amazon.com/cloudfront/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an infrastructure monitoring service. The company is building a new feature that will enable the\nservice to monitor data in customer AWS accounts. The new feature will call AWS APIs in customer accounts to\ndescribe Amazon EC2 instances and read Amazon CloudWatch metrics.\nWhat should the company do to obtain access to customer accounts in the MOST secure way?",
    "options": {},
    "answer": "A",
    "explanation": "The most secure way for the monitoring service to access customer AWS accounts is through IAM roles with\ntrust policies. Option A proposes that customers create an IAM role in their account granting read-only access\nto EC2 and CloudWatch, with a trust policy that allows the company's AWS account to assume the role. This\napproach, known as cross-account access using IAM roles, is the best practice for granting permissions\nbetween AWS accounts without sharing long-term credentials. The customer retains control over the\npermissions granted to the monitoring service and can revoke access at any time by modifying or deleting the\nIAM role. The monitoring service uses the AWS Security Token Service (STS) to assume the role, receiving\ntemporary credentials that are used to access resources in the customer account.\nOption B is less suitable because it involves creating and managing a serverless API to vend temporary\ncredentials. While this may provide temporary credentials, it adds unnecessary complexity and introduces a\npotential point of failure and security risk. Option C is highly insecure because it involves creating IAM users\nin customer accounts and storing their long-term access keys, which presents a significant security risk if the\nkeys are compromised. Option D introduces Amazon Cognito, which is typically used for authenticating users,\nnot for cross-account access in this scenario. Cognito does not provide a mechanism for allowing access to\nAWS resources in a different account using credentials.\nTherefore, Option A provides the most secure and efficient method to access customer accounts because it\nadheres to the principle of least privilege and utilizes IAM roles for cross-account access, eliminating the\nneed to share long-term credentials.\nRelevant links:\nIAM Roles for cross-account access\nAWS STS",
    "links": []
  },
  {
    "question": "CertyIQ\nA company needs to connect several VPCs in the us-east-1 Region that span hundreds of AWS accounts. The\ncompany's networking team has its own AWS account to manage the cloud network.\nWhat is the MOST operationally efficient solution to connect the VPCs?",
    "options": {
      "C": "In summary, Transit Gateway provides the most efficient and scalable solution for connecting numerous VPCs"
    },
    "answer": "C",
    "explanation": "The most operationally efficient solution for connecting hundreds of VPCs across multiple AWS accounts in\nthe us-east-1 Region, with a centralized networking team managing the cloud network, is to use AWS Transit\nGateway (TGW).\nOption C is correct because TGW acts as a hub, simplifying VPC connectivity. Instead of creating numerous\npeer-to-peer connections, each VPC attaches to the TGW. This significantly reduces the management\noverhead and complexity associated with VPC peering, which would quickly become unmanageable with\nhundreds of VPCs, and the associated route table maintenance (as stated in option A). TGW is designed for\nthis type of use case. It is a central hub in the networking team's account simplifies network management and\nrouting across the many VPCs that can be connected via the TGW attachment. Route tables within the TGW\nmanage the traffic between the VPCs.\nOption A is not operationally efficient. Managing hundreds of VPC peering connections and constantly\nupdating associated route tables introduces considerable overhead and a high risk of misconfiguration. The\nadministrative burden scales quadratically with the number of VPCs.\nOption B, using NAT gateways and internet gateways to connect VPCs over the internet, is insecure and\ninefficient. This exposes traffic to the public internet and does not leverage the private networking\ncapabilities within AWS. Also, egress costs would be significant.\nOption D, utilizing VPN gateways and a transit VPC, is an older approach compared to Transit Gateway and\nadds complexity. While a transit VPC can centralize VPN connections, it requires managing instances and\nrouting within the transit VPC, which increases operational burden, and introduces potential bottlenecks. TGW\nprovides a more scalable and managed solution than a transit VPC.\nIn summary, Transit Gateway provides the most efficient and scalable solution for connecting numerous VPCs\nacross many AWS accounts because it centralizes network management, simplifies routing, and reduces\noperational complexity compared to VPC peering, internet-based connections, or transit VPCs.\nAuthoritative Links:\nAWS Transit Gateway: https://aws.amazon.com/transit-gateway/\nAWS Transit Gateway Documentation: https://docs.aws.amazon.com/transit-gateway/index.html",
    "links": [
      "https://aws.amazon.com/transit-gateway/",
      "https://docs.aws.amazon.com/transit-gateway/index.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has Amazon EC2 instances that run nightly batch jobs to process data. The EC2 instances run in an\nAuto Scaling group that uses On-Demand billing. If a job fails on one instance, another instance will reprocess the\njob. The batch jobs run between 12:00 AM and 06:00 AM local time every day.\nWhich solution will provide EC2 instances to meet these requirements MOST cost-effectively?",
    "options": {
      "C": "Create a new launch template for the Auto Scaling group. Set the",
      "A": "Savings Plan: While Savings Plans offer discounts, they commit you to a consistent usage level over a",
      "B": "Reserved Instances: Similar to Savings Plans, Reserved Instances commit you to a specific instance type",
      "D": "Increasing Instance Size: Increasing the instance size will lead to higher costs for the entire 6 hour run"
    },
    "answer": "C",
    "explanation": "The most cost-effective solution is C. Create a new launch template for the Auto Scaling group. Set the\ninstances to Spot Instances. Set a policy to scale out based on CPU usage.\nHere's why:\nSpot Instances: Spot Instances offer significant cost savings (up to 90% compared to On-Demand) by utilizing\nspare EC2 capacity. The nightly batch job processing, which tolerates interruptions (as evidenced by the job\nreprocessing mechanism), is a perfect use case for Spot Instances. If a Spot Instance is terminated due to\nprice fluctuations, another instance will automatically launch to reprocess the failed job, maintaining\nreliability.\nLaunch Template: Using a launch template simplifies instance configuration and allows easy updates to the\nAuto Scaling group.\nScaling Policy based on CPU: Scaling out based on CPU usage ensures that instances are launched only\nwhen needed to handle the batch job's workload, optimizing resource utilization and minimizing costs. Scaling\npolicies linked to CPU utilization offer direct and adaptive responses to workload demands.\nWhy other options are less ideal:\nA. Savings Plan: While Savings Plans offer discounts, they commit you to a consistent usage level over a\nlonger period. Since the batch jobs only run for 6 hours each night, the commitment for the other 18 hours\ncould lead to wasted resources.\nB. Reserved Instances: Similar to Savings Plans, Reserved Instances commit you to a specific instance type\nand operating system for a longer period. The risk of underutilization is also present, although less than the\nsavings plan, given the batch job's limited duration.\nD. Increasing Instance Size: Increasing the instance size will lead to higher costs for the entire 6 hour run\ntime. Scaling out based on CPU with the correct sized instances offers a more granular approach to costs.\nAuthoritative Links:\nSpot Instances: https://aws.amazon.com/ec2/spot/\nAuto Scaling Groups: https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html\nLaunch Templates: https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-templates.html\nSavings Plan: https://aws.amazon.com/savingsplans/\nReserved Instances: https://aws.amazon.com/ec2/pricing/reserved-instances/\nIn summary, leveraging Spot Instances within an Auto Scaling group, configured through a launch template,\nand scaling based on CPU usage provides the most cost-effective approach for the given scenario because it\naligns with the application's tolerance for interruption and the specific time window of operation.",
    "links": [
      "https://aws.amazon.com/ec2/spot/",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-templates.html",
      "https://aws.amazon.com/savingsplans/",
      "https://aws.amazon.com/ec2/pricing/reserved-instances/"
    ]
  },
  {
    "question": "CertyIQ\nA social media company is building a feature for its website. The feature will give users the ability to upload\nphotos. The company expects significant increases in demand during large events and must ensure that the\nwebsite can handle the upload traffic from users.\nWhich solution meets these requirements with the MOST scalability?",
    "options": {},
    "answer": "C",
    "explanation": "The most scalable solution for handling user photo uploads in a social media application with expected surges\nin traffic is option C: generating Amazon S3 presigned URLs. Here's why:\nDirect Upload to S3: Presigned URLs allow users to upload files directly to Amazon S3 without passing\nthrough the application servers. This offloads the upload traffic from the application tier, freeing up resources\nand significantly improving scalability.\nScalability of S3: S3 is designed for virtually unlimited scalability and handles massive amounts of data and\nrequests. It automatically scales to meet demand without requiring any manual intervention.\nhttps://aws.amazon.com/s3/\nReduced Application Server Load: By bypassing the application servers, the solution minimizes their load,\nenabling them to focus on other critical tasks like processing and serving user requests. This ensures the\napplication remains responsive during peak upload periods.\nCost Efficiency: S3 is cost-effective for storing large amounts of data. Since users upload directly to S3, you\nonly pay for the storage and data transfer used, avoiding costs associated with application server resources\nfor handling the uploads.\nSecurity: Presigned URLs are time-limited and provide granular control over upload permissions. The\napplication controls the parameters of the presigned URL, defining the allowed actions (e.g., uploading),\nexpiration time, and target bucket/object.\nOther options are less scalable:\nA (Upload through Application Servers): This bottlenecks the application servers, making it less scalable,\nparticularly during peak upload periods.\nB (AWS Storage Gateway): While Storage Gateway can provide on-premises access to cloud storage, it is\nless suitable for directly handling high-volume browser uploads. It introduces additional complexity and\npotential latency.\nD (Amazon EFS): EFS is designed for shared file storage access across EC2 instances and not for direct client\nuploads. EFS also typically has higher costs compared to S3 for simple storage of user uploads.\nTherefore, generating S3 presigned URLs offers the optimal combination of scalability, cost-effectiveness,\nand security for handling large-scale user photo uploads.",
    "links": [
      "https://aws.amazon.com/s3/"
    ]
  },
  {
    "question": "CertyIQ\nA company has a web application for travel ticketing. The application is based on a database that runs in a single\ndata center in North America. The company wants to expand the application to serve a global user base. The\ncompany needs to deploy the application to multiple AWS Regions. Average latency must be less than 1 second on\nupdates to the reservation database.\nThe company wants to have separate deployments of its web platform across multiple Regions. However, the\ncompany must maintain a single primary reservation database that is globally consistent.\nWhich solution should a solutions architect recommend to meet these requirements?",
    "options": {
      "B": "Use a global table for the center reservation table. Use",
      "A": "Convert the application to use Amazon DynamoDB. Use a global table for the center"
    },
    "answer": "A",
    "explanation": "The best solution is A. Convert the application to use Amazon DynamoDB. Use a global table for the center\nreservation table. Use the correct Regional endpoint in each Regional deployment.\nHere's why:\nGlobal Consistency: The key requirement is maintaining a single, globally consistent reservation database.\nAmazon DynamoDB Global Tables are specifically designed for this purpose. They provide multi-Region,\nmulti-active database replication, ensuring data consistency across multiple Regions with low latency.\nhttps://aws.amazon.com/dynamodb/global-tables/\nLow Latency Updates: DynamoDB Global Tables are designed for low-latency global writes. Writes to any\nRegion are replicated to other Regions, typically within milliseconds.\nRegional Deployments: Using Regional endpoints in each deployment allows the application to access the\nDynamoDB Global Table from the nearest Region, minimizing latency for both reads and writes.\nAlternatives are unsuitable: Options B, C and D use a relational database. While Aurora and RDS offer read\nreplicas for improved read performance in different Regions, they struggle to meet the requirement for a\nsingle, globally consistent, writable database with low-latency updates. Aurora Serverless in option D\nintroduces complexity by using Lambda functions for data synchronization. This does not align well with the\nconsistency and latency requirement of this application.\nIn summary, DynamoDB Global Tables are perfectly suited for applications requiring a single, globally\nconsistent database with low-latency writes across multiple Regions, making option A the correct choice.",
    "links": [
      "https://aws.amazon.com/dynamodb/global-tables/"
    ]
  },
  {
    "question": "CertyIQ\nA company has migrated multiple Microsoft Windows Server workloads to Amazon EC2 instances that run in the\nus-west-1 Region. The company manually backs up the workloads to create an image as needed.\nIn the event of a natural disaster in the us-west-1 Region, the company wants to recover workloads quickly in the\nus-west-2 Region. The company wants no more than 24 hours of data loss on the EC2 instances. The company also\nwants to automate any backups of the EC2 instances.\nWhich solutions will meet these requirements with the LEAST administrative effort? (Choose two.)",
    "options": {
      "D": "Here's a detailed justification:"
    },
    "answer": "B",
    "explanation": "The correct answer is BD. Here's a detailed justification:\nOption B: Creating an AMI lifecycle policy with twice-daily backups and automated copying to us-west-2\ndirectly addresses the requirements. AMI lifecycle policies provide a managed way to automate AMI creation\nand deletion based on tags, minimizing administrative effort. Configuring the copy to us-west-2 ensures a\nreplicated image for disaster recovery. The twice-daily schedule ensures that the Recovery Point Objective\n(RPO) of 24 hours is met (or exceeded). This option leverages native AWS features for backups and\nreplication, leading to a simple and efficient\nsetup.https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/image-lifecycle.html\nOption D: AWS Backup is a centralized backup service that offers automated and managed backups across\nvarious AWS services, including EC2. By creating a backup plan based on tags and specifying us-west-2 as\nthe destination for the copy, the company automates the backup and replication process in a single solution.\nConfiguring twice-daily backups directly fulfills the RPO requirement. This provides ease of management and\nconsistent backup policies across AWS resources.https://aws.amazon.com/backup/\nWhy other options are incorrect:\nA: This is not the most efficient. Copying on demand is an administrative overhead.\nC: Using a Lambda function to copy data to us-west-2 adds complexity and overhead compared to using AWS\nBackup's built-in cross-region copy feature. It's extra administrative burden.\nE: Copying on demand is an administrative overhead. AWS Backup offers a simpler automated approach for\ncross-region replication.",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/image-lifecycle.html",
      "https://aws.amazon.com/backup/"
    ]
  },
  {
    "question": "CertyIQ\nA company operates a two-tier application for image processing. The application uses two Availability Zones, each\nwith one public subnet and one private subnet. An Application Load Balancer (ALB) for the web tier uses the public\nsubnets. Amazon EC2 instances for the application tier use the private subnets.\nUsers report that the application is running more slowly than expected. A security audit of the web server log files\nshows that the application is receiving millions of illegitimate requests from a small number of IP addresses. A\nsolutions architect needs to resolve the immediate performance problem while the company investigates a more\npermanent solution.\nWhat should the solutions architect recommend to meet this requirement?",
    "options": {
      "B": "Modify the network ACL for the web tier subnets. Add an inbound deny rule for the",
      "A": "Modify the inbound security group for the web tier: Security groups are stateful and operate at the",
      "C": "Modify the inbound security group for the application tier: The illegitimate requests are targeting the web",
      "D": "Modify the network ACL for the application tier subnets: Similar to option C, the application tier is not the"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Modify the network ACL for the web tier subnets. Add an inbound deny rule for the\nIP addresses that are consuming resources.\nHere's why:\nNetwork ACLs (NACLs) vs. Security Groups: Both control network traffic, but they operate at different layers\nand have different characteristics. Security Groups are stateful and operate at the instance level, allowing\nyou to control traffic to and from individual instances. NACLs, on the other hand, are stateless and operate at\nthe subnet level, allowing you to control traffic entering and exiting subnets. Because NACLs work at the\nsubnet level and are stateless, they are a more efficient and immediate way to block traffic from specific IP\naddresses affecting the entire web tier.\nImmediate Performance Problem: The question emphasizes resolving the immediate performance problem.\nNACLs are processed before Security Groups, so changes to NACLs take effect immediately. Security Group\nchanges might take longer to propagate and have less immediate impact across all instances.\nWeb Tier Focus: The illegitimate requests are hitting the web tier (ALB in public subnets). The fastest way to\nmitigate this is to block the traffic at the point of entry to the web tier subnets.\nDeny Rule for Problematic IPs: Adding an inbound deny rule to the NACL for the web tier subnets will\nimmediately block the malicious traffic from reaching the ALB and the EC2 instances behind it, freeing up\nresources and improving application performance.\nWhy other options are less suitable:\nA. Modify the inbound security group for the web tier: Security groups are stateful and operate at the\ninstance level. While this would block traffic, it's less efficient and potentially less immediate than a NACL\nchange, especially if the number of instances is large.\nC. Modify the inbound security group for the application tier: The illegitimate requests are targeting the web\ntier, not the application tier directly. Blocking traffic at the application tier would not prevent the web tier\nfrom being overloaded.\nD. Modify the network ACL for the application tier subnets: Similar to option C, the application tier is not the\nprimary target. Blocking traffic at the application tier would not resolve the immediate performance problem\nat the web tier.\nSupporting Concepts:\nDefense in Depth: This scenario illustrates the concept of defense in depth, where multiple layers of security\nare implemented to protect resources. NACLs and Security Groups provide complementary security controls.\nNetwork Segmentation: Subnets provide a way to segment your network and control traffic flow.\nAuthoritative Links:\nAWS Documentation on Network ACLs\nAWS Documentation on Security Groups",
    "links": []
  },
  {
    "question": "CertyIQ\nA global marketing company has applications that run in the ap-southeast-2 Region and the eu-west-1 Region.\nApplications that run in a VPC in eu-west-1 need to communicate securely with databases that run in a VPC in ap-\nsoutheast-2.\nWhich network design will meet these requirements?",
    "options": {
      "C": "Option D is incorrect because while Transit Gateway can connect VPCs, it's generally more complex and"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Here's a detailed justification:\nVPC Peering creates a direct networking connection between two VPCs, enabling them to route traffic\nbetween each other privately. This is a suitable option when you need to connect two VPCs in different\nregions. For VPC Peering to work, route tables in each VPC must be updated to point to the peer VPC's CIDR\nblock. This allows traffic to be routed correctly. Security groups control inbound and outbound traffic at the\ninstance level. To allow traffic from the eu-west-1 application servers to the ap-southeast-2 database servers,\nthe database security group in ap-southeast-2 needs an inbound rule. This rule should specifically allow\ntraffic from the CIDR block or the specific IP addresses of the application servers in eu-west-1. Using security\ngroup IDs in the other region is not supported and won't work due to the regional boundary.\nOption A is incorrect because while VPC peering is valid, referencing a security group from another region's\nsecurity group is invalid. Security Groups can only reference other Security Groups within the same AWS\nregion and VPC.\nOption B is incorrect for the same reason as option A; Security Groups can only reference other Security\nGroups within the same AWS region and VPC.\nOption D is incorrect because while Transit Gateway can connect VPCs, it's generally more complex and\ncostly than VPC peering for a simple two-VPC connection. A Transit Gateway introduces unnecessary\ncomplexity and costs when a simple peering connection will suffice. Also, just like option A and B, referencing\nsecurity group IDs in another region does not work.\nHere are some helpful links for further research:\nVPC Peering: https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\nSecurity Groups: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\nTransit Gateway: https://docs.aws.amazon.com/vpc/latest/tgw/what-is-transit-gateway.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html",
      "https://docs.aws.amazon.com/vpc/latest/tgw/what-is-transit-gateway.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is developing software that uses a PostgreSQL database schema. The company needs to configure\nmultiple development environments and databases for the company's developers. On average, each development\nenvironment is used for half of the 8-hour workday.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "C": "Configure each development environment with its own Amazon Aurora"
    },
    "answer": "C",
    "explanation": "The most cost-effective solution is C. Configure each development environment with its own Amazon Aurora\nOn-Demand PostgreSQL-Compatible database.\nHere's why:\nCost Optimization: Aurora On-Demand allows you to pay only for the compute resources your database\nconsumes, per second, with no minimum charges. This aligns perfectly with the requirement that each\ndevelopment environment is used for only half of the workday. During the idle periods, the database cost will\nbe significantly reduced, leading to substantial savings compared to always-on RDS or Aurora instances.\nAurora PostgreSQL Compatibility: Aurora PostgreSQL is wire-compatible with standard PostgreSQL,\nensuring the company's software works seamlessly without code modifications.\nDevelopment Environment Isolation: Each developer gets their isolated database environment, preventing\nconflicts and ensuring independent development.\nScalability and Performance: Even in on-demand mode, Aurora PostgreSQL offers excellent performance\nand scalability compared to Single-AZ RDS instances.\nSingle-AZ RDS Inefficiency: RDS Single-AZ instances (option B) are always running, regardless of usage,\nmaking them less cost-effective when instances are idle for significant portions of the day.\nAurora vs RDS: Aurora is designed for high availability and performance than RDS.\nS3 Object Select Inapplicability: Amazon S3 Object Select (option D) is used for querying data within S3\nobjects and is completely irrelevant for setting up a PostgreSQL database for development environments.\nAurora Provisioned Instances Inefficiency: Option A, using standard Aurora, would be expensive since the\ninstances would be charged regardless of whether they are being used or not.\nIn Summary: On-demand Aurora PostgreSQL clusters enable users to launch a fully compliant and completely\nisolated version of Aurora PostgreSQL in minutes. You can immediately reduce the cost of running non-\nproduction databases by only paying for the capacity you consume.\nFurther reading on Aurora On-Demand can be found here:\nAWS Aurora On-Demand Pricing",
    "links": []
  },
  {
    "question": "CertyIQ\nA company uses AWS Organizations with resources tagged by account. The company also uses AWS Backup to\nback up its AWS infrastructure resources. The company needs to back up all AWS resources.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Here's why:"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's why:\nAWS Organizations and Tagging: The company already leverages AWS Organizations and tags resources by\naccount. This establishes a foundation for organized resource management.\nAWS Backup and Tag-Based Backups: AWS Backup natively supports tag-based backups. This allows you to\ndynamically include resources in backup plans based on their tags, minimizing manual configuration.\nIdentifying Untagged Resources with AWS Config: The problem states that the company needs to back up\nall resources. Therefore, any resources not currently tagged will not be backed up using their existing\nmethod. AWS Config can continuously assess your AWS resources, making it ideal for finding resources\nwithout specific tags.\nProgrammatic Tagging: Once AWS Config identifies untagged resources, you can use scripts (e.g., AWS CLI,\nSDKs) to apply the necessary tags programmatically. This ensures consistent tagging and avoids manual\nerrors.\nLeast Operational Overhead: This approach minimizes manual effort and ongoing maintenance. By\nautomating the tagging process, you avoid the need for constant manual checks and updates to backup plans.\nOptions C and D would add complexity in the form of manual review or focus on security rather than backup\ncoverage.\nIncorrect Options:\nB: AWS Config can identify many different aspects of the AWS environment, however, this is an over-\nengineered solution that does not directly address the tagging requirement.\nC: This places a burden on individual account owners to remember to identify and tag their respective\nresources. This is also error prone, time consuming, and adds a large amount of operational overhead.\nD: Amazon Inspector is designed for security vulnerability management, not resource inventory or tagging.\nSupporting Documentation:\nAWS Backup Tag-Based Backups: https://docs.aws.amazon.com/aws-backup/latest/devguide/tags.html\nAWS Config: https://aws.amazon.com/config/",
    "links": [
      "https://docs.aws.amazon.com/aws-backup/latest/devguide/tags.html",
      "https://aws.amazon.com/config/"
    ]
  },
  {
    "question": "CertyIQ\nA social media company wants to allow its users to upload images in an application that is hosted in the AWS\nCloud. The company needs a solution that automatically resizes the images so that the images can be displayed on\nmultiple device types. The application experiences unpredictable traffic patterns throughout the day. The\ncompany is seeking a highly available solution that maximizes scalability.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Here's why:"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's why:\nScalability and High Availability: AWS Lambda is a serverless compute service, meaning it automatically\nscales to handle fluctuating workloads without requiring management of underlying infrastructure. Amazon\nS3 provides highly durable and scalable object storage, ideal for storing images.\nCost-Effectiveness: Lambda functions are billed based on execution time, making it a cost-effective solution\nfor unpredictable traffic patterns. S3 offers various storage classes that can be used to optimize costs based\non access frequency.\nSuitability for Image Processing: Lambda functions can be easily triggered by S3 events (e.g., object\ncreation), allowing for automated image resizing upon upload.\nStatic Website Hosting: S3 can host static websites directly, serving the front-end of the application.\nOption B is incorrect: AWS Step Functions are for orchestrating complex workflows, which is overkill for\nsimple image resizing. Amazon RDS is a relational database service and not suitable for storing images\ndirectly.\nOption C is incorrect: Running a process on an EC2 instance requires managing the instance and its scaling,\nwhich adds complexity and is less cost-effective compared to Lambda. EC2 might not scale quickly enough\nfor unpredictable traffic.\nOption D is incorrect: While ECS can scale, it involves managing container clusters. SQS adds complexity that\nis unnecessary for a straightforward image resizing task. It's more complex and costly than using Lambda\ndirectly.\nAuthoritative Links:\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon S3: https://aws.amazon.com/s3/\nServerless Architectures: https://aws.amazon.com/serverless/",
    "links": [
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/serverless/"
    ]
  },
  {
    "question": "CertyIQ\nA company is running a microservices application on Amazon EC2 instances. The company wants to migrate the\napplication to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster for scalability. The company must\nconfigure the Amazon EKS control plane with endpoint private access set to true and endpoint public access set to\nfalse to maintain security compliance. The company must also put the data plane in private subnets. However, the\ncompany has received error notifications because the node cannot join the cluster.\nWhich solution will allow the node to join the cluster?",
    "options": {
      "B": "Create interface VPC endpoints to allow nodes to access the control plane.",
      "A": "Grant the required permission in AWS Identity and Access Management (IAM) to the",
      "C": "Recreate nodes in the public subnet. Restrict security groups for EC2 nodes: Moving the nodes to public",
      "D": "Allow outbound traffic in the security group of the nodes: Allowing outbound traffic (even if restricted) is"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Create interface VPC endpoints to allow nodes to access the control plane.\nHere's a detailed justification:\nThe problem states that the EKS control plane has private access enabled and public access disabled. This\nmeans that the control plane's API server is only accessible from within the VPC where the EKS cluster\nresides. The worker nodes, which form the data plane, are in private subnets. Therefore, they don't have direct\ninternet access (unless a NAT gateway is configured, which isn't mentioned).\nSince the nodes cannot reach the control plane directly via public internet, and public access is disabled on\nthe control plane, they need a private network path. This is achieved using VPC endpoints. Specifically,\ninterface VPC endpoints provide private connectivity to AWS services within your VPC, without requiring\ninternet access or a NAT gateway. By creating an interface VPC endpoint for EKS, the worker nodes in the\nprivate subnets can communicate with the EKS control plane over the AWS private network, enabling them to\njoin the cluster.\nLet's examine why the other options are less suitable:\nA. Grant the required permission in AWS Identity and Access Management (IAM) to the\nAmazonEKSNodeRole IAM role: IAM permissions control authorization, not network connectivity. While the\nnodes need the correct IAM permissions to interact with the cluster, this won't solve the underlying problem\nof the nodes being unable to reach the control plane due to network isolation.\nC. Recreate nodes in the public subnet. Restrict security groups for EC2 nodes: Moving the nodes to public\nsubnets and then attempting to lock them down with security groups goes against the initial requirement of\nmaintaining security compliance. Moreover, it creates an unnecessary public exposure surface, increasing the\nsecurity risk. The initial setup was attempting to avoid public exposure, and this option reverts that.\nD. Allow outbound traffic in the security group of the nodes: Allowing outbound traffic (even if restricted) is\nnot enough. With the control plane set to private access only, outbound traffic from the nodes needs to reach\nthe control plane privately. Simply allowing general outbound traffic doesn't establish the necessary private\nconnection. Outbound traffic alone will likely still try to reach the public endpoint, which is disabled. You need\na dedicated path.\nIn summary, VPC endpoints provide a secure and compliant way for nodes in private subnets to communicate\nwith the EKS control plane when public access is disabled.\nSupporting Links:\nAmazon EKS Cluster Endpoint Access Control\nVPC Endpoints",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is migrating an on-premises application to AWS. The company wants to use Amazon Redshift as a\nsolution.\nWhich use cases are suitable for Amazon Redshift in this scenario? (Choose three.)",
    "options": {
      "B": "Supporting client-side and server-side encryption: Amazon Redshift supports both client-side and server-",
      "C": "Building analytics workloads during specified hours and when the application is not active: Redshift is",
      "A": "Supporting data APIs to access data with traditional, containerized, and event-driven applications: While",
      "D": "Caching data to reduce the pressure on the backend database: Redshift is a data warehouse, not a"
    },
    "answer": "B",
    "explanation": "The correct answer is BCE. Here's why:\nB. Supporting client-side and server-side encryption: Amazon Redshift supports both client-side and server-\nside encryption to protect data at rest and in transit. This is a crucial security feature for sensitive data being\nmigrated to the cloud. https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-encryption.html\nC. Building analytics workloads during specified hours and when the application is not active: Redshift is\ndesigned for analytical workloads, such as running complex queries on large datasets. It is suited to batch\nprocessing and scheduled analysis, making it ideal for workloads executed during off-peak hours to avoid\nimpacting application performance. Amazon Redshift's concurrency scaling features can automatically add\nquery processing power during periods of high analytical demand, and then scale back down when the\nworkload diminishes.\nE. Scaling globally to support petabytes of data and tens of millions of requests per minute: Redshift is a\nmassively parallel processing (MPP) data warehouse that can scale to petabytes of data. Redshift also can\nhandle high query concurrency, making it suitable for handling tens of millions of requests, though the exact\nperformance is dependent on various factors like query complexity and cluster configuration.\nhttps://aws.amazon.com/redshift/features/\nHere's why the other options are incorrect:\nA. Supporting data APIs to access data with traditional, containerized, and event-driven applications: While\nyou can access Redshift via APIs, it's not primarily designed to be a general-purpose data API provider for all\ntypes of applications. Redshift focuses on analytical querying. Options like Amazon RDS with the Data API, or\nDynamoDB are better suited for general data API access.\nD. Caching data to reduce the pressure on the backend database: Redshift is a data warehouse, not a\ncaching layer. Caching is typically handled by services like Amazon ElastiCache or Amazon CloudFront.\nF. Creating a secondary replica of the cluster by using the AWS Management Console: While you can take\nsnapshots of Redshift clusters for backups and use them to restore to a new cluster, creating a secondary\nreplica in the sense of a real-time failover replica via the console is not the typical way Redshift high\navailability is managed. Redshift leverages features like automatic snapshots and cross-region restores for\ndisaster recovery, rather than continuously replicating to a hot standby. Redshift also allows for automated\nbackups.",
    "links": [
      "https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-encryption.html",
      "https://aws.amazon.com/redshift/features/"
    ]
  },
  {
    "question": "CertyIQ\nA company provides an API interface to customers so the customers can retrieve their financial information. he\ncompany expects a larger number of requests during peak usage times of the year.\nThe company requires the API to respond consistently with low latency to ensure customer satisfaction. The\ncompany needs to provide a compute host for the API.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "B": "Use Amazon API Gateway and AWS Lambda functions with provisioned"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Use Amazon API Gateway and AWS Lambda functions with provisioned\nconcurrency.\nHere's why:\nLow Latency & Consistent Performance: Lambda functions with provisioned concurrency ensure that a\nspecified number of Lambda function instances are initialized and ready to respond instantly to requests. This\neliminates cold starts, providing consistent low latency, which is crucial during peak usage periods. API\nGateway acts as a front door, managing requests and routing them efficiently to Lambda.\nLeast Operational Overhead: Lambda is a serverless compute service. AWS manages the underlying\ninfrastructure, operating system patching, and scaling, significantly reducing operational overhead compared\nto container-based solutions like ECS or EKS. API Gateway also simplifies API management, authorization,\nand monitoring.\nWhy other options are less suitable:\nA & C (ECS/EKS): Managing container orchestration platforms like ECS or EKS requires significant\noperational overhead. You need to manage the cluster, scaling, and container deployments.\nD (Reserved Concurrency): While similar to provisioned concurrency, the option given of reserved\nconcurrency is not the same feature provided by Lambda. Provisioned concurrency actively pre-warms\ninstances, and using the \"reserved\" setting only prevents other functions from using allocated resources.\nBenefits of API Gateway and Lambda:\nScalability: Both services automatically scale to handle increased traffic during peak periods.\nCost-Effectiveness: You only pay for the compute time consumed by Lambda functions.\nSecurity: API Gateway provides features like authentication, authorization, and request validation.\nIn summary, leveraging Amazon API Gateway and AWS Lambda functions with provisioned concurrency offers\nthe optimal balance of low latency, consistent performance, scalability, and minimal operational overhead for\na high-traffic API service.\nAuthoritative Links:\nAWS Lambda Provisioned Concurrency: https://docs.aws.amazon.com/lambda/latest/dg/configuration-\nconcurrency.html\nAmazon API Gateway: https://aws.amazon.com/api-gateway/",
    "links": [
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-",
      "https://aws.amazon.com/api-gateway/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to send all AWS Systems Manager Session Manager logs to an Amazon S3 bucket for archival\npurposes.\nWhich solution will meet this requirement with the MOST operational efficiency?",
    "options": {
      "A": "Enable S3 logging in the Systems Manager console. Choose an S3 bucket to send the session"
    },
    "answer": "A",
    "explanation": "The most operationally efficient solution for archiving Systems Manager Session Manager logs to an S3\nbucket is A. Enable S3 logging in the Systems Manager console. Choose an S3 bucket to send the session\ndata to.\nHere's why:\nDirect Integration: Systems Manager Session Manager has native integration with S3 for logging. This means\nyou can configure logging directly within the Systems Manager console, avoiding the need for additional\nservices or complex configurations.\nSimplified Configuration: Enabling S3 logging in the Systems Manager console is a straightforward process\ninvolving selecting an S3 bucket as the destination. This requires minimal setup and maintenance.\nOperational Overhead: Solution A has the least operational overhead. It leverages the built-in functionality of\nSystems Manager Session Manager, which is designed to directly stream logs to S3. This eliminates the need\nfor deploying, configuring, and managing additional components, making it operationally efficient.\nCost-Effectiveness: By minimizing the number of services and configurations, Solution A reduces the\npotential for errors and the need for monitoring and troubleshooting, thereby lowering operational costs.\nAlternatives: Solution B (CloudWatch Agent and exporting logs to S3) introduces extra layers of complexity.\nInstalling and configuring agents on instances and managing CloudWatch log groups adds overhead.\nFurthermore, exporting logs from CloudWatch requires additional configuration and potentially incurs\nadditional costs. Solutions C introduces complexities of SSM documents and eventbridge schedules.\nIn summary, leveraging the native S3 logging feature of Systems Manager Session Manager provides the\nsimplest, most direct, and operationally efficient way to archive session logs to an S3 bucket. It avoids\nunnecessary complexity and minimizes the management burden.\nReference:\nAWS Documentation on Systems Manager Session Manager Logging",
    "links": []
  },
  {
    "question": "CertyIQ\nAn application uses an Amazon RDS MySQL DB instance. The RDS database is becoming low on disk space. A\nsolutions architect wants to increase the disk space without downtime.\nWhich solution meets these requirements with the LEAST amount of effort?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A: Enable storage autoscaling in RDS. This is the simplest and most efficient way to\naddress the low disk space issue without downtime. Storage autoscaling automatically increases storage\ncapacity when needed, up to a user-defined maximum, preventing applications from running out of space. This\neliminates the need for manual intervention and potential downtime associated with manual scaling\noperations.\nOption B, increasing the RDS database instance size, affects the compute resources (CPU, memory) but\ndoesn't directly address storage capacity. It's an unnecessary change for resolving a storage problem. Option\nC, changing the storage type to Provisioned IOPS, primarily focuses on improving I/O performance rather than\nincreasing capacity. While it might offer some incidental capacity increase, it's not the primary solution.\nOption D, backing up, increasing storage, and restoring, involves significant downtime, which is contrary to the\nquestion's requirements.\nStorage autoscaling is specifically designed for this scenario and is the least intrusive. It seamlessly extends\nthe storage without interrupting the application. Amazon RDS monitors the free storage space and\nautomatically scales the storage when it detects that the database is approaching its capacity limit. This\nfeature is enabled with a few clicks in the AWS Management Console or using the AWS CLI. It offers a cost-\neffective and hands-off approach to managing storage requirements.\nFor further research, refer to the official AWS documentation on RDS Storage Autoscaling:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.Autoscaling.html and Managing\nStorage Capacity:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ConfigureStorage.html. These resources\nwill provide comprehensive information on enabling and managing storage autoscaling, along with best\npractices.",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.Autoscaling.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ConfigureStorage.html."
    ]
  },
  {
    "question": "CertyIQ\nA consulting company provides professional services to customers worldwide. The company provides solutions\nand tools for customers to expedite gathering and analyzing data on AWS. The company needs to centrally\nmanage and deploy a common set of solutions and tools for customers to use for self-service purposes.\nWhich solution will meet these requirements?",
    "options": {
      "B": "Create AWS Service Catalog products for the customers.",
      "A": "Create AWS CloudFormation templates for the customers: While CloudFormation is a valid infrastructure-",
      "C": "Create AWS Systems Manager templates for the customers: AWS Systems Manager is primarily for",
      "D": "Create AWS Config items for the customers: AWS Config is for auditing and compliance; it does not"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Create AWS Service Catalog products for the customers.\nHere's why:\nAWS Service Catalog enables organizations to create and manage catalogs of IT services that are approved\nfor use on AWS. These services can include everything from virtual machines and databases to complete\nmulti-tier application architectures. The key benefit here is centralized management and deployment. The\nconsulting company can define a standardized set of solutions and tools as Service Catalog products.\nBy creating these products, the company provides a self-service portal for customers to deploy these pre-\napproved, configured solutions. This ensures consistency, reduces errors, and simplifies the process for the\ncustomers. Service Catalog products are versioned, allowing for controlled updates and rollbacks. Customers\ncan self-provision these solutions without needing extensive AWS expertise or direct interaction with the\nconsulting company. This aligns with the requirement for self-service and standardized tools.\nOptions A, C, and D are not as suitable:\nA. Create AWS CloudFormation templates for the customers: While CloudFormation is a valid infrastructure-\nas-code tool, it doesn't provide the same level of abstraction and governance as Service Catalog.\nCloudFormation templates require customers to have a certain level of AWS understanding, which defeats\nthe purpose of self-service. There's no central management and versioning like Service Catalog.\nC. Create AWS Systems Manager templates for the customers: AWS Systems Manager is primarily for\noperational management and automation tasks on existing AWS resources. Although it includes automation\ncapabilities, it's not designed for creating and managing self-service catalogs of IT services. It's more suitable\nfor operational tasks after the infrastructure is deployed.\nD. Create AWS Config items for the customers: AWS Config is for auditing and compliance; it does not\nprovide provisioning capabilities. It's used to assess, audit, and evaluate the configurations of AWS resources.\nIt can't deploy solutions.\nIn summary, AWS Service Catalog is the most appropriate solution for providing a centrally managed, self-\nservice portal for customers to deploy pre-defined solutions and tools.\nAuthoritative Links:\nAWS Service Catalog Documentation: https://aws.amazon.com/servicecatalog/\nAWS Service Catalog Features: https://aws.amazon.com/servicecatalog/features/",
    "links": [
      "https://aws.amazon.com/servicecatalog/",
      "https://aws.amazon.com/servicecatalog/features/"
    ]
  },
  {
    "question": "CertyIQ\nA company is designing a new web application that will run on Amazon EC2 Instances. The application will use\nAmazon DynamoDB for backend data storage. The application traffic will be unpredictable. The company expects\nthat the application read and write throughput to the database will be moderate to high. The company needs to\nscale in response to application traffic.\nWhich DynamoDB table configuration will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "B",
    "explanation": "The most cost-effective DynamoDB configuration for an application with unpredictable traffic and moderate\nto high throughput requirements is using on-demand mode with the DynamoDB Standard table class (Option\nB).\nHere's why:\nUnpredictable Traffic: On-demand mode eliminates the need to provision read and write capacity units\n(RCUs/WCUs). DynamoDB automatically scales capacity in response to application traffic. This is ideal for\nunpredictable workloads because you only pay for the reads and writes your application actually performs.\nCost-Effectiveness: While provisioned mode with auto-scaling can handle traffic fluctuations, it requires you\nto estimate peak capacity and set scaling parameters. On-demand mode avoids over-provisioning, which is a\ncommon issue with auto-scaling when workloads are highly variable, leading to cost savings.\nDynamoDB Standard: DynamoDB Standard is the general-purpose table class designed for frequently\naccessed data. Since the application expects moderate to high throughput, it's assumed the data will be\naccessed regularly, making Standard the appropriate choice.\nDynamoDB Standard-IA is less suitable since it is optimized for infrequently accessed data and has higher\nread and write costs. It's not the optimal option for the expected workload.\nIn summary, on-demand mode's pay-per-request pricing model aligns perfectly with unpredictable traffic,\nensuring that costs are minimized while dynamically scaling to meet the application's throughput demands.\nRefer to the following AWS documentation for more information:\nDynamoDB Pricing: https://aws.amazon.com/dynamodb/pricing/\nChoosing Between On-Demand and Provisioned Capacity:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\nDynamoDB Table Classes: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/table-\nclasses.html",
    "links": [
      "https://aws.amazon.com/dynamodb/pricing/",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/table-"
    ]
  },
  {
    "question": "CertyIQ\nA retail company has several businesses. The IT team for each business manages its own AWS account. Each team\naccount is part of an organization in AWS Organizations. Each team monitors its product inventory levels in an\nAmazon DynamoDB table in the team's own AWS account.\nThe company is deploying a central inventory reporting application into a shared AWS account. The application\nmust be able to read items from all the teams' DynamoDB tables.\nWhich authentication option will meet these requirements MOST securely?",
    "options": {
      "B": "Configure the application to use the correct certificate to authenticate and read the DynamoDB"
    },
    "answer": "C",
    "explanation": "Option C is the most secure solution because it leverages AWS's recommended practice of cross-account IAM\nroles for accessing resources across different AWS accounts. This approach provides temporary credentials\nwith least privilege, enhancing security posture.\nHere's a detailed breakdown:\nIAM Roles over IAM Users: IAM roles are preferred over IAM users for cross-account access because roles\nprovide temporary credentials through the AssumeRole API, eliminating the need to manage long-term access\nkeys which are vulnerable to leakage.\nCross-Account Access with AssumeRole: The described setup uses AssumeRole in a secure manner. The\napplication in the central inventory account (APP_ROLE) assumes the role in each business account\n(BU_ROLE).\nLeast Privilege: Each BU_ROLE is granted specific permissions to access only the DynamoDB table in that\nbusiness account. The trust policy of BU_ROLE ensures that only the designated APP_ROLE can assume it,\npreventing unauthorized access.\nCentralized Control: The APP_ROLE in the inventory application account handles the assumption of roles\nacross all business accounts, enabling centralized control and auditing of access.\nNo Hardcoded Credentials: No long-term credentials (like access keys) need to be stored or managed within\nthe application, reducing the risk of exposure.\nOption A is less secure because Secrets Manager would require storing database credentials, which is a valid\napproach, but not the best in a multi-account scenario. It does not intrinsically facilitate cross-account access\nas effectively or securely as cross-account IAM roles.\nOption B is highly insecure because it involves managing and rotating long-term IAM user access keys, which\nis a significant security risk. Manual rotation is prone to errors and delays. Exposing access keys opens up a\nlarge attack surface if compromised.\nOption D is incorrect. DynamoDB does not directly integrate with ACM for authentication in this manner. ACM\nis primarily for managing SSL/TLS certificates for secure communication. Identity certificates are not the\nstandard way to authenticate DynamoDB access.\nIn summary, leveraging cross-account IAM roles with AssumeRole ensures secure, auditable, and easily\nmanageable access to DynamoDB tables across multiple AWS accounts, adhering to the principle of least\nprivilege and utilizing temporary credentials.\nReferences:\nIAM roles:\nGranting access to multiple AWS accounts:\nAssumeRole API:",
    "links": []
  },
  {
    "question": "CertyIQ\nA company runs container applications by using Amazon Elastic Kubernetes Service (Amazon EKS). The company's\nworkload is not consistent throughout the day. The company wants Amazon EKS to scale in and out according to\nthe workload.\nWhich combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)",
    "options": {
      "B": "Use the Kubernetes Metrics Server to activate horizontal pod autoscaling.",
      "C": "Use the Kubernetes Cluster Autoscaler to manage the number of nodes in the cluster.",
      "A": "Use an AWS Lambda function to resize the EKS cluster: While possible, using Lambda for this purpose is",
      "D": "Use Amazon API Gateway and connect it to Amazon EKS: API Gateway is used for managing and exposing"
    },
    "answer": "B",
    "explanation": "The optimal solution for dynamically scaling an Amazon EKS cluster based on workload with minimal\noperational overhead involves two key components: the Kubernetes Metrics Server and the Kubernetes\nCluster Autoscaler.\nB. Use the Kubernetes Metrics Server to activate horizontal pod autoscaling.\nThe Kubernetes Metrics Server collects resource usage data (CPU, memory) from pods and nodes. This data is\ncrucial for Horizontal Pod Autoscaler (HPA). HPA automatically scales the number of pods in a deployment or\nreplication controller based on observed CPU utilization or other select metrics. By using the Metrics Server,\nthe HPA can dynamically increase or decrease the number of pods running to meet demand, thereby\nefficiently utilizing resources and improving application responsiveness. The Metrics Server is a lightweight,\nreadily available tool for this purpose and is simple to deploy and manage.\nKubernetes Metrics Server\nHorizontal Pod Autoscaling\nC. Use the Kubernetes Cluster Autoscaler to manage the number of nodes in the cluster.\nThe Kubernetes Cluster Autoscaler automatically adjusts the size of the EKS cluster (i.e., the number of\nworker nodes) based on the resource requirements of the pods that are scheduled or unscheduled. If the HPA\nscales out and new pods cannot be scheduled due to insufficient node resources, the Cluster Autoscaler will\nautomatically provision new nodes to accommodate them. Conversely, if pods are scaled down by the HPA\nand nodes become underutilized, the Cluster Autoscaler will terminate unnecessary nodes. This dynamic\nscaling of the worker nodes ensures that the cluster has the right amount of resources to meet the\napplication's needs.\nKubernetes Cluster Autoscaler\nScaling worker nodes\nWhy other options are incorrect:\nA. Use an AWS Lambda function to resize the EKS cluster: While possible, using Lambda for this purpose is\nsignificantly more complex and requires custom coding to monitor metrics, determine scaling needs, and\ninteract with the EKS API. This approach introduces substantial operational overhead.\nD. Use Amazon API Gateway and connect it to Amazon EKS: API Gateway is used for managing and exposing\nAPIs, not for cluster scaling. It's irrelevant to the workload-based scaling requirement.\nE. Use AWS App Mesh to observe network activity: App Mesh is a service mesh that provides visibility and\ncontrol over microservices communication. While helpful for observability and traffic management, it doesn't\ndirectly address cluster scaling based on workload. App Mesh focuses on the application layer, not on the\ninfrastructure capacity needed to support the application.",
    "links": []
  },
  {
    "question": "CertyIQ\nA company runs a microservice-based serverless web application. The application must be able to retrieve data\nfrom multiple Amazon DynamoDB tables A solutions architect needs to give the application the ability to retrieve\nthe data with no impact on the baseline performance of the application.\nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "options": {
      "B": "Amazon CloudFront with [email protected] functions.",
      "A": "AWS AppSync pipeline resolvers: While AppSync is suitable for data aggregation from multiple sources, it",
      "C": "Edge-optimized Amazon API Gateway with AWS Lambda functions: This option is similar to",
      "D": "Amazon Athena Federated Query with a DynamoDB connector: Athena is designed for analyzing data"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Amazon CloudFront with [email protected] functions.\nHere's why:\nOperational Efficiency: Lambda@Edge functions are deployed globally within CloudFront's infrastructure.\nThis close proximity to users reduces latency as requests don't need to travel back to the origin server (in this\ncase, the serverless application) for simple data retrieval logic. This minimizes the performance impact on the\nbaseline application.\nData Aggregation and Transformation: Lambda@Edge allows you to intercept requests before they reach the\norigin and responses before they reach the user. In this scenario, Lambda@Edge can be configured to fetch\ndata from the multiple DynamoDB tables, aggregate the results, and transform them into the required format\nbefore sending the consolidated response to the user. This offloads the data aggregation from the origin\nserver, contributing to better performance.\nCaching Benefits: CloudFront's caching capabilities can further reduce the load on the DynamoDB tables and\nthe serverless application. By caching frequently accessed data close to the users, CloudFront eliminates the\nneed to repeatedly fetch the same data from the DynamoDB tables, enhancing performance and reducing\noperational costs.\nWhy the other options are less optimal:\nA. AWS AppSync pipeline resolvers: While AppSync is suitable for data aggregation from multiple sources, it\nintroduces a separate managed service and requires restructuring the existing microservice-based\narchitecture. The additional hop to AppSync increases latency and complexity compared to Lambda@Edge.\nThe question asks for a solution with no impact on the baseline performance.\nC. Edge-optimized Amazon API Gateway with AWS Lambda functions: This option is similar to\nLambda@Edge, but API Gateway is optimized for API management and security. It adds extra complexity\nwithout solving the performance issue since the Lambda would still need to query the DynamoDB tables.\nD. Amazon Athena Federated Query with a DynamoDB connector: Athena is designed for analyzing data\nstored in DynamoDB using SQL. While it can retrieve data from multiple DynamoDB tables, it's primarily used\nfor batch processing and querying large datasets, not for real-time data retrieval in a web application. It will\nimpact the latency of the responses, failing the no performance impact requirement.\nTherefore, Lambda@Edge with CloudFront is the most operationally efficient and performant solution for\nretrieving data from multiple DynamoDB tables in a serverless web application with minimal impact on the\nbaseline performance.\nAuthoritative Links:\nLambda@Edge: https://aws.amazon.com/lambda/edge/\nAmazon CloudFront: https://aws.amazon.com/cloudfront/",
    "links": [
      "https://aws.amazon.com/lambda/edge/",
      "https://aws.amazon.com/cloudfront/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to analyze and troubleshoot Access Denied errors and Unauthorized errors that are related to\nIAM permissions. The company has AWS CloudTrail turned on.\nWhich solution will meet these requirements with the LEAST effort?",
    "options": {},
    "answer": "C",
    "explanation": "The optimal solution for analyzing IAM-related Access Denied and Unauthorized errors from CloudTrail logs\nwith minimal effort is option C: using Amazon Athena.\nAthena directly queries data stored in Amazon S3, where CloudTrail typically delivers its logs. It leverages\nstandard SQL, making analysis relatively straightforward for individuals familiar with SQL. The key advantage\nis that Athena eliminates the need for complex ETL (Extract, Transform, Load) processes, unlike AWS Glue\n(option A), which necessitates creating and managing ETL jobs to prepare the data for querying. While AWS\nBatch (option B) could be used to run custom scripts against CloudTrail logs, this approach involves more\noperational overhead and script development, which negates the 'least effort' requirement.\nAmazon QuickSight (option D) is primarily a business intelligence tool for visualization and creating\ndashboards. While QuickSight can connect to Athena, it's an additional layer of complexity for the initial\ntroubleshooting stage. Constructing a complete dashboard before identifying error patterns introduces\nunnecessary work. Athena provides the capability to query and filter CloudTrail logs directly to identify these\nerrors, after which, insights can be visualized for deeper analysis. This makes Athena a more targeted and\nefficient first step. It also supports federated queries so you can access multiple data stores, which is\nirrelevant to the question.\nTherefore, Athena's ability to directly query CloudTrail logs using SQL, without requiring ETL or dashboard\ncreation upfront, makes it the most efficient solution to identify the specified IAM errors.\nSupporting Links:\nAWS CloudTrail Logging IAM API Calls\nAnalyzing CloudTrail Logs with Amazon Athena",
    "links": []
  },
  {
    "question": "CertyIQ\nA company wants to add its existing AWS usage cost to its operation cost dashboard. A solutions architect needs\nto recommend a solution that will give the company access to its usage cost programmatically. The company must\nbe able to access cost data for the current year and forecast costs for the next 12 months.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A: Access usage cost-related data by using the AWS Cost Explorer API with pagination.\nHere's why:\nProgrammatic Access: The requirement specifies programmatic access to cost data. The Cost Explorer API\ndirectly addresses this, allowing the company to retrieve data through code. Options B, C, and D require\nmanual intervention or rely on delivery mechanisms less suitable for automated integration with a dashboard.\nCurrent Year and Forecast Data: The AWS Cost Explorer API provides access to historical cost data for the\ncurrent year and forecasting capabilities for the next 12 months, meeting the data requirements.\nLeast Operational Overhead: The Cost Explorer API, when used with pagination, minimizes the overhead.\nPagination allows the retrieval of data in manageable chunks, preventing large data transfers that could\nstrain resources. Options C and D require configuring and managing budgets and delivery mechanisms like\nFTP or SMTP, increasing operational complexity. Option B would require manual downloading and processing\nof CSV files.\nAutomation and Integration: The API facilitates direct integration with the company's operation cost\ndashboard, enabling automated data updates and analysis. This aligns with best practices for cloud cost\nmanagement and operational efficiency.\nScalability: The API-based approach can easily scale to accommodate increasing data volumes and\ncomplexity as the company's AWS usage grows. The pagination feature helps maintain performance during\nscaling.\nCost Optimization: By providing granular cost data, the Cost Explorer API can help the company identify cost\noptimization opportunities and improve its overall cloud cost efficiency.\nAuthoritative Links:\nAWS Cost Explorer API: https://docs.aws.amazon.com/aws-cost-management/latest/APIReference/\nAWS Cost Explorer: https://aws.amazon.com/aws-cost-management/aws-cost-explorer/",
    "links": [
      "https://docs.aws.amazon.com/aws-cost-management/latest/APIReference/",
      "https://aws.amazon.com/aws-cost-management/aws-cost-explorer/"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is reviewing the resilience of an application. The solutions architect notices that a database\nadministrator recently failed over the application's Amazon Aurora PostgreSQL database writer instance as part of\na scaling exercise. The failover resulted in 3 minutes of downtime for the application.\nWhich solution will reduce the downtime for scaling exercises with the LEAST operational overhead?",
    "options": {
      "D": "Set up an Amazon RDS proxy for the database. Update the application to use the",
      "A": "Create more Aurora PostgreSQL read replicas: Read replicas are for read scaling and don't help with write",
      "B": "Secondary Aurora PostgreSQL cluster: This involves a much more complex setup, including data",
      "C": "Amazon ElastiCache for Memcached: ElastiCache is a caching service. While caching can reduce database"
    },
    "answer": "D",
    "explanation": "The best solution to minimize downtime during Aurora PostgreSQL writer instance failovers, with the least\noperational overhead, is D. Set up an Amazon RDS proxy for the database. Update the application to use the\nproxy endpoint.\nHere's why:\nRDS Proxy's Primary Benefit: RDS Proxy sits between your application and your database. Its core\nfunctionality is to manage database connections efficiently, maintain connection pools, and most importantly,\nautomatically reconnect to the new writer instance during a failover event.\nhttps://aws.amazon.com/rds/proxy/\nReduced Downtime: By automatically handling reconnection, RDS Proxy significantly reduces application\ndowntime during failovers. The application doesn't need to implement complex retry logic or be aware of the\ndatabase's internal failover process. This typically reduces downtime from minutes to seconds.\nConnection Management: RDS Proxy handles connection management, preventing connection storms that\ncan overload the database during failover. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-\nproxy.html\nMinimal Operational Overhead: Setting up RDS Proxy is relatively straightforward and requires minimal\nchanges to the application code. The application only needs to be configured to connect to the RDS Proxy\nendpoint instead of the database endpoint.\nWhy other options are less suitable:\nA. Create more Aurora PostgreSQL read replicas: Read replicas are for read scaling and don't help with write\noperations during a writer failover. The problem is the unavailability of the writer instance.\nB. Secondary Aurora PostgreSQL cluster: This involves a much more complex setup, including data\nreplication and application failover logic. It has a significantly higher operational overhead. Maintaining a\nsecondary cluster requires continuous synchronization and testing of failover procedures.\nC. Amazon ElastiCache for Memcached: ElastiCache is a caching service. While caching can reduce database\nload, it doesn't address the problem of the application being unable to write to the database during a writer\nfailover. Data loss or inconsistency can also be major concerns.\nRDS Proxy is specifically designed for this use case: It provides a managed service for connection pooling\nand automatic failover, simplifying database management and increasing application availability. Therefore,\noption D is the most effective and efficient solution.",
    "links": [
      "https://aws.amazon.com/rds/proxy/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-"
    ]
  },
  {
    "question": "CertyIQ\nA company has a regional subscription-based streaming service that runs in a single AWS Region. The architecture\nconsists of web servers and application servers on Amazon EC2 instances. The EC2 instances are in Auto Scaling\ngroups behind Elastic Load Balancers. The architecture includes an Amazon Aurora global database cluster that\nextends across multiple Availability Zones.\nThe company wants to expand globally and to ensure that its application has minimal downtime.\nWhich solution will provide the MOST fault tolerance?",
    "options": {},
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the best solution for maximizing fault tolerance while\nexpanding a regional streaming service globally:\nThe primary goal is to minimize downtime during a regional failure. Option D achieves this most effectively by\nreplicating the application and database tiers to a second region and using Route 53 for failover.\nApplication Tier Replication: Deploying the web and application tiers to a second region provides redundancy.\nIf the primary region experiences issues, Route 53 can direct traffic to the healthy second region. This\nensures application availability.\nAurora Global Database for Database Replication: Aurora Global Database is crucial. It replicates data\nacross AWS Regions with low latency, typically under a second. This ensures that the second region's\ndatabase is nearly up-to-date, minimizing data loss during failover.\nRoute 53 Failover: Amazon Route 53 health checks monitor the health of the application in both regions. A\nfailover routing policy automatically redirects traffic to the second region if the primary region becomes\nunavailable.\nControlled Failover (Promotion): Promoting the secondary Aurora instance to primary is essential for write\noperations. The controlled failover approach minimizes downtime and ensures data consistency.\nOption A is less desirable because it suggests extending Auto Scaling groups across regions. While\ntechnically possible with some configurations, it introduces unnecessary complexity and potential latency\nissues related to cross-region networking for application components.\nOption B uses Aurora PostgreSQL cross-Region Aurora Replica. While an Aurora replica does replicate data,\npromotion can be more complex and slower than using the built-in global database capabilities. Moreover, it\ndoesn't utilize the automatic failover benefits of Aurora Global Database.\nOption C uses AWS DMS for database replication. DMS is suitable for migrations or scenarios requiring data\ntransformation, but it's not the best choice for a constantly replicating, highly available database setup.\nDMS's replication latency is higher than Aurora Global Database, leading to a greater potential for data loss\nduring failover.\nTherefore, option D balances redundancy, performance, and ease of failover, making it the most fault-tolerant\nsolution.\nAuthoritative Links:\nAmazon Aurora Global Database: https://aws.amazon.com/rds/aurora/global-database/\nAmazon Route 53 Failover: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html\nAWS Database Migration Service (DMS): https://aws.amazon.com/dms/",
    "links": [
      "https://aws.amazon.com/rds/aurora/global-database/",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html",
      "https://aws.amazon.com/dms/"
    ]
  },
  {
    "question": "CertyIQ\nA data analytics company wants to migrate its batch processing system to AWS. The company receives thousands\nof small data files periodically during the day through FTP. An on-premises batch job processes the data files\novernight. However, the batch job takes hours to finish running.\nThe company wants the AWS solution to process incoming data files as soon as possible with minimal changes to\nthe FTP clients that send the files. The solution must delete the incoming data files after the files have been\nprocessed successfully. Processing for each file needs to take 3-8 minutes.\nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "options": {},
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the most operationally efficient solution:\nOption D leverages several AWS services optimally for the given scenario. First, AWS Transfer Family is a\nmanaged service specifically designed for secure file transfers to and from AWS storage services, minimizing\nthe operational overhead of managing an EC2-based FTP server. It directly integrates with S3, simplifying the\nfile storage aspect.https://aws.amazon.com/transfer/\nStoring the files in Amazon S3 Standard provides immediate availability and scalability. The use of S3 event\nnotifications triggers an AWS Lambda function immediately when a file arrives. AWS Lambda is ideal for\nprocessing individual files that take only a few minutes to process, fitting the 3-8 minute requirement.\nLambda functions are serverless, automatically scaling based on the number of invocations, thus reducing\noperational burden.https://aws.amazon.com/lambda/\nThe Lambda function can process the file and then delete it from S3, automating the cleanup process. This\nentire solution requires minimal manual intervention and scales automatically based on the workload.\nOptions A, B, and C are less optimal for several reasons. Options A and B use Amazon EC2 to manage the FTP\nserver. This increases operational complexity because you need to patch, scale, and manage the EC2\ninstance. They also use AWS Batch nightly, which is not what the customer asked for. Option A moves the\ndata to Glacier Flexible Retrieval, which is not a use-case for immediate processing. The process of moving\nobjects to Glacier is not immediate. Option C unnecessarily uses EBS, which adds persistent storage costs and\ndoes not readily integrate with event-driven processing. Option C uses AWS Batch, which may introduce more\ncomplexity to the design since Lambda can more easily process incoming files and delete them.",
    "links": [
      "https://aws.amazon.com/transfer/",
      "https://aws.amazon.com/lambda/"
    ]
  },
  {
    "question": "CertyIQ\nA company is migrating its workloads to AWS. The company has transactional and sensitive data in its databases.\nThe company wants to use AWS Cloud solutions to increase security and reduce operational overhead for the\ndatabases.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B: Migrate the databases to Amazon RDS and configure encryption at rest.\nHere's why:\nAmazon RDS (Relational Database Service) is a managed database service. It significantly reduces\noperational overhead by automating tasks like patching, backups, and recovery, which the company desires.\nThis removes the need for the company to manage the underlying infrastructure, improving efficiency.\nEncryption at rest in Amazon RDS directly addresses the security requirement for transactional and sensitive\ndata. RDS provides encryption options using AWS Key Management Service (KMS) for managing encryption\nkeys. Data is encrypted while stored on disk (at rest), enhancing data protection.\nOption A (Amazon EC2): While EC2 allows you to host databases, it increases operational overhead because\nyou are responsible for managing the entire infrastructure, including backups, patching, and security. This\ncontradicts the requirement to reduce operational overhead.\nOption C (Amazon S3 and Macie): Amazon S3 is primarily object storage, not designed for transactional\ndatabases. Amazon Macie is a data security and data privacy service that uses machine learning and pattern\nmatching to discover and protect sensitive data in AWS. It is not suitable for hosting a transactional database.\nOption D (Amazon RDS and CloudWatch Logs): While RDS provides a managed database solution, CloudWatch\nLogs is primarily for monitoring and logging. While logs can be used for security analysis, CloudWatch Logs\ndoes not encrypt data at rest, therefore not directly addressing data protection for transactional databases.\nTherefore, it doesn't satisfy the security requirement.\nTherefore, migrating to Amazon RDS and enabling encryption at rest provides both reduced operational\noverhead and increased security for sensitive data.\nHere are some authoritative links for further research:\nAmazon RDS: https://aws.amazon.com/rds/\nAmazon RDS Encryption:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\nAWS Key Management Service (KMS): https://aws.amazon.com/kms/",
    "links": [
      "https://aws.amazon.com/rds/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html",
      "https://aws.amazon.com/kms/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an online gaming application that has TCP and UDP multiplayer gaming capabilities. The company\nuses Amazon Route 53 to point the application traffic to multiple Network Load Balancers (NLBs) in different AWS\nRegions. The company needs to improve application performance and decrease latency for the online game in\npreparation for user growth.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The best solution for improving application performance and decreasing latency for a global online game with\nTCP and UDP traffic, while using Route 53 and NLBs, is to use AWS Global Accelerator.\nDetailed Justification:\nAWS Global Accelerator (C): Global Accelerator uses the AWS global network to direct user traffic to the\noptimal endpoint based on proximity, health, and configuration. It supports both TCP and UDP protocols,\nwhich are essential for the game's multiplayer capabilities. It provides static anycast IP addresses that serve\nas a fixed entry point to the application, improving resilience. Global Accelerator's endpoint groups can be\nconfigured to use different ports for the TCP and UDP listeners on the NLBs. By directing traffic based on\nnetwork proximity, latency is minimized.\nCloudFront (A): CloudFront excels at caching static content, which is less relevant for real-time gaming\ntraffic that requires low latency and constant updates. Additionally, CloudFront primarily works with HTTP\nand HTTPS traffic and isn't designed for general TCP/UDP acceleration. While caching can help, the dynamic\nnature of game data makes CloudFront less effective than Global Accelerator.\nApplication Load Balancers (ALBs) with Latency-Based Routing (B): Replacing NLBs with ALBs doesn't\ndirectly address the global latency issue as efficiently as Global Accelerator. While latency-based routing in\nRoute 53 helps direct users to the closest region, it does not leverage the optimized AWS global network for\nlow-latency routing between the user and the region, which Global Accelerator provides. ALBs also do not\nhandle UDP traffic natively.\nAPI Gateway (D): API Gateway is designed for managing APIs and doesn't directly improve latency for real-\ntime TCP/UDP gaming traffic. It introduces an additional layer of complexity and isn't suitable for the gaming\nuse case described.\nWhy Global Accelerator is optimal:\nGlobal Accelerator is specifically designed to optimize performance for applications with a global user base,\nparticularly for use cases requiring consistent, low-latency connectivity over TCP and UDP. It reduces latency\nby leveraging the highly available and congestion-free AWS global network. The anycast static IPs provide a\nstable entry point to the application, masking regional failures and changes.\nAuthoritative Links:\nAWS Global Accelerator: https://aws.amazon.com/global-accelerator/\nAWS Global Accelerator Features: https://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-\nglobal-accelerator.html",
    "links": [
      "https://aws.amazon.com/global-accelerator/",
      "https://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to integrate with a third-party data feed. The data feed sends a webhook to notify an external\nservice when new data is ready for consumption. A developer wrote an AWS Lambda function to retrieve data\nwhen the company receives a webhook callback. The developer must make the Lambda function available for the\nthird party to call.\nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A: Create a function URL for the Lambda function. Provide the Lambda function URL to\nthe third party for the webhook.\nHere's a detailed justification:\nLambda function URLs are a dedicated, built-in feature that allows you to directly invoke your Lambda\nfunctions over HTTPS without needing to configure and manage separate API Gateways. This represents the\nmost operationally efficient solution for exposing a Lambda function as a webhook endpoint to a third party.\nOption B, deploying an ALB in front of Lambda, adds significant complexity and operational overhead. ALB\nintroduces the need to manage load balancing, listener rules, and target groups, which are unnecessary for a\nsimple webhook integration.\nOption C, using SNS, introduces an unnecessary level of indirection. SNS is a pub/sub messaging service,\nwhich is suitable for decoupling applications but adds unnecessary complexity for a direct webhook\nintegration. The third party would need to publish to the SNS topic, and the Lambda function would then be\ntriggered by SNS, adding latency and more points of failure.\nOption D, using SQS, is also an unnecessarily complex approach. SQS is a queuing service designed for\nasynchronous message processing. For a simple webhook integration where you expect an immediate action\nfrom the Lambda function, SQS introduces unnecessary queuing and latency.\nLambda function URLs offer a direct, simplified way to expose a Lambda function as a publicly accessible\nHTTPS endpoint. They are cost-effective, require minimal configuration, and are designed for scenarios like\nwebhook integrations.\nIn summary, Lambda Function URLs provide the simplest, most direct, and most operationally efficient\nmethod to expose a Lambda function to a third-party service as a webhook endpoint.\nHere are some authoritative links for further research:\nAWS Lambda Function URLs: https://docs.aws.amazon.com/lambda/latest/dg/lambda-urls.html\nAWS Lambda: https://aws.amazon.com/lambda/",
    "links": [
      "https://docs.aws.amazon.com/lambda/latest/dg/lambda-urls.html",
      "https://aws.amazon.com/lambda/"
    ]
  },
  {
    "question": "CertyIQ\nA company has a workload in an AWS Region. Customers connect to and access the workload by using an Amazon\nAPI Gateway REST API. The company uses Amazon Route 53 as its DNS provider. The company wants to provide\nindividual and secure URLs for all customers.\nWhich combination of steps will meet these requirements with the MOST operational efficiency? (Choose three.)",
    "options": {
      "A": "Register the required domain in a registrar. Create a wildcard custom domain name in a Route 53 hosted",
      "D": "Request a wildcard certificate that matches the custom domain name in AWS Certificate Manager (ACM)",
      "B": "Request a wildcard certificate that matches the domains in AWS Certificate Manager (ACM) in a",
      "C": "Create hosted zones for each customer as required in Route 53. Create zone records that point to the API"
    },
    "answer": "A",
    "explanation": "The correct answer is ADF. Here's why:\nA. Register the required domain in a registrar. Create a wildcard custom domain name in a Route 53 hosted\nzone and record in the zone that points to the API Gateway endpoint. This establishes the foundation for\nusing custom domains and Route 53 for managing DNS records. A wildcard domain (e.g., *.example.com)\nallows you to handle multiple subdomains without creating individual records for each. This dramatically\nsimplifies DNS management. https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-name-\nbasics.html\nD. Request a wildcard certificate that matches the custom domain name in AWS Certificate Manager (ACM)\nin the same Region. To secure the custom domain with HTTPS, you need a certificate. Using a wildcard\ncertificate (e.g., *.example.com) covers all subdomains under your main domain, streamlining certificate\nmanagement. The certificate must be in the same region as your API Gateway instance.\nhttps://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html\nF. Create a custom domain name in API Gateway for the REST API. Import the certificate from AWS\nCertificate Manager (ACM). This step links the secured custom domain to your API Gateway. You configure\nAPI Gateway to use the wildcard certificate from ACM, enabling secure access through the custom domain.\nThis is a critical part of providing personalized URLs.\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html\nWhy other options are incorrect:\nB. Request a wildcard certificate that matches the domains in AWS Certificate Manager (ACM) in a\ndifferent Region. ACM certificates used with API Gateway must be in the same region.\nC. Create hosted zones for each customer as required in Route 53. Create zone records that point to the API\nGateway endpoint. Creating individual hosted zones for each customer adds unnecessary complexity and\nmanagement overhead. A wildcard record in a single hosted zone is much more efficient.\nE. Create multiple API endpoints for each customer in API Gateway. Creating multiple API endpoints\nsignificantly increases management overhead. Using custom domains allows for a single API endpoint to be\naccessed through different URLs.",
    "links": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-name-",
      "https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html"
    ]
  },
  {
    "question": "CertyIQ\nA company stores data in Amazon S3. According to regulations, the data must not contain personally identifiable\ninformation (PII). The company recently discovered that S3 buckets have some objects that contain PII. The\ncompany needs to automatically detect PII in S3 buckets and to notify the companys security team.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Here's why:"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's why:\nAmazon Macie is a fully managed data security and data privacy service that uses machine learning and\npattern matching to discover and protect sensitive data in AWS. It's designed specifically for identifying PII\nand other sensitive information within S3 buckets.\nMacie for PII Detection: Macie is built to automatically discover PII in S3, which directly addresses the\ncompany's requirement.\nEventBridge for Event-Driven Architecture: Amazon EventBridge allows you to build event-driven\napplications by routing events between AWS services. In this case, Macie findings are events.\nFiltering Sensitive Data Events: An EventBridge rule can be configured to specifically listen for the\nSensitiveData event type from Macie findings. This ensures that the security team is only notified when PII is\ndetected, avoiding unnecessary alerts for other types of findings.\nSNS for Notification: Amazon Simple Notification Service (SNS) is a fully managed messaging service that\ncan be used to send notifications to various endpoints, including email, SMS, and HTTP endpoints, which is\nsuitable for notifying the security team.\nWhy other options are incorrect:\nGuardDuty: GuardDuty focuses on threat detection and monitoring malicious activity, not specifically\nidentifying PII in S3. It's not designed for data privacy in the same way Macie is.\nSQS: While SQS is a useful queuing service, sending notifications directly to SQS wouldn't notify the security\nteam. SNS is a better choice for direct notifications.\nCRITICAL event type: GuardDuty's CRITICAL event type is related to security threats and is not designed to\nfind PII.Using Macie, EventBridge, and SNS creates an automated pipeline: Macie scans S3, detects PII,\nEventBridge filters the alerts for sensitive data, and SNS sends a notification to the security team for review\nand action.\nAuthoritative Links:\nAmazon Macie: https://aws.amazon.com/macie/\nAmazon EventBridge: https://aws.amazon.com/eventbridge/\nAmazon SNS: https://aws.amazon.com/sns/",
    "links": [
      "https://aws.amazon.com/macie/",
      "https://aws.amazon.com/eventbridge/",
      "https://aws.amazon.com/sns/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to build a logging solution for its multiple AWS accounts. The company currently stores the logs\nfrom all accounts in a centralized account. The company has created an Amazon S3 bucket in the centralized\naccount to store the VPC flow logs and AWS CloudTrail logs. All logs must be highly available for 30 days for\nfrequent analysis, retained for an additional 60 days for backup purposes, and deleted 90 days after creation.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "C",
    "explanation": "Here's a breakdown of why option C is the most cost-effective solution for the logging requirements:\nThe core requirement is to store logs for 90 days with frequent analysis for the first 30 and backup retention\nfor the next 60. Then, delete the logs. The most important factor is cost-effectiveness.\nWhy S3 Glacier Flexible Retrieval (formerly S3 Glacier) after 30 days? S3 Glacier Flexible Retrieval offers\nthe lowest storage cost of the S3 storage classes designed for data archiving and backup. Since the logs are\nfor backup after 30 days, they will rarely be accessed. S3 Glacier Flexible Retrieval fits well.\nWhy not S3 Standard-IA or S3 One Zone-IA? While these are cheaper than S3 Standard, they are still more\nexpensive than S3 Glacier Flexible Retrieval, and are designed for more frequent access than required after\nthe initial 30 days. Also, S3 One Zone-IA data is lost if the AZ fails, not suitable for backup retention.\nWhy use an expiration action? Expiration actions are the built-in, automated way within S3 to delete objects\nafter a specified time. This ensures compliance with the 90-day deletion requirement.\nLet's analyze why the other options are not the most cost-effective:\nOption A (S3 Standard): S3 Standard is the most expensive S3 storage class, making it unsuitable for long-\nterm storage of logs intended for infrequent access after the initial 30 days. This drastically increases the\nstorage cost without adding any value for logs mainly used for backup.\nOption B (S3 Standard-IA then Glacier Flexible Retrieval): While it uses Glacier Flexible Retrieval eventually,\nthe initial 30 days in S3 Standard-IA add unnecessary cost. The logs can be moved to Glacier Flexible\nRetrieval after 30 days to reduce storage costs. Moving to Glacier after 90 days would defeat the purpose of\nthe backup retention requirements, since the intention is deletion at 90.\nOption D (S3 One Zone-IA then Glacier Flexible Retrieval): S3 One Zone-IA's cost is lower than S3 Standard-\nIA, however data is lost if the single availability zone is compromised, which does not align with data retention\nfor audit purposes. There's also the unnecessary transition to S3 One Zone-IA before moving it to Glacier.\nTherefore, Option C provides the most cost-effective solution by using the S3 Glacier Flexible Retrieval\nstorage class for the 60-day backup phase and automating deletion with an S3 lifecycle policy.\nSupporting Documentation:\nS3 Storage Classes: https://aws.amazon.com/s3/storage-classes/\nS3 Lifecycle Management: https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-\nmanagement.html",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-"
    ]
  },
  {
    "question": "CertyIQ\nA company is building an Amazon Elastic Kubernetes Service (Amazon EKS) cluster for its workloads. All secrets\nthat are stored in Amazon EKS must be encrypted in the Kubernetes etcd key-value store.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B: Create a new AWS Key Management Service (AWS KMS) key. Enable Amazon EKS\nKMS secrets encryption on the Amazon EKS cluster.\nHere's a detailed justification:\nThe requirement is to encrypt Kubernetes secrets stored in the etcd key-value store of an Amazon EKS\ncluster. Kubernetes stores secrets in etcd, and by default, these secrets are only base64 encoded, not\nencrypted. Therefore, an encryption mechanism is needed.\nOption A, using AWS Secrets Manager, is incorrect because while Secrets Manager can store secrets\nsecurely, it doesn't directly encrypt the secrets within the Kubernetes etcd datastore. Secrets Manager would\nrequire the application code to retrieve secrets from Secrets Manager, which is a different operational model\nthan directly encrypting secrets managed within Kubernetes. This does not address the direct encryption of\nsecrets stored in etcd.\nOption B directly addresses the requirement. Amazon EKS offers the capability to encrypt secrets at rest in\netcd using a KMS key. When this feature is enabled, all secrets stored in the etcd database are encrypted\nusing envelope encryption. The data encryption key (DEK) used to encrypt the secrets is itself encrypted by\nthe KMS key (KEK) you provide. This ensures that even if someone gains unauthorized access to the etcd\nstore, the secrets remain encrypted and unreadable without the correct KMS key. Creating a new KMS key\nprovides you with control over the encryption key, aligning with security best practices.\nOption C is incorrect because using the EBS CSI driver is for managing persistent volumes attached to the\nEKS cluster nodes, it doesn't encrypt the secrets stored in etcd. It only deals with the storage and encryption\nof data on EBS volumes.\nOption D is partially correct as EBS encryption encrypts the data at rest on EBS volumes, but it is irrelevant\nfor encrypting secrets within the etcd database. The alias/aws/ebs KMS key is used for EBS volume encryption\nand is separate from the encryption of secrets within EKS.\nTherefore, option B provides the solution that directly fulfills the stated requirement of encrypting\nKubernetes secrets in etcd within the EKS cluster using a dedicated KMS key.\nSupporting documentation:\nAmazon EKS Secrets Encryption: https://docs.aws.amazon.com/eks/latest/userguide/managing-secrets.html\nEncrypting Secrets at Rest: https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/",
    "links": [
      "https://docs.aws.amazon.com/eks/latest/userguide/managing-secrets.html",
      "https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to provide data scientists with near real-time read-only access to the company's production\nAmazon RDS for PostgreSQL database. The database is currently configured as a Single-AZ database. The data\nscientists use complex queries that will not affect the production database. The company needs a solution that is\nhighly available.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "C",
    "explanation": "The requirement is to provide near real-time read-only access to an RDS PostgreSQL database for data\nscientists without impacting the production database, while maintaining high availability and cost-\neffectiveness.\nOption A is incorrect because scaling the production database does not address the need for read-only access\nand could still impact production performance, especially with complex queries. It also doesn't inherently\nimprove high availability.\nOption B is partially correct in enabling high availability by switching to Multi-AZ. However, providing data\nscientists direct access to the secondary standby instance is not the intended use case and may interfere with\nfailover operations. The secondary is primarily for failover and not optimized for heavy read workloads.\nOption C offers the best balance of requirements. Converting to a Multi-AZ deployment provides high\navailability. Then creating read replicas off the primary instance allows data scientists to run their complex\nqueries without impacting the production workload. Multiple read replicas further improve read scalability\nand availability.\nOption D is similar to option C but utilizes a Multi-AZ cluster deployment with readable standby instances.\nThis is typically associated with Aurora PostgreSQL, not standard RDS PostgreSQL. Furthermore, while\nreadable standby instances are a good solution, providing two additional read replicas as in option C offers\nmore flexibility in distributing the read load and ensuring resilience for the data scientist workload within the\ncontext of standard RDS PostgreSQL, potentially at a lower cost compared to migrating to Aurora.\nTherefore, option C is the most cost-effective solution that meets all the requirements: near real-time read-\nonly access, minimal impact on the production database, and high availability for the data scientists'\nworkload.\nRelevant Links:\nAmazon RDS Read Replicas\nAmazon RDS Multi-AZ Deployments",
    "links": []
  },
  {
    "question": "CertyIQ\nA company runs a three-tier web application in the AWS Cloud that operates across three Availability Zones. The\napplication architecture has an Application Load Balancer, an Amazon EC2 web server that hosts user session\nstates, and a MySQL database that runs on an EC2 instance. The company expects sudden increases in application\ntraffic. The company wants to be able to scale to meet future application capacity demands and to ensure high\navailability across all three Availability Zones.\nWhich solution will meet these requirements?",
    "options": {
      "B": "Migrate the web server to an Auto Scaling group that is in three Availability"
    },
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A is the best solution:\nThe primary goal is to achieve high availability and scalability for a three-tier web application in AWS across\nthree Availability Zones while handling sudden traffic increases.\nDatabase: Migrating the on-premises MySQL database to Amazon RDS for MySQL with a Multi-AZ\ndeployment provides high availability. In case of a failure in the primary AZ, RDS automatically fails over to a\nstandby replica in another AZ. This eliminates single points of failure for the database layer.\nhttps://aws.amazon.com/rds/mysql/\nSession Management: Using Amazon ElastiCache for Redis with high availability is ideal for storing session\ndata. Redis is an in-memory data store that provides fast read and write operations, crucial for handling user\nsessions efficiently. The high availability feature of ElastiCache ensures that session data remains accessible\neven if a node fails. Redis also supports more advanced data structures beneficial for session management.\nhttps://aws.amazon.com/elasticache/redis/\nWeb Tier Scalability: Migrating the EC2 web server to an Auto Scaling group across three Availability Zones\nenables the application to scale horizontally based on demand. Auto Scaling automatically launches and\nterminates EC2 instances based on predefined metrics, ensuring the application can handle sudden traffic\nspikes. Spreading instances across multiple AZs provides fault tolerance.\nhttps://aws.amazon.com/autoscaling/\nWhy other options are less suitable:\nOption B: While Memcached is a viable caching solution, Redis offers more advanced features for session\nmanagement, such as persistence and more complex data structures. For session storage, Redis generally\nprovides better functionality and capabilities.\nOption C: Migrating the MySQL database to Amazon DynamoDB could be a valid solution if the applications\ndata model and access patterns are suitable for a NoSQL database. However, DynamoDB is a major\narchitectural shift and requires significant application changes. Also, storing session data in DynamoDB may\nnot be as efficient as using a dedicated in-memory cache like Redis. While DynamoDB Accelerator (DAX)\nimproves performance, it's primarily designed for hot data in DynamoDB, and Redis provides a more targeted\nsolution for session data.\nOption D: Deploying the MySQL database in a single Availability Zone introduces a single point of failure. If\nthat Availability Zone experiences an outage, the entire application could become unavailable. This violates\nthe requirement for high availability.\nTherefore, option A provides the best solution by addressing scalability and high availability requirements\nacross all three tiers of the application using appropriate AWS services.",
    "links": [
      "https://aws.amazon.com/rds/mysql/",
      "https://aws.amazon.com/elasticache/redis/",
      "https://aws.amazon.com/autoscaling/"
    ]
  },
  {
    "question": "CertyIQ\nA global video streaming company uses Amazon CloudFront as a content distribution network (CDN). The company\nwants to roll out content in a phased manner across multiple countries. The company needs to ensure that viewers\nwho are outside the countries to which the company rolls out content are not able to view the content.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A: \"Add geographic restrictions to the content in CloudFront by using an allow list. Set\nup a custom error message.\"\nHere's a detailed justification:\nThe scenario requires a phased rollout of video content based on geographic location. Users outside the\ntarget countries should not be able to access the content. CloudFront offers built-in geographic restrictions\n(Geo Restriction) that are ideal for this requirement. An allow list (or a whitelist) approach specifies the\ncountries where the content is allowed. This aligns directly with the need to allow content only in rolled-out\ncountries.\nUsing geographic restrictions, CloudFront automatically inspects the viewer's IP address and compares it\nagainst the configured allow list. If the viewer's location is not in the allowed countries, CloudFront blocks the\nrequest and can display a custom error message. This is the functionality exactly asked for in the prompt.\nOption B is incorrect because Signed URLs and Cookies primarily control access based on authentication and\nauthorization rather than geographic location. While they can be combined with other methods to achieve\ngeographic restriction, it's a more complex and less efficient solution than using CloudFront's built-in Geo\nRestriction. Signed URLs are more appropriate for restricting access based on user credentials or time-based\naccess, not location.\nOption C is incorrect because encrypting the data doesn't prevent access based on geographic location.\nEncryption protects the confidentiality of the data but doesn't control who can access it. Encryption\naddresses data security, not geographic access control. A user outside of the allowed region could\ntheoretically still download the encrypted data even if they cannot decrypt and view it.\nOption D is incorrect. Time-restricted access policies are not a solution to the problem.\nTherefore, CloudFront's Geo Restriction configured with an allow list offers the simplest, most efficient, and\nmost direct solution for the stated requirement.\nRefer to the official AWS documentation on CloudFront Geo Restriction for more information:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to use the AWS Cloud to improve its on-premises disaster recovery (DR) configuration. The\ncompany's core production business application uses Microsoft SQL Server Standard, which runs on a virtual\nmachine (VM). The application has a recovery point objective (RPO) of 30 seconds or fewer and a recovery time\nobjective (RTO) of 60 minutes. The DR solution needs to minimize costs wherever possible.\nWhich solution will meet these requirements?",
    "options": {
      "B": "Configure a warm standby Amazon RDS for SQL Server database on AWS."
    },
    "answer": "B",
    "explanation": "The correct answer is B. Configure a warm standby Amazon RDS for SQL Server database on AWS.\nConfigure AWS Database Migration Service (AWS DMS) to use change data capture (CDC).\nHere's why:\nThis solution strikes a balance between meeting the RPO and RTO requirements while minimizing costs, which\nis a primary concern. Using Amazon RDS for SQL Server in a warm standby configuration keeps a SQL Server\ninstance running on AWS but not actively serving production traffic. AWS DMS with change data capture\n(CDC) continuously replicates changes from the on-premises SQL Server to the RDS instance. This ensures\nthat the standby database is kept relatively up-to-date, meeting the 30-second RPO.\nIn the event of a disaster, the RDS instance can be promoted to become the primary database. While a warm\nstandby takes longer than an active/active setup, the cost savings are significant. The RDS instance is pre-\nconfigured and ready, reducing the time needed for recovery and aiming to meet the 60-minute RTO.\nOption A (Always On availability groups with SQL Server Enterprise) is very expensive due to the licensing\ncosts associated with SQL Server Enterprise. This option is also more complex to manage.\nOption C (AWS Elastic Disaster Recovery as pilot light) might not consistently meet the 30-second RPO, as it\ninvolves spinning up instances after a disaster. While Elastic Disaster Recovery can reduce costs through\nminimal infrastructure during normal operation, the restore process itself can impact the RTO if the database\nrequires significant recovery time.\nOption D (Backup software to S3) will not meet the RTO. Restoring from backups stored in S3 will take\nsignificantly longer than 60 minutes, especially for a substantial database.\nHere's a breakdown of why each solution is or isn't the best fit for RPO, RTO, and cost:\nRPO (Recovery Point Objective): The objective is 30 seconds. AWS DMS CDC keeps the warm standby\ndatabase reasonably close to real-time changes from the on-premises database. Options C and D fail here.\nRTO (Recovery Time Objective): The objective is 60 minutes. An already provisioned RDS instance can be\nfailed over relatively quickly. Options C and D require more setup time, exceeding the RTO. Option A will meet\nthe RTO but it is too expensive.\nCost: RDS warm standby with DMS is significantly cheaper than running SQL Server Enterprise with Always\nOn Availability Groups (Option A). Options C and D may be cheaper in terms of upfront costs, but they fail to\nmeet the recovery objectives.\nTherefore, option B is the most balanced solution considering all constraints.\nRelevant links:\nAWS DMS: https://aws.amazon.com/dms/\nAmazon RDS for SQL Server: https://aws.amazon.com/rds/sqlserver/\nAWS Elastic Disaster Recovery: https://aws.amazon.com/elastic-disaster-recovery/",
    "links": [
      "https://aws.amazon.com/dms/",
      "https://aws.amazon.com/rds/sqlserver/",
      "https://aws.amazon.com/elastic-disaster-recovery/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an on-premises server that uses an Oracle database to process and store customer information.\nThe company wants to use an AWS database service to achieve higher availability and to improve application\nperformance. The company also wants to offload reporting from its primary database system.\nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "options": {},
    "answer": "C",
    "explanation": "The most operationally efficient solution is C, utilizing Amazon RDS in a Multi-AZ cluster deployment for\nOracle. This addresses the requirements of high availability, improved application performance, and\noffloading reporting.\nMulti-AZ deployments inherently provide high availability by replicating data across multiple availability\nzones. If the primary instance fails, automatic failover to a standby instance ensures minimal downtime. This\nmeets the \"higher availability\" requirement efficiently, without needing to manage cross-region replication\n(option A).\nAn RDS Multi-AZ cluster provides a reader endpoint, also known as a reader instance, specifically designed\nfor read-heavy workloads such as reporting. By directing reporting functions to this reader instance, the load\non the primary database is significantly reduced, improving the performance of the primary database for\ntransaction processing. This fulfills the need to \"offload reporting from its primary database system\" and\n\"improve application performance.\"\nOption A, using AWS DMS for cross-region replication, adds operational complexity. DMS requires continuous\nreplication, which consumes resources and introduces potential latency. Furthermore, managing databases\nacross multiple regions increases operational overhead.\nOption B, using RDS in a Single-AZ deployment, contradicts the high availability requirement. A read replica in\nthe same AZ offers no resilience against AZ failures.\nOption D suggests using Amazon Aurora, which is a different database engine, which might require\napplication changes and a more complex migration. The question implied that the existing Oracle database\nshould be maintained as a critical requirement by mentioning \"Oracle database to process and store customer\ninformation\". Thus making it the most disruptive solution.\nTherefore, Option C strikes the best balance between meeting the requirements and minimizing operational\noverhead, as it provides high availability and read replica capabilities within a single, managed RDS cluster\nusing the same Oracle database engine.\nRelevant links:\nAmazon RDS Multi-AZ Deployments\nAmazon RDS Read Replicas",
    "links": []
  },
  {
    "question": "CertyIQ\nA company wants to build a web application on AWS. Client access requests to the website are not predictable and\ncan be idle for a long time. Only customers who have paid a subscription fee can have the ability to sign in and use\nthe web application.\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose three.)",
    "options": {
      "B": "ECS with an ALB and RDS, while viable, is a less cost-effective option for unpredictable workloads. ECS",
      "A": "Create an AWS Lambda function to retrieve user information from Amazon DynamoDB. Create an",
      "C": "Create an Amazon Cognito user pool to authenticate users. Cognito user pools are designed to handle",
      "D": "Cognito identity pools are used for providing temporary AWS credentials to users who are already"
    },
    "answer": "A",
    "explanation": "The correct answer is ACE. Here's why:\nA. Create an AWS Lambda function to retrieve user information from Amazon DynamoDB. Create an\nAmazon API Gateway endpoint to accept RESTful APIs. Send the API calls to the Lambda function. This is a\nsuitable backend architecture. API Gateway provides a managed, scalable entry point for the web application,\nhandling incoming requests and routing them to the Lambda function. Lambda offers a serverless, cost-\neffective way to process these requests and interact with DynamoDB. DynamoDB's NoSQL nature is excellent\nfor storing and retrieving user information with low latency. Since client access requests are unpredictable\nand can be idle, the serverless nature of Lambda ensures that you only pay for actual usage, making it cost-\neffective.\nC. Create an Amazon Cognito user pool to authenticate users. Cognito user pools are designed to handle\nuser registration, authentication, and password recovery. This addresses the requirement that only paying\nsubscribers can access the web application by controlling access to the application based on successful\nauthentication against the user pool. Cognito simplifies the authentication process and integrates seamlessly\nwith other AWS services.\nE. Use AWS Amplify to serve the frontend web content with HTML, CSS, and JS. Use an integrated Amazon\nCloudFront configuration. AWS Amplify is an ideal solution for hosting static frontend content. Amplify\noffers built-in capabilities for hosting, continuous deployment, and integration with other AWS services.\nIntegrating it with Amazon CloudFront provides a content delivery network (CDN) for faster content delivery\nand improved user experience globally. This is also cost-effective due to pay-as-you-go pricing and CDN\ncaching minimizing origin requests.\nWhy other options are less suitable:\nB. ECS with an ALB and RDS, while viable, is a less cost-effective option for unpredictable workloads. ECS\nrequires maintaining servers or containers even when idle, and RDS incurs database costs regardless of\nusage. Lambda scales down to zero when not in use, making it more efficient in terms of cost.\nD. Cognito identity pools are used for providing temporary AWS credentials to users who are already\nauthenticated, whether through a user pool or another identity provider. It doesn't directly handle user\nauthentication itself but rather grants access to AWS resources.\nF. S3 static web hosting, without Amplify, would necessitate managing complex configurations, and using\nPHP for static web hosting is generally incorrect. Also, Amplify provides additional features such as CI/CD\npipeline to make the whole process easier.\nSupporting Links:\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon API Gateway: https://aws.amazon.com/api-gateway/\nAmazon DynamoDB: https://aws.amazon.com/dynamodb/\nAmazon Cognito: https://aws.amazon.com/cognito/\nAWS Amplify: https://aws.amazon.com/amplify/\nAmazon CloudFront: https://aws.amazon.com/cloudfront/",
    "links": [
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/api-gateway/",
      "https://aws.amazon.com/dynamodb/",
      "https://aws.amazon.com/cognito/",
      "https://aws.amazon.com/amplify/",
      "https://aws.amazon.com/cloudfront/"
    ]
  },
  {
    "question": "CertyIQ\nA media company uses an Amazon CloudFront distribution to deliver content over the internet. The company wants\nonly premium customers to have access to the media streams and file content. The company stores all content in\nan Amazon S3 bucket. The company also delivers content on demand to customers for a specific purpose, such as\nmovie rentals or music downloads.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B: Generate and provide CloudFront signed URLs to premium customers.\nHere's a detailed justification:\nThe media company needs a way to restrict access to its content stored in S3, delivered via CloudFront, so\nthat only premium customers can access it. Several options are available, but CloudFront signed URLs are the\nmost appropriate for on-demand content delivery, like movie rentals and music downloads.\nSigned URLs are dynamically generated, time-limited URLs that grant access to specific resources on S3\nthrough the CloudFront distribution. The company can generate these URLs when a premium customer\nrequests content (e.g., rents a movie). The URL contains a cryptographic signature which verifies the\nauthenticity and validity of the request. By implementing this approach, the company retains precise control\nover who can access the content and for how long, facilitating its on-demand content business model.\nOption A, using S3 signed cookies, is more suited for controlling access to multiple restricted files, for\ninstance, an entire \"premium content\" section of a website. While it provides access control, it's less granular\nfor on-demand scenarios.\nOption C, using Origin Access Control (OAC), primarily secures the S3 bucket by allowing only CloudFront to\naccess it. While necessary for overall security, OAC doesn't differentiate between premium and non-premium\ncustomers. It only controls access to the origin (S3), not who is requesting content from CloudFront. Premium\ncustomer differentiation still needs to be handled within CloudFront.\nOption D, field-level encryption, focuses on encrypting specific data fields for security purposes during transit\nand at rest. It is not directly related to access control and wouldn't prevent non-premium users from\nattempting to access the media files.\nTherefore, generating and providing CloudFront signed URLs provides the most fine-grained and suitable\naccess control mechanism for the media company's requirements, specifically tailored to its on-demand\ncontent model.\nReference:\nUsing Signed URLs - Amazon CloudFront",
    "links": []
  },
  {
    "question": "CertyIQ\nA company runs Amazon EC2 instances in multiple AWS accounts that are individually bled. The company recently\npurchased a Savings Pian. Because of changes in the companys business requirements, the company has\ndecommissioned a large number of EC2 instances. The company wants to use its Savings Plan discounts on its\nother AWS accounts.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": {
      "A": "From the AWS Account Management Console of the management account, turn on discount sharing"
    },
    "answer": "A",
    "explanation": "The correct answer is AE because it leverages AWS Organizations to enable Savings Plan discount sharing\nacross accounts. Here's a detailed justification:\nA. From the AWS Account Management Console of the management account, turn on discount sharing\nfrom the billing preferences section.\nThis step is correct because it's the mechanism AWS Organizations provides for distributing the benefits of\nSavings Plans to other accounts within the organization. Without enabling discount sharing, Savings Plan\nbenefits are confined to the account that purchased it. The management account (payer account in AWS\nOrganizations) is the central point for controlling billing and cost management. This is done in the Billing\nconsole, specifically under Billing preferences where you configure Savings Plans sharing. This ensures the\ndiscounts aren't wasted and are efficiently applied.\nE. Create an organization in AWS Organizations in the existing AWS account with the existing EC2\ninstances and Savings Plan. Invite the other AWS accounts to join the organization from the management\naccount.\nThis step is necessary to group the AWS accounts together under a single management account. AWS\nOrganizations provides centralized governance and management across multiple AWS accounts. By creating\nan organization and inviting other accounts to join, you can leverage features like consolidated billing and,\nmost importantly in this case, Savings Plan sharing. Placing the Savings Plan-owning account as the\nmanagement account ensures the Savings Plan is readily available to be shared across the newly formed\norganization. Moving all accounts into an Organization allows centralized visibility and control.\nWhy the other options are incorrect:\nB: While discount sharing is configured from the existing account with the Savings Plan, doing it from the\n\"AWS Account Management Console\" and not correctly setting up an AWS Organization would not be the\ncorrect mechanism, as it has to be paired with AWS Organizations to work effectively. This option fails to\nconsider the underlying organizational structure required for proper sharing.\nC: AWS RAM (Resource Access Manager) is generally used for sharing specific resources within an AWS\norganization, not for broadly sharing Savings Plan benefits. While you can share specific compute resources\nwith RAM, savings plan discounts need to be organization-wide sharing which is handled directly by the\norganization settings. The purpose of RAM is different.\nD: Creating a new payer account adds unnecessary complexity. The savings plan and the associated EC2\ninstance utilization already exists in a specific account. Migrating everything to a new organization and payer\naccount is an overcomplicated solution. This would require migrating existing AWS infrastructure.\nAuthoritative Links:\nAWS Organizations: https://aws.amazon.com/organizations/\nSavings Plans Sharing in AWS Organizations:\nhttps://docs.aws.amazon.com/savingsplans/latest/userguide/sp-sharing.html\nAWS Resource Access Manager (RAM): https://aws.amazon.com/ram/",
    "links": [
      "https://aws.amazon.com/organizations/",
      "https://docs.aws.amazon.com/savingsplans/latest/userguide/sp-sharing.html",
      "https://aws.amazon.com/ram/"
    ]
  },
  {
    "question": "CertyIQ\nA retail company uses a regional Amazon API Gateway API for its public REST APIs. The API Gateway endpoint is a\ncustom domain name that points to an Amazon Route 53 alias record. A solutions architect needs to create a\nsolution that has minimal effects on customers and minimal data loss to release the new version of APIs.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Create a canary release deployment stage for API Gateway. Deploy the latest API"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Create a canary release deployment stage for API Gateway. Deploy the latest API\nversion. Point an appropriate percentage of traffic to the canary stage. After API verification, promote the\ncanary stage to the production stage.\nHere's a detailed justification:\nCanary Deployment Minimizes Risk: Canary deployment involves releasing a new version of the API to a\nsmall subset of users (defined by percentage of traffic). This allows you to test the new version in a real-world\nenvironment with minimal impact if issues arise.\nControlled Rollout: By directing a percentage of traffic to the canary stage, you can monitor the performance\nand stability of the new API version before exposing it to the entire user base. If any issues are detected, the\ntraffic can be quickly reverted to the original production version.\nMinimal Data Loss: A canary deployment inherently minimizes data loss as only a fraction of the requests are\nrouted to the new version initially. Any errors or data discrepancies will affect only a small group of users,\nlimiting the potential impact.\nSeamless Transition: Once the canary version is validated, promoting it to the production stage effectively\nreplaces the old version without disrupting the entire user base at once. This provides a smoother transition.\nAPI Gateway Support: Amazon API Gateway provides built-in support for canary deployments through its\ndeployment stages and traffic management features.\nAlternatives and Why They Aren't Best:\nB and C (Import-to-Update): While import-to-update can be used for API updates, it lacks the controlled rollout\nand risk mitigation of a canary deployment. An overwrite approach (option C) is particularly risky, as it\nimmediately replaces the existing API and could lead to widespread issues. Merging (option B) can be\ncomplex to manage and doesn't isolate the rollout for initial validation.\nD (New API Gateway Endpoint): Creating a new API Gateway endpoint with a custom domain name and\nchanging the Route 53 record leads to a potentially disruptive switchover. All users would immediately be\ndirected to the new API. While blue/green deployments can be useful, they require more infrastructure and\naren't as gradual as canary releases. Furthermore, it needs a DNS change, creating possible propagation\nissues.\nIn summary, a canary release in API Gateway provides a controlled, low-risk approach for deploying new API\nversions, ensuring minimal impact on customers and data loss.\nReference:AWS API Gateway Canary DeploymentsAWS API Gateway Deployment Stages",
    "links": []
  },
  {
    "question": "CertyIQ\nA company wants to direct its users to a backup static error page if the company's primary website is unavailable.\nThe primary website's DNS records are hosted in Amazon Route 53. The domain is pointing to an Application Load\nBalancer (ALB). The company needs a solution that minimizes changes and infrastructure overhead.\nWhich solution will meet these requirements?",
    "options": {
      "B": "D.Update the Route 53 records to use a multivalue answer routing policy. Create a health check. Direct traffic"
    },
    "answer": "B",
    "explanation": "The correct answer is B: Set up a Route 53 active-passive failover configuration. Direct traffic to a static error\npage that is hosted in an Amazon S3 bucket when Route 53 health checks determine that the ALB endpoint is\nunhealthy.\nHere's why this is the best solution and why the others are not ideal:\nActive-Passive Failover: Route 53's active-passive failover is designed precisely for scenarios where you have\na primary endpoint (the ALB in this case) and a backup endpoint (the static error page). Route 53 continuously\nmonitors the health of the primary endpoint using health checks. If the health check fails (indicating the ALB\nis down), Route 53 automatically starts routing traffic to the backup endpoint. This provides a seamless\nfailover experience for users.\nS3 Static Website Hosting: Hosting the static error page in an S3 bucket configured for static website\nhosting is a cost-effective and highly available solution. S3 offers high durability and availability, making it\nideal for serving static content.\nMinimizes Changes and Overhead: This solution minimizes changes because it leverages existing Route 53\nfunctionality and S3, which are common AWS services. It avoids the need for complex configurations or\nadditional infrastructure like EC2 instances specifically for error pages.\nLet's examine why the other options are less suitable:\nA (Latency Routing Policy): Latency routing routes traffic to the endpoint with the lowest latency. It's not\ndesigned for failover scenarios. If the ALB is unhealthy, latency routing won't necessarily direct traffic away\nfrom it.\nC (Active-Active with EC2): While active-active configurations are suitable for distributing traffic, using an\nEC2 instance to host a static error page adds unnecessary overhead and management complexity compared\nto S3. Furthermore, an EC2 instance requires patching, monitoring, and scaling, which are not needed for a\nsimple error page. Also, configuring health checks to only fail over to the EC2 instance after the ALB fails\nrequires custom configuration and is less efficient than Route 53's built-in failover mechanism.\nD (Multivalue Answer Routing Policy): While multivalue answer routing can provide redundancy, it's not\ndesigned for a true failover scenario. If the ALB is unhealthy, Route 53 might still return its IP address\nalongside the S3 bucket endpoint, potentially leading to inconsistent behavior for users. It distributes the\ntraffic across multiple healthy resources, not to a single backup resource when the primary is unhealthy.\nAuthoritative Links:\nRoute 53 Health Checks: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-\ncreating-deleting.html\nRoute 53 Failover: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html\nS3 Static Website Hosting: https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html",
    "links": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html"
    ]
  },
  {
    "question": "CertyIQ\nA recent analysis of a company's IT expenses highlights the need to reduce backup costs. The company's chief\ninformation officer wants to simplify the on-premises backup infrastructure and reduce costs by eliminating the\nuse of physical backup tapes. The company must preserve the existing investment in the on-premises backup\napplications and workflows.\nWhat should a solutions architect recommend?",
    "options": {
      "D": "Set up AWS Storage Gateway to connect with the backup applications using the",
      "A": "NFS interface with Storage Gateway: While the NFS interface could allow backup applications to write to",
      "B": "& C. Amazon EFS: Amazon EFS is a file storage service ideal for shared file systems but not typically used"
    },
    "answer": "D",
    "explanation": "The best solution is D. Set up AWS Storage Gateway to connect with the backup applications using the\niSCSI-virtual tape library (VTL) interface.\nHere's why:\nPreserves Existing Investment: The question explicitly requires maintaining the existing on-premises backup\napplications and workflows. The VTL interface of AWS Storage Gateway is designed to seamlessly integrate\nwith existing backup software that's traditionally used with physical tape libraries. It presents itself as a\nvirtual tape library to the on-premises application, requiring minimal or no changes to the backup process.\nEliminates Physical Tapes: The VTL interface allows the backup application to write data to virtual tapes\nstored in AWS. These virtual tapes are then stored in Amazon S3 or Amazon S3 Glacier Deep Archive,\neffectively replacing the physical tapes and eliminating the need for physical tape management.\nCost Reduction: By moving backups to AWS storage services (S3 or Glacier), the company can leverage the\ncost-effective nature of cloud storage, particularly S3 Glacier Deep Archive for long-term archival, leading to\nsignificant cost savings compared to managing physical tapes.\nAWS Storage Gateway's Capabilities: AWS Storage Gateway provides a bridge between on-premises\nenvironments and AWS. Its VTL interface is specifically designed for tape backup replacement.\nWhy Other Options Are Less Suitable:\nA. NFS interface with Storage Gateway: While the NFS interface could allow backup applications to write to\nAWS, it's less tailored to the existing backup workflow using tape libraries. It might require more significant\nchanges to the backup application configuration.\nB. & C. Amazon EFS: Amazon EFS is a file storage service ideal for shared file systems but not typically used\nas a direct replacement for tape-based backups. Integrating EFS with backup software designed for tape\ndrives would require more substantial application modifications and may not be the most efficient approach.\nThe iSCSI interface of EFS is primarily intended for database, web server and development workloads, not\nlarge scale backup scenarios.\nIn conclusion, the VTL interface of AWS Storage Gateway directly addresses the core requirement of\nreplacing physical tapes while preserving the existing on-premises backup applications and workflows with\nminimal disruption, enabling cost reduction through the use of AWS storage services.\nFurther Research:\nAWS Storage Gateway: https://aws.amazon.com/storagegateway/\nAWS Storage Gateway VTL: https://docs.aws.amazon.com/storagegateway/latest/userguide/VTL.html",
    "links": [
      "https://aws.amazon.com/storagegateway/",
      "https://docs.aws.amazon.com/storagegateway/latest/userguide/VTL.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has data collection sensors at different locations. The data collection sensors stream a high volume of\ndata to the company. The company wants to design a platform on AWS to ingest and process high-volume\nstreaming data. The solution must be scalable and support data collection in near real time. The company must\nstore the data in Amazon S3 for future reporting.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A: Use Amazon Kinesis Data Firehose to deliver streaming data to Amazon S3. Here's\nwhy:\nKinesis Data Firehose is specifically designed for near real-time ingestion, transformation, and loading of\nstreaming data into data lakes, data stores, and analytics services like Amazon S3. It provides a managed\nservice, thus minimizing operational overhead. Firehose automatically scales to handle varying data volumes,\nbuffers data, and can optionally transform it before delivering to S3. It also handles error management and\nretries, simplifying the data ingestion pipeline.\nOption B is incorrect because AWS Glue is primarily an ETL (Extract, Transform, Load) service focused on\npreparing data for analytics, typically from databases or data warehouses. While Glue can process data, it is\nnot optimized for continuous, high-volume streaming data ingestion like Kinesis Data Firehose is. It involves\nmore setup and operational overhead for streaming data scenarios.\nOption C is incorrect because while Lambda can process data, directly using Lambda to handle high-volume\nstreaming data and store it in S3 would lead to increased operational complexity. Managing scaling, error\nhandling, and concurrency for Lambda functions in such a scenario requires significant effort. Furthermore,\nLambda functions have execution time limits and payload size restrictions that might hinder processing high-\nvolume streaming data reliably. Lambda is typically used for event-driven processing, often triggered by\nFirehose or other streaming services.\nOption D is incorrect because AWS DMS is designed for database migration, not for general streaming data\ningestion. It is used to migrate data between different database platforms, not to collect and deliver\nstreaming data to Amazon S3 for analytical purposes. Using DMS for this purpose would be an inappropriate\nand highly inefficient solution.\nIn summary, Kinesis Data Firehose is the best choice because it is a managed service specifically built for\ningesting streaming data into S3 with minimal operational overhead, automatic scaling, and built-in buffering\nand transformation capabilities.\nFor further reading:\nAmazon Kinesis Data Firehose: https://aws.amazon.com/kinesis/data-firehose/\nAWS Glue: https://aws.amazon.com/glue/\nAWS Lambda: https://aws.amazon.com/lambda/\nAWS Database Migration Service: https://aws.amazon.com/dms/",
    "links": [
      "https://aws.amazon.com/kinesis/data-firehose/",
      "https://aws.amazon.com/glue/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/dms/"
    ]
  },
  {
    "question": "CertyIQ\nA company has separate AWS accounts for its finance, data analytics, and development departments. Because of\ncosts and security concerns, the company wants to control which services each AWS account can use.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B: Create organization units (OUs) for each department in AWS Organizations. Attach\nservice control policies (SCPs) to the OUs.\nHere's a detailed justification:\nAWS Organizations enables centralized management and governance across multiple AWS accounts. By\ncreating separate OUs for each department (finance, data analytics, and development), the company can\ngroup the accounts logically based on their functional areas. Service Control Policies (SCPs) are a powerful\nfeature of AWS Organizations that allows you to define policies that specify the maximum permissions\navailable to the accounts within an OU. These policies act as guardrails, preventing accounts from performing\nactions that are not explicitly allowed, regardless of the IAM permissions granted within individual accounts.\nSCPs address the company's need to control which services each AWS account can use, aligning with both\ncost and security concerns. The SCPs restrict service access at the organizational level, providing centralized\ncontrol. This centralized approach minimizes operational overhead because policies are defined and managed\nin one place (the AWS Organizations management account), rather than individually across multiple accounts.\nSCPs inherit down the organizational hierarchy, ensuring consistent policy enforcement.\nOption A is incorrect because AWS Systems Manager templates are primarily used for automating\ninfrastructure management tasks within a single account, not for controlling service access across multiple\naccounts in an organization. It lacks the centralized policy enforcement capability offered by SCPs.\nOption C is not the best solution because AWS CloudFormation is designed for infrastructure-as-code and\nprovisioning resources. While you can control which services are used through CloudFormation templates, it\ndoesn't provide a centralized, organization-wide policy enforcement mechanism like SCPs. Managing separate\nCloudFormation stacks for each department to restrict service usage would increase operational complexity.\nOption D is incorrect because AWS Service Catalog focuses on creating and managing catalogs of IT services\nthat users can request. It helps standardize the deployment of applications and resources but does not\nprevent accounts from directly accessing or using restricted AWS services. It's more about managing product\nofferings, not controlling access across accounts.\nIn summary, AWS Organizations with SCPs provides the most efficient and centralized way to control service\naccess across multiple AWS accounts, minimizing operational overhead and meeting the company's\nrequirements.\nRelevant links:\nAWS Organizations: https://aws.amazon.com/organizations/\nService Control Policies (SCPs):\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html",
    "links": [
      "https://aws.amazon.com/organizations/",
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has created a multi-tier application for its ecommerce website. The website uses an Application Load\nBalancer that resides in the public subnets, a web tier in the public subnets, and a MySQL cluster hosted on\nAmazon EC2 instances in the private subnets. The MySQL database needs to retrieve product catalog and pricing\ninformation that is hosted on the internet by a third-party provider. A solutions architect must devise a strategy\nthat maximizes security without increasing operational overhead.\nWhat should the solutions architect do to meet these requirements?",
    "options": {
      "C": "Directly routing the database tier's traffic to the internet gateway would expose the database instances to",
      "B": "Deploy a NAT gateway in the public subnets. Modify the private subnet route table"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Deploy a NAT gateway in the public subnets. Modify the private subnet route table\nto direct all internet-bound traffic to the NAT gateway.\nHere's a detailed justification:\nThe requirement is to allow the MySQL database instances in the private subnets to access the internet to\nretrieve product catalog information from a third-party provider while maximizing security and minimizing\noperational overhead. The database instances should not be directly exposed to the internet.\nNAT Gateway (Option B): A NAT (Network Address Translation) gateway allows instances in a private subnet\nto connect to the internet or other AWS services, but prevents the internet from initiating a connection with\nthose instances. This is a key security feature. Deploying a NAT gateway in the public subnets and routing the\ndatabase tier's internet-bound traffic through it ensures that the database instances can reach the third-party\nservice without being directly exposed. NAT gateway is also managed by AWS, so the operational overhead is\nminimal. It automatically scales based on demand.\nNAT Instance (Option A): While a NAT instance can provide similar functionality to a NAT gateway, it requires\nmore manual configuration and management. You need to manage patching, scaling, and high availability. This\nincreases operational overhead, which contradicts the requirements. Further, the NAT instance can become a\nbottleneck as it is a single EC2 instance.\nInternet Gateway (Option C): An internet gateway allows direct internet access to resources within the VPC.\nDirectly routing the database tier's traffic to the internet gateway would expose the database instances to\nthe internet, which compromises security. This violates the requirement to maximize security. This would\nessentially make the subnet a public subnet.\nVirtual Private Gateway (Option D): A virtual private gateway is used to connect the VPC to an on-premises\nnetwork using VPN or Direct Connect. It is not used for general internet access from private subnets. It's\npurpose is to allow access into the VPC, which is the opposite of what is needed.\nTherefore, a NAT gateway is the optimal solution because it provides secure internet access from private\nsubnets without direct internet exposure and has minimal operational overhead due to being a managed\nservice.\nAuthoritative Links:\nNAT Gateway: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\nNAT Instances vs NAT Gateways: https://aws.amazon.com/premiumsupport/knowledge-center/nat-gateway-\ninstance/",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/nat-gateway-"
    ]
  },
  {
    "question": "CertyIQ\nA company is using AWS Key Management Service (AWS KMS) keys to encrypt AWS Lambda environment\nvariables. A solutions architect needs to ensure that the required permissions are in place to decrypt and use the\nenvironment variables.\nWhich steps must the solutions architect take to implement the correct permissions? (Choose two.)",
    "options": {
      "B": "Add AWS KMS permissions in the Lambda execution role: The Lambda function's execution role is the",
      "D": "Allow the Lambda execution role in the AWS KMS key policy: The KMS key policy defines who can use the",
      "A": "Add AWS KMS permissions in the Lambda resource policy: The Lambda resource policy grants",
      "C": "Add AWS KMS permissions in the Lambda function policy: The Lambda function policy isn't a typical term."
    },
    "answer": "B",
    "explanation": "Let's break down why options B and D are the correct choices for managing KMS permissions when Lambda\nuses KMS-encrypted environment variables.\nB. Add AWS KMS permissions in the Lambda execution role: The Lambda function's execution role is the\nIAM role assumed by Lambda when it executes your code. It needs permission to decrypt the environment\nvariables using the KMS key. Therefore, the execution role must explicitly grant the kms:Decrypt permission\n(and potentially kms:GenerateDataKey if data keys are being used). This allows the Lambda function, when\nrunning, to call the KMS API to decrypt the environment variables before the function code executes.\nD. Allow the Lambda execution role in the AWS KMS key policy: The KMS key policy defines who can use the\nkey. It needs to explicitly allow the Lambda execution role to use the key for decryption. This is achieved by\nadding a statement to the key policy granting the kms:Decrypt permission to the Lambda execution role's\nARN. Without this, KMS will deny the Lambda function's request to decrypt the environment variables.\nEssentially, the execution role says, \"I need to decrypt something,\" and the KMS key policy says, \"Okay, I trust\nthis role to decrypt using this key.\"\nLet's look at why the other options are incorrect:\nA. Add AWS KMS permissions in the Lambda resource policy: The Lambda resource policy grants\npermissions to entities outside of Lambda to invoke the function. It's primarily used to allow services like API\nGateway or S3 to trigger the Lambda function. It is not used for managing permissions related to decrypting\nenvironment variables within the Lambda function's execution environment.\nC. Add AWS KMS permissions in the Lambda function policy: The Lambda function policy isn't a typical term.\nLambda uses a resource-based policy (the resource policy mentioned above) to manage who can invoke it, and\nuses an execution role to manage what the Lambda function can access. There isn't a separate \"function\npolicy\" that would be relevant here.\nE. Allow the Lambda resource policy in the AWS KMS key policy: The resource policy controls who can\ninvoke the Lambda function. The decryption process is related to what the Lambda function does after it's\ninvoked. Therefore, putting the resource policy in the KMS key policy is incorrect. The key policy needs to\ntrust the execution role which is doing the decrypting, not whoever is invoking the Lambda function.\nIn short, Lambda's execution role assumes permissions to interact with AWS resources and the KMS key\npolicy grants those permissions to decrypt the variables.\nFurther research links:\nAWS KMS Key Policies\nUsing AWS Lambda environment variables\nAWS Lambda Execution Role",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has a financial application that produces reports. The reports average 50 KB in size and are stored in\nAmazon S3. The reports are frequently accessed during the first week after production and must be stored for\nseveral years. The reports must be retrievable within 6 hours.\nWhich solution meets these requirements MOST cost-effectively?",
    "options": {},
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A is the most cost-effective solution, along with supporting\nconcepts and links:\nThe problem requires frequent access for the first week and long-term archival with a retrieval time of under\n6 hours. The cost-effectiveness is paramount.\nOption A leverages S3 Standard for the initial frequent access period. S3 Standard offers high availability,\ndurability, and performance suitable for frequently accessed objects. After 7 days, the lifecycle rule\nautomatically transitions the reports to S3 Glacier.\nS3 Glacier is designed for low-cost archival storage. It's significantly cheaper than S3 Standard and S3\nStandard-IA for long-term storage. The retrieval time for S3 Glacier, even Glacier Flexible Retrieval, meets\nthe requirement of being retrievable within 6 hours.\nOption B is less cost-effective because S3 Standard-IA is more expensive than S3 Glacier for long-term\nstorage. While S3 Standard-IA provides faster retrieval than Glacier, this isn't necessary after the first week,\nso the extra cost is not justified.\nOption C, S3 Intelligent-Tiering, automatically moves data between tiers based on access patterns. However,\nsince the access pattern is clearly defined (frequent for 1 week, infrequent after), manually controlled\ntransitions using Lifecycle rules provide better cost optimization in this scenario. Intelligent Tiering also has a\nmonitoring cost that may be unnecessary.\nOption D, transitioning to S3 Glacier Deep Archive after 7 days, would not be ideal since the minimum retrieval\ntime for Glacier Deep Archive is 12 hours, violating the 6-hour requirement. Also, for archival, it would be even\nmore cost-effective than Glacier, the slower retreival time is not useful.\nTherefore, the best balance of cost and retrieval time is achieved by storing the data initially in S3 Standard\nand then transitioning it to S3 Glacier using a lifecycle rule after one week.\nKey Concepts:\nS3 Storage Classes: Understanding the different S3 storage classes (Standard, Standard-IA, Glacier, Deep\nArchive) and their cost/performance trade-offs is crucial.\nS3 Lifecycle Rules: Lifecycle rules automate the movement of objects between storage classes or their\ndeletion, reducing storage costs.\nCost Optimization: Choosing the most cost-effective storage solution involves analyzing access patterns and\nselecting the appropriate storage class with the desired balance of availability, durability, and retrieval time.\nAuthoritative Links:\nAmazon S3 Storage Classes: https://aws.amazon.com/s3/storage-classes/\nS3 Lifecycle Management: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-\nconfiguration-examples.html\nS3 Glacier: https://aws.amazon.com/glacier/",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-",
      "https://aws.amazon.com/glacier/"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to optimize the cost of its Amazon EC2 instances. The company also needs to change the type\nand family of its EC2 instances every 2-3 months.\nWhat should the company do to meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the correct answer, along with supporting information:\nThe company's primary goals are cost optimization for EC2 instances and flexibility to change instance types\nand families frequently (every 2-3 months). Let's analyze why each option is either suitable or unsuitable:\nOption A: Purchase Partial Upfront Reserved Instances for a 3-year term. While Reserved Instances offer\ndiscounts, a 3-year term introduces inflexibility. The company needs to change instance types frequently,\nmaking a long-term commitment less desirable. If instance types are changed, the Reserved Instance might\nnot apply, wasting money.\nOption B: Purchase a No Upfront Compute Savings Plan for a 1-year term. Compute Savings Plans provide\nsignificant discounts compared to On-Demand pricing, similar to Reserved Instances. The key advantage is\nflexibility. Compute Savings Plans apply to EC2 instances regardless of instance family, size, OS, or tenancy,\nas long as compute usage stays within the committed amount. Because it is a \"no upfront\" option, the\ncompany can benefit from savings while retaining the ability to change instance types as needed. A 1-year\nterm balances cost savings with flexibility better than a 3-year term.\nOption C: Purchase All Upfront Reserved Instances for a 1-year term. Similar to option A, Reserved Instances\nlack the flexibility the company needs. An \"all upfront\" payment means the company pays the full cost\nupfront, which might not be ideal if resources change quickly.\nOption D: Purchase an All Upfront EC2 Instance Savings Plan for a 1-year term. Instance Savings Plans offer\ndiscounts specific to instance families within a region. While providing savings, these plans are less flexible\nthan Compute Savings Plans because they are tied to specific instance types/families. The all upfront cost is\nnot desirable, and the limited scope compared to compute savings plans makes this option worse.\nWhy Compute Savings Plans are optimal:\nCompute Savings Plans are designed for scenarios where consistent compute usage is expected, but the\nspecific EC2 instance configurations might change. They provide a commitment to a certain amount of\ncompute spending per hour across a region and are suitable for different instance types, sizes, operating\nsystems, and tenancies. This flexibility aligns perfectly with the company's need to change instance types\nfrequently.\nIn Summary: Option B, purchasing a No Upfront Compute Savings Plan for a 1-year term, provides the best\nbalance of cost savings and flexibility to accommodate the company's evolving EC2 instance requirements.\nAuthoritative Links:\nAWS Savings Plans: https://aws.amazon.com/savingsplans/\nAmazon EC2 Pricing: https://aws.amazon.com/ec2/pricing/ (compare Savings Plans to other options)",
    "links": [
      "https://aws.amazon.com/savingsplans/",
      "https://aws.amazon.com/ec2/pricing/"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect needs to review a company's Amazon S3 buckets to discover personally identifiable\ninformation (PII). The company stores the PII data in the us-east-1 Region and us-west-2 Region.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A because Amazon Macie is specifically designed for discovering and protecting\nsensitive data like PII stored in Amazon S3. Macie automates the process of identifying PII by leveraging\nmachine learning and pattern matching techniques. Configuring Macie in each region (us-east-1 and us-west-\n2) and creating a job to analyze the S3 data allows the solution architect to pinpoint the exact locations where\nPII is stored. This requires minimal configuration and ongoing maintenance.\nOption B is incorrect because AWS Security Hub, while providing a consolidated view of security alerts and\ncompliance status, doesn't inherently analyze the contents of S3 buckets for PII. Creating an AWS Config rule\nmight identify if the S3 buckets have appropriate encryption or access controls, but it won't perform content\nanalysis to locate PII. Security Hub primarily aggregates findings from other services.\nOption C is wrong because Amazon Inspector is a vulnerability management service focused on assessing EC2\ninstances and container images for software vulnerabilities and unintended network exposure. It doesn't\ndirectly analyze data within S3 buckets for PII.\nOption D is incorrect because Amazon GuardDuty is a threat detection service that monitors for malicious\nactivity and unauthorized behavior within the AWS environment. It doesn't inspect the contents of S3 buckets\nfor PII. GuardDuty focuses on network traffic, API calls, and log analysis to detect threats, not data content.\nTherefore, Macie provides the most efficient and specialized solution for the problem of identifying PII within\nS3 buckets with the least operational overhead. It directly addresses the requirement to discover sensitive\ndata stored within S3.\nFurther research:\nAmazon Macie: https://aws.amazon.com/macie/",
    "links": [
      "https://aws.amazon.com/macie/"
    ]
  },
  {
    "question": "CertyIQ\nA company's SAP application has a backend SQL Server database in an on-premises environment. The company\nwants to migrate its on-premises application and database server to AWS. The company needs an instance type\nthat meets the high demands of its SAP database. On-premises performance data shows that both the SAP\napplication and the database have high memory utilization.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C because the problem statement explicitly mentions high memory utilization for both\nthe SAP application and the SQL Server database.\nOption A is incorrect because while the application server might benefit from compute optimization, the\ndatabase server requires memory optimization due to the stated high memory usage. Compute-optimized\ninstances focus on CPU performance rather than memory.\nOption B is incorrect because storage-optimized instances are designed for applications that require high,\nsequential read and write access to large datasets on local storage. This does not address the problem's core\nrequirement of high memory utilization for the application and database.\nOption D is incorrect because HPC-optimized instances are tailored for computationally intensive workloads\nlike scientific simulations, weather modeling, and deep learning, which are not implied in this scenario of\nmigrating an SAP application and SQL Server database. The primary constraint here is memory, not extreme\ncomputational power.\nMemory-optimized instances are designed to deliver fast performance for workloads that process large\ndatasets in memory. Instance families like R5, R6i, and X2gd are excellent choices for memory-intensive\napplications. The description of the scenario clearly indicates the importance of in-memory performance for\nboth the SAP application and the SQL Server database. Therefore, using memory-optimized instances for both\nensures that the applications can efficiently access and process data, preventing performance bottlenecks\ndue to memory constraints.\nFurther research:\nAWS EC2 Instance Types: https://aws.amazon.com/ec2/instance-types/ - This page provides detailed\ninformation about all EC2 instance types and their intended use cases.\nMemory Optimized Instances: https://aws.amazon.com/ec2/instance-types/#Memory_optimized -\nSpecifically reviews the families best for in memory intensive tasks.",
    "links": [
      "https://aws.amazon.com/ec2/instance-types/",
      "https://aws.amazon.com/ec2/instance-types/#Memory_optimized"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an application in a VPC with public and private subnets. The VPC extends across multiple\nAvailability Zones. The application runs on Amazon EC2 instances in private subnets. The application uses an\nAmazon Simple Queue Service (Amazon SQS) queue.\nA solutions architect needs to design a secure solution to establish a connection between the EC2 instances and\nthe SQS queue.\nWhich solution will meet these requirements?",
    "options": {
      "B": "SQS requires an interface endpoint."
    },
    "answer": "A",
    "explanation": "The correct solution to securely connect EC2 instances in private subnets to an SQS queue is to use an\ninterface VPC endpoint configured within the private subnets. Here's why:\nInterface VPC Endpoints for SQS: Interface endpoints provide private connectivity to AWS services, in this\ncase, SQS, without exposing traffic to the public internet. This enhances security and reduces latency.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\nPlacement in Private Subnets: Placing the interface endpoint in the private subnets ensures that EC2\ninstances can reach SQS privately, as the instances themselves reside within the private subnets and are not\nexposed to the public internet.\nSecurity Group Rules: Security groups act as virtual firewalls. An inbound rule on the endpoint's security\ngroup should allow traffic from the EC2 instances' security group. This controls which instances can access\nthe SQS queue through the endpoint.\nVPC Endpoint Policies: While VPC endpoint policies can restrict access, the primary control here is through\nthe security group associated with the interface endpoint, simplifying the configuration and security\nmanagement.\nWhy other options are incorrect:\nPublic subnets: Placing the endpoint in public subnets defeats the purpose of private connectivity.\nGateway Endpoints: Gateway endpoints only support S3 and DynamoDB. SQS requires an interface endpoint.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html\nNAT Gateway + IAM Role: While a NAT gateway enables outbound internet access from private subnets and\nan IAM role grants permissions, this approach still requires traffic to traverse the public internet to reach\nSQS, which is less secure. Furthermore, it's an unnecessary and costlier setup when private connectivity using\nan interface endpoint is available.\nSQS Access Policy: While SQS access policies are relevant, in the context of VPC endpoints the endpoint\npolicy and security group configurations together provide a better approach for defining access based on the\nVPC endpoint itself and the security contexts of the resources making the requests.\nTherefore, implementing an interface VPC endpoint in the private subnets with appropriate security group\nrules provides the most secure and efficient solution.",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is using an AWS CloudFormation template to deploy a three-tier web application. The web\napplication consists of a web tier and an application tier that stores and retrieves user data in Amazon DynamoDB\ntables. The web and application tiers are hosted on Amazon EC2 instances, and the database tier is not publicly\naccessible. The application EC2 instances need to access the DynamoDB tables without exposing API credentials\nin the template.\nWhat should the solutions architect do to meet these requirements?",
    "options": {
      "B": "Option C is incorrect because it involves asking the user to manually input access and secret keys. This is bad"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Here's a detailed justification:\nThe core requirement is to grant EC2 instances in the application tier access to DynamoDB tables securely\nwithout hardcoding or exposing credentials within the CloudFormation template or instance configuration.\nIAM roles provide a secure way to grant permissions to AWS resources without managing long-term\ncredentials directly within the application or infrastructure.\nOption B utilizes IAM roles and instance profiles, which is the recommended approach for granting\npermissions to EC2 instances. An IAM role is created with the necessary permissions (read and write access)\nto DynamoDB tables. This role is then associated with an EC2 instance profile. The instance profile acts as a\ncontainer for the IAM role and is automatically applied to the EC2 instances when they are launched. When\nthe application running on the EC2 instance makes calls to DynamoDB, the AWS SDK automatically uses the\ncredentials provided by the instance profile to authenticate. This eliminates the need to store or manage\naccess keys directly on the instance or within the CloudFormation template. This approach aligns with the\nprinciple of least privilege, granting only the necessary permissions to the EC2 instances.\nOption A is partially correct because it suggests using an IAM role. However, it doesn't explicitly mention\nassigning write permissions to the IAM role. Therefore, it's less complete than Option B.\nOption C is incorrect because it involves asking the user to manually input access and secret keys. This is bad\npractice because it requires manual intervention, poses a security risk if keys are compromised, and is not a\nscalable or automated solution. It goes against the principle of minimizing human intervention.\nOption D is also incorrect and a very bad practice. Creating IAM users within the CloudFormation template and\nattempting to retrieve and pass credentials via user data is insecure. Secret and access keys are long-term\ncredentials that should never be passed via user data because user data is easily accessible and can be\nlogged. Furthermore, hardcoding credentials in templates increases the risk of accidental exposure.\nIn summary, leveraging IAM roles and instance profiles provides a secure, automated, and auditable\nmechanism for granting EC2 instances access to other AWS services like DynamoDB, fulfilling the\nrequirements without compromising security.\nFor further research:\nIAM Roles for Amazon EC2: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_ec2.html\nGranting Applications Running on Amazon EC2 Instances Access to AWS Resources:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_delegation.html\nAWS CloudFormation: https://aws.amazon.com/cloudformation/",
    "links": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_ec2.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_delegation.html",
      "https://aws.amazon.com/cloudformation/"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect manages an analytics application. The application stores large amounts of semistructured\ndata in an Amazon S3 bucket. The solutions architect wants to use parallel data processing to process the data\nmore quickly. The solutions architect also wants to use information that is stored in an Amazon Redshift database\nto enrich the data.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Use Amazon Athena to process the S3 data. Use AWS Glue with the Amazon",
      "B": "Use Amazon EMR to process the S3 data. Use Amazon EMR with the Amazon Redshift data to enrich the",
      "C": "Use Amazon EMR to process the S3 data. Use Amazon Kinesis Data Streams to move the S3 data into",
      "D": "Use AWS Glue to process the S3 data. Use AWS Lake Formation with the Amazon Redshift data to enrich"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Use Amazon Athena to process the S3 data. Use AWS Glue with the Amazon\nRedshift data to enrich the S3 data.\nHere's why:\nAmazon Athena for S3 data processing: Athena is a serverless, interactive query service that makes it easy\nto analyze data directly in Amazon S3 using standard SQL. It's well-suited for processing large amounts of\nsemi-structured data stored in S3 because it doesn't require setting up or managing any infrastructure.\nAthena excels at parallel data processing directly on S3 data, allowing for faster query execution.\nhttps://aws.amazon.com/athena/\nAWS Glue for data enrichment: AWS Glue is a fully managed extract, transform, and load (ETL) service that\nmakes it easy to prepare and load data for analytics. Glue can access data in Amazon Redshift and transform\nit to enrich the data stored in S3. Glue's capabilities include data cataloging, code generation, and job\nscheduling, which can be leveraged to orchestrate the data enrichment process. Glue is also capable of\nperforming parallel processing using Spark. https://aws.amazon.com/glue/\nNow, let's analyze why the other options are less suitable:\nB. Use Amazon EMR to process the S3 data. Use Amazon EMR with the Amazon Redshift data to enrich the\nS3 data. While EMR can process S3 data in parallel, it's generally an overkill for simple SQL queries. Athena is\nmore cost-effective and easier to manage for such use cases. Using EMR to enrich data from Redshift might\ninvolve more complex setup and maintenance than using Glue, which is specifically designed for ETL tasks.\nC. Use Amazon EMR to process the S3 data. Use Amazon Kinesis Data Streams to move the S3 data into\nAmazon Redshift so that the data can be enriched. Kinesis Data Streams is designed for real-time data\nstreaming, which isn't the core requirement here. The problem describes a batch-oriented analytics\napplication. Moving all the S3 data into Redshift simply for enrichment is less efficient and more costly\ncompared to enriching it in place using Glue.\nD. Use AWS Glue to process the S3 data. Use AWS Lake Formation with the Amazon Redshift data to enrich\nthe S3 data. Glue is typically used for ETL, not for interactive querying like the solutions architect intended to\ndo. While Lake Formation helps manage and secure data lakes, it's not directly involved in the data\nenrichment process itself. Glue performs the actual enrichment, potentially making use of Lake Formation's\ngovernance features for data access.",
    "links": [
      "https://aws.amazon.com/athena/",
      "https://aws.amazon.com/glue/"
    ]
  },
  {
    "question": "CertyIQ\nA company has two VPCs that are located in the us-west-2 Region within the same AWS account. The company\nneeds to allow network traffic between these VPCs. Approximately 500 GB of data transfer will occur between the\nVPCs each month.\nWhat is the MOST cost-effective solution to connect these VPCs?",
    "options": {},
    "answer": "C",
    "explanation": "The most cost-effective solution for connecting two VPCs within the same region and account, with a\nmoderate data transfer requirement (500 GB/month), is VPC peering. VPC peering allows direct networking\nbetween VPCs using AWS's internal network backbone.\nOption A, AWS Transit Gateway, is more suitable for connecting many VPCs (hub-and-spoke model) or\nconnecting to on-premises networks. While it offers more features than VPC peering, it incurs higher costs,\nincluding hourly charges for the Transit Gateway itself and data processing charges.\nOption B, AWS Site-to-Site VPN, introduces overhead related to IPsec encryption and decryption. It's better\nsuited for connecting to on-premises networks or across regions where direct peering isn't possible. VPN\ntunnels also have bandwidth limitations which could impact performance for larger data transfers.\nOption D, AWS Direct Connect, is designed for establishing a dedicated network connection between on-\npremises infrastructure and AWS. It's overkill for connecting VPCs within the same region and account and is\nsignificantly more expensive due to monthly port fees and hourly charges. It's targeted towards hybrid cloud\nenvironments where consistent and high-bandwidth connectivity is critical between the on-premises data\ncenter and AWS.\nVPC peering involves setting up a peering connection request and accepting it. Once established, route tables\nin each VPC are updated to direct traffic destined for the other VPC's CIDR block through the peering\nconnection. The primary cost associated with VPC peering is data transfer charges between the VPCs, which\nare generally lower than the costs associated with Transit Gateway, VPN, or Direct Connect for this particular\nuse case. Therefore, VPC peering is the simplest and most cost-effective solution for the given scenario.\nRefer to these links for further information:\nVPC Peering: https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\nAWS Transit Gateway: https://aws.amazon.com/transit-gateway/\nAWS Site-to-Site VPN: https://aws.amazon.com/vpn/\nAWS Direct Connect: https://aws.amazon.com/directconnect/",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html",
      "https://aws.amazon.com/transit-gateway/",
      "https://aws.amazon.com/vpn/",
      "https://aws.amazon.com/directconnect/"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts multiple applications on AWS for different product lines. The applications use different compute\nresources, including Amazon EC2 instances and Application Load Balancers. The applications run in different AWS\naccounts under the same organization in AWS Organizations across multiple AWS Regions. Teams for each\nproduct line have tagged each compute resource in the individual accounts.\nThe company wants more details about the cost for each product line from the consolidated billing feature in\nOrganizations.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is BE. Here's a detailed justification:\nThe problem requires cost allocation based on product lines using tags that are already applied to resources\nacross multiple AWS accounts within an Organization. AWS Organizations provides a consolidated billing\nfeature, which allows for aggregated cost tracking. To leverage this feature effectively with existing tags,\ncertain steps are essential within the AWS Billing console.\nOption B is correct because the company has already applied user-defined tags (specific to product lines) to\ntheir resources. To utilize these tags for cost allocation, they must be activated in the AWS Billing console.\nAWS-generated tags typically serve other purposes and don't align with the specific product line breakdown\nthe company requires, making option A incorrect.\nOption E is correct because tag activation for cost allocation must occur from the AWS Organizations\nmanagement account. This centralized activation ensures that the tag is enabled across all member accounts\ncontributing to the consolidated bill, enabling the cost allocation to be displayed correctly in billing reports.\nActivating the tags in individual accounts (option D) would not centrally manage the cost allocation across the\nentire Organization. The AWS Resource Groups console (option C) is designed for managing and organizing\nresources, not for activating tags for cost allocation purposes in billing.\nIn summary, selecting the appropriate user-defined tag in the AWS Billing console and activating it from the\nOrganizations management account enables the cost allocation based on product lines across all member\naccounts within the AWS Organization, thus meeting the company's requirements.\nSupporting Documentation:\nAWS Organizations Consolidated Billing\nUsing Cost Allocation Tags",
    "links": []
  },
  {
    "question": "CertyIQ\nA company's solutions architect is designing an AWS multi-account solution that uses AWS Organizations. The\nsolutions architect has organized the company's accounts into organizational units (OUs).\nThe solutions architect needs a solution that will identify any changes to the OU hierarchy. The solution also needs\nto notify the company's operations team of any changes.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "B": "AWS Config aggregated rules: While AWS Config can monitor configurations, aggregated rules are best",
      "C": "AWS Service Catalog and CloudTrail: AWS Service Catalog helps manage and provision services. While",
      "D": "CloudFormation and drift detection on stacks: CloudFormation is excellent for infrastructure as code."
    },
    "answer": "A",
    "explanation": "The correct answer is A: Provision the AWS accounts by using AWS Control Tower. Use account drift\nnotifications to identify the changes to the OU hierarchy.\nHere's why:\nAWS Control Tower simplifies multi-account management: AWS Control Tower provides a high-level\nabstraction for managing multiple AWS accounts through AWS Organizations. It establishes guardrails and\nautomates account provisioning, making it easier to enforce compliance and security best practices across\nyour AWS environment.\nAccount drift detection directly addresses the requirement: The core problem is detecting changes (drift)\nfrom the desired, governed state. Control Tower's account drift notifications are specifically designed to\ndetect configuration changes within provisioned accounts that deviate from the established guardrails. These\nchanges could indicate modifications to the OU hierarchy or other important configurations.\nNotifications minimize operational overhead: Control Tower automatically sends notifications when drift is\ndetected. This proactive monitoring eliminates the need for manual checks or custom scripting, significantly\nreducing the operational overhead for the company's operations team.\nLet's examine why the other options are less suitable:\nB. AWS Config aggregated rules: While AWS Config can monitor configurations, aggregated rules are best\nsuited for assessing compliance across multiple accounts based on specific configuration rules, not for\ndetecting structural changes in the OU hierarchy. Setting up these rules and maintaining them would require\nmore operational effort than Control Tower's built-in drift detection.\nC. AWS Service Catalog and CloudTrail: AWS Service Catalog helps manage and provision services. While\nCloudTrail can log API calls, including those related to OU changes, this solution requires the operations team\nto sift through CloudTrail logs to identify OU changes. This manual process increases the operational burden.\nD. CloudFormation and drift detection on stacks: CloudFormation is excellent for infrastructure as code.\nHowever, using it to manage accounts directly in Organizations is less common than using Control Tower.\nFurthermore, CloudFormation drift detection is specific to changes within a stack, not the broader OU\nhierarchy. This approach would require creating stacks specifically for managing the OU structure and\nmonitoring drift on those stacks, increasing complexity and overhead.\nIn summary, AWS Control Tower's built-in account drift notifications provide the most straightforward and\nefficient way to identify and notify the operations team about changes to the OU hierarchy, thereby\nminimizing operational overhead.\nSupporting Documentation:\nAWS Control Tower: https://aws.amazon.com/controltower/\nAWS Organizations: https://aws.amazon.com/organizations/\nAWS Control Tower Account Factory: https://docs.aws.amazon.com/controltower/latest/userguide/account-\nfactory.html",
    "links": [
      "https://aws.amazon.com/controltower/",
      "https://aws.amazon.com/organizations/",
      "https://docs.aws.amazon.com/controltower/latest/userguide/account-"
    ]
  },
  {
    "question": "CertyIQ\nA company's website handles millions of requests each day, and the number of requests continues to increase. A\nsolutions architect needs to improve the response time of the web application. The solutions architect determines\nthat the application needs to decrease latency when retrieving product details from the Amazon DynamoDB table.\nWhich solution will meet these requirements with the LEAST amount of operational overhead?",
    "options": {
      "B": "This tight integration eliminates the complexities of managing a general-purpose"
    },
    "answer": "A",
    "explanation": "The question focuses on reducing latency for retrieving product details from DynamoDB with minimal\noperational overhead. Let's analyze why option A, using DynamoDB Accelerator (DAX), is the best choice.\nDAX is purpose-built for DynamoDB: DAX is a fully managed, highly available, in-memory cache specifically\ndesigned for DynamoDB. This tight integration eliminates the complexities of managing a general-purpose\ncache like Redis or Memcached.\nLowest Operational Overhead: DAX requires minimal configuration and management. AWS handles the\nunderlying infrastructure, patching, and scaling. This contrasts with ElastiCache solutions, which require more\nmanual setup and ongoing maintenance.\nSeamless Integration: DAX works directly with DynamoDB, requiring minimal application code changes. You\nsimply point your application to the DAX cluster.\nMicrosecond Latency: DAX is optimized for extremely low latency, providing single-digit millisecond\nresponse times, often even in the microsecond range, significantly improving read performance.\nNo Data Consistency Concerns (for this scenario): The question focuses on product details. This suggests\nrelatively static data that is suitable for caching without strong consistency requirements. DAX provides\neventual consistency, which is acceptable in many read-heavy scenarios where stale data is tolerable for a\nshort period.\nWhy other options are less suitable:\nElastiCache (Redis/Memcached): While ElastiCache can be used for caching DynamoDB data, it involves\nmore operational overhead. You need to manage the ElastiCache cluster, handle data\nserialization/deserialization, and implement the caching logic within your application. Furthermore, it doesn't\nnatively understand DynamoDB's data model.\nDynamoDB Streams + Lambda + ElastiCache: This is a complex solution that introduces significant\noperational overhead. It requires setting up DynamoDB Streams, configuring a Lambda function to process\nthe stream, populating ElastiCache, and managing the entire pipeline. This is overkill for simply caching read\ndata and increases points of failure. It also introduces latency due to the asynchronous nature of the stream\nprocessing.\nIn summary, DAX provides the simplest, most efficient, and lowest-overhead solution for accelerating\nDynamoDB reads by providing a managed in-memory cache specifically optimized for DynamoDB, minimizing\nlatency without the need for extensive configuration or application modifications.\nAuthoritative Links:\nDynamoDB Accelerator (DAX):\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html\nAmazon ElastiCache: https://aws.amazon.com/elasticache/",
    "links": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html",
      "https://aws.amazon.com/elasticache/"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect needs to ensure that API calls to Amazon DynamoDB from Amazon EC2 instances in a VPC do\nnot travel across the internet.\nWhich combination of steps should the solutions architect take to meet this requirement? (Choose two.)",
    "options": {
      "B": "C.Create an interface endpoint for Amazon EC2.",
      "C": "Gateway endpoints are"
    },
    "answer": "A",
    "explanation": "The goal is to ensure DynamoDB traffic from EC2 instances within a VPC doesn't traverse the public internet.\nOption A, creating a route table entry for the endpoint, is essential. A route table directs network traffic within\na VPC. By adding a route that directs traffic destined for DynamoDB (identified by the endpoint's prefix list) to\nthe gateway endpoint, we ensure traffic stays within the VPC.\nOption B, creating a gateway endpoint for DynamoDB, is also crucial. A gateway endpoint is a VPC resource\nthat enables private connections to supported AWS services (like DynamoDB) without requiring an internet\ngateway, NAT device, or VPN connection. This prevents traffic from leaving the VPC. Gateway endpoints are\nhighly available and scaled automatically to support high traffic volumes.\nOption C, creating an interface endpoint for Amazon EC2, is incorrect. Interface endpoints (powered by\nPrivateLink) are used for connecting to AWS services or other VPC endpoints privately, but specifically for\nservices that are not supported by gateway endpoints. DynamoDB is supported by gateway endpoints, so\ninterface endpoints are not the correct choice here. Furthermore, an interface endpoint is used to connect to a\nservice, not to be connected from.\nOption D, creating an elastic network interface for the endpoint in each subnet, is incorrect. This is not how\ngateway endpoints work. Gateway endpoints do not require explicit ENIs. They're managed by AWS.\nOption E, creating a security group entry in the endpoint's security group to provide access, is incorrect.\nGateway endpoints do not use security groups. Instead, access is controlled by the resource policies of the\nservice being accessed (DynamoDB in this case). Resource policies specify which VPCs and accounts are\nallowed to access the service. While security groups still control traffic to the EC2 instance, they don't\ndirectly control the path of traffic that traverses the gateway endpoint.\nTherefore, creating a route table entry and creating a gateway endpoint for DynamoDB is the correct\napproach.\nHere are helpful links for more information:\nVPC Endpoints: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\nGateway VPC Endpoints for DynamoDB:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-access.html\nRoute Tables: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-access.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs its applications on both Amazon Elastic Kubernetes Service (Amazon EKS) clusters and on-\npremises Kubernetes clusters. The company wants to view all clusters and workloads from a central location.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "B",
    "explanation": "The question requires a solution that centralizes Kubernetes cluster and workload visibility across Amazon\nEKS and on-premises environments with minimal operational overhead.\nOption B, using Amazon EKS Connector, is the most suitable solution. EKS Connector allows you to connect\nany conformant Kubernetes cluster (including on-premises) to Amazon EKS. Once connected, you can view\nthese clusters and their associated resources (like deployments, pods, and services) directly from the AWS\nManagement Console. This offers a centralized view without requiring extensive configuration or\nmanagement of monitoring agents. The integration leverages AWS IAM for authentication and authorization.\nOption A, CloudWatch Container Insights, focuses primarily on performance monitoring and metrics\ncollection. While it can provide insights into cluster performance, it doesn't offer a unified console for viewing\nall clusters and workloads without significant configuration and management of agents on each cluster.\nOption C, AWS Systems Manager, is a powerful management service but primarily focuses on managing EC2\ninstances and on-premises servers. It can execute commands on instances within the clusters, but it doesn't\ninherently provide a centralized Kubernetes-aware view of all clusters and their workloads like EKS\nConnector does.\nOption D, Amazon EKS Anywhere, creates EKS-compatible clusters on-premises. While it helps standardize\nthe Kubernetes experience, it doesn't directly address the requirement of viewing existing on-premises\nclusters alongside EKS clusters from a central location without migrating the on-premises workloads to EKS\nAnywhere. Using EKS Anywhere as a primary viewing cluster would involve significant operational overhead.\nTherefore, EKS Connector offers the simplest and most direct way to meet the requirements of centralized\nvisibility with minimal operational overhead by connecting existing clusters to the EKS console.\nAmazon EKS Connector DocumentationAmazon EKS Anywhere Documentation",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is building an ecommerce application and needs to store sensitive customer information. The company\nneeds to give customers the ability to complete purchase transactions on the website. The company also needs to\nensure that sensitive customer data is protected, even from database administrators.\nWhich solution meets these requirements?",
    "options": {
      "B": "Store sensitive data in Amazon RDS for MySQL. Use AWS Key Management"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Store sensitive data in Amazon RDS for MySQL. Use AWS Key Management\nService (AWS KMS) client-side encryption to encrypt the data.\nHere's why:\nRequirement: Storing sensitive customer information for an e-commerce application: RDS for MySQL is a\nsuitable database solution for structured data like customer information and transaction details.\nRequirement: Protecting data even from database administrators: Client-side encryption using AWS KMS\nensures that the data is encrypted before it's stored in the database. This means the database administrators\n(DBAs) only see encrypted data and cannot decrypt it without the proper KMS key.\nClient-Side Encryption: With client-side encryption, the application handles the encryption and decryption of\nthe data using the KMS key. The RDS instance never has direct access to the key itself, making it impossible\nfor the database to reveal the plain-text data.\nAWS KMS: AWS KMS is a managed service that makes it easy for you to create and control the encryption\nkeys used to encrypt your data. KMS provides a secure and centralized way to manage cryptographic keys.\nWhy other options are incorrect:\nA: Storing sensitive data on EBS volumes with EBS encryption only protects data at rest on the volume itself.\nIf someone with proper IAM permissions gets access to the instance and the mounted volume, they can\naccess the unencrypted data if the application isn't specifically encrypting data at the application level. It\ndoes not protect against DBAs if they have instance access to the database EC2 instance.\nC: S3 is generally used for unstructured data (objects). While S3 server-side encryption protects data at rest,\nit doesn't provide the same level of control as client-side encryption for protecting against DBAs.\nFurthermore, using S3 for transactional data in an e-commerce application is not an efficient or standard\ndesign pattern.\nD: Amazon FSx is a file system service. While you can store data on FSx and restrict access using Windows\nfile permissions, this option doesn't address the requirement of protecting data from database administrators\nsince file storage is not designed for transactional data.\nAuthoritative Links:\nAWS KMS: https://aws.amazon.com/kms/\nAmazon RDS: https://aws.amazon.com/rds/\nClient-Side Encryption: https://docs.aws.amazon.com/kms/latest/developerguide/services-rds.html",
    "links": [
      "https://aws.amazon.com/kms/",
      "https://aws.amazon.com/rds/",
      "https://docs.aws.amazon.com/kms/latest/developerguide/services-rds.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has an on-premises MySQL database that handles transactional data. The company is migrating the\ndatabase to the AWS Cloud. The migrated database must maintain compatibility with the company's applications\nthat use the database. The migrated database also must scale automatically during periods of increased demand.\nWhich migration solution will meet these requirements?",
    "options": {
      "B": "Configure"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it addresses all the requirements outlined: compatibility, managed database\nservice, and auto-scaling.\nCompatibility: Amazon Aurora with MySQL compatibility is designed to be a drop-in replacement for MySQL\ndatabases. This ensures that the existing applications that rely on MySQL will function correctly with minimal\nor no code changes. https://aws.amazon.com/rds/aurora/mysql-features/\nMigration: AWS Database Migration Service (DMS) is specifically designed to migrate databases from on-\npremises environments to AWS. It supports heterogeneous database migrations, but for a MySQL-to-Aurora\nmigration, it's a homogeneous migration, which simplifies the process. https://aws.amazon.com/dms/\nAuto-Scaling: Aurora Auto Scaling automatically adjusts the number of Aurora DB instances in an Aurora\ncluster. This is vital to maintain performance during peak demand periods without manual intervention,\nmeeting the scalability requirement.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Scaling.html\nOption A is partially correct (RDS for MySQL allows elastic storage scaling), but it doesn't offer the superior\nperformance and scalability of Aurora, especially for demanding workloads. Native MySQL tools for migration\ncan be complex and time-consuming compared to DMS. Elastic storage scaling is not the same as scaling the\ncompute resources/instances, which Aurora Auto Scaling provides.\nOption B suggests migrating to Amazon Redshift, which is a data warehouse service optimized for analytical\nworkloads, not transactional data. Redshift is incompatible with MySQL applications and is not suitable for a\ndirect database migration requiring compatibility. Furthermore, Auto Scaling in Redshift refers to scaling the\nnumber of compute nodes, not intended for auto-scaling based on transaction load like Aurora.\nOption D proposes migrating to Amazon DynamoDB, a NoSQL database. This option fails on the compatibility\nrequirement. DynamoDB uses a different data model and query language than MySQL, meaning significant\napplication changes would be needed, which the problem statement aims to avoid.",
    "links": [
      "https://aws.amazon.com/rds/aurora/mysql-features/",
      "https://aws.amazon.com/dms/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Scaling.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs multiple Amazon EC2 Linux instances in a VPC across two Availability Zones. The instances host\napplications that use a hierarchical directory structure. The applications need to read and write rapidly and\nconcurrently to shared storage.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "C": "This"
    },
    "answer": "B",
    "explanation": "The best solution for rapidly and concurrently reading and writing to shared storage from multiple EC2 Linux\ninstances across Availability Zones, while maintaining a hierarchical directory structure, is to use Amazon\nElastic File System (Amazon EFS).\nOption A is incorrect because Amazon S3 is object storage, not file storage. While S3 can store files, it lacks\nthe hierarchical directory structure required and isn't designed for the rapid and concurrent read/write\noperations needed for applications expecting a traditional file system interface. S3 is better suited for storing\nlarge files, backups, or static content.\nOption C is incorrect because attaching a single EBS volume to multiple EC2 instances is not supported. EBS\nvolumes are block storage and are designed for single-instance attachment. Attempting to attach the same\nEBS volume to multiple instances can lead to data corruption. Also, even if possible, io2 EBS would only be in\none Availability Zone, impacting availability across the two zones.\nOption D is incorrect because synchronizing EBS volumes across multiple instances would be complex to set\nup and manage, prone to errors, and would not be a native AWS solution. It would also introduce significant\nlatency and overhead, hindering the performance requirements. Furthermore, it wouldn't offer the same level\nof availability and resilience as a managed service.\nEFS, on the other hand, is a fully managed network file system specifically designed for use with AWS\ncompute services like EC2. It provides a simple, scalable, elastic, and highly available file system that can be\nsimultaneously accessed by multiple EC2 instances across multiple Availability Zones within a VPC. This\nmakes it ideal for shared storage scenarios requiring a traditional file system interface, such as those\ndescribed in the question. Mounting the EFS file system on each EC2 instance allows the applications to read\nand write to the shared storage rapidly and concurrently, while automatically handling data replication and\navailability across Availability Zones. EFS uses a hierarchical directory structure natively.\nAmazon EFS Documentation",
    "links": []
  },
  {
    "question": "CertyIQ\nA solutions architect is designing a workload that will store hourly energy consumption by business tenants in a\nbuilding. The sensors will feed a database through HTTP requests that will add up usage for each tenant. The\nsolutions architect must use managed services when possible. The workload will receive more features in the\nfuture as the solutions architect adds independent components.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "B": "Option D involves similar operational overhead to option B because it requires managing EC2 instances. While"
    },
    "answer": "A",
    "explanation": "The correct answer is A because it offers the least operational overhead and best aligns with managed\nservices and future scalability requirements. Let's break down why:\nOption A leverages fully managed services: Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. API\nGateway handles the HTTP requests from the sensors, Lambda provides serverless compute to process the\ndata, and DynamoDB offers a NoSQL database that's highly scalable and requires minimal administration. This\narchitecture is designed for event-driven processing and scales automatically with the workload. DynamoDB\nis also optimized for key-value data, making it ideal for storing tenant usage data.\nOption B involves managing EC2 instances within an Auto Scaling group and an Elastic Load Balancer (ELB).\nWhile ELB is managed, the EC2 instances require patching, scaling, and monitoring, increasing operational\noverhead. Storing processed data in S3 is a good choice for archival purposes, but not ideal for real-time\nquerying and updates.\nOption C introduces a Microsoft SQL Server Express database on an EC2 instance. This requires managing the\ndatabase server, including patching, backups, and ensuring high availability. Although API Gateway and\nLambda are beneficial, the SQL Server significantly increases operational overhead compared to DynamoDB.\nOption D involves similar operational overhead to option B because it requires managing EC2 instances. While\nEFS is a managed service, its primary use case is sharing file systems, not storing structured data like tenant\nconsumption. Storing usage data on EFS wouldn't be the most efficient approach.\nTherefore, Option A is the most efficient choice due to the combination of managed services, scalability, and\nevent-driven architecture, minimizing the administrative burden and facilitating future feature additions.\nUsing DynamoDB instead of a file system, relational database, or object storage is better suited for quick and\neasy updates to tenant usage data based on sensor readings.\nSupporting Resources:\nAmazon API Gateway: https://aws.amazon.com/api-gateway/\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon DynamoDB: https://aws.amazon.com/dynamodb/\nAmazon EC2 Auto Scaling: https://aws.amazon.com/autoscaling/",
    "links": [
      "https://aws.amazon.com/api-gateway/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/dynamodb/",
      "https://aws.amazon.com/autoscaling/"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is designing the storage architecture for a new web application used for storing and viewing\nengineering drawings. All application components will be deployed on the AWS infrastructure.\nThe application design must support caching to minimize the amount of time that users wait for the engineering\ndrawings to load. The application must be able to store petabytes of data.\nWhich combination of storage and caching should the solutions architect use?",
    "options": {},
    "answer": "A",
    "explanation": "The correct solution is Amazon S3 with Amazon CloudFront. Here's why:\nAmazon S3 is ideal for storing large amounts of unstructured data like engineering drawings. It offers\nvirtually unlimited storage, scalability, and high durability. S3 is designed for object storage, making it perfect\nfor storing individual files accessible via HTTP/HTTPS.\nAmazon CloudFront, a content delivery network (CDN), integrates seamlessly with S3. It caches content (like\nthose engineering drawings) at edge locations globally, reducing latency for users accessing the web\napplication. When a user requests an engineering drawing, CloudFront first checks its cache. If the drawing is\ncached, it delivers it from the nearest edge location, resulting in faster load times. If the drawing is not\ncached, CloudFront retrieves it from the S3 origin and caches it for future requests.\nOption B is incorrect because Amazon S3 Glacier is designed for archival storage of data that is infrequently\naccessed, which doesn't align with the need for fast access for viewing engineering drawings. ElastiCache is\nan in-memory caching service typically used for databases, not for serving large files like engineering\ndrawings.\nOption C is incorrect because EBS volumes are block storage devices primarily used for persistent storage for\nEC2 instances. While EBS can store data, it isn't a scalable and cost-effective solution for storing petabytes of\ndata compared to S3. Also, EBS isn't directly integrated with CloudFront for content caching in the same way\nthat S3 is.\nOption D is incorrect because AWS Storage Gateway is a hybrid storage service that connects on-premises\nsoftware appliances to AWS cloud storage. It isn't appropriate for a fully cloud-native web application storing\npetabytes of data within AWS. ElastiCache isn't a substitute for a CDN like CloudFront, which optimizes\ncontent delivery globally.\nTherefore, S3 provides the scalable storage for petabytes of drawings, and CloudFront provides caching for\nlow latency delivery, fulfilling the application's requirements.\nFurther research:\nAmazon S3: https://aws.amazon.com/s3/\nAmazon CloudFront: https://aws.amazon.com/cloudfront/",
    "links": [
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/cloudfront/"
    ]
  },
  {
    "question": "CertyIQ\nAn Amazon EventBridge rule targets a third-party API. The third-party API has not received any incoming traffic. A\nsolutions architect needs to determine whether the rule conditions are being met and if the rule's target is being\ninvoked.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Check for metrics in Amazon CloudWatch in the namespace for AWS/Events.",
      "B": "Review events in the Amazon Simple Queue Service (Amazon SQS) dead-letter queue: A dead-letter",
      "C": "Check for the events in Amazon CloudWatch Logs: While CloudWatch Logs can capture detailed logs from",
      "D": "Check the trails in AWS CloudTrail for the EventBridge events: AWS CloudTrail records API calls made to"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Check for metrics in Amazon CloudWatch in the namespace for AWS/Events.\nHere's a detailed justification:\nAmazon EventBridge emits metrics to Amazon CloudWatch under the AWS/Events namespace. These metrics\nprovide valuable insights into the performance and behavior of EventBridge rules. Specifically, metrics like\nInvocations, FailedInvocations, TriggeredRules, and MatchedEvents can help determine if the rule conditions are\nbeing met and if the target is being invoked.\nInvocations: This metric indicates the number of times the target has been invoked by the EventBridge rule. If\nthe target is a third-party API, a zero value suggests that the rule hasn't invoked it yet, indicating a potential\nissue with rule conditions or event matching.\nFailedInvocations: This metric represents the number of times the target invocation failed. A high value\nindicates a problem with the target or the EventBridge configuration.\nTriggeredRules: This metric indicates how many times a rule was matched to an incoming event. This is useful\nfor confirming that the rule conditions are being met by incoming events.\nMatchedEvents: This metric represents the number of events that matched a particular EventBridge rule.\nBy monitoring these metrics, a solutions architect can quickly ascertain whether the EventBridge rule is\ntriggering based on the defined conditions and if the target API is being successfully invoked. If the\nInvocations metric remains at zero, it suggests that either the rule conditions are not being met, or the events\nare not being routed correctly. If FailedInvocations is high, the API endpoint itself might have problems.\nWhy other options are incorrect:\nB. Review events in the Amazon Simple Queue Service (Amazon SQS) dead-letter queue: A dead-letter\nqueue (DLQ) is used to capture events that could not be processed successfully after multiple retry attempts.\nWhile a DLQ might contain information about failed invocations, it won't help determine if the rule is being\ntriggered in the first place. DLQs only get populated after a failed target invocation (after retries), not if the\nrule never fired initially.\nC. Check for the events in Amazon CloudWatch Logs: While CloudWatch Logs can capture detailed logs from\nvarious AWS services and applications, EventBridge doesn't automatically log every event it processes or rule\nit triggers to CloudWatch Logs unless explicitly configured to do so using CloudWatch Logs as a target.\nMoreover, logging every event can become costly and inefficient for debugging this specific scenario.\nD. Check the trails in AWS CloudTrail for the EventBridge events: AWS CloudTrail records API calls made to\nAWS services, including EventBridge. CloudTrail can show the creation, modification, or deletion of\nEventBridge rules, but it doesn't directly track the number of times a rule is triggered or the target is invoked.\nCloudTrail is primarily for auditing purposes, not for real-time monitoring of rule performance.\nAuthoritative Links:\nMonitoring Amazon EventBridge - Amazon EventBridge\nAWS/Events Metrics - Amazon CloudWatch",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has a large workload that runs every Friday evening. The workload runs on Amazon EC2 instances that\nare in two Availability Zones in the us-east-1 Region. Normally, the company must run no more than two instances\nat all times. However, the company wants to scale up to six instances each Friday to handle a regularly repeating\nincreased workload.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the best solution for scaling EC2 instances for a recurring\nFriday workload with minimal operational overhead:\nThe core requirement is to automatically scale the EC2 instance count based on a predictable schedule (every\nFriday). Auto Scaling groups are designed to automatically manage the number of EC2 instances based on\ndefined parameters. Option B, using a scheduled action within an Auto Scaling group, directly addresses this\nneed. A scheduled action allows you to predefine instance count adjustments (minimum, maximum, desired\ncapacity) that occur at specific times or recurring intervals. This eliminates manual intervention each Friday.\nOption A, using Amazon EventBridge (formerly CloudWatch Events) to remind someone to scale the instances,\nintroduces manual steps. While EventBridge can trigger actions, a simple reminder still requires someone to\nlog in and adjust the instance count. This increases operational overhead and the risk of human error.\nOption C, manual scaling, defeats the purpose of automation entirely. It would require someone to manually\nadjust the Auto Scaling group's parameters (desired capacity) every Friday, which is exactly what the\nquestion aims to avoid.\nOption D, automatic scaling, is usually based on metrics like CPU utilization or network traffic. While it can\nhandle fluctuating workloads, it is less appropriate for predictable, time-based scaling. Automatic scaling\nwould react to the increased workload only after it has already started, potentially delaying the scaling and\nimpacting performance. Setting up and maintaining the metrics for automatic scaling adds complexity.\nTherefore, a scheduled action within an Auto Scaling group provides the most streamlined and automated\napproach to managing the EC2 instance count for a recurring Friday workload with minimal operational\noverhead, perfectly aligning with the stated requirements.\nRelevant links for further research:\nAWS Auto Scaling: https://aws.amazon.com/autoscaling/\nAuto Scaling Scheduled Actions:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\nAmazon EventBridge: https://aws.amazon.com/eventbridge/",
    "links": [
      "https://aws.amazon.com/autoscaling/",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html",
      "https://aws.amazon.com/eventbridge/"
    ]
  },
  {
    "question": "CertyIQ\nA company is creating a REST API. The company has strict requirements for the use of TLS. The company requires\nTLSv1.3 on the API endpoints. The company also requires a specific public third-party certificate authority (CA) to\nsign the TLS certificate.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Why Option C and D are Incorrect: Lambda function URLs don't directly offer the same level of custom",
      "B": "Here's why:"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Here's why:\nAWS Certificate Manager (ACM): ACM is the preferred service for provisioning, managing, and deploying\nSSL/TLS certificates for use with AWS services. ACM simplifies the process of obtaining and renewing\ncertificates. https://aws.amazon.com/certificate-manager/\nImporting Certificates: ACM allows you to import certificates issued by a third-party CA, which is a key\nrequirement in this scenario. https://docs.aws.amazon.com/acm/latest/userguide/import-certificate.html\nAPI Gateway Custom Domains: Amazon API Gateway supports custom domain names for your APIs. To use\nHTTPS with a custom domain, you need to associate a TLS certificate with it. ACM makes this integration\nseamless. https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html\nHTTP API and TLS Requirements: HTTP APIs in API Gateway support TLS. The question specifies TLSv1.3.\nAPI Gateway manages TLS versions and supports configuring a secure policy.\nWhy Option A is Incorrect: Option A suggests creating a certificate on a local machine and then importing.\nWhile possible, ACM's primary function is certificate management; relying on a local machine introduces\ncomplexities and defeats the purpose of using ACM. The certificate creation and signing should be handled\nby the third party CA.\nWhy Option C and D are Incorrect: Lambda function URLs don't directly offer the same level of custom\ncertificate management and custom domain integration as API Gateway. While you can handle TLS within the\nLambda function itself, this is less efficient and more complex than using API Gateway's built-in features. API\nGateway is specifically designed to handle API traffic, including TLS termination.\nTherefore, the most efficient and manageable approach is to import the certificate issued by the third-party\nCA into ACM and then associate it with a custom domain in API Gateway for your HTTP API, satisfying the\nTLSv1.3 requirement.",
    "links": [
      "https://aws.amazon.com/certificate-manager/",
      "https://docs.aws.amazon.com/acm/latest/userguide/import-certificate.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an application on AWS. The application receives inconsistent amounts of usage. The application\nuses AWS Direct Connect to connect to an on-premises MySQL-compatible database. The on-premises database\nconsistently uses a minimum of 2 GiB of memory.\nThe company wants to migrate the on-premises database to a managed AWS service. The company wants to use\nauto scaling capabilities to manage unexpected workload increases.\nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": {},
    "answer": "C",
    "explanation": "The optimal solution is to provision an Amazon Aurora Serverless v2 database with a minimum capacity of 1\nAurora capacity unit (ACU). This choice best addresses the requirements for a managed database service with\nauto-scaling capabilities and minimal administrative overhead, suitable for handling inconsistent workloads\nafter migrating from an on-premises MySQL-compatible database.\nOption A, using Amazon DynamoDB, is not ideal because it's a NoSQL database. While DynamoDB scales very\nwell, migrating a MySQL-compatible database to DynamoDB would require a significant application rewrite\nand redesign, increasing complexity and administrative overhead, especially considering the existing\napplication's design.\nOption B, provisioning a standard Amazon Aurora database with a minimum capacity of 1 ACU, fulfills the\nrequirements for a managed MySQL-compatible service and provides scalability. However, Aurora's standard\nprovisioned instances are not as efficient at scaling to zero when the application has near-zero or\nunpredictable usage. Administratively, Aurora requires more effort to right-size compared to Serverless v2.\nOption D, provisioning an Amazon RDS for MySQL database with 2 GiB of memory, meets the minimum\nmemory requirement but lacks the automatic scaling capability needed to manage unpredictable workload\nincreases efficiently. While RDS MySQL can be scaled, manual intervention or complex custom automation\nwould be required, increasing administrative burden.\nAurora Serverless v2 (Option C) excels because it combines compatibility with MySQL, automatic scaling, and\nminimal administrative overhead. It starts with a minimum of 0.5 ACUs (effectively using about 1 GiB memory)\nand automatically scales up or down based on application demand. This ensures efficient resource utilization\nand cost optimization during periods of low usage, aligning perfectly with the need to manage inconsistent\nworkloads. Aurora Serverless v2 also manages patching and other maintenance tasks. The cost of the service\nscales directly with usage, which can be more efficient than a provisioned Aurora DB if the workload is\nextremely variable. It effectively handles unexpected workload increases without manual intervention.\nTherefore, Aurora Serverless v2 provides the best balance between compatibility, scalability, cost-\neffectiveness, and ease of management for this scenario.\nReference:\nAmazon Aurora Serverless v2\nAmazon RDS\nAmazon DynamoDB",
    "links": []
  },
  {
    "question": "CertyIQ\nA company wants to use an event-driven programming model with AWS Lambda. The company wants to reduce\nstartup latency for Lambda functions that run on Java 11. The company does not have strict latency requirements\nfor the applications. The company wants to reduce cold starts and outlier latencies when a function scales up.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "A": "Configure Lambda provisioned concurrency: Provisioned concurrency keeps Lambda functions initialized",
      "B": "Increase the timeout of the Lambda functions: Increasing the timeout merely allows longer execution",
      "C": "Increase the memory of the Lambda functions: While increasing memory can sometimes improve",
      "D": "Configure Lambda SnapStart: SnapStart optimizes for Java 11 functions. It captures a snapshot of the"
    },
    "answer": "D",
    "explanation": "The correct answer is D: Configure Lambda SnapStart. Here's why:\nThe scenario focuses on reducing startup latency (\"cold starts\") for Java 11 Lambda functions without strict\nlatency requirements for the application as a whole. We also want to reduce outlier latencies during function\nscale-up and cost-effectiveness.\nA. Configure Lambda provisioned concurrency: Provisioned concurrency keeps Lambda functions initialized\nand ready to respond, directly addressing cold starts. However, it comes at a cost: you pay for the allocated\nconcurrency regardless of whether it's used. While effective, it's not the most cost-effective solution when\nstrict latency requirements are absent, and it does not fundamentally address the underlying cold start\nprocess.\nB. Increase the timeout of the Lambda functions: Increasing the timeout merely allows longer execution\ntimes but does nothing to reduce cold start latency itself. Cold starts occur before the function code begins\nexecuting. This choice is irrelevant to the problem.\nC. Increase the memory of the Lambda functions: While increasing memory can sometimes improve\nperformance by providing more resources during execution, it doesn't fundamentally address the cold start\nissue. The JVM initialization and class loading, which are major contributors to cold starts, are still present. It\nalso directly increases costs, as you pay for the provisioned memory.\nD. Configure Lambda SnapStart: SnapStart optimizes for Java 11 functions. It captures a snapshot of the\ninitialized execution environment (including the JVM) when the function is first deployed or updated. When the\nfunction is invoked for the first time or needs to scale, Lambda resumes from the snapshot instead of starting\nfrom scratch. This drastically reduces cold start latency, approaching near-instant startup. This makes it the\nmost cost-effective choice because you only pay for the actual invocation time of the function from the\nresumed state. It is particularly suited when there aren't strict latency requirements.\nTherefore, SnapStart is the best option as it directly tackles the cold start issue for Java 11, reduces outlier\nlatencies during scaling, and is more cost-effective than provisioned concurrency. It is also the most suitable\nsolution when strict latency requirements are not an issue.\nSupporting documentation:\nAWS Lambda SnapStart: https://aws.amazon.com/blogs/aws/lambda-snapstart-for-java-functions-generally-\navailable-increase-startup-speed-by-up-to-10x/\nOptimizing Lambda performance: https://docs.aws.amazon.com/lambda/latest/dg/optimization-tips.html",
    "links": [
      "https://aws.amazon.com/blogs/aws/lambda-snapstart-for-java-functions-generally-",
      "https://docs.aws.amazon.com/lambda/latest/dg/optimization-tips.html"
    ]
  },
  {
    "question": "CertyIQ\nA financial services company launched a new application that uses an Amazon RDS for MySQL database. The\ncompany uses the application to track stock market trends. The company needs to operate the application for only\n2 hours at the end of each week. The company needs to optimize the cost of running the database.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "B",
    "explanation": "The optimal solution is to migrate the existing RDS for MySQL database to an Aurora MySQL database\ncluster. Here's why:\nAurora MySQL is a cost-effective option for intermittent workloads:\nCost Optimization: Aurora MySQL offers cost-effective scaling, especially when the application operates for\na short duration each week. It does not have the capability to pause completely, but the cost will be low.\nPerformance and Scalability: Aurora offers enhanced performance and scalability compared to standard\nMySQL, which is not directly related to the prompt.\nManaged Service: RDS and Aurora handle patching, backups, and maintenance, reducing operational\noverhead compared to managing MySQL on EC2.\nAlternatives: While Aurora Serverless v2 could seem appealing because it scales to zero capacity when idle,\nthe prompt did not specify that the application be completely idle.\nWhy other options are less suitable:\nOption A (Aurora Serverless v2 MySQL): Although Aurora Serverless v2 scales to zero when idle, it may not\nbe the most cost-effective option since the prompt did not specify it needs to scale to zero.\nOption C (EC2 with MySQL): Running MySQL on EC2 requires more manual management of the database,\nbackups, patching, and OS, increasing operational overhead. Instance reservations don't address the\nfundamental issue of wasted resources during the idle period.\nOption D (ECS with MySQL): Deploying MySQL in containers on ECS introduces complexity and overhead for\nmanaging containers and orchestration, without a significant cost advantage over using Aurora.\nSupporting Documentation:\nAmazon Aurora: https://aws.amazon.com/rds/aurora/\nAmazon RDS Pricing: https://aws.amazon.com/rds/pricing/",
    "links": [
      "https://aws.amazon.com/rds/aurora/",
      "https://aws.amazon.com/rds/pricing/"
    ]
  },
  {
    "question": "CertyIQ\nA company deploys its applications on Amazon Elastic Kubernetes Service (Amazon EKS) behind an Application\nLoad Balancer in an AWS Region. The application needs to store data in a PostgreSQL database engine. The\ncompany wants the data in the database to be highly available. The company also needs increased capacity for\nread workloads.\nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C: Create an Amazon RDS database with Multi-AZ DB cluster deployment. Here's why:\nHigh Availability: Multi-AZ DB cluster deployments in Amazon RDS are designed to provide high availability\nfor PostgreSQL database engines. They involve creating a primary DB instance along with multiple\nsynchronous standby DB instances in different Availability Zones within the same AWS Region. In case of a\nfailure of the primary instance, RDS automatically fails over to one of the standby instances, minimizing\ndowntime. This addresses the requirement for high availability.\nIncreased Read Capacity: Multi-AZ DB clusters support read workloads through reader endpoints.\nApplications can direct read traffic to these reader endpoints, which distribute the load across the available\nstandby instances. This increases the read capacity without impacting the performance of the primary\ninstance handling write operations.\nOperational Efficiency: Amazon RDS simplifies database management tasks like backups, patching, and\nrecovery. With Multi-AZ DB clusters, these tasks are handled automatically, reducing the operational\noverhead for the company. This efficiency is crucial for environments where the company desires to minimize\nadministrative burden.\nOption A is incorrect because DynamoDB, while highly available, is a NoSQL database and not suitable for\napplications requiring a PostgreSQL database engine. Option B offers High Availability but it doesn't offer\ndirect support for scaling read workloads as efficiently as multi-az DB clusters. Option D provides read\nreplicas but requires cross-region configuration, adding complexity and increased latency. It is also more\ncomplex than a multi-az DB cluster.\nTherefore, using an Amazon RDS database with Multi-AZ DB cluster deployment provides the best balance of\nhigh availability, increased read capacity, and operational efficiency for the company's PostgreSQL database\nneeds.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.htmlhttps://aws.amazon.com/rds/features/",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.htmlhttps://aws.amazon.com/rds/features/"
    ]
  },
  {
    "question": "CertyIQ\nA company is building a RESTful serverless web application on AWS by using Amazon API Gateway and AWS\nLambda. The users of this web application will be geographically distributed, and the company wants to reduce the\nlatency of API requests to these users.\nWhich type of endpoint should a solutions architect use to meet these requirements?",
    "options": {
      "D": "Edge-optimized endpoint.",
      "C": "It also does not"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Edge-optimized endpoint.\nHere's why: Edge-optimized endpoints in Amazon API Gateway are designed to minimize latency for\ngeographically distributed users. When a user makes a request to an edge-optimized API, the request is\nrouted to the nearest AWS edge location via Amazon CloudFront. CloudFront is a content delivery network\n(CDN) that caches content and routes requests through its global network of edge locations, reducing the\ndistance the request needs to travel. This results in lower latency and faster response times for users\nregardless of their location.\nA regional endpoint, on the other hand, serves requests from within a specific AWS Region. While it offers low\nlatency for users in that region, it doesn't provide the global distribution benefits of an edge-optimized\nendpoint.\nA private endpoint is used to access APIs from within a VPC without exposing them to the public internet. It\ndoesn't directly address latency reduction for geographically distributed users.\nAn Interface VPC endpoint is used to connect to AWS services privately from within a VPC. It also does not\nprovide geographical distribution for latency reduction like an edge-optimized endpoint.\nTherefore, for the requirement of reducing latency for geographically distributed users accessing a serverless\nweb application, the edge-optimized endpoint is the most suitable choice due to its integration with Amazon\nCloudFront and global edge location network.\nRelevant links for further research:\nAmazon API Gateway Endpoints\nAmazon CloudFront",
    "links": []
  },
  {
    "question": "CertyIQ\nA company uses an Amazon CloudFront distribution to serve content pages for its website. The company needs to\nensure that clients use a TLS certificate when accessing the company's website. The company wants to automate\nthe creation and renewal of the TLS certificates.\nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C: Use AWS Certificate Manager (ACM) to create a certificate and use DNS validation\nfor the domain. Here's why:\nThe requirement is to automate TLS certificate creation and renewal for a CloudFront distribution with\nminimal operational overhead. AWS Certificate Manager (ACM) is specifically designed for this purpose. ACM\nintegrates directly with CloudFront and can automatically provision, deploy, and renew SSL/TLS certificates\nwithout manual intervention.\nDNS validation is the preferred method for ACM certificate validation because it allows automated renewals\nwithout requiring manual email verification each time. When using DNS validation, you add a CNAME record to\nyour DNS configuration, which ACM periodically checks. This removes the need to respond to emails for\nvalidation and makes the renewal process fully automated and significantly more operationally efficient.\nOption A is incorrect because CloudFront security policies don't create certificates. They configure the\nsecurity protocols and ciphers that CloudFront uses when communicating with viewers. Option B is incorrect\nbecause Origin Access Control (OAC) secures the communication between CloudFront and the origin server\n(e.g., S3 bucket), not the communication between clients and CloudFront, and OAC doesn't handle certificate\ncreation. Option D, while using ACM, utilizes email validation, which requires manual intervention to validate\nthe certificate upon creation and renewal. This is less efficient than DNS validation, which fully automates the\nprocess. ACM handles the entire lifecycle of the certificate automatically when paired with DNS validation.\nTherefore, ACM with DNS validation offers the most automated and least operationally burdensome method\nfor managing TLS certificates for CloudFront.\nSupporting Links:\nAWS Certificate Manager (ACM): https://aws.amazon.com/certificate-manager/\nACM DNS Validation: https://docs.aws.amazon.com/acm/latest/userguide/gs-acm-validate-dns.html\nUsing SSL/TLS Certificates with CloudFront:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront.html",
    "links": [
      "https://aws.amazon.com/certificate-manager/",
      "https://docs.aws.amazon.com/acm/latest/userguide/gs-acm-validate-dns.html",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront.html"
    ]
  },
  {
    "question": "CertyIQ\nA company deployed a serverless application that uses Amazon DynamoDB as a database layer. The application\nhas experienced a large increase in users. The company wants to improve database response time from\nmilliseconds to microseconds and to cache requests to the database.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "B": "It's designed specifically to accelerate read performance without requiring developers to manage"
    },
    "answer": "A",
    "explanation": "The correct answer is A: Use DynamoDB Accelerator (DAX).\nHere's why:\nThe requirement is to improve DynamoDB response time from milliseconds to microseconds and to cache\nrequests with the least operational overhead. DAX is a fully managed, highly available, in-memory cache for\nDynamoDB. It's designed specifically to accelerate read performance without requiring developers to manage\ncache invalidation, data population, or cluster management. This minimizes operational overhead. DAX sits\nbetween the application and DynamoDB, caching frequently accessed data. Subsequent requests for the\nsame data are served directly from the DAX cache, significantly reducing latency to microseconds.\nOption B, migrating to Amazon Redshift, is inappropriate. Redshift is a data warehousing solution optimized\nfor analytical queries on large datasets, not for transactional workloads requiring microsecond latency. It\nwould be a significant operational undertaking and would not directly address the caching requirement.\nOption C, migrating to Amazon RDS, is also unsuitable. While RDS offers various database engines, it doesn't\ninherently solve the caching problem without implementing a separate caching layer (like ElastiCache).\nMigrating would involve significant operational effort and wouldn't guarantee microsecond latency for all\nread operations without additional caching mechanisms. RDS primarily addresses relational database needs,\nnot necessarily the NoSQL caching requirements.\nOption D, using Amazon ElastiCache for Redis, while a valid caching solution, introduces more operational\noverhead compared to DAX. You would need to manage the ElastiCache cluster, configure caching strategies,\nand handle cache invalidation. DAX is purpose-built for DynamoDB, simplifying these aspects and providing a\ntighter integration with the database. DAX requires less configuration because it understands DynamoDB\naccess patterns and can handle the underlying data structures automatically.\nIn summary, DAX provides the simplest and most efficient solution to improve DynamoDB response time to\nmicroseconds through caching, with minimal operational overhead due to its tight integration and fully\nmanaged nature.\nHere are some authoritative links for further research:\nDynamoDB Accelerator (DAX): https://aws.amazon.com/dynamodb/dax/\nDynamoDB Documentation - DAX:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html",
    "links": [
      "https://aws.amazon.com/dynamodb/dax/",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an application that uses Amazon RDS for PostgreSQL. The application receives traffic only on\nweekdays during business hours. The company wants to optimize costs and reduce operational overhead based on\nthis usage.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A: Use the Instance Scheduler on AWS to configure start and stop schedules.\nHere's why:\nThe primary requirement is cost optimization for an RDS PostgreSQL database that's only needed during\nweekday business hours. The Instance Scheduler directly addresses this by automatically starting and\nstopping the RDS instance based on a predefined schedule. This eliminates compute costs during off-peak\nhours (nights and weekends) when the application isn't in use.\nOption B is incorrect because turning off automatic backups creates a significant risk of data loss. Manual\nsnapshots, while possible, increase operational overhead and require careful management to ensure backups\nare taken consistently and retained appropriately. Automatic backups are a fundamental best practice for\ndatabase management.\nOption C is less efficient than Instance Scheduler. While a Lambda function could technically achieve the\nsame outcome, it requires custom coding, deployment, and maintenance. The Instance Scheduler is a pre-\nbuilt, managed service specifically designed for this purpose, reducing operational overhead and providing a\nsimpler configuration interface. Relying on CPU utilization as a trigger can be unreliable, as even idle\ndatabases consume some CPU resources, potentially leading to the instance staying on longer than\nnecessary.\nOption D is incorrect because All Upfront reserved DB instances are a cost optimization strategy for\ndatabases running consistently. They don't address the requirement of turning off the database during off-\npeak hours, and the company would be paying for compute resources it isn't using. Reserved instances are\nmost effective for consistent, high-utilization workloads.\nTherefore, using the Instance Scheduler offers the best balance of cost optimization and operational\nsimplicity, directly aligning with the company's requirement to reduce costs during off-peak hours while\nminimizing management effort.\nFurther Research:\nAWS Instance Scheduler: https://aws.amazon.com/solutions/implementations/instance-scheduler/\nRDS Cost Optimization: https://aws.amazon.com/rds/pricing/",
    "links": [
      "https://aws.amazon.com/solutions/implementations/instance-scheduler/",
      "https://aws.amazon.com/rds/pricing/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses locally attached storage to run a latency-sensitive application on premises. The company is using\na lift and shift method to move the application to the AWS Cloud. The company does not want to change the\napplication architecture.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "D",
    "explanation": "The best solution is to host the application on an Amazon EC2 instance and use an Amazon Elastic Block Store\n(Amazon EBS) GP3 volume.\nHere's why:\nLift and Shift: The company wants to move the application without significant architectural changes. This\nimplies a direct migration of the existing application stack to AWS.\nLatency-Sensitive Application: The application requires low latency storage. EBS is designed for block\nstorage and provides low latency access.\nCost-Effectiveness: GP3 volumes offer a balance of performance and cost, making them generally more\ncost-effective than other EBS volume types for many workloads.\nLet's eliminate the other options:\nOption A (FSx for Lustre): FSx for Lustre is a high-performance file system optimized for compute-intensive\nworkloads. While it offers low latency, it's more complex to set up and generally more expensive than EBS,\nmaking it less cost-effective for a simple lift and shift.\nOption B (EBS GP2): GP2 volumes are older generation EBS volumes. GP3 volumes offer better performance\nand pricing in many cases.\nOption C (FSx for OpenZFS): FSx for OpenZFS is a fully managed file system service built on ZFS. While it\nprovides features like data compression and snapshots, it adds complexity and can be more expensive than\nusing EBS directly. It isn't the most cost-effective for a simple lift and shift of an application using locally\nattached storage.\nIn essence, using EBS directly attached to EC2 instance mirrors the on-premises setup of an application\nrunning with locally attached storage. Using the newer generation EBS volumes can provide cost and\nperformance advantages.Here are some resources that could be helpful.\nAmazon EBS Volume Types\nAmazon FSx",
    "links": []
  },
  {
    "question": "CertyIQ\nA company runs a stateful production application on Amazon EC2 instances. The application requires at least two\nEC2 instances to always be running.\nA solutions architect needs to design a highly available and fault-tolerant architecture for the application. The\nsolutions architect creates an Auto Scaling group of EC2 instances.\nWhich set of additional steps should the solutions architect take to meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B because it ensures high availability and fault tolerance for the application. Here's why:\nMinimum Capacity of Two: The requirement is that at least two EC2 instances must always be running.\nSetting the minimum capacity of the Auto Scaling group to two guarantees that the group will always attempt\nto maintain at least this number of instances.\nHigh Availability Through Multiple Availability Zones: Distributing instances across multiple Availability\nZones (AZs) is a core principle of high availability in AWS. If one AZ experiences an outage, the instances in\nthe other AZ remain operational, ensuring the application continues to function. Deploying instances into two\nAZs meets this requirement. The question does not require spot instances.\nTherefore, deploying two On-Demand Instances in one Availability Zone and two On-Demand Instances in a\nsecond Availability Zone ensures that the application is both fault-tolerant (can withstand the failure of an\ninstance) and highly available (can withstand the failure of an Availability Zone). Setting the minimum capacity\nto four allows the Auto Scaling Group to function if an instance in either AZ fails.\nOption A is incorrect because while it satisfies the minimum capacity requirement of two instances, it doesn't\nprovide enough redundancy within each Availability Zone. A single instance failure in an AZ could impact\nperformance or availability.\nOption C and D use spot instances. Spot instances are cheaper than on-demand but come with a risk of\ninterruption that doesn't align with the \"always be running\" condition of the application.\nFurther research:https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-\ngroups.htmlhttps://wa.aws.amazon.com/wellarchitected/2020-07-02T19-33-23/best-\npractices/rel12017_design_for_high_availability.en.html",
    "links": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-",
      "https://wa.aws.amazon.com/wellarchitected/2020-07-02T19-33-23/best-"
    ]
  },
  {
    "question": "CertyIQ\nAn ecommerce company uses Amazon Route 53 as its DNS provider. The company hosts its website on premises\nand in the AWS Cloud. The company's on-premises data center is near the us-west-1 Region. The company uses the\neu-central-1 Region to host the website. The company wants to minimize load time for the website as much as\npossible.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Set up a geolocation routing policy. Send the traffic that is near us-west-1 to the",
      "B": "Simple routing policy: Simple routing policies provide no intelligent distribution based on location or",
      "C": "Latency routing policy: While latency routing aims to minimize latency, it only considers the latency",
      "D": "Weighted routing policy: Weighted routing balances traffic across multiple resources but doesn't consider"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Set up a geolocation routing policy. Send the traffic that is near us-west-1 to the\non-premises data center. Send the traffic that is near eu-central-1 to eu-central-1.\nHere's why:\nThe primary goal is to minimize website load time for users globally. Geolocation routing in Route 53 allows\ndirecting users to specific resources based on their geographic location. By directing users geographically\nclose to the on-premises data center (near us-west-1) to that data center and users geographically close to\nthe eu-central-1 region to the AWS eu-central-1 Region, the solution reduces latency, and thus speeds up load\ntimes, for the majority of users. This approach minimizes the distance data travels, leading to faster response\ntimes.\nLet's analyze why the other options are not optimal:\nB. Simple routing policy: Simple routing policies provide no intelligent distribution based on location or\nlatency, making it less effective for minimizing load times across different geographic areas. It will not\neffectively distribute traffic based on user location proximity.\nC. Latency routing policy: While latency routing aims to minimize latency, it only considers the latency\nbetween the user's DNS resolver and the AWS regions. It doesn't take into account the on-premises data\ncenter's location.\nD. Weighted routing policy: Weighted routing balances traffic across multiple resources but doesn't consider\nthe user's location. It may not efficiently minimize latency for users in specific regions.\nTherefore, geolocation routing provides the best approach to serve users from the geographically closest\nlocation, resulting in the lowest latency and best user experience.\nFor further research, consider these resources:\nAWS Route 53 Routing Policies: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-\npolicy.html\nGeolocation Routing: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-geo.html",
    "links": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-geo.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has 5 PB of archived data on physical tapes. The company needs to preserve the data on the tapes for\nanother 10 years for compliance purposes. The company wants to migrate to AWS in the next 6 months. The data\ncenter that stores the tapes has a 1 Gbps uplink internet connectivity.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "C",
    "explanation": "The most cost-effective solution for migrating 5 PB of tape-archived data to AWS for a 10-year compliance\nretention period, given a limited 1 Gbps internet uplink, is option C, leveraging AWS Snowball Edge with Tape\nGateway.\nHere's a detailed justification:\n1. High Data Volume & Low Bandwidth: Migrating 5 PB over a 1 Gbps link would take an impractically\nlong time (potentially years). Network-based transfer solutions (options A & B) become highly\ninefficient due to the sheer data volume and bandwidth limitations.\n2. AWS Snowball Edge (Data Transfer Device): Snowball Edge devices provide a secure, physically\ntransportable way to move massive amounts of data to AWS, bypassing the bandwidth constraints of\nthe internet connection. https://aws.amazon.com/snowball/\n3. Tape Gateway Integration: Snowball Edge can be configured with Tape Gateway, allowing the\ncompany to treat the Snowball device as a virtual tape library on-premises. This enables direct\ncopying of data from physical tapes to virtual tapes stored on the Snowball.\nhttps://aws.amazon.com/storagegateway/tape-gateway/\n4. S3 Glacier Deep Archive (Cost-Effective Archival): Amazon S3 Glacier Deep Archive is the lowest-\ncost storage class, ideal for long-term data archiving with infrequent access requirements.\nhttps://aws.amazon.com/glacier/pricing/\n5. Lifecycle Policy Automation: A lifecycle policy in S3 can automatically transition the virtual tapes\nuploaded from Snowball to S3 Glacier Deep Archive after a specified period. This ensures the data is\nstored in the most cost-effective storage tier for long-term retention.\n6. Cost Efficiency Comparison: While option B might seem appealing in its simplicity, the prolonged\ntransfer time over the limited internet connection would incur significant operational costs (network\nusage, staff time). Option A involves staging data in NFS storage, adding an unnecessary\nintermediate step and infrastructure cost. Option C minimizes infrastructure and network costs by\nusing a physical appliance and leveraging the lowest-cost archive tier.\n7. Compliance Considerations: S3 Glacier Deep Archive offers features like data encryption and\nversioning, which support compliance requirements for long-term data retention and integrity.\nSnowball transfer are also secured by KMS encryption.\nTherefore, ordering multiple Snowball devices equipped with Tape Gateway, copying tapes to Snowball,\nshipping them to AWS, and utilizing a lifecycle policy to archive to Glacier Deep Archive provides the most\ncost-effective and practical solution for the company's needs, circumventing the bandwidth limitation and\nmeeting the 10-year compliance requirement at minimal cost.",
    "links": [
      "https://aws.amazon.com/snowball/",
      "https://aws.amazon.com/storagegateway/tape-gateway/",
      "https://aws.amazon.com/glacier/pricing/"
    ]
  },
  {
    "question": "CertyIQ\nA company is deploying an application that processes large quantities of data in parallel. The company plans to\nuse Amazon EC2 instances for the workload. The network architecture must be configurable to prevent groups of\nnodes from sharing the same underlying hardware.\nWhich networking solution meets these requirements?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A: Run the EC2 instances in a spread placement group. Here's why:\nThe requirement is to prevent groups of nodes (EC2 instances) from sharing the same underlying hardware for\nan application processing large quantities of data in parallel. This isolates instances, minimizing the impact of\nfailures or performance issues on one host affecting others.\nSpread placement groups are designed to meet precisely this requirement. They ensure that each instance\nwithin the group runs on distinct underlying hardware. This provides high availability and minimizes correlated\nfailures. AWS guarantees that a small number of instances within a spread placement group won't share the\nsame physical hardware.\nOption B (Separate accounts) technically isolates instances at a management level, but it doesn't guarantee\nhardware separation without additional configurations and can significantly increase management overhead.\nIt is not primarily a network configuration choice.\nOption C (Dedicated tenancy) ensures EC2 instances run on hardware dedicated solely to the account,\navoiding sharing with other AWS customers. While this provides isolation, it is more costly and doesn't\nnecessarily prevent instances within the account from potentially sharing hardware unless combined with\nplacement groups.\nOption D (Shared tenancy) is the default tenancy and allows EC2 instances to share hardware with other\nAWS customers, which is the opposite of what's required.\nTherefore, spread placement groups are the most appropriate solution because they focus on distributing\ninstances across distinct hardware while staying within a single AWS account for simplified management and\nnetwork configuration. The other options either don't directly address the hardware isolation requirement\n(separate accounts) or contradict it (shared tenancy) or are more expensive for the same result (dedicated\ntenancy, without necessarily the needed spread).\nAuthoritative Links:\nAWS EC2 Placement Groups: Detailed documentation on placement groups, including spread placement\ngroups.\nAWS EC2 Tenancy Options: Information on dedicated tenancy and its use cases.",
    "links": []
  },
  {
    "question": "CertyIQ\nA solutions architect is designing a disaster recovery (DR) strategy to provide Amazon EC2 capacity in a failover\nAWS Region. Business requirements state that the DR strategy must meet capacity in the failover Region.\nWhich solution will meet these requirements?",
    "options": {
      "D": "Purchase a Capacity Reservation in the failover Region."
    },
    "answer": "D",
    "explanation": "The correct answer is D. Purchase a Capacity Reservation in the failover Region.\nJustification:\nThe primary requirement is to meet capacity in the failover region during a disaster recovery event. Capacity\nReservations guarantee that EC2 capacity will be available in a specific Availability Zone when you need it.\nThis directly addresses the requirement of ensuring capacity for EC2 instances in the failover region.\nOption A (On-Demand Instances): While On-Demand Instances are readily available, they do not guarantee\ncapacity. During a disaster, demand for resources in the failover Region may surge, potentially leading to\ninsufficient capacity to launch all required instances.\nOption B (EC2 Savings Plan): Savings Plans offer cost savings based on committed usage. However, Savings\nPlans do not guarantee capacity. They simply offer discounted pricing for EC2 usage. The instances still\ncompete for available capacity.\nOption C (Regional Reserved Instances): Regional Reserved Instances provide a discount on EC2 usage and\nprovide a capacity reservation at a regional level. Although these instances provide increased availability of\ncapacity, they do not guarantee the availability of resources during a widespread disaster.\nBy using Capacity Reservations, the organization can ensure that the necessary EC2 resources are always\navailable, even during a DR event where demand for resources spikes. This proactively addresses the capacity\nconstraint and ensures the DR strategy can successfully meet the business requirements.\nAuthoritative Links:\nCapacity Reservations\nEC2 Instance Purchasing Options",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has five organizational units (OUs) as part of its organization in AWS Organizations. Each OU correlates\nto the five businesses that the company owns. The company's research and development (R&D) business is\nseparating from the company and will need its own organization. A solutions architect creates a separate new\nmanagement account for this purpose.\nWhat should the solutions architect do next in the new management account?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B: Invite the R&D AWS account to be part of the new organization after the R&D AWS\naccount has left the prior organization.\nHere's why:\nThe goal is to move the R&D business's AWS account from the existing AWS Organization to a completely\nseparate one. AWS Organizations does not allow an account to be part of two organizations simultaneously.\nTherefore, the R&D account must first leave the original organization before it can join the new one.\nOption A is incorrect because an AWS account cannot be a member of two AWS Organizations at the same\ntime. This violates the fundamental structure and control mechanisms of AWS Organizations.\nOption C is not the most efficient approach. Creating a new account and migrating resources would involve\nsignificant time, effort, and potential disruption. Direct account transfer, after detachment, is a more\nstreamlined method.\nOption D is incorrect because the new management account (which represents the new R&D organization)\nshould not be a member of the prior organization. The R&D business is meant to be entirely independent.\nMaking the new management account a member of the prior organization would defeat the purpose of the\nseparation and create a dependency.\nThe correct procedure involves first removing the R&D account from the initial organization. After this\ndetachment, the new organization's management account can send an invitation to the detached R&D\naccount. Once accepted, the R&D account becomes a member of the new organization. This ensures a clean\nseparation and transfer of the AWS account to its new, independent organizational structure.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_remove.htmlhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invite.html",
    "links": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_remove.htmlhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invite.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is designing a solution to capture customer activity in different web applications to process analytics\nand make predictions. Customer activity in the web applications is unpredictable and can increase suddenly. The\ncompany requires a solution that integrates with other web applications. The solution must include an\nauthorization step for security purposes.\nWhich solution will meet these requirements?",
    "options": {
      "B": "B.Configure an Amazon API Gateway endpoint in front of an Amazon Kinesis data stream that stores the",
      "C": "Option B uses Kinesis Data Streams. While Streams can handle high-velocity data, Data Firehose is a more"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Here's a detailed justification:\nThe problem requires a solution for capturing unpredictable customer activity from web applications,\nintegrating with existing apps, and implementing authorization.\nOption C leverages Amazon API Gateway, which is designed to handle API requests from various sources,\nincluding web applications. This satisfies the integration requirement. API Gateway can scale automatically to\nhandle unpredictable surges in traffic, meeting the scalability requirement.\nKinesis Data Firehose is a suitable choice for ingesting high volumes of streaming data directly into\ndestinations like Amazon S3. Storing the data in S3 allows for cost-effective storage and future analysis\nusing services like Athena or Redshift. This fulfills the data capture and storage requirements.\nCrucially, Option C uses an API Gateway Lambda authorizer. Lambda authorizers (formerly known as custom\nauthorizers) are powerful tools that allow you to implement custom authorization logic for your API Gateway\nendpoints. This directly addresses the authorization requirement, providing fine-grained control over who can\naccess your API and the data it processes. The Lambda function can verify identity, check permissions, and\nreturn an IAM policy that determines what resources the caller is authorized to access.\nOptions A and D use Gateway Load Balancers (GWLBs) and ECS containers. While GWLB handles network\ntraffic, it's not the primary service for API management and authorization in the context of web applications.\nECS containers storing data on EFS are also not the most efficient or scalable solution for handling streaming\ndata from web applications. The authorization methods in A and D are also less integrated than the API\nGateway Lambda Authorizer in C.\nOption B uses Kinesis Data Streams. While Streams can handle high-velocity data, Data Firehose is a more\nsuitable choice when you need to reliably load data into data lakes or data warehouses like S3, especially with\na large volume of web application event activity. Furthermore, the Lambda function used for authorization in\noption B is outside of the AWS API Gateway and might be more difficult to maintain for complex API\nauthorizations.\nIn summary, option C best meets all the requirements by providing a scalable API endpoint with authorization\nand efficient data ingestion and storage, making it the optimal solution.\nSupporting links:\nAmazon API Gateway: https://aws.amazon.com/api-gateway/\nAmazon Kinesis Data Firehose: https://aws.amazon.com/kinesis/data-firehose/\nAmazon S3: https://aws.amazon.com/s3/\nAPI Gateway Lambda Authorizers:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html",
    "links": [
      "https://aws.amazon.com/api-gateway/",
      "https://aws.amazon.com/kinesis/data-firehose/",
      "https://aws.amazon.com/s3/",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html"
    ]
  },
  {
    "question": "CertyIQ\nAn ecommerce company wants a disaster recovery solution for its Amazon RDS DB instances that run Microsoft\nSQL Server Enterprise Edition. The company's current recovery point objective (RPO) and recovery time objective\n(RTO) are 24 hours.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "A": "Create a cross-Region read replica and promote the read replica to the primary instance: Read replicas",
      "B": "Use AWS Database Migration Service (AWS DMS) to create RDS cross-Region replication: AWS DMS is",
      "C": "Use cross-Region replication every 24 hours to copy native backups to an Amazon S3 bucket: Although a",
      "D": "Copy automatic snapshots to another Region every 24 hours: RDS SQL Server has a native feature to"
    },
    "answer": "D",
    "explanation": "The question asks for the most cost-effective disaster recovery solution for RDS SQL Server with a 24-hour\nRPO/RTO. Let's analyze the options:\nA. Create a cross-Region read replica and promote the read replica to the primary instance: Read replicas\nprovide faster failover times compared to snapshots, potentially meeting a shorter RTO. However, for a 24-\nhour RTO, this option is generally more expensive because read replicas continuously replicate data, incurring\nhigher operational costs. For this 24-hour RTO, using Snapshots is enough.\nB. Use AWS Database Migration Service (AWS DMS) to create RDS cross-Region replication: AWS DMS is\nmore suitable for heterogeneous database migrations and ongoing replication. For disaster recovery with a\n24-hour RPO/RTO for RDS SQL Server, it adds complexity and cost compared to native snapshot replication.\nC. Use cross-Region replication every 24 hours to copy native backups to an Amazon S3 bucket: Although a\nvalid option, this manual copy approach may be less efficient than using automated snapshot copy feature\noffered by AWS. The snapshot copy automates the process of moving backups across regions.\nD. Copy automatic snapshots to another Region every 24 hours: RDS SQL Server has a native feature to\nautomate snapshot creation and cross-Region copy. Configuring automatic snapshots with a cross-Region\ncopy every 24 hours meets the RPO/RTO of 24 hours. This approach is cost-effective because you only pay for\nstorage and the occasional snapshot copy operation.\nTherefore, copying automated snapshots cross-Region is the most cost-effective approach for a 24-hour\nRPO/RTO, utilizing the native capabilities of AWS RDS.\nRefer to\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html\nand https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.Procedural.Importing.html for\ndetails on RDS SQL Server backups and disaster recovery. Also read more about RTO/RPO best practices\nhttps://aws.amazon.com/blogs/storage/backup-and-restore-strategies-for-amazon-elastic-file-system-\namazon-efs/",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.Procedural.Importing.html",
      "https://aws.amazon.com/blogs/storage/backup-and-restore-strategies-for-amazon-elastic-file-system-"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a web application on Amazon EC2 instances in an Auto Scaling group behind an Application Load\nBalancer that has sticky sessions enabled. The web server currently hosts the user session state. The company\nwants to ensure high availability and avoid user session state loss in the event of a web server outage.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The requirement is to ensure high availability and prevent session state loss when EC2 instances running a\nweb application in an Auto Scaling group behind an ALB fail. Since the current setup stores the user session\nstate on the web server itself, a server outage leads to session loss.\nOption B, using Amazon ElastiCache for Redis, is the best solution because it provides a highly available and\npersistent store for user session data. Redis is an in-memory data structure store that can be configured for\npersistence, which prevents data loss in case of failure. By updating the application to use ElastiCache for\nRedis, the session state is stored externally to the EC2 instances. Therefore, even if an instance fails, the\nsession data is preserved in the Redis cluster.\nOption A, using ElastiCache for Memcached, is less ideal. While Memcached offers fast caching, it is primarily\ndesigned for caching data that can be easily recreated and doesn't natively support persistence. This means\nin case of a Memcached instance failure, the session data is lost.\nOption C, utilizing AWS Storage Gateway cached volume, is not suitable for storing session state. Storage\nGateway primarily focuses on bridging on-premises storage with AWS cloud storage. It introduces significant\nlatency compared to an in-memory cache, negatively impacting application performance. Session data needs\nto be rapidly accessible.\nOption D, using Amazon RDS, while a persistent storage option, is generally not optimized for the speed and\naccess patterns required for session management. Relational databases like RDS introduce overhead that\nsignificantly impacts performance compared to in-memory caching solutions like Redis or Memcached. RDS is\nmuch slower at retrieving user session data.Redis offers better performance.\nTherefore, ElastiCache for Redis addresses the high availability and session state persistence requirement in\nthe most effective manner.\nAmazon ElastiCache for RedisSession Management with ElastiCache",
    "links": []
  },
  {
    "question": "CertyIQ\nA company migrated a MySQL database from the company's on-premises data center to an Amazon RDS for\nMySQL DB instance. The company sized the RDS DB instance to meet the company's average daily workload. Once\na month, the database performs slowly when the company runs queries for a report. The company wants to have\nthe ability to run reports and maintain the performance of the daily workloads.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "A",
    "explanation": "Option A, creating a read replica and directing report queries to it, is the most suitable solution because it\ndirectly addresses the problem of performance degradation during report generation without impacting the\nprimary database's daily workload. Read replicas in Amazon RDS provide a near real-time, read-only copy of\nthe primary database. By offloading the resource-intensive report queries to the read replica, the primary\ndatabase remains available and performs optimally for regular transactions and operations. This avoids\nperformance bottlenecks for daily users while still allowing for reporting functionality. It's also a cost-\neffective approach, as the read replica can be sized appropriately for the reporting workload, and the data\nreplication is managed by RDS. Option B, while isolating the reporting workload, introduces complexity with\ncreating and restoring backups, which can be time-consuming. Furthermore, restoring a backup to a new DB\ninstance could potentially lead to data staleness, as it represents a point-in-time snapshot of the data, and not\nthe current database state. Option C, while potentially scalable, involves exporting data to S3 and using\nAthena, which is suited for analyzing large datasets. This approach is overkill for a simple reporting\nrequirement, adds complexity and potential latency. Option D, resizing the DB instance, addresses the\nsymptom but not the root cause. It is cost-inefficient because the larger instance would be underutilized most\nof the time. Using a read replica provides a more targeted and cost-effective scaling strategy only during\nreport generation periods.\nAuthoritative Links:\nAmazon RDS Read Replicas:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.ReadReplicas.html\nAmazon Athena: https://aws.amazon.com/athena/",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.ReadReplicas.html",
      "https://aws.amazon.com/athena/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a container application by using Amazon Elastic Kubernetes Service (Amazon EKS). The\napplication includes microservices that manage customers and place orders. The company needs to route\nincoming requests to the appropriate microservices.\nWhich solution will meet this requirement MOST cost-effectively?",
    "options": {
      "D": "Use Amazon API Gateway to connect the requests to Amazon EKS."
    },
    "answer": "D",
    "explanation": "The most cost-effective solution for routing incoming requests to microservices within an Amazon EKS cluster\nfor the given scenario is D. Use Amazon API Gateway to connect the requests to Amazon EKS.\nHere's a detailed justification:\nAPI Gateway's Role: Amazon API Gateway is designed to handle API requests at scale. It acts as a single\nentry point for all incoming requests to your application. It can route these requests to different backend\nservices based on the request's path, headers, or other attributes.\nCost-Effectiveness:\nPay-as-you-go Pricing: API Gateway operates on a pay-as-you-go model, charging only for the API calls you\nreceive and the data transferred. This is often more cost-effective than running a dedicated load balancer\nconstantly, especially for applications with variable traffic.\nReduced Infrastructure Overhead: By offloading API management tasks to API Gateway, you reduce the\noperational burden on your EKS cluster and simplify your infrastructure.\nFunctionality:\nRequest Routing: API Gateway offers sophisticated request routing capabilities based on URL paths, HTTP\nheaders, query parameters, and more. This allows precise control over how requests are directed to your\nmicroservices.\nSecurity: API Gateway provides security features such as authentication, authorization, and request validation\nto protect your application.\nTraffic Management: API Gateway can manage traffic by throttling requests to prevent overload on the EKS\ncluster.\nIntegration with EKS: API Gateway can integrate with EKS through the use of Ingress Controller and Service\nDiscovery.\nWhy other options are less cost-effective or less suitable:\nA & B (Load Balancers): While AWS Load Balancer Controller can provision Application Load Balancer (ALB)\nor Network Load Balancer (NLB), they are typically more expensive than API Gateway for scenarios where you\nrequire complex routing and API management features. Load balancers also require continuous operation,\nleading to higher costs even when the application is not heavily used. Furthermore, using an NLB would\nrequire you to manage TLS termination and additional complexity that ALB and API Gateway can offload.\nC (Lambda Function): Using Lambda functions as a gateway to EKS is typically not recommended. Lambda\nhas execution time limits and might not be cost-effective for handling high-volume, long-running requests. It\nalso adds complexity to the application architecture.\nIn conclusion, Amazon API Gateway provides the most cost-effective and feature-rich solution for routing\nrequests to microservices within an Amazon EKS cluster by offering fine-grained control over request routing,\nsecurity, and traffic management, while adhering to a pay-as-you-go pricing model.\nAuthoritative Links:\nAmazon API Gateway: https://aws.amazon.com/api-gateway/\nAWS Load Balancer Controller: https://github.com/kubernetes-sigs/aws-load-balancer-controller",
    "links": [
      "https://aws.amazon.com/api-gateway/",
      "https://github.com/kubernetes-sigs/aws-load-balancer-controller"
    ]
  },
  {
    "question": "CertyIQ\nA company uses AWS and sells access to copyrighted images. The companys global customer base needs to be\nable to access these images quickly. The company must deny access to users from specific countries. The\ncompany wants to minimize costs as much as possible.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D because it leverages both S3 and CloudFront to meet the requirements of fast global\naccess, geographic restrictions, and cost optimization.\nHere's a detailed justification:\nS3 for Storage: Amazon S3 provides highly durable, scalable, and cost-effective storage for the images.\nStoring the images in S3 as the origin simplifies the overall architecture and management overhead.\nCloudFront for Distribution: CloudFront is a Content Delivery Network (CDN) that caches content at edge\nlocations worldwide. This ensures low latency and fast access to the images for global customers, fulfilling\nthe first requirement.\nGeographic Restrictions: CloudFront allows you to configure geographic restrictions, also known as geo-\nfiltering or geo-blocking, to deny access to content based on the viewer's country. This directly addresses the\nrequirement to deny access to users from specific countries.\n(https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html)\nSigned URLs: Signed URLs provide controlled access to the images. They allow you to grant temporary\naccess to specific images to authorized users without exposing the underlying S3 bucket directly. This\nenhances security and control. This approach ensures that users can access the images only when they have a\nvalid, signed URL.\nCost Optimization: Using CloudFront helps optimize costs by caching content closer to users, reducing the\nload on the S3 bucket and potentially lowering S3 data transfer costs. CloudFront's pay-as-you-go pricing\nmodel also aligns with the company's cost minimization goal.\nWhy other options are not optimal:\nA: MFA and public bucket access are contradictory to each other. MFA is to secure bucket while Public\nBucket exposes the bucket and is insecure.\nB: Creating IAM users for each customer is not scalable and can be difficult to manage as the customer base\ngrows. It also doesn't directly address the geographic restriction requirement.\nC: Deploying EC2 instances in specific countries could be complex to manage, and it's not an optimal method\nto restrict customer access globally. It also might not provide the best performance for users outside those\nspecific countries and cost inefficient compared to CloudFront.\nIn conclusion, option D is the most suitable solution because it efficiently combines S3 for storage,\nCloudFront for global content delivery and geo-restrictions, and signed URLs for controlled access, while\naligning with the company's cost optimization goals.",
    "links": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html)"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is designing a highly available Amazon ElastiCache for Redis based solution. The solutions\narchitect needs to ensure that failures do not result in performance degradation or loss of data locally and within\nan AWS Region. The solution needs to provide high availability at the node level and at the Region level.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Use Multi-AZ Redis replication groups with shards that contain multiple nodes.",
      "B": "Use Redis shards that contain multiple nodes with Redis append only files (AOF) turned on: AOF provides",
      "C": "Use a Multi-AZ Redis cluster with more than one read replica in the replication group: This option is",
      "D": "Use Redis shards that contain multiple nodes with Auto Scaling turned on: Auto Scaling will not provide"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Use Multi-AZ Redis replication groups with shards that contain multiple nodes.\nHere's a detailed justification:\nHigh Availability at the Node Level (within a shard): Redis replication groups with multiple nodes in each\nshard provide node-level high availability. A primary node handles writes, and one or more read replicas\nasynchronously replicate the data. If the primary node fails, one of the read replicas is automatically\npromoted to primary, minimizing downtime.\nMulti-AZ for Regional High Availability: Deploying the Redis replication group across multiple Availability\nZones (Multi-AZ) provides resilience against AZ-level failures. If an entire AZ goes down, the automatic\nfailover mechanism will promote a replica in a different AZ to become the new primary. This ensures that the\napplication continues to function without significant interruption.\nData Durability: Redis replication ensures that data is not lost in case of a node failure within a shard. The\nasynchronous replication mechanism keeps replicas updated with the changes made to the primary. Even\nthough asynchronous replication may lead to slight data loss in extreme scenarios, it offers the best balance\nbetween performance and data safety.\nWhy other options are incorrect:\nB. Use Redis shards that contain multiple nodes with Redis append only files (AOF) turned on: AOF provides\ndata durability in case of server restarts and is not a solution for node-level high availability or region-level\nhigh availability. AOF stores every write operation in a log. If a node crashes and is restarted, Redis can\nreconstruct the dataset by replaying the AOF log. However, it does not automatically failover to another node\nin case of a primary failure.\nC. Use a Multi-AZ Redis cluster with more than one read replica in the replication group: This option is\npartially correct in its usage of Multi-AZ and read replicas, but the term \"cluster\" isn't appropriate here.\nElastiCache for Redis uses replication groups (for non-cluster mode) or clusters (for cluster mode). Also, the\nreplication group consists of one primary and multiple replicas.\nD. Use Redis shards that contain multiple nodes with Auto Scaling turned on: Auto Scaling will not provide\nhigh availability since the process of scaling up the number of shards will take time to complete and there\nwould still be data loss in case of node failure. It helps manage the capacity of your ElastiCache cluster based\non demand but does not offer an immediate failover mechanism for high availability. Auto Scaling is more\nsuitable for addressing scalability concerns rather than immediate failure recovery.\nAuthoritative Links:\nAmazon ElastiCache for Redis: High Availability\nReplication: Redis Documentation",
    "links": []
  },
  {
    "question": "CertyIQ\nA company plans to migrate to AWS and use Amazon EC2 On-Demand Instances for its application. During the\nmigration testing phase, a technical team observes that the application takes a long time to launch and load\nmemory to become fully productive.\nWhich solution will reduce the launch time of the application during the next testing phase?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C: Launch the EC2 On-Demand Instances with hibernation turned on. Configure EC2\nAuto Scaling warm pools during the next testing phase. This approach effectively reduces application launch\ntime.\nHere's why:\nHibernation: Hibernation saves the in-memory state of an EC2 instance to the EBS root volume when the\ninstance is stopped. Upon restart, the instance resumes from this saved state, drastically reducing the time it\ntakes to load the application and its data back into memory.\n(https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html)\nWarm Pools: EC2 Auto Scaling warm pools allow you to pre-initialize a group of EC2 instances before they are\nneeded. These instances are already launched and configured, waiting in a \"ready\" state. This eliminates the\ndelay of launching and configuring instances when demand increases.\n(https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-warm-pools.html)\nBy combining hibernation and warm pools, instances can be pre-launched, pre-configured, and their state\nsaved. When needed, instances resume quickly from their hibernated state within the warm pool, significantly\ndecreasing the time it takes for the application to become fully productive during the next testing phase.\nOption A is less effective because simply launching more instances and using auto-scaling doesn't address\nthe underlying issue of slow application loading. Auto-scaling helps with scaling in response to demand but\ndoesn't inherently speed up individual instance launch and initialization.\nOption B, using Spot Instances, introduces the risk of instance interruption if Spot prices rise above the bid\nprice, which is unsuitable for predictable testing. Furthermore, it doesn't solve the problem of the initial slow\nloading time.\nOption D, Capacity Reservations, guarantees EC2 capacity but doesn't inherently reduce application launch\ntime. It ensures resources are available, but the application will still take the same time to load into memory\non newly launched instances. The addition of extra instances during the testing phase will have no benefit.",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html)",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-warm-pools.html)"
    ]
  },
  {
    "question": "CertyIQ\nA company's applications run on Amazon EC2 instances in Auto Scaling groups. The company notices that its\napplications experience sudden traffic increases on random days of the week. The company wants to maintain\napplication performance during sudden traffic increases.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "C": "Use dynamic scaling to change the size of the Auto Scaling group."
    },
    "answer": "C",
    "explanation": "The most cost-effective solution is C. Use dynamic scaling to change the size of the Auto Scaling group.\nHere's why:\nDynamic Scaling (also known as Target Tracking or Step Scaling): This approach automatically adjusts the\nnumber of EC2 instances in response to real-time changes in demand, based on pre-defined metrics like CPU\nutilization or network traffic. Because the traffic increases are sudden and on random days, a dynamic scaling\npolicy allows the Auto Scaling group to react immediately, maintaining performance.\nWhy other options are not as suitable:\nManual Scaling: Requires human intervention to adjust the Auto Scaling group, which is not ideal for sudden\ntraffic increases. This approach is time-consuming and may result in performance degradation during the\nscaling process.\nPredictive Scaling: Analyzes historical traffic patterns to anticipate future demand and proactively adjust the\nAuto Scaling group. While effective for predictable patterns, it is less suitable for random traffic spikes as it\nrelies on predictable patterns.\nScheduled Scaling: Scales the Auto Scaling group based on a pre-defined schedule. Since the traffic\nincreases occur on random days, scheduled scaling is not a viable option.\nDynamic scaling is the optimal choice as it provides real-time responsiveness to fluctuating workloads without\nmanual intervention or reliance on historical data, ensuring application performance is maintained during\nunexpected traffic increases in a cost-effective manner.\nHere are authoritative links for further research:\nAWS Auto Scaling Documentation: https://docs.aws.amazon.com/autoscaling/ec2/userguide/\nScaling Your Application on AWS: https://aws.amazon.com/blogs/architecture/scaling-your-application-on-\naws/\nAWS Well-Architected Framework: https://wa.aws.amazon.com/ (Focus on the Performance Efficiency pillar\nfor scaling strategies).",
    "links": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/",
      "https://aws.amazon.com/blogs/architecture/scaling-your-application-on-",
      "https://wa.aws.amazon.com/"
    ]
  },
  {
    "question": "CertyIQ\nAn ecommerce application uses a PostgreSQL database that runs on an Amazon EC2 instance. During a monthly\nsales event, database usage increases and causes database connection issues for the application. The traffic is\nunpredictable for subsequent monthly sales events, which impacts the sales forecast. The company needs to\nmaintain performance when there is an unpredictable increase in traffic.\nWhich solution resolves this issue in the MOST cost-effective way?",
    "options": {},
    "answer": "A",
    "explanation": "The best solution is to migrate the PostgreSQL database to Amazon Aurora Serverless v2. Here's why:\nAurora Serverless v2 Cost Efficiency: Aurora Serverless v2 automatically scales database capacity based on\napplication needs. This is ideal for unpredictable traffic patterns like the monthly sales event. You only pay for\nthe capacity you consume, making it highly cost-effective compared to constantly running a larger,\npotentially underutilized, instance.\nRDS for PostgreSQL Scaling Limitations: While RDS for PostgreSQL with a larger instance type (Option C)\ncan handle increased load, it involves vertical scaling (changing the instance size). This requires downtime and\ncan be more expensive than Aurora Serverless v2 if the increased capacity is only needed intermittently.\nEC2 Auto Scaling Complexities: Enabling auto-scaling for PostgreSQL on an EC2 instance (Option B) is more\ncomplex and requires significant configuration for replication, failover, and data consistency. It involves\ncreating and managing multiple EC2 instances, which adds operational overhead and can be less cost-\neffective than Aurora Serverless v2, especially given the unpredictable workload.\nRedshift is Inappropriate: Amazon Redshift (Option D) is a data warehouse service designed for analytical\nworkloads (OLAP), not transactional workloads (OLTP) like an ecommerce application's database. Migrating to\nRedshift would require significant application changes and is not suitable for resolving database connection\nissues during sales events.\nMinimal Downtime Migration: Aurora offers migration tools and strategies that can minimize downtime\nduring the migration from PostgreSQL on EC2.\nSimplified Management: Aurora Serverless v2 simplifies database management, reducing operational\noverhead compared to managing a PostgreSQL database on EC2 or using EC2 auto scaling. It takes care of\npatching, backups, and other maintenance tasks.\nIn summary, Aurora Serverless v2 provides the most cost-effective and scalable solution for handling\nunpredictable traffic spikes in a PostgreSQL database used by an ecommerce application. It eliminates the\nneed for over-provisioning resources, simplifies management, and minimizes downtime during migration.\nHere are some authoritative links for further research:\nAmazon Aurora Serverless v2: https://aws.amazon.com/rds/aurora/serverless/\nAmazon RDS: https://aws.amazon.com/rds/\nAmazon Redshift: https://aws.amazon.com/redshift/",
    "links": [
      "https://aws.amazon.com/rds/aurora/serverless/",
      "https://aws.amazon.com/rds/",
      "https://aws.amazon.com/redshift/"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts an internal serverless application on AWS by using Amazon API Gateway and AWS Lambda. The\ncompanys employees report issues with high latency when they begin using the application each day. The\ncompany wants to reduce latency.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B: Set up a scheduled scaling to increase Lambda provisioned concurrency before\nemployees begin to use the application each day. Here's a detailed justification:\nThe problem describes high latency at the beginning of each day when employees start using the serverless\napplication. This suggests a cold start issue with the Lambda functions. Lambda functions operate on a pay-\nper-use model, meaning AWS only provisions resources when the function is invoked. When a function hasn't\nbeen used for a while, it enters a \"cold state,\" and the first invocation triggers a cold start. A cold start\ninvolves provisioning the execution environment, loading the code, and initializing the function, which adds\nsignificant latency.\nProvisioned Concurrency (PC) addresses this cold start issue by pre-initializing a specified number of Lambda\nfunction execution environments. These environments are ready to respond to requests immediately,\neliminating the cold start latency. By scheduling an increase in provisioned concurrency before employees\nbegin their workday, the necessary execution environments are already warm and waiting, significantly\nreducing latency for the initial requests.\nOption A, increasing the API Gateway throttling limit, addresses a different problem. Throttling limits are in\nplace to prevent API Gateway from being overwhelmed with requests. High latency caused by throttling\nwould manifest as errors and request rejections, not simply slow response times.\nOption C, creating a CloudWatch alarm to initiate a Lambda function, is an overly complex solution. It\nessentially triggers a \"dummy\" function to try to keep the function warm. While it might help slightly, it's less\neffective and more resource-intensive than provisioned concurrency. PC is specifically designed to manage\ncold starts and handle predictable traffic patterns.\nOption D, increasing Lambda function memory, can reduce latency in some cases by providing more resources\nfor code execution. However, it doesn't directly address the cold start problem. While increased memory\nmight slightly improve the performance of subsequent invocations, it won't eliminate the initial cold start\npenalty.\nTherefore, provisioned concurrency offers the most direct and efficient solution to mitigate the latency\ncaused by Lambda cold starts at the beginning of each day. Scheduled scaling automates this process,\nensuring consistent performance during peak usage.\nSupporting documentation:\nAWS Lambda Provisioned Concurrency: https://docs.aws.amazon.com/lambda/latest/dg/configuration-\nconcurrency.html\nImproving Lambda Cold Start Time: https://aws.amazon.com/blogs/compute/operating-lambda-\nperformance-optimization-part-1/",
    "links": [
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-",
      "https://aws.amazon.com/blogs/compute/operating-lambda-"
    ]
  },
  {
    "question": "CertyIQ\nA research company uses on-premises devices to generate data for analysis. The company wants to use the AWS\nCloud to analyze the data. The devices generate .csv files and support writing the data to an SMB file share.\nCompany analysts must be able to use SQL commands to query the data. The analysts will run queries periodically\nthroughout the day.\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose three.)",
    "options": {
      "A": "Deploy an AWS Storage Gateway on premises in Amazon S3 File Gateway mode: AWS Storage Gateway",
      "C": "Set up an AWS Glue crawler to create a table based on the data that is in Amazon S3: AWS Glue is a fully",
      "B": "Deploy an AWS Storage Gateway on premises in Amazon FSx File Gateway mode: While FSx File",
      "D": "Set up an Amazon EMR cluster with EMR File System (EMRFS) to query the data that is in Amazon S3."
    },
    "answer": "A",
    "explanation": "The correct answer is ACF. Here's a detailed justification:\nA. Deploy an AWS Storage Gateway on premises in Amazon S3 File Gateway mode: AWS Storage Gateway\nsimplifies connecting on-premises environments to AWS storage. The S3 File Gateway mode allows the\nresearch company's on-premises devices that write .csv files to SMB file shares to seamlessly write data\ndirectly to an S3 bucket. This avoids complex data transfer mechanisms and facilitates data ingestion into\nAWS for analysis. https://aws.amazon.com/storagegateway/file-gateway/\nC. Set up an AWS Glue crawler to create a table based on the data that is in Amazon S3: AWS Glue is a fully\nmanaged ETL (extract, transform, and load) service. Its crawler feature can automatically discover the\nschema of the .csv files in the S3 bucket and create a corresponding table in the AWS Glue Data Catalog. This\nmetadata catalog is essential for querying the data using SQL. https://aws.amazon.com/glue/\nF. Set up Amazon Athena to query the data that is in Amazon S3. Provide access to analysts: Amazon\nAthena is a serverless query service that allows analysts to use standard SQL to analyze data directly in S3.\nAthena uses the AWS Glue Data Catalog to understand the structure of the data. By pointing Athena to the\ntable created by Glue, the analysts can directly query the .csv files without needing to set up or manage any\ninfrastructure. It's also cost-effective since you pay only for the queries you run.\nhttps://aws.amazon.com/athena/\nWhy other options are not ideal or less cost-effective:\nB. Deploy an AWS Storage Gateway on premises in Amazon FSx File Gateway mode: While FSx File\nGateway allows on-premises access to Amazon FSx file systems, it's typically used when you need a fully-\nmanaged, highly performant file system in the cloud for workloads like video editing or large-scale\nsimulations. For simple .csv files, directly using S3 is more cost-effective.\nD. Set up an Amazon EMR cluster with EMR File System (EMRFS) to query the data that is in Amazon S3.\nProvide access to analysts: While EMR can query data in S3, setting up a full EMR cluster is overkill for\nsimply running SQL queries on .csv files. EMR is best suited for complex data processing tasks that require\ndistributed computing, such as data transformation, machine learning, or running Spark jobs.\nE. Set up an Amazon Redshift cluster to query the data that is in Amazon S3. Provide access to analysts:\nRedshift is a data warehouse designed for complex analytical workloads. While Redshift Spectrum can query\nS3 data, loading the .csv data into Redshift is less cost-effective than using Athena for periodic queries.\nRedshift requires a persistent cluster, resulting in higher costs compared to Athena's pay-per-query model.",
    "links": [
      "https://aws.amazon.com/storagegateway/file-gateway/",
      "https://aws.amazon.com/glue/",
      "https://aws.amazon.com/athena/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to use Amazon Elastic Container Service (Amazon ECS) clusters and Amazon RDS DB instances\nto build and run a payment processing application. The company will run the application in its on-premises data\ncenter for compliance purposes.\nA solutions architect wants to use AWS Outposts as part of the solution. The solutions architect is working with the\ncompany's operational team to build the application.\nWhich activities are the responsibility of the company's operational team? (Choose three.)",
    "options": {
      "A": "Providing resilient power and network connectivity to the Outposts racks: AWS Outposts extends AWS",
      "C": "Physical security and access controls of the data center environment: Since the Outposts racks are",
      "B": "Managing the virtualization hypervisor, storage systems, and the AWS services that run on Outposts:",
      "D": "Availability of the Outposts infrastructure including the power supplies, servers, and networking"
    },
    "answer": "A",
    "explanation": "The correct answer is ACE. Here's why:\nA. Providing resilient power and network connectivity to the Outposts racks: AWS Outposts extends AWS\ninfrastructure and services to on-premises environments. The customer is responsible for providing the\nphysical environment including power, cooling, and network connectivity. AWS manages the services once\nthey are deployed on Outposts. This is a shared responsibility model. (https://aws.amazon.com/outposts/)\nC. Physical security and access controls of the data center environment: Since the Outposts racks are\nlocated within the company's data center, the company retains responsibility for the physical security of that\nenvironment. This includes access controls, surveillance, and other security measures. AWS is responsible for\nthe security of the Outposts hardware itself.\nE. Physical maintenance of Outposts components: While AWS owns and manages the hardware components\nwithin the Outposts racks, the physical maintenance to be performed on-site (under AWS guidance and\nsupport) is the responsibility of the customer. AWS handles complex repairs and replacements, but the\noperational team will generally handle the regular physical maintenance.\nHere's why the other options are incorrect:\nB. Managing the virtualization hypervisor, storage systems, and the AWS services that run on Outposts:\nAWS manages the virtualization hypervisor and AWS services running on the Outposts. This aligns with AWS's\nmanaged service model.\nD. Availability of the Outposts infrastructure including the power supplies, servers, and networking\nequipment within the Outposts racks: While the customer provides the environment, AWS is responsible for\nthe availability of the Outposts infrastructure itself (power supplies, servers, networking hardware). This is\npart of the AWS service level agreement.\nF. Providing extra capacity for Amazon ECS clusters to mitigate server failures and maintenance events:\nScaling the Amazon ECS clusters is primarily an application-level activity to maintain application availability,\nand AWS is not responsible for managing application scaling. The operational team is responsible for setting\nup Auto Scaling policies for the ECS clusters to ensure capacity during failures, or for scaling the ECS\ninfrastructure.",
    "links": [
      "https://aws.amazon.com/outposts/)"
    ]
  },
  {
    "question": "CertyIQ\nA company is planning to migrate a TCP-based application into the company's VP",
    "options": {
      "C": "The application is publicly",
      "A": "Deploy a Network Load Balancer (NLB). Configure the NLB to be publicly"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Deploy a Network Load Balancer (NLB). Configure the NLB to be publicly\naccessible over the TCP port that the application requires.\nHere's why:\nThe primary requirement is to replicate the existing on-premises application's performance in AWS,\nspecifically handling 3 million requests per second with low latency over a non-standard TCP port. NLBs are\ndesigned for high-performance, low-latency traffic routing at the transport layer (Layer 4), making them ideal\nfor TCP-based applications. NLBs can handle millions of requests per second while maintaining low latency,\naligning perfectly with the stated performance requirement. They also support listening on non-standard\nports.\nOption B, using an Application Load Balancer (ALB), is not suitable. ALBs operate at the application layer\n(Layer 7) and are designed for HTTP/HTTPS traffic. While ALBs can handle a significant load, they are not\noptimized for the raw throughput and low latency needed for a TCP-based application that requires 3 million\nrequests per second. They are more suited for routing based on content, headers, or hostnames.\nOption C, using Amazon CloudFront with an ALB, is also inappropriate. CloudFront is a Content Delivery\nNetwork (CDN) primarily for caching static and dynamic content closer to users to improve latency for web\ncontent. It adds unnecessary complexity and potential latency for a TCP-based application that requires\ndirect, low-latency connections. CloudFront is best suited for content distribution, not as a direct replacement\nfor a load balancer handling TCP traffic volume.\nOption D, using Amazon API Gateway with Lambda functions, is unsuitable due to scalability and latency\nconcerns. API Gateway introduces overhead and might not handle 3 million requests per second with the\nrequired low latency, especially when integrated with Lambda functions. Lambda functions, even with\nprovisioned concurrency, might not be able to scale rapidly enough and maintain the needed throughput for\nthis volume of requests. API Gateway is also generally optimized for HTTP APIs, not raw TCP connections.\nIn summary, the NLB provides the necessary performance, protocol support (TCP), and port flexibility to meet\nthe application's requirements in AWS.\nAuthoritative Links:\nAWS Network Load Balancer:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\nAWS Application Load Balancer:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\nAmazon CloudFront: https://docs.aws.amazon.com/cloudfront/latest/developerguide/Introduction.html\nAmazon API Gateway: https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html",
    "links": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://docs.aws.amazon.com/cloudfront/latest/developerguide/Introduction.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs its critical database on an Amazon RDS for PostgreSQL DB instance. The company wants to\nmigrate to Amazon Aurora PostgreSQL with minimal downtime and data loss.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "B": "Create an Aurora read replica of the RDS for"
    },
    "answer": "B",
    "explanation": "The best solution to migrate an RDS for PostgreSQL database to Aurora PostgreSQL with minimal downtime\nand data loss, and with the least operational overhead, is B. Create an Aurora read replica of the RDS for\nPostgreSQL DB instance. Promote the Aurora read replica to a new Aurora PostgreSQL DB cluster.\nHere's why:\nMinimal Downtime: Creating an Aurora read replica allows continuous replication from the source RDS\ndatabase. This means that changes are constantly applied to the Aurora replica. Promoting the replica to be\nthe primary instance involves a cutover, which results in much less downtime compared to other migration\nstrategies.\nMinimal Data Loss: Because of the continuous replication, data loss is minimized to the replication lag during\nthe final cutover.\nLeast Operational Overhead: Aurora read replicas provide a straightforward and automated method for\nmigrating data. It avoids the complexities of managing manual backups, restores, or data import/export\nprocesses.\nData import from S3 (Option C) and pg_dump (Option D): require more operational overhead compared to\nAurora read replica. They also might involve a longer downtime window for backing up, transferring, and\nrestoring data.\nDB snapshot (Option A): A DB snapshot is a point-in-time copy of the data, and will not capture any\ntransactions happening on the RDS database after the snapshot is taken.\nAurora Read Replicas are specifically designed to enable migrations with minimal downtime and data loss.\nPromoting the read replica avoids manual data transfer, reducing the risk of human error and data\ninconsistencies.\nSupporting Links:\nAmazon Aurora Migration: https://aws.amazon.com/rds/aurora/migration/\nCreating an Amazon Aurora Read Replica:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Aurora.CreateReplica.html",
    "links": [
      "https://aws.amazon.com/rds/aurora/migration/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Aurora.CreateReplica.html"
    ]
  },
  {
    "question": "CertyIQ\nA company's infrastructure consists of hundreds of Amazon EC2 instances that use Amazon Elastic Block Store\n(Amazon EBS) storage. A solutions architect must ensure that every EC2 instance can be recovered after a\ndisaster.\nWhat should the solutions architect do to meet this requirement with the LEAST amount of effort?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C, using AWS Backup. Here's why:\nAWS Backup is a fully managed backup service designed to centralize and automate the backup of data\nacross AWS services, including EC2 instances and their associated EBS volumes. This means less manual\neffort compared to managing snapshots individually. AWS Backup enables you to create backup plans that\ndefine backup frequency, retention policies, and target backup vaults. You can easily assign these plans to\ngroups of EC2 instances. This solution inherently simplifies the process of ensuring every EC2 instance can be\nrecovered.\nOption A, taking individual snapshots and using CloudFormation, requires you to manually manage snapshots\nfor hundreds of instances and create/maintain complex CloudFormation templates. This increases complexity\nand management overhead. Moreover, CloudFormation focuses on infrastructure as code, not specifically\nbackup and restore.\nOption B, using Elastic Beanstalk, is primarily for deploying and managing web applications, not for backing\nup and restoring EC2 instances. Attaching EBS storage through Beanstalk can be complex and isn't a native\nbackup solution.\nOption D, using Lambda to manage snapshots and AMIs, necessitates custom code and adds operational\nburden. You would need to write, test, and maintain the Lambda functions, increasing complexity compared to\nusing a managed service like AWS Backup. Additionally, AMIs are instance-specific and don't inherently\ninclude the data from EBS volumes, thus not guaranteeing data recovery.\nAWS Backup, on the other hand, provides a centralized dashboard for monitoring backup jobs and simplifying\nthe restore process. The AWS Backup API or CLI can then be used to further expedite the restoration of\nmultiple instances, thereby minimizing the time required to recover from a disaster, hence fulfilling the \"least\namount of effort\" criterion.\nTherefore, AWS Backup provides the most straightforward, scalable, and manageable solution for backing up\nhundreds of EC2 instances and ensuring their recoverability.\nSupporting links:\nAWS Backup: https://aws.amazon.com/backup/\nAWS Backup Documentation: https://docs.aws.amazon.com/aws-\nbackup/latest/devguide/whatisawsbackup.html",
    "links": [
      "https://aws.amazon.com/backup/",
      "https://docs.aws.amazon.com/aws-"
    ]
  },
  {
    "question": "CertyIQ\nA company recently migrated to the AWS Cloud. The company wants a serverless solution for large-scale parallel\non-demand processing of a semistructured dataset. The data consists of logs, media files, sales transactions, and\nIoT sensor data that is stored in Amazon S3. The company wants the solution to process thousands of items in the\ndataset in parallel.\nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": {},
    "answer": "B",
    "explanation": "The company requires a serverless, operationally efficient solution for large-scale parallel processing of\nsemistructured data in S3. The ideal solution should handle thousands of items in parallel.\nOption A, using Step Functions Map state in Inline mode, is unsuitable for large-scale parallelism. Inline mode\nhas limitations on the number of iterations and payload sizes, making it less efficient for processing\nthousands of items.\nOption B, using Step Functions Map state in Distributed mode, is the most appropriate choice. Distributed\nmode is specifically designed for high-throughput parallel processing of large datasets. It can handle\nthousands or even millions of parallel executions, making it suitable for the company's requirements. This\nmode leverages AWS resources efficiently, providing operational simplicity and scalability.\nOption C, using AWS Glue, while capable of processing data in parallel, is primarily designed for ETL (Extract,\nTransform, Load) operations. It may be overkill for simple processing tasks, and the operational overhead\ncould be higher compared to Step Functions Distributed Map. Glue is a powerful service, but its ETL focus\nmakes it less suitable for the described scenario.\nOption D, using multiple Lambda functions directly, requires manual management of parallelism and scaling.\nThis approach would be operationally complex and less efficient compared to using Step Functions Map state\nin Distributed mode, which automatically manages the parallel execution and resource allocation.\nIn summary, Step Functions Distributed Map provides the best balance between serverless architecture,\noperational efficiency, scalability, and the ability to process thousands of items in parallel. It is specifically\ntailored for this type of high-throughput data processing.\nTherefore, option B is the correct answer.\nSupporting Links:\nAWS Step Functions Map State: https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-\nlanguage-map-state.html\nStep Functions Distributed Map vs. Inline Map: https://aws.amazon.com/blogs/compute/introducing-\ndistributed-map-in-aws-step-functions/",
    "links": [
      "https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-",
      "https://aws.amazon.com/blogs/compute/introducing-"
    ]
  },
  {
    "question": "CertyIQ\nA company will migrate 10 PB of data to Amazon S3 in 6 weeks. The current data center has a 500 Mbps uplink to\nthe internet. Other on-premises applications share the uplink. The company can use 80% of the internet bandwidth\nfor this one-time migration task.\nWhich solution will meet these requirements?",
    "options": {
      "B": "To transfer this data in 6 weeks (6 weeks 7 days/week 24 hours/day * 3600 seconds/hour"
    },
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D, using AWS Snowball devices, is the most suitable solution for\nmigrating 10 PB of data to Amazon S3 within 6 weeks, given the limited network bandwidth:\nOption D (AWS Snowball) is the best approach because it bypasses the internet bottleneck. With only a 500\nMbps uplink, and only 80% usable, the effective bandwidth is 400 Mbps (50 MBps). Transferring 10 PB (10,000\nTB) of data over this connection in 6 weeks is highly impractical. 10,000 TB 1024 GB/TB 1024 MB/GB =\n10,485,760,000 MB. To transfer this data in 6 weeks (6 weeks 7 days/week 24 hours/day * 3600 seconds/hour\n= 3,628,800 seconds), you would need a throughput of 10,485,760,000 MB / 3,628,800 seconds = ~2890\nMBps. This vastly exceeds the available 50 MBps.\nAWS Snowball allows for offline data transfer. The data is copied to the Snowball devices on-premises,\nshipped to AWS, and then uploaded to S3. This eliminates dependence on the internet bandwidth, reducing\ntransfer time.\nOptions A, B, and C rely on the limited internet connection. While AWS DataSync (A) could be used, it's\nconstrained by the 400 Mbps. Rsync (B) and the AWS CLI (C) are also limited by the available bandwidth.\nCalculating the total transfer time using the internet connection would significantly exceed the 6-week\ndeadline, making these options infeasible. These methods are more suitable for smaller data volumes or when\na high-bandwidth connection is available. Snowball provides a faster solution that is unaffected by the uplink\nlimitations. Using Snowball devices allows for parallel transfer which improves the time taken to migrate the\ndata.https://aws.amazon.com/snowball/",
    "links": [
      "https://aws.amazon.com/snowball/"
    ]
  },
  {
    "question": "CertyIQ\nA company has several on-premises Internet Small Computer Systems Interface (ISCSI) network storage servers.\nThe company wants to reduce the number of these servers by moving to the AWS Cloud. A solutions architect\nmust provide low-latency access to frequently used data and reduce the dependency on on-premises servers with\na minimal number of infrastructure changes.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the best solution:\nThe core requirements are low-latency access to frequently used data, reduced dependency on on-premises\nservers, and minimal infrastructure changes during migration.\nOption D, deploying an AWS Storage Gateway volume gateway configured with cached volumes, directly\naddresses these requirements. A cached volume gateway stores the entire dataset in Amazon S3, while\nfrequently accessed data is cached locally on the on-premises servers. This provides low-latency access to\nfrequently used data, as it's readily available locally. The bulk of the data resides in S3, fulfilling the\nrequirement of reducing dependency on on-premises servers and decreasing the needed storage on the local\nISCSI infrastructure.\nOption A, deploying an Amazon S3 File Gateway, is less suitable because it's primarily designed for file-based\naccess, not block-level access required by ISCSI. Converting ISCSI volumes to files and then using S3 File\nGateway would involve significant infrastructure changes.\nOption B, deploying Amazon EBS storage with backups to Amazon S3, requires migrating data entirely to\nAWS. This is a significant infrastructure change and doesn't provide low-latency access to on-premises users\nwithout completely migrating the applications.\nOption C, deploying an AWS Storage Gateway volume gateway configured with stored volumes, stores the\nentire dataset locally on the on-premises servers. While this reduces dependency on the original servers, it\ndoes not provide low-latency local access to the application server because it would need to access the\nstorage via Storage Gateway.\nTherefore, the cached volumes configuration with AWS Storage Gateway strikes the best balance between\nlow latency, reduced on-premises dependency, and minimal infrastructure changes by leveraging the on-\npremises servers to cache frequently accessed data.\nFor further research, you can refer to these authoritative links:\nAWS Storage Gateway Documentation: https://aws.amazon.com/storagegateway/\nCached Volumes: https://docs.aws.amazon.com/storagegateway/latest/userguide/cached-volumes.html",
    "links": [
      "https://aws.amazon.com/storagegateway/",
      "https://docs.aws.amazon.com/storagegateway/latest/userguide/cached-volumes.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is designing an application that will allow business users to upload objects to Amazon S3. The\nsolution needs to maximize object durability. Objects also must be readily available at any time and for any length\nof time. Users will access objects frequently within the first 30 days after the objects are uploaded, but users are\nmuch less likely to access objects that are older than 30 days.\nWhich solution meets these requirements MOST cost-effectively?",
    "options": {
      "B": "Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the\nobjects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.\nHere's a detailed justification:\nDurability: S3 Standard provides 99.999999999% (11 9's) of data durability because it stores data\nredundantly across multiple devices in multiple Availability Zones (AZs). S3 Standard-IA also offers this high\nlevel of durability as it is designed for infrequently accessed data but still needs the same resilience as S3\nStandard. This meets the requirement for maximizing object durability.\nAvailability: S3 Standard offers high availability (99.99%). S3 Standard-IA also offers high availability\n(99.9%) suitable for data that needs to be accessed when needed, though with a slightly lower availability\nguarantee than standard.\nAccessibility: Both S3 Standard and S3 Standard-IA provide immediate accessibility. This is important\nbecause the objects must be \"readily available at any time.\"\nCost-Effectiveness: Because most accesses occur within the first 30 days, storing the data initially in S3\nStandard makes sense because it is optimized for frequent access. After 30 days, transitioning to S3\nStandard-IA reduces storage costs for the less frequently accessed data.\nLifecycle Policies: S3 Lifecycle policies allow you to automatically transition objects between storage classes\nbased on predefined rules. This automation simplifies storage management and reduces costs.\nWhy the other options are less suitable:\nA (S3 Glacier): S3 Glacier is designed for long-term archival, not for data that needs to be readily available.\nRetrievals from Glacier can take several hours, which violates the \"readily available at any time\" requirement.\nC (S3 One Zone-IA): S3 One Zone-IA stores data in only one Availability Zone. While cost-effective, it\ncompromises durability. If the AZ is destroyed, data could be lost. This violates the requirement to maximize\nobject durability.\nD (S3 Intelligent-Tiering transitioning to S3 Standard-IA): While S3 Intelligent-Tiering automatically moves\ndata between frequent and infrequent access tiers based on usage patterns, using a lifecycle rule to\ntransition to S3 Standard-IA after 30 days provides a more predictable cost optimization for the specific\naccess pattern described (frequent access for 30 days, then infrequent access). Intelligent-Tiering is more\nbeneficial when access patterns are unknown or vary significantly. Using both Intelligent Tiering with a\nlifecycle rule transitioning to Standard-IA might introduce unnecessary complexity and potentially higher\ncosts if the 30-day access pattern holds true.\nTherefore, storing the objects in S3 Standard initially and then transitioning to S3 Standard-IA after 30 days\nusing a lifecycle rule strikes the best balance between durability, availability, accessibility, and cost-\neffectiveness for the given requirements.\nAuthoritative Links for Further Research:\nAmazon S3 Storage Classes: https://aws.amazon.com/s3/storage-classes/\nAmazon S3 Lifecycle: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-\nexamples.html",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-"
    ]
  },
  {
    "question": "CertyIQ\nA company has migrated a two-tier application from its on-premises data center to the AWS Cloud. The data tier is\na Multi-AZ deployment of Amazon RDS for Oracle with 12 TB of General Purpose SSD Amazon Elastic Block Store\n(Amazon EBS) storage. The application is designed to process and store documents in the database as binary large\nobjects (blobs) with an average document size of 6 M",
    "options": {
      "B": "In summary, moving the BLOBs to S3 frees up valuable database resources, lowers storage costs, improves"
    },
    "answer": "C",
    "explanation": "The most cost-effective and performant solution is to offload the BLOB storage to Amazon S3 (Option C).\nHere's why:\nCost: EBS storage, especially large volumes and Provisioned IOPS, is significantly more expensive than S3. S3\noffers cost-effective object storage, optimized for storing large binary files like documents.\nPerformance: Storing BLOBs directly in the database bloats its size, impacting query performance and overall\ndatabase responsiveness. Moving BLOBs to S3 reduces database size and improves performance by allowing\nit to focus on structured data.\nScalability and Durability: S3 provides virtually unlimited storage and high durability (11 9's), ensuring data\navailability and resilience.\nDatabase Optimization: The database can then focus on storing metadata (e.g., document name, S3 object\nkey) for each document, which is much smaller and faster to query.\nOther Options' Drawbacks:\nOptions A and B suggest increasing EBS storage, which is counterproductive given the cost and performance\nissues associated with storing BLOBs in the database. Magnetic storage (Option A) is also unsuitable for\nperformance-sensitive applications. DynamoDB (Option D) is a NoSQL database, which would require a\nsignificant application rewrite and data migration effort, adding complexity and cost, and is likely not\nnecessary for storing document data, whose metadata can easily be held within the relational RDS DB.\nIn summary, moving the BLOBs to S3 frees up valuable database resources, lowers storage costs, improves\napplication performance, and leverages S3's scalability and durability features, while storing metadata within\nthe existing database maintains the integrity and structure of the data.\nAuthoritative Links:\nAmazon S3: https://aws.amazon.com/s3/\nAmazon RDS: https://aws.amazon.com/rds/\nEBS Storage Types: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\nAmazon DynamoDB: https://aws.amazon.com/dynamodb/",
    "links": [
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/rds/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html",
      "https://aws.amazon.com/dynamodb/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an application that serves clients that are deployed in more than 20.000 retail storefront locations\naround the world. The application consists of backend web services that are exposed over HTTPS on port 443. The\napplication is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The retail locations\ncommunicate with the web application over the public internet. The company allows each retail location to register\nthe IP address that the retail location has been allocated by its local ISP.\nThe company's security team recommends to increase the security of the application endpoint by restricting\naccess to only the IP addresses registered by the retail locations.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "B": "Update the ingress"
    },
    "answer": "A",
    "explanation": "The correct answer is A because it leverages AWS WAF, a service specifically designed for protecting web\napplications from common web exploits. AWS WAF allows you to create rules, called web access control lists\n(web ACLs), to control which traffic is allowed or blocked based on criteria like IP addresses, HTTP headers,\nHTTP body, URI strings, SQL injection, and cross-site scripting. Associating a web ACL with the ALB enables\nfiltering traffic based on registered IP addresses. Implementing IP rule sets within AWS WAF is a direct and\nscalable method to allow only traffic originating from the registered retail location IP addresses.\nOption B is incorrect because AWS Firewall Manager is designed to centrally manage firewall rules across\nmultiple AWS accounts and resources, not to enforce IP address restrictions at the application level. While\nFirewall Manager can leverage WAF, the core functionality needed here is within WAF itself.\nOption C is less optimal. While Lambda authorization can achieve the desired outcome, it introduces\nunnecessary complexity and potential latency. Every request would trigger a Lambda function invocation,\nwhich can impact application performance and increase operational overhead. WAF offers a more efficient\nand purpose-built solution for IP address filtering.\nOption D is not recommended. Network ACLs operate at the subnet level and are stateless, meaning ingress\nand egress rules must be configured separately. Managing 20,000+ IP addresses in network ACL rules is\ncumbersome, error-prone, and can impact network performance due to the sheer number of rules.\nFurthermore, network ACLs have a limited number of rules per list, making this option infeasible. WAF\nprovides a much more manageable and scalable solution for filtering traffic at the application layer.\nTherefore, using AWS WAF with IP rule sets offers the most efficient, scalable, and secure method to restrict\naccess to the application endpoint based on the registered IP addresses of the retail locations.\nRelevant links for further research:\nAWS WAF: https://aws.amazon.com/waf/\nAWS Firewall Manager: https://aws.amazon.com/firewall-manager/\nAmazon DynamoDB: https://aws.amazon.com/dynamodb/\nAWS Lambda: https://aws.amazon.com/lambda/\nNetwork ACLs: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html",
    "links": [
      "https://aws.amazon.com/waf/",
      "https://aws.amazon.com/firewall-manager/",
      "https://aws.amazon.com/dynamodb/",
      "https://aws.amazon.com/lambda/",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is building a data analysis platform on AWS by using AWS Lake Formation. The platform will ingest\ndata from different sources such as Amazon S3 and Amazon RDS. The company needs a secure solution to prevent\naccess to portions of the data that contain sensitive information.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B: Create data filters to implement row-level security and cell-level security.\nHere's why: AWS Lake Formation allows you to define fine-grained access control policies directly on your\ndata lake. Data filters provide a mechanism to implement row-level and cell-level security without requiring\ncustom code or complex infrastructure.\nOption B leverages Lake Formation's built-in capabilities for security. Row-level security limits access to\nspecific rows based on defined criteria (e.g., only allowing certain users to see records for their region). Cell-\nlevel security restricts access to specific columns or attributes, masking sensitive data elements within a row.\nBoth are natively supported through Lake Formation's data filter feature. This means you don't have to build\nand maintain additional ETL pipelines or custom access control mechanisms.\nOption A, creating an IAM role with permissions to access Lake Formation tables, provides a basic level of\naccess control but doesn't offer the granularity needed to prevent access to specific rows or cells containing\nsensitive information. It's a necessary component for Lake Formation overall, but insufficient for the specific\nsecurity requirements of the question.\nOption C and D involve creating AWS Lambda functions to remove or modify sensitive data. These options\nintroduce significant operational overhead. Option C modifies the data before it's even stored, potentially\nimpacting downstream analysis. Option D requires periodic processing and introduces complexity in managing\nand scheduling these Lambda functions. They also require the creation and maintenance of custom code,\nincreasing the possibility of errors and vulnerabilities. These options add unnecessary complexity because\nLake Formation provides a native solution.\nTherefore, using Lake Formation's data filters for row-level and cell-level security is the most efficient and\nsecure method for restricting access to sensitive data within the data lake, minimizing operational overhead.\nFor further research, refer to the AWS documentation on Lake Formation data filtering:\nAWS Lake Formation Data Filtering: https://docs.aws.amazon.com/lake-formation/latest/dg/data-\nfiltering.html\nLake Formation Fine-Grained Access Control: https://aws.amazon.com/blogs/big-data/fine-grained-access-\ncontrol-using-aws-lake-formation/",
    "links": [
      "https://docs.aws.amazon.com/lake-formation/latest/dg/data-",
      "https://aws.amazon.com/blogs/big-data/fine-grained-access-"
    ]
  },
  {
    "question": "CertyIQ\nA company deploys Amazon EC2 instances that run in a VP",
    "options": {
      "C": "This allows the servers in the on-premises data center to securely",
      "B": "EC2 itself doesn't require a VPC endpoint for its normal functioning within a VPC. An"
    },
    "answer": "B",
    "explanation": "The correct answer is B because it provides a solution that ensures data transfer between EC2 instances and\nS3 buckets, and between the VPC and the on-premises network, without traversing the public internet.\nA gateway VPC endpoint for S3 creates a direct, private connection to S3 from within the VPC. It avoids the\nneed to route traffic through the internet gateway, NAT gateway, or VPC peering connections to access S3.\nThis directly addresses the compliance requirement of avoiding public internet transmission for S3 data\naccess. Gateway endpoints are designed specifically for S3 and DynamoDB.\nAn AWS Direct Connect connection establishes a private network connection between the company's on-\npremises data center and the VPC. This allows the servers in the on-premises data center to securely\nconsume the output from the EC2 instances without sending data over the internet. Direct Connect offers\nconsistent, low-latency connectivity compared to internet-based connections.\nOption A is incorrect because interface VPC endpoints are used to privately connect to AWS services other\nthan S3 and DynamoDB. EC2 itself doesn't require a VPC endpoint for its normal functioning within a VPC. An\nAWS Site-to-Site VPN connection addresses the on-premises connectivity but not the S3 traffic routing\nrequirements.\nOption C is incorrect because Transit Gateway is used to connect multiple VPCs and on-premises networks,\nbut does not provide a private connection to S3. While a VPN addresses on-premises connectivity, it doesn't\nisolate S3 traffic from the internet.\nOption D is overly complex and inefficient. Using proxy EC2 instances with NAT gateways still involves traffic\npotentially traversing public internet infrastructure at some point. It also adds unnecessary overhead and cost\nto the architecture. Using gateway VPC endpoints is the recommended, more performant and cost-effective\napproach for private S3 access.\nRelevant Links:\nAWS Direct Connect: https://aws.amazon.com/directconnect/\nVPC Endpoints: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
    "links": [
      "https://aws.amazon.com/directconnect/",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has an application with a REST-based interface that allows data to be received in near-real time from a\nthird-party vendor. Once received, the application processes and stores the data for further analysis. The\napplication is running on Amazon EC2 instances.\nThe third-party vendor has received many 503 Service Unavailable Errors when sending data to the application.\nWhen the data volume spikes, the compute capacity reaches its maximum limit and the application is unable to\nprocess all requests.\nWhich design should a solutions architect recommend to provide a more scalable solution?",
    "options": {},
    "answer": "A",
    "explanation": "The recommended solution is A: Use Amazon Kinesis Data Streams to ingest the data. Process the data\nusing AWS Lambda functions.\nHere's why:\nScalability and Decoupling: Kinesis Data Streams provides a highly scalable and durable data ingestion\nservice. It can handle large volumes of data in real-time and decouples the data source (third-party vendor)\nfrom the processing application. This decoupling is crucial because it allows the application to process data at\nits own pace, preventing overload. https://aws.amazon.com/kinesis/data-streams/\nElastic Processing: AWS Lambda functions offer a serverless compute environment that can automatically\nscale in response to the incoming data stream from Kinesis. Each Lambda function can process individual\nrecords from the stream, enabling parallel processing and efficient utilization of resources. This eliminates\nthe bottleneck caused by the EC2 instances reaching their maximum capacity.\nhttps://aws.amazon.com/lambda/\nReal-time Processing: Kinesis Data Streams and Lambda enable near-real-time processing of data, meeting\nthe application's requirements.\nCost-Effectiveness: Lambda's pay-per-execution model can be more cost-effective than running EC2\ninstances continuously, especially during periods of low data volume.\nHere's why the other options are less suitable:\nB: Amazon API Gateway with usage plans: While API Gateway can provide rate limiting, it doesn't address the\nunderlying issue of limited compute capacity on the EC2 instances. It might reduce the frequency of 503\nerrors but won't enable the application to handle high data volumes.\nC: Amazon SNS and Auto Scaling group behind an Application Load Balancer: SNS is a publish/subscribe\nmessaging service and not ideal for ingesting a continuous data stream. While Auto Scaling would help with\nhorizontal scaling of EC2 instances, it doesn't offer the same level of scalability and elasticity as Kinesis and\nLambda. Additionally, managing the Auto Scaling group adds operational overhead. SNS has a maximum\nmessage size limitation as well, which makes it less suitable for large data payloads.\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-limits.html\nD: Amazon ECS with Auto Scaling: ECS provides container orchestration, which can improve resource\nutilization and deployment. However, it still relies on underlying compute resources (EC2 instances), and the\nscaling may not be as responsive or cost-effective as Lambda for handling unpredictable data spikes. It\nrequires more operational overhead for maintaining the ECS cluster.\nIn summary, Kinesis Data Streams and Lambda provide a scalable, cost-effective, and near-real-time solution\nfor ingesting and processing data from the third-party vendor, addressing the issue of 503 errors and\ncompute capacity limitations.",
    "links": [
      "https://aws.amazon.com/kinesis/data-streams/",
      "https://aws.amazon.com/lambda/",
      "https://docs.aws.amazon.com/sns/latest/dg/sns-limits.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has an application that runs on Amazon EC2 instances in a private subnet. The application needs to\nprocess sensitive information from an Amazon S3 bucket. The application must not use the internet to connect to\nthe S3 bucket.\nWhich solution will meet these requirements?",
    "options": {
      "C": "It's an unnecessary complexity for internal VPC"
    },
    "answer": "D",
    "explanation": "The correct answer is D: Configure a VPC endpoint. Update the S3 bucket policy to allow access from the VPC\nendpoint. Update the application to use the new VPC endpoint.\nHere's why:\nVPC endpoints provide private connectivity between your VPC and supported AWS services, including S3,\nwithout requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. This\nmeans traffic between your EC2 instances in the private subnet and the S3 bucket stays within the AWS\nnetwork, enhancing security and reducing latency.\nOption A is incorrect because an internet gateway provides access to the internet, which violates the\nrequirement of not using the internet to connect to S3.\nOption B is incorrect because while a VPN connection does provide a secure connection, it is primarily used to\nconnect your on-premises network to your VPC. It's an unnecessary complexity for internal VPC\ncommunication.\nOption C is incorrect because a NAT gateway allows instances in a private subnet to connect to the internet\n(e.g., for software updates), but it doesn't provide private connectivity to S3. It also utilizes the internet, which\nagain goes against the problem's requirements.\nOption D, by using a VPC endpoint, specifically a Gateway Endpoint for S3, provides a secure, private\nconnection between the EC2 instances in the private subnet and the S3 bucket. The S3 bucket policy can be\nupdated to only allow access from that specific VPC endpoint, further enhancing security by restricting\naccess to only the EC2 instances. The application is configured to use the VPC endpoint, ensuring traffic flows\nprivately within AWS.\nGateway endpoints are a cost-effective solution when compared to VPNs and do not introduce extra hops like\nNAT gateways. They are specifically designed for this kind of direct S3 access.\nFor further reading:\nAWS VPC Endpoints: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\nGateway VPC Endpoints for S3: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html"
    ]
  },
  {
    "question": "CertyIQ\nA company uses Amazon Elastic Kubernetes Service (Amazon EKS) to run a container application. The EKS cluster\nstores sensitive information in the Kubernetes secrets object. The company wants to ensure that the information is\nencrypted.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "B": "Enable secrets encryption in the EKS cluster by using AWS Key Management Service"
    },
    "answer": "B",
    "explanation": "The best solution is B. Enable secrets encryption in the EKS cluster by using AWS Key Management Service\n(AWS KMS).\nHere's why:\nNative Integration: EKS natively supports secrets encryption at rest using KMS. This is the most\nstraightforward and least operationally heavy approach because it leverages built-in functionality. No custom\ncode or external services are required.\nCentralized Key Management: KMS provides a centralized and secure way to manage the encryption keys.\nAWS handles the key lifecycle, rotation, and access control.\nEase of Implementation: Enabling secrets encryption in EKS involves configuring the control plane to use a\nKMS key. Once enabled, all newly created or updated secrets are automatically encrypted.\nReduced Development Effort: Option A requires the container application to handle encryption, increasing\nthe complexity of the application code and potentially exposing the key within the application. Option C\nintroduces a Lambda function which creates an additional component to manage and monitor.\nSecrets Management: While Systems Manager Parameter Store (Option D) can store sensitive information,\nit's not designed to replace Kubernetes secrets for application deployment within an EKS cluster. Using\nParameter Store would require additional logic within the application to fetch and manage the secrets.\nTherefore, enabling secrets encryption in EKS using KMS directly addresses the requirement with the least\noperational overhead by utilizing native functionality and centralized key management.\nAuthoritative Links:\nEncrypting Secrets at Rest\nAWS Key Management Service (KMS)",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is designing a new multi-tier web application that consists of the following components:\nWeb and application servers that run on Amazon EC2 instances as part of Auto Scaling groups\nAn Amazon RDS DB instance for data storage\nA solutions architect needs to limit access to the application servers so that only the web servers can access them.\nWhich solution will meet these requirements?",
    "options": {
      "C": "Using network ACLs for this level"
    },
    "answer": "D",
    "explanation": "The correct solution is D: Deploy an Application Load Balancer (ALB) with a target group containing the\napplication servers' Auto Scaling group, and configure the security group to allow only the web servers to\naccess the application servers. Here's why:\nALB for Application Tier Traffic: ALBs are designed for HTTP/HTTPS traffic, typical for communication\nbetween web servers and application servers in a multi-tier architecture. They provide advanced routing\ncapabilities based on content, hostnames, or other request attributes. This makes them suitable for directing\ntraffic from the web tier to the application tier.\nTarget Group for Dynamic Scaling: Using a target group linked to the application servers' Auto Scaling group\nallows the ALB to automatically adapt as the number of application servers scales up or down. The ALB\ndynamically registers and deregisters instances as they are launched or terminated by Auto Scaling, ensuring\nhigh availability and scalability.\nSecurity Group for Fine-Grained Access Control: Security groups act as virtual firewalls at the instance level.\nBy configuring the security group of the application servers to allow traffic only from the web servers'\nsecurity group, we can effectively limit access to the application tier. This ensures that only authorized web\nservers can communicate with the application servers, enhancing security and preventing unauthorized\naccess.\nWhy not A: AWS PrivateLink is used to privately connect your VPC to supported AWS services, services\nhosted by other AWS accounts (called endpoint services), and supported AWS Marketplace partner services.\nIt's overkill for internal communication between tiers within the same VPC. Using network ACLs for this level\nof granularity can become complex and harder to manage than security groups.\nWhy not B: VPC endpoints are used to connect to AWS services privately without traversing the public\ninternet. Similar to PrivateLink, it's designed for external AWS service communication, not internal tier\ncommunication. Security groups are again, more effective for instance-level access control in this scenario.\nWhy not C: While Network Load Balancers (NLBs) provide high throughput and low latency, they operate at\nthe transport layer (TCP/UDP). They lack the application-level awareness of ALBs and are less suited for\nHTTP/HTTPS traffic routing based on request content. Also, security groups are easier to manage for this\nlevel of security than network ACLs.\nIn summary, using an ALB for the application tier along with a well-defined security group rule achieves the\ngoal of restricting access to the application servers specifically to the web servers in a scalable and\nmanageable way.\nAuthoritative Links:\nApplication Load Balancer\nSecurity Groups\nTarget Groups for your Application Load Balancers",
    "links": []
  },
  {
    "question": "CertyIQ\nA company runs a critical, customer-facing application on Amazon Elastic Kubernetes Service (Amazon EKS). The\napplication has a microservices architecture. The company needs to implement a solution that collects,\naggregates, and summarizes metrics and logs from the application in a centralized location.\nWhich solution meets these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D because it directly addresses the need for centralized metrics and log aggregation\nfrom an EKS cluster running a microservices application. Amazon CloudWatch Container Insights is\nspecifically designed for monitoring containerized applications, including those running on EKS. It\nautomatically discovers container environments and collects performance data such as CPU, memory,\nnetwork, and disk utilization, as well as application logs. These metrics and logs are then readily available in\nthe CloudWatch console, providing a single pane of glass for monitoring the entire application.\nOption A is partially correct, as the CloudWatch agent can collect logs and some metrics. However, it requires\nmore manual configuration and doesn't offer the same level of pre-built insights and dashboards tailored for\ncontainer environments as Container Insights.\nOption B, AWS App Mesh, is a service mesh that focuses on managing and securing communication between\nmicroservices. While App Mesh provides metrics related to service-to-service communication (e.g., latency,\nrequest rates, error rates), it doesn't directly address the broader requirement of collecting all metrics and\nlogs from the entire application across the EKS cluster. Additionally, viewing logs and metrics primarily\nfocuses on inter-service communication, not the entire cluster.\nOption C, using AWS CloudTrail, is designed for auditing API calls made to AWS services. While CloudTrail\ncaptures data events, it doesn't collect the granular application metrics and logs needed for performance\nmonitoring within a Kubernetes cluster. Also, querying CloudTrail logs in OpenSearch is not a suitable\nreplacement for metrics dashboards.\nTherefore, CloudWatch Container Insights provides the most comprehensive and efficient solution for\ncollecting, aggregating, and summarizing metrics and logs from an EKS application in a centralized location. It\nis designed to automatically discover and monitor containerized environments.\nReferences:\nAmazon CloudWatch Container Insights\nMonitoring Amazon EKS with CloudWatch Container Insights",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has deployed its newest product on AWS. The product runs in an Auto Scaling group behind a Network\nLoad Balancer. The company stores the products objects in an Amazon S3 bucket.\nThe company recently experienced malicious attacks against its systems. The company needs a solution that\ncontinuously monitors for malicious activity in the AWS account, workloads, and access patterns to the S3 bucket.\nThe solution must also report suspicious activity and display the information on a dashboard.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The correct solution is C: Configure Amazon GuardDuty to monitor and report findings to AWS Security Hub.\nHere's why:\nAmazon GuardDuty is a threat detection service that continuously monitors for malicious activity and\nunauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. It analyzes\nVPC Flow Logs, CloudTrail logs, and DNS logs to identify threats. This addresses the requirement of\ncontinuously monitoring for malicious activity within the AWS account, workloads, and S3 bucket access\npatterns.\nAWS Security Hub is a security service that provides a comprehensive view of your security state in AWS. It\ncollects security findings from various AWS security services like GuardDuty, Inspector, and Macie, as well as\nfrom integrated third-party partner products. It then correlates and prioritizes these findings to help you\nidentify the most important security issues. Security Hub provides a central dashboard to view and manage\nthese findings. This satisfies the need to report suspicious activity and display it on a dashboard.\nOption A is incorrect because while Amazon Macie can identify sensitive data stored in S3, it doesn't provide\ncomprehensive threat detection across the entire AWS environment like GuardDuty. Also, AWS Config is\nprimarily a configuration management service and doesn't provide threat detection capabilities.\nOption B is incorrect because Amazon Inspector focuses on identifying software vulnerabilities and\nunintended network exposure within EC2 instances and container images. While valuable, it doesn't monitor\naccess patterns to S3 buckets or provide a central dashboard. CloudTrail logs API calls, but is not designed to\nautomatically generate security findings or provide a dashboard like Security Hub.\nOption D is incorrect because AWS Config monitors the configuration of your AWS resources, and Amazon\nEventBridge is an event bus service. While Config can detect configuration changes that might indicate a\nsecurity issue, it doesn't have built-in threat detection capabilities. EventBridge can trigger actions based on\nevents, but it doesn't provide the threat intelligence needed for identifying malicious activity.\nIn summary, GuardDuty provides the threat detection and monitoring capabilities, and Security Hub provides\nthe central dashboard for reporting and managing security findings, making option C the best solution.\nRelevant Documentation:\nAmazon GuardDuty: https://aws.amazon.com/guardduty/\nAWS Security Hub: https://aws.amazon.com/security-hub/",
    "links": [
      "https://aws.amazon.com/guardduty/",
      "https://aws.amazon.com/security-hub/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to migrate an on-premises data center to AWS. The data center hosts a storage server that\nstores data in an NFS-based file system. The storage server holds 200 GB of data. The company needs to migrate\nthe data without interruption to existing services. Multiple resources in AWS must be able to access the data by\nusing the NFS protocol.\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": {
      "B": "Create an Amazon Elastic File System (Amazon EFS) file system: Amazon EFS is a fully managed,",
      "A": "Create an Amazon FSx for Lustre file system: While FSx for Lustre is a high-performance file system, it's",
      "C": "Create an Amazon S3 bucket to receive the data: S3 is object storage, not a file system. While data could",
      "D": "Manually use an operating system copy command to push the data into the AWS destination: This is not"
    },
    "answer": "B",
    "explanation": "The correct answer is BE. Here's why:\nB. Create an Amazon Elastic File System (Amazon EFS) file system: Amazon EFS is a fully managed,\nscalable, elastic, and NFS file system. It's designed for use with AWS Cloud services and on-premises\nresources. Given the requirement that multiple AWS resources need to access the data via the NFS protocol,\nEFS is a natural fit. It supports concurrent access from multiple EC2 instances, containers, and serverless\nfunctions.\nE. Install an AWS DataSync agent in the on-premises data center. Use a DataSync task between the on-\npremises location and AWS: AWS DataSync is a data transfer service that simplifies, automates, and\naccelerates moving data between on-premises storage and AWS storage services. It's designed to handle\nactive datasets and provides features such as incremental transfers and encryption. Deploying a DataSync\nagent on-premises allows for efficient and secure data transfer from the existing NFS server to the EFS file\nsystem in AWS, without interrupting existing services because DataSync handles the data movement in the\nbackground.\nHere's why the other options are not the best choice:\nA. Create an Amazon FSx for Lustre file system: While FSx for Lustre is a high-performance file system, it's\ntypically used for compute-intensive workloads such as machine learning, high-performance computing\n(HPC), and video processing. It's not the most cost-effective option for general-purpose file storage and\nsharing via NFS.\nC. Create an Amazon S3 bucket to receive the data: S3 is object storage, not a file system. While data could\ntechnically be moved to S3, it wouldn't natively support the NFS protocol requirement. Directly copying the\ndata into S3 would mean a change in accessing protocol to S3 API calls, which is not in line with original\nrequirement.\nD. Manually use an operating system copy command to push the data into the AWS destination: This is not\npractical or efficient for a 200GB dataset, especially with the requirement for zero downtime. It lacks built-in\nfeatures for data integrity, error handling, and automated synchronization.\nAuthoritative Links:\nAmazon EFS: https://aws.amazon.com/efs/\nAWS DataSync: https://aws.amazon.com/datasync/",
    "links": [
      "https://aws.amazon.com/efs/",
      "https://aws.amazon.com/datasync/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to use Amazon FSx for Windows File Server for its Amazon EC2 instances that have an SMB file\nshare mounted as a volume in the us-east-1 Region. The company has a recovery point objective (RPO) of 5 minutes\nfor planned system maintenance or unplanned service disruptions. The company needs to replicate the file system\nto the us-west-2 Region. The replicated data must not be deleted by any user for 5 years.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The correct solution is option C because it addresses the RPO, cross-region replication, and data retention\nrequirements. Here's a breakdown:\n1. Multi-AZ Deployment: A Multi-AZ deployment type is essential for meeting the 5-minute RPO\nrequirement. Multi-AZ provides high availability, automatically failing over to a standby file server in\ncase of a failure in the primary Availability Zone. Single-AZ deployments do not offer this level of\nfault tolerance. https://docs.aws.amazon.com/fsx/latest/WindowsGuide/high-availability-multi-\nAZ.html\n2. AWS Backup for Cross-Region Replication: AWS Backup is used to create a daily backup of the file\nsystem in us-east-1 and copy it to us-west-2. This meets the requirement of replicating the file\nsystem to another region for disaster recovery purposes. AWS Backup enables scheduled backups\nand retention policies, vital for data protection and compliance. https://aws.amazon.com/backup/\n3. AWS Backup Vault Lock in Compliance Mode: To ensure the replicated data cannot be deleted by\nany user for 5 years, AWS Backup Vault Lock in compliance mode is used. Compliance mode prevents\nany modifications to the retention policy once it's configured, ensuring immutability. Governance\nmode allows some modifications under specific conditions, which doesn't meet the strict requirement\nof no deletion. https://docs.aws.amazon.com/aws-backup/latest/devguide/vault-lock.html\n4. Retention Period: Configuring a minimum duration of 5 years in the Vault Lock ensures that the\nbackups cannot be deleted before the specified period, satisfying the data retention policy.\nOptions A and D are incorrect because they use Single-AZ deployment, which doesn't meet the high\navailability requirements for the given RPO. Option B uses governance mode for Vault Lock, which doesn't\nguarantee the data will not be deleted. Therefore, option C is the only option that meets all requirements.",
    "links": [
      "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/high-availability-multi-",
      "https://aws.amazon.com/backup/",
      "https://docs.aws.amazon.com/aws-backup/latest/devguide/vault-lock.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is designing a security solution for a company that wants to provide developers with\nindividual AWS accounts through AWS Organizations, while also maintaining standard security controls. Because\nthe individual developers will have AWS account root user-level access to their own accounts, the solutions\narchitect wants to ensure that the mandatory AWS CloudTrail configuration that is applied to new developer\naccounts is not modified.\nWhich action meets these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C because it utilizes Service Control Policies (SCPs) to centrally enforce mandatory\nconfigurations across AWS Organizations accounts, addressing the specific requirement of preventing\nmodifications to the CloudTrail configuration in developer accounts.\nHere's a breakdown of the justification:\nService Control Policies (SCPs): SCPs are a feature of AWS Organizations that allow administrators to define\nguardrails and controls over the AWS services and actions that users can perform within member accounts (in\nthis case, the developer accounts). These policies are applied at the organizational unit (OU) or account level,\neffectively setting boundaries for permitted actions.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html\nPreventing CloudTrail Modification: By creating an SCP that explicitly denies actions related to modifying or\ndeleting CloudTrail configurations (e.g., cloudtrail:UpdateTrail, cloudtrail:DeleteTrail), the solutions architect can\nensure that even with root user access in the developer accounts, developers cannot alter the mandatory\nCloudTrail setup defined at the organization level.\nCentralized Enforcement: SCPs are managed from the AWS Organizations management account, providing a\nsingle point of control for enforcing security policies across all member accounts. This ensures consistency\nand simplifies governance.\nRoot User Limitation: While developers have root user access in their individual accounts, SCPs override\nthese permissions. The SCP acts as a \"guardrail,\" preventing actions regardless of the user's IAM permissions\nwithin the account. This addresses the core requirement of securing the CloudTrail configuration despite the\ndevelopers having root user privileges.\nWhy other options are incorrect:\nA: Applying an IAM policy to the root user within each developer account is not scalable and would require\nmanual configuration in each account. Furthermore, if a developer with root user access compromises the\naccount, they could potentially modify or delete the policy itself.\nB: Creating a new trail from the developer account does not meet the request. If the initial CloudTrail is\nchanged and the only record is the additional trail, this becomes a misconfiguration.\nD: Service-linked roles are used for services to access other services and do not restrict permissions for users\nwithin the developer accounts. It will also not prevent developers from modifying or deleting CloudTrail\nconfigurations.\nIn summary, using SCPs is the most effective and scalable way to enforce mandatory CloudTrail\nconfigurations across multiple AWS accounts in an organization, ensuring that developers cannot bypass\ncentrally defined security controls even with root user access to their individual accounts.",
    "links": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is planning to deploy a business-critical application in the AWS Cloud. The application requires durable\nstorage with consistent, low-latency performance.\nWhich type of storage should a solutions architect recommend to meet these requirements?",
    "options": {
      "A": "Instance store volume: Instance store provides very high performance but is ephemeral. Data is lost if the",
      "B": "Amazon ElastiCache for Memcached cluster: ElastiCache is a caching service. While it offers low latency,",
      "C": "Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume: Provisioned IOPS (io1 and io2)",
      "D": "Throughput Optimized HDD Amazon Elastic Block Store (Amazon EBS) volume: Throughput Optimized"
    },
    "answer": "C",
    "explanation": "The correct answer is C, Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume. Here's why:\nThe application requires durable storage, consistent, and low-latency performance. Let's evaluate each\noption:\nA. Instance store volume: Instance store provides very high performance but is ephemeral. Data is lost if the\ninstance fails or is stopped. This violates the durability requirement.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\nB. Amazon ElastiCache for Memcached cluster: ElastiCache is a caching service. While it offers low latency,\nit is primarily for caching frequently accessed data and is not designed for durable storage. The data within\nMemcached instances is not persistent across failures. It's not suitable as the primary storage for a business-\ncritical application. https://aws.amazon.com/elasticache/\nC. Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume: Provisioned IOPS (io1 and io2)\nEBS volumes are designed for I/O-intensive workloads and offer consistent, low-latency performance by\nallowing you to specify the number of IOPS (Input/Output Operations Per Second) that the volume can\nsupport. EBS provides durable, block-level storage that is independent of the EC2 instance lifecycle. This\nensures data persistence and meets the requirements. The io2 Block Express volumes provide the highest\nperformance. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\nD. Throughput Optimized HDD Amazon Elastic Block Store (Amazon EBS) volume: Throughput Optimized\nHDD (st1) volumes are designed for frequently accessed, throughput-intensive workloads, such as big data,\ndata warehouses, and log processing. While they are cost-effective, they don't offer the consistent, low-\nlatency performance required for a business-critical application. HDD drives have slower seek times\ncompared to SSDs.\nTherefore, only Provisioned IOPS SSD EBS volumes provide the necessary durability, consistency, and low\nlatency for the application.",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html",
      "https://aws.amazon.com/elasticache/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html"
    ]
  },
  {
    "question": "CertyIQ\nAn online photo-sharing company stores its photos in an Amazon S3 bucket that exists in the us-west-1 Region.\nThe company needs to store a copy of all new photos in the us-east-1 Region.\nWhich solution will meet this requirement with the LEAST operational effort?",
    "options": {
      "A": "Create a second S3 bucket in us-east-1. Use S3 Cross-Region Replication to copy"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Create a second S3 bucket in us-east-1. Use S3 Cross-Region Replication to copy\nphotos from the existing S3 bucket to the second S3 bucket.\nHere's why:\nS3 Cross-Region Replication (CRR) is specifically designed to automatically and asynchronously replicate\nobjects between S3 buckets in different AWS Regions. It's the most straightforward and efficient way to\nachieve the stated requirement of storing copies of new photos in a different Region with minimal operational\noverhead. CRR handles the replication process seamlessly, ensuring data consistency and minimizing manual\nintervention.\nOption B is incorrect because CORS (Cross-Origin Resource Sharing) is a browser security mechanism that\nallows web pages from one domain to access resources from a different domain. It doesn't facilitate S3 object\nreplication across regions.\nOption C is incorrect because S3 Lifecycle rules are primarily for managing object storage lifecycle, such as\ntransitioning objects to different storage classes (e.g., Standard to Glacier) or deleting them after a certain\nperiod. While lifecycle rules can move data within a region, they are not the primary method for replicating\ndata between regions.\nOption D is incorrect because while Lambda could be used to copy the objects, it involves significantly more\noperational effort than CRR. It requires writing, deploying, and maintaining Lambda function code, configuring\nS3 event notifications, and managing potential scaling and error handling issues. This approach introduces\nunnecessary complexity compared to the built-in functionality of CRR. CRR handles the replication in a\nmanaged way.In terms of 'least operational effort', CRR requires the least hands-on management because the\nreplication is automatically managed by AWS once configured.\nFor further research, refer to the AWS documentation on S3 Cross-Region Replication:\nAmazon S3 Cross-Region Replication",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is creating a new web application for its subscribers. The application will consist of a static single page\nand a persistent database layer. The application will have millions of users for 4 hours in the morning, but the\napplication will have only a few thousand users during the rest of the day. The company's data architects have\nrequested the ability to rapidly evolve their schema.\nWhich solutions will meet these requirements and provide the MOST scalability? (Choose two.)",
    "options": {
      "D": "Here's why:",
      "B": "E: Deploy the web servers for static content across a fleet of Amazon EC2 instances in Auto Scaling groups."
    },
    "answer": "C",
    "explanation": "The correct answer is CD. Here's why:\nOption C: Deploy Amazon DynamoDB as the database solution. Ensure that DynamoDB auto scaling is\nenabled.\nDynamoDB is a fully managed NoSQL database service. Its key advantage in this scenario is its ability to scale\nautomatically based on demand. Given the huge spike in users during the morning hours and a significant\ndrop-off for the rest of the day, DynamoDB auto-scaling is crucial for handling the fluctuating workload\nefficiently and cost-effectively. DynamoDB's NoSQL nature aligns with the architects' need to rapidly evolve\nthe schema as it provides more flexibility than a relational database.https://aws.amazon.com/dynamodb/\nOption D: Deploy the static content into an Amazon S3 bucket. Provision an Amazon CloudFront\ndistribution with the S3 bucket as the origin.\nS3 is highly scalable and provides a durable storage for static content. By pairing it with CloudFront, a content\ndelivery network (CDN), the application can effectively serve millions of users globally. CloudFront caches\ncontent at edge locations, reducing latency and offloading the traffic from S3. Since the application will\nconsist of a static single page, serving it directly from S3 through CloudFront is the most cost-effective and\nscalable solution.https://aws.amazon.com/s3/https://aws.amazon.com/cloudfront/\nWhy other options are not optimal:\nA: Deploy Amazon DynamoDB as the database solution. Provision on-demand capacity. While DynamoDB is\na good choice, provisioning on-demand capacity requires estimating the peak load and pre-allocating\nresources. This approach is less flexible and can be more expensive than auto-scaling, especially with the\ndrastically changing user load. It doesn't adapt dynamically to usage changes as well as autoscaling.\nB: Deploy Amazon Aurora as the database solution. Choose the serverless DB engine mode. Aurora\nServerless could be a good choice, but it may not be the best option if rapid schema evolution is required.\nRelational databases generally involve more rigid schemas and schema migrations than NoSQL databases\nlike DynamoDB.\nE: Deploy the web servers for static content across a fleet of Amazon EC2 instances in Auto Scaling groups.\nConfigure the instances to periodically refresh the content from an Amazon Elastic File System (Amazon\nEFS) volume. Deploying a fleet of EC2 instances to serve static content is overkill and more complex. Using\nS3 and CloudFront is much simpler, more scalable, and cheaper for this purpose. EFS is designed for shared\nfile storage, not for serving static web content to a large number of users. EC2 is best suited for compute-\nintensive tasks or running dynamic applications, not for directly serving static content.",
    "links": [
      "https://aws.amazon.com/dynamodb/",
      "https://aws.amazon.com/s3/https://aws.amazon.com/cloudfront/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses Amazon API Gateway to manage its REST APIs that third-party service providers access. The\ncompany must protect the REST APIs from SQL injection and cross-site scripting attacks.\nWhat is the MOST operationally efficient solution that meets these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The most operationally efficient solution to protect REST APIs managed by Amazon API Gateway from SQL\ninjection and cross-site scripting (XSS) attacks is to configure AWS WAF (Web Application Firewall).\nHere's why:\nAWS WAF's Core Functionality: AWS WAF is specifically designed to protect web applications from common\nweb exploits like SQL injection and XSS. It allows you to define customizable rules to filter malicious traffic\nbased on patterns in HTTP requests.\nDirect Integration with API Gateway: AWS WAF can be directly associated with API Gateway, providing a\nseamless and efficient way to protect APIs. This direct integration minimizes operational overhead.\nPre-configured Rules: AWS WAF offers pre-configured rule sets (managed rules) specifically designed to\nmitigate common web vulnerabilities, including those associated with SQL injection and XSS. Using managed\nrules simplifies deployment and maintenance.\nOperational Efficiency: Using AWS WAF directly on API Gateway avoids the added complexity and\noperational overhead of implementing a CDN like CloudFront solely for WAF integration. The other solutions\nare not as operationally efficient.\nAWS Shield's Limitations: AWS Shield primarily protects against DDoS attacks, not SQL injection or XSS.\nWhile Shield is valuable for availability, it doesn't address the stated security requirements.\nCloudFront and WAF Combination (Suboptimal): While combining CloudFront with AWS WAF is a valid\napproach for protecting web applications, it's less operationally efficient for API Gateway protection\ncompared to directly associating WAF with API Gateway. The CloudFront layer introduces additional\ncomplexity, and it's not strictly necessary when the primary goal is to protect the API layer.\nTherefore, AWS WAF provides the most operationally efficient solution, by allowing you to define rules or\nleverage managed rules to filter traffic and protect against SQL injection and XSS attacks directly at the API\nGateway level.\nSupporting Links:\nAWS WAF: https://aws.amazon.com/waf/\nAWS WAF and API Gateway: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-\ncontrol-access-aws-waf.html\nAWS Shield: https://aws.amazon.com/shield/",
    "links": [
      "https://aws.amazon.com/waf/",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-",
      "https://aws.amazon.com/shield/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to provide users with access to AWS resources. The company has 1,500 users and manages their\naccess to on-premises resources through Active Directory user groups on the corporate network. However, the\ncompany does not want users to have to maintain another identity to access the resources. A solutions architect\nmust manage user access to the AWS resources while preserving access to the on-premises resources.\nWhat should the solutions architect do to meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the best solution:\nThe scenario describes a common use case: integrating on-premises Active Directory (AD) with AWS for\nseamless user authentication and authorization. The core requirement is to allow existing users to access\nAWS resources without creating separate AWS identities, leveraging their existing AD credentials.\nOption D, using SAML 2.0-based federation, directly addresses this need. SAML (Security Assertion Markup\nLanguage) is an open standard for exchanging authentication and authorization data between security\ndomains, namely an Identity Provider (IdP) and a Service Provider (SP). In this case, Active Directory acts as\nthe IdP, and AWS acts as the SP.\nHere's how it works:\n1. A user attempts to access an AWS resource.\n2. AWS redirects the user to the AD Federation Services (ADFS) server, which acts as the SAML IdP.\n3. The user authenticates with their existing AD credentials.\n4. ADFS creates a SAML assertion containing information about the user's identity and group\nmemberships.\n5. ADFS sends the SAML assertion to AWS.\n6. AWS uses the information in the SAML assertion to determine which IAM role the user should\nassume. You configure this mapping in IAM.\n7. AWS grants the user temporary credentials based on the permissions defined in the assumed role.\n8. The user can now access the AWS resource with the permissions granted by the role.\nBy mapping AD groups to IAM roles, you can control access to AWS resources based on the user's existing\ngroup memberships in Active Directory. This approach avoids creating and managing individual IAM users for\neach of the 1,500 users, simplifying administration and maintaining consistency with on-premises access\ncontrols. SAML federation provides a single sign-on (SSO) experience, enhancing user convenience.\nThe other options are less suitable:\nOption A, creating individual IAM users, is unmanageable for a large number of users (1,500) and defeats the\npurpose of leveraging the existing Active Directory.\nOption B, using Amazon Cognito with an Active Directory user pool, is primarily designed for authenticating\napplication users and doesn't directly integrate with IAM roles for resource access in the same way SAML\ndoes. Cognito does not directly map AD groups to IAM roles. While cognito can authenticate against AD, it is\nnot the most straightforward solution to the described problem.\nOption C, defining cross-account roles, is mainly used for granting access to resources in other AWS\naccounts, not for federating with an external identity provider like Active Directory. This option also doesn't\nsolve the problem of linking AD identities to AWS access.\nAuthoritative links:\nAWS Documentation on SAML\nAWS Whitepaper on Federated Authentication with AWS",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is hosting a website behind multiple Application Load Balancers. The company has different\ndistribution rights for its content around the world. A solutions architect needs to ensure that users are served the\ncorrect content without violating distribution rights.\nWhich configuration should the solutions architect choose to meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C: Configure Amazon Route 53 with a geolocation policy.\nGeolocation routing in Amazon Route 53 allows you to route traffic to different resources based on the\ngeographic location of your users. This is perfectly suited for scenarios where content distribution rights vary\nby region. By configuring Route 53 to direct users from specific countries or regions to the appropriate\nApplication Load Balancer and origin server holding the content licensed for that area, the company can\nensure compliance with distribution rights. Each record in Route 53 would specify a geographic location (e.g.,\na country) and the corresponding Application Load Balancer endpoint.\nOption A, using CloudFront with AWS WAF, primarily addresses security concerns and content caching for\nperformance improvements but doesn't directly facilitate geographic content distribution. While CloudFront\nsupports geo-restriction, it mainly blocks access rather than routing to different content origins based on\nlocation.\nOption B, using Application Load Balancers with AWS WAF, focuses on securing the web application layer and\ndoesn't inherently offer geolocation-based routing capabilities. While WAF can identify the origin of a request\nthrough IP addresses, it cannot dynamically route traffic to different servers based on that information.\nOption D, using Route 53 with geoproximity routing, is designed to route traffic based on the physical\nproximity of users to your resources. While it uses geographical information, it's primarily for optimizing\nlatency and not for complying with content distribution rights as precisely as geolocation routing.\nGeoproximity considers distances between resources and users, making it less suitable when adherence to\ndefined regional boundaries for legal reasons is paramount. Geolocation offers precise control over region-\nbased content delivery.\nFor further research, refer to the following resources:\nAmazon Route 53 Geolocation Routing:\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo\nAWS WAF: https://aws.amazon.com/waf/\nAmazon CloudFront Geo Restriction:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html\nAmazon Route 53 Geoproximity Routing:\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-\ngeoproximity",
    "links": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo",
      "https://aws.amazon.com/waf/",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-"
    ]
  },
  {
    "question": "CertyIQ\nA company stores its data on premises. The amount of data is growing beyond the company's available capacity.\nThe company wants to migrate its data from the on-premises location to an Amazon S3 bucket. The company\nneeds a solution that will automatically validate the integrity of the data after the transfer.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B: Deploy an AWS DataSync agent on premises. Configure the DataSync agent to\nperform the online data transfer to an S3 bucket.\nHere's why: AWS DataSync is specifically designed for efficiently and securely transferring large amounts of\ndata between on-premises storage and AWS services like S3. A key feature of DataSync is its built-in data\nintegrity validation. During the transfer process, DataSync performs checksum calculations on both the\nsource and destination data. These checksums are compared to ensure that the data transferred is identical\nto the original data, thus automatically validating integrity. DataSync also offers features like encryption\nduring transit and at rest, compression, and automatic retries for failed transfers. This ensures reliable and\nsecure data migration.\nOption A (AWS Snowball Edge) is suitable for large datasets when network bandwidth is limited or unreliable.\nWhile Snowball Edge validates data during import to AWS, it doesn't automatically validate the integrity\nduring an online transfer from on-premises, as the question requires. It's primarily an offline data transfer\nsolution.\nOption C (Amazon S3 File Gateway) provides a way to access S3 objects as files on premises, effectively\ncreating a hybrid storage solution. While it can transfer data, its primary purpose isn't bulk data migration with\nautomatic integrity validation like DataSync. It's more focused on providing local file system access to S3.\nOption D (Amazon S3 Transfer Acceleration) leverages AWS's globally distributed edge locations to\naccelerate uploads to S3. While it can improve transfer speeds, it doesn't provide the automatic data integrity\nvalidation feature that DataSync offers. It focuses solely on optimizing transfer speeds.\nIn summary, DataSync's core functionalities are efficient data transfer and automatic data integrity validation,\nprecisely meeting the requirements of the question.\nFurther Reading:\nAWS DataSync: https://aws.amazon.com/datasync/\nAWS Snowball Edge: https://aws.amazon.com/snowball/\nAmazon S3 File Gateway: https://aws.amazon.com/storagegateway/file/\nAmazon S3 Transfer Acceleration: https://aws.amazon.com/s3/transfer-acceleration/",
    "links": [
      "https://aws.amazon.com/datasync/",
      "https://aws.amazon.com/snowball/",
      "https://aws.amazon.com/storagegateway/file/",
      "https://aws.amazon.com/s3/transfer-acceleration/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to migrate two DNS servers to AWS. The servers host a total of approximately 200 zones and\nreceive 1 million requests each day on average. The company wants to maximize availability while minimizing the\noperational overhead that is related to the management of the two servers.\nWhat should a solutions architect recommend to meet these requirements?",
    "options": {},
    "answer": "A",
    "explanation": "The best solution to migrate the DNS servers to AWS, maximizing availability and minimizing operational\noverhead, is option A: creating 200 new hosted zones in Amazon Route 53 and importing zone files.\nHere's why:\nManaged Service: Route 53 is a highly available and scalable managed DNS service. This means AWS\nhandles the underlying infrastructure, patching, scaling, and maintenance, greatly reducing operational\noverhead for the company compared to managing EC2 instances. https://aws.amazon.com/route53/\nHigh Availability: Route 53 is designed for high availability, automatically distributing DNS records across\nmultiple authoritative DNS servers. The company benefits from this inherent redundancy without having to\nconfigure and manage it themselves.\nScalability: Route 53 can easily handle the 1 million requests per day. Its architecture scales to handle large\nquery volumes without manual intervention.\nSimple Migration: Importing zone files is a straightforward process, making the migration relatively simple\nand less prone to errors. It maintains the existing DNS configurations.\nCost-Effective: While Route 53 has associated costs for queries, these are likely to be lower than the costs\nand operational overhead associated with running EC2 instances, especially considering the HA requirements.\nNo Server Management: Unlike options B, C, and D, Option A avoids the need to manage servers, apply\npatches, and deal with operating system level issues. The managed service model abstracts these operational\nburdens away.\nAuto Scaling is Unnecessary: The auto scaling features in Option D are not pertinent as Route 53 is a highly\navailable service.\nOptions B, C, and D involve managing EC2 instances, which increases operational overhead. Launching an\nAmazon EC2 instance and running the DNS server on EC2 will require ongoing server maintenance, security\npatching, and scaling efforts. While options C and D try to automate the deployment of servers, they introduce\nadded layers of complexity to manage.\nTherefore, using Route 53 (Option A) is the most efficient and cost-effective solution for migrating DNS\nservers to AWS, maximizing availability, and minimizing operational overhead.",
    "links": [
      "https://aws.amazon.com/route53/"
    ]
  },
  {
    "question": "CertyIQ\nA global company runs its applications in multiple AWS accounts in AWS Organizations. The company's\napplications use multipart uploads to upload data to multiple Amazon S3 buckets across AWS Regions. The\ncompany wants to report on incomplete multipart uploads for cost compliance purposes.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C: Configure S3 Storage Lens to report the incomplete multipart upload object count.\nHere's a detailed justification:\nS3 Storage Lens provides organization-wide visibility into object storage usage, activity trends, and makes\nactionable recommendations to optimize costs and apply data protection best practices. It aggregates data\nacross all S3 buckets within an AWS organization (or a single account), including metrics related to\nincomplete multipart uploads. It offers pre-built dashboards that are accessible through the S3 console,\nmaking setup and reporting straightforward.\nOption A (AWS Config) is not ideal because while Config can track configuration changes to S3 buckets, it\ndoesn't directly report on metrics like incomplete multipart uploads in a cost-effective and aggregated\nmanner across multiple accounts and Regions. Implementing a custom Config rule for this purpose would\ninvolve significant operational overhead and custom logic.\nOption B (SCP) is inappropriate. SCPs control what IAM principals (users and roles) within the organization can\ndo. They are not designed to collect or report on S3 metrics. SCPs prevent actions; they don't provide visibility\nor reporting capabilities for storage-related costs.\nOption D (S3 Multi-Region Access Point) is irrelevant to the problem. Multi-Region Access Points provide a\nsingle global endpoint for S3 data stored in multiple Regions, improving application availability. They do not\naddress the requirement of reporting on incomplete multipart uploads.\nS3 Storage Lens directly addresses the reporting requirement with the least operational overhead because\nit's designed to provide this type of aggregated storage metrics across accounts and Regions in AWS\nOrganizations. It offers a dedicated dashboard to analyze and report incomplete multipart uploads for cost\noptimization.\nHere are some authoritative links for further research:\nAWS S3 Storage Lens: https://aws.amazon.com/s3/storage-lens/\nUsing S3 Storage Lens: https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-lens.html\nUnderstanding S3 Storage Lens Metrics: https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-\nlens-metrics-values.html",
    "links": [
      "https://aws.amazon.com/s3/storage-lens/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-lens.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a production database on Amazon RDS for MySQL. The company wants to upgrade the database\nversion for security compliance reasons. Because the database contains critical data, the company wants a quick\nsolution to upgrade and test functionality without losing any data.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D: Use Amazon RDS Blue/Green Deployments to deploy and test production changes.\nHere's a detailed justification:\nAmazon RDS Blue/Green Deployments offer the simplest and fastest method for database engine upgrades\nwhile minimizing downtime and data loss risk. They allow creating a fully functional, isolated staging\nenvironment (the \"green\" environment) that mirrors the production environment (\"blue\"). This makes it\npossible to test the upgraded MySQL version without affecting the production database.\nUsing Blue/Green Deployments, the company can create a green environment with the upgraded MySQL\nversion. RDS handles the data replication from the blue to the green environment. The company can then\nperform thorough testing in the green environment, ensuring all applications function correctly with the new\nversion. If testing is successful, a simple switchover promotes the green environment to production, making it\nthe new blue environment. This process involves a brief outage, considerably less than other migration\nmethods. Crucially, the entire process is managed by RDS, reducing manual intervention and operational\noverhead. If issues arise, the switchover can be reversed, falling back to the original blue environment.\nOption A is not ideal because taking a manual snapshot and restoring it to a new, upgraded instance involves\nmanual steps and potential downtime for the restore process. Option B, using native backup and restore, also\nincreases operational overhead. Option C, AWS DMS, is more suited for migrating between different database\nengines or moving databases to AWS and typically incurs more overhead than a simple version upgrade within\nRDS. Blue/Green deployments are specifically designed for these types of upgrade scenarios within RDS.\nTherefore, Amazon RDS Blue/Green Deployments is the most efficient approach for upgrading the MySQL\ndatabase version with minimal downtime, reduced operational overhead, and a safe testing environment.\nRefer to the AWS documentation for more information:\nAmazon RDS Blue/Green Deployments: https://aws.amazon.com/rds/blue-green-deployments/\nPerforming blue/green deployments for Amazon RDS for MySQL:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/blue-green-deployments.MySQL.html",
    "links": [
      "https://aws.amazon.com/rds/blue-green-deployments/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/blue-green-deployments.MySQL.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is creating a data processing job that runs once daily and can take up to 2 hours to complete.\nIf the job is interrupted, it has to restart from the beginning.\nHow should the solutions architect address this issue in the MOST cost-effective manner?",
    "options": {},
    "answer": "C",
    "explanation": "The most cost-effective solution is to use an Amazon ECS Fargate task triggered by an Amazon EventBridge\nscheduled event (Option C). Here's why:\nFargate's \"Pay-as-you-go\" Model: Fargate allows you to run containers without managing servers. You pay\nonly for the compute resources your container uses while it's running, making it efficient for sporadic\nworkloads.\nEventBridge Scheduling: EventBridge can reliably trigger the ECS task at the scheduled time (once daily).\nNo Idle EC2 Costs: Options A and D involve running EC2 instances, which incur costs even when idle. Since\nthe job only runs for up to 2 hours daily, maintaining a dedicated EC2 instance is wasteful. Reserved Instances\n(Option A) could potentially reduce EC2 costs, but require long-term commitment and might not be the most\ncost-effective if utilization is low.\nLambda Time Constraints: AWS Lambda (Option B) has execution time limits (currently 15 minutes). Since the\ndata processing job can take up to 2 hours, Lambda is not a suitable option.\nFault Tolerance: ECS Fargate automatically handles underlying infrastructure and availability. It restarts the\ncontainer if it fails during execution.\nSimplified Management: Fargate abstracts away the complexities of EC2 instance management, making the\nsolution simpler to deploy and maintain.\nIn summary, ECS Fargate combined with EventBridge provides a serverless, scalable, and cost-optimized\nsolution for the scheduled data processing job that can handle interruptions by restarting the task, all while\nadhering to the budget-conscious constraint.\nSupporting Links:\nAmazon ECS Fargate: https://aws.amazon.com/fargate/\nAmazon EventBridge: https://aws.amazon.com/eventbridge/\nAWS Lambda Limits: https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html",
    "links": [
      "https://aws.amazon.com/fargate/",
      "https://aws.amazon.com/eventbridge/",
      "https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html"
    ]
  },
  {
    "question": "CertyIQ\nA social media company wants to store its database of user profiles, relationships, and interactions in the AWS\nCloud. The company needs an application to monitor any changes in the database. The application needs to\nanalyze the relationships between the data entities and to provide recommendations to users.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "B",
    "explanation": "The optimal solution leverages Amazon Neptune and Neptune Streams.\nAmazon Neptune: Neptune is a purpose-built graph database ideal for storing and querying data with\ncomplex relationships, such as social media user profiles, connections, and interactions. This directly\naddresses the company's need to represent relationships between data entities.\n(https://aws.amazon.com/neptune/)\nNeptune Streams: Neptune Streams provide a managed mechanism for capturing changes (insertions,\nupdates, deletions) happening within the Neptune graph database. This allows the company to monitor\ndatabase changes for analysis and recommendations. This is a feature tightly integrated with Neptune,\nminimizing operational overhead compared to external streaming solutions.\n(https://docs.aws.amazon.com/neptune/latest/userguide/streams.html)\nOption A is less optimal because Kinesis Data Streams, while a robust streaming service, requires additional\nconfiguration and integration to extract change data from Neptune, increasing operational overhead\ncompared to the native Neptune Streams feature.\nOptions C and D use Amazon QLDB, which is a ledger database designed for maintaining a verifiable and\nimmutable history of data changes. While QLDB ensures data integrity, it is not optimized for analyzing\ncomplex relationships or providing recommendations based on graph patterns. It excels at auditing, not the\nrelationship analysis required in the scenario. Further, Neptune Streams can not be used with QLDB, as it is a\nNeptune specific service. Thus, QLDB is not suitable for the requirement, and the combination of QLDB and\nKinesis Data Streams introduces unnecessary complexity compared to Neptune and Neptune Streams.",
    "links": [
      "https://aws.amazon.com/neptune/)",
      "https://docs.aws.amazon.com/neptune/latest/userguide/streams.html)"
    ]
  },
  {
    "question": "CertyIQ\nA company is creating a new application that will store a large amount of data. The data will be analyzed hourly\nand will be modified by several Amazon EC2 Linux instances that are deployed across multiple Availability Zones.\nThe needed amount of storage space will continue to grow for the next 6 months.\nWhich storage solution should a solutions architect recommend to meet these requirements?",
    "options": {
      "C": "Store the data in an Amazon Elastic File System (Amazon EFS) file system. Mount",
      "A": "Amazon S3 Glacier: Glacier is designed for long-term archival storage where infrequent access is",
      "D": "Amazon EBS: EBS is a block storage service that is directly attached to a single EC2 instance (with"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Store the data in an Amazon Elastic File System (Amazon EFS) file system. Mount\nthe file system on the application instances.\nHere's why:\nShared Access: The scenario specifies that multiple EC2 instances in different Availability Zones need to\nmodify the data concurrently. Amazon EFS is designed for shared file storage, allowing multiple instances to\naccess the same data simultaneously. EBS, in contrast, can only be attached to a single instance at a time\n(unless using EBS multi-attach, but this is a more complex solution for few instances).\nScalability: The requirement states that the storage space will grow significantly in the next 6 months. EFS is\ndesigned to scale automatically as data is added, without the need for manual intervention or pre-\nprovisioning. This makes it suitable for dynamic storage needs.\nAvailability Zones: EFS is a regional service, meaning it replicates data across multiple Availability Zones.\nThis provides high availability and durability, ensuring that the data is accessible even if one Availability Zone\nexperiences an outage.\nPerformance: EFS offers different performance modes and throughput options, including Bursting\nThroughput and Provisioned Throughput, allowing the company to choose the performance level that best\nmeets its analysis requirements.\nWhy the other options are incorrect:\nA. Amazon S3 Glacier: Glacier is designed for long-term archival storage where infrequent access is\nexpected. Hourly analysis and modification would make Glacier unsuitable due to its retrieval times and cost\nstructure.\nB & D. Amazon EBS: EBS is a block storage service that is directly attached to a single EC2 instance (with\nlimited multi-attach use cases). While EBS can be used, sharing a standard or Provisioned IOPS EBS volume\namong multiple instances is not natively supported (without complex solutions). EBS is also limited to a single\nAZ per Volume, and while snapshots can be replicated across AZs, that is not a solution for a shared\nfilesystem. Additionally, EBS requires manual resizing to increase the storage capacity.\nTherefore, using Amazon EFS offers the most efficient and scalable solution for storing and sharing data\namong multiple EC2 instances in different Availability Zones, aligning perfectly with the specified\nrequirements.\nAuthoritative Links:\nAmazon EFS: https://aws.amazon.com/efs/\nAmazon EBS: https://aws.amazon.com/ebs/\nAmazon S3 Glacier: https://aws.amazon.com/glacier/",
    "links": [
      "https://aws.amazon.com/efs/",
      "https://aws.amazon.com/ebs/",
      "https://aws.amazon.com/glacier/"
    ]
  },
  {
    "question": "CertyIQ\nA company manages an application that stores data on an Amazon RDS for PostgreSQL Multi-AZ DB instance.\nIncreases in traffic are causing performance problems. The company determines that database queries are the\nprimary reason for the slow performance.\nWhat should a solutions architect do to improve the application's performance?",
    "options": {},
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the correct answer:\nThe problem clearly states that increasing traffic is causing performance problems specifically related to\ndatabase queries. The RDS PostgreSQL instance is a Multi-AZ deployment, which is primarily for high\navailability and disaster recovery, not for scaling read operations. The standby replica in a Multi-AZ\nconfiguration is not directly accessible for serving read traffic in a standard configuration; it's only activated\nin case of a failover. Therefore, option A is incorrect.\nOption B, configuring the DB instance to use Transfer Acceleration, is irrelevant. Transfer Acceleration is an\nAmazon S3 feature for accelerating data transfers into and out of S3 buckets. It doesn't apply to RDS\ndatabase performance or query optimization.\nOption D, using Amazon Kinesis Data Firehose between the application and RDS, is also incorrect. Kinesis Data\nFirehose is designed for streaming data into data lakes, data stores, and analytics services. It's not intended\nas a solution for optimizing database query performance or increasing concurrency of database requests in a\ntransactional database.\nOption C, creating a read replica from the source DB instance and serving read traffic from the read replica, is\nthe correct solution. Read replicas are designed specifically to offload read traffic from the primary database\ninstance. By directing read queries to the read replica, the primary instance is freed up to handle write\noperations and other critical tasks, significantly improving overall application performance and reducing the\nload on the primary database. This approach leverages the concept of read scaling, a common pattern for\nimproving the performance of read-heavy applications.\nHere are some authoritative links for further research:\nAmazon RDS Read Replicas:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.ReadReplicas.html\nAmazon RDS Multi-AZ Deployments:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\nScaling with RDS: https://aws.amazon.com/rds/scaling/",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.ReadReplicas.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html",
      "https://aws.amazon.com/rds/scaling/"
    ]
  },
  {
    "question": "CertyIQ\nA company collects 10 GB of telemetry data daily from various machines. The company stores the data in an\nAmazon S3 bucket in a source data account.\nThe company has hired several consulting agencies to use this data for analysis. Each agency needs read access to\nthe data for its analysts. The company must share the data from the source data account by choosing a solution\nthat maximizes security and operational efficiency.\nWhich solution will meet these requirements?",
    "options": {
      "C": "Configure cross-account access for the S3 bucket to the accounts that the agencies"
    },
    "answer": "C",
    "explanation": "The best solution is C. Configure cross-account access for the S3 bucket to the accounts that the agencies\nown.\nHere's why:\nSecurity: Cross-account access using IAM roles is the most secure approach. It avoids sharing credentials or\nmaking the bucket public. The agencies' analysts access the data through their own AWS accounts, ensuring\nthat the company retains control over its data while granting specific permissions. This aligns with the\nprinciple of least privilege.\nOperational Efficiency: Managing access via cross-account roles is operationally efficient. The company\ndoesn't need to manage individual IAM users for each analyst within their account. Changes to analyst access\nare handled within the agency's AWS account.\nAvoids data duplication: Cross-account access avoids replicating the data as S3 global tables would. This\nsaves on storage costs and reduces operational overhead.\nLimited access duration and Public access are not secure Making the bucket public even for a limited time\n(option B) introduces significant security risks. It's vulnerable to unauthorized access and potential data\nbreaches.\nIAM for individual analyst management is complex and not efficient: Creating IAM users for each analyst in\nthe company's account (option D) is not scalable or efficient, creating complexity and operational overhead.\nS3 Global tables (replication) is not required: Replicating 10GB of data daily to separate buckets for each\nagency (option A) introduces significant storage and data transfer costs and delays. S3 global tables is meant\nfor low-latency access across regions, not a distribution mechanism for data analysis.\nCross-account access: grants specific and auditable permissions, while using the agencies' existing\naccounts, avoiding unnecessary IAM user management.\nSupporting Documentation:\nAWS Documentation on Cross-Account Access:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\nAWS Documentation on S3 Bucket Permissions:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html",
    "links": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html"
    ]
  },
  {
    "question": "CertyIQ\nA company uses Amazon FSx for NetApp ONTAP in its primary AWS Region for CIFS and NFS file shares.\nApplications that run on Amazon EC2 instances access the file shares. The company needs a storage disaster\nrecovery (DR) solution in a secondary Region. The data that is replicated in the secondary Region needs to be\naccessed by using the same protocols as the primary Region.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "C": "Create an FSx for ONTAP instance in the secondary Region. Use NetApp"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Create an FSx for ONTAP instance in the secondary Region. Use NetApp\nSnapMirror to replicate data from the primary Region to the secondary Region.\nHere's why:\nMeeting Requirements: The primary requirement is a DR solution for FSx for ONTAP with the least\noperational overhead, accessible via the same CIFS/NFS protocols in the DR region.\nSnapMirror's Efficiency: NetApp SnapMirror is a native feature of FSx for ONTAP specifically designed for\nblock-level, incremental replication between ONTAP systems. This makes it very efficient in replicating only\nthe changes, thus minimizing bandwidth usage and replication time. It directly addresses the need for DR\nacross regions.\nProtocol Consistency: SnapMirror replicates the data while preserving the underlying file system structure\nand protocol support (CIFS/NFS). The DR FSx for ONTAP instance will natively serve the replicated data using\nthe same protocols as the primary.\nLeast Operational Overhead: SnapMirror setup is relatively straightforward within the AWS console/API,\nrequiring configuration of source and destination file systems. Once configured, replication can be automated\nwith minimal ongoing manual intervention.\nLet's analyze why the other options are less suitable:\nA (Lambda and S3): This introduces unnecessary complexity and overhead. Data would need to be extracted\nfrom FSx for ONTAP, converted to an object format suitable for S3, and then re-hydrated into a file system\nformat in the DR region. This process is slow, complex, and doesn't natively support CIFS/NFS directly from\nS3.\nB (AWS Backup): AWS Backup can be used to create backups of FSx for ONTAP volumes and copy them\nacross regions. While this provides a point-in-time recovery option, it's not the most efficient DR solution,\nespecially for minimizing recovery time objective (RTO). Restoring from backup in DR would be slower than\nfailing over to a replica maintained by SnapMirror. Also, it would require periodic backup schedules which\nadds to operational overhead compared to SnapMirror.\nD (Amazon EFS): Migrating from FSx for ONTAP to EFS is a significant undertaking that introduces\ncomplexity and potential compatibility issues. Moreover, it's not a direct replacement for the FSx for ONTAP\nenvironment. EFS supports NFS, but not CIFS directly, which means the applications accessing the CIFS\nshares will require modification. This also fails to meet the core requirements and dramatically increases the\noperational overhead.\nAuthoritative Links:\nAmazon FSx for NetApp ONTAP Replication: https://aws.amazon.com/blogs/storage/implement-cross-\nregion-disaster-recovery-with-amazon-fsx-for-netapp-ontap-using-netapp-snapmirror/\nNetApp SnapMirror: https://docs.netapp.com/us-en/ontap/data-\nprotection/concept_snapmirror_replication.html",
    "links": [
      "https://aws.amazon.com/blogs/storage/implement-cross-",
      "https://docs.netapp.com/us-en/ontap/data-"
    ]
  },
  {
    "question": "CertyIQ\nA development team is creating an event-based application that uses AWS Lambda functions. Events will be\ngenerated when files are added to an Amazon S3 bucket. The development team currently has Amazon Simple\nNotification Service (Amazon SNS) configured as the event target from Amazon S3.\nWhat should a solutions architect do to process the events from Amazon S3 in a scalable way?",
    "options": {
      "C": "Create an SNS subscription that sends the event to Amazon Simple Queue Service"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Create an SNS subscription that sends the event to Amazon Simple Queue Service\n(Amazon SQS). Configure the SQS queue to trigger a Lambda function.\nHere's why:\nScalability and Decoupling: Directly invoking Lambda from S3 via SNS can become problematic at scale. If\nS3 generates a high volume of events, SNS might overwhelm Lambda, leading to throttling and potential\nevent loss. Using SQS as an intermediary decouples S3 event generation from Lambda processing. SQS acts\nas a buffer, holding events until Lambda can process them.\nReliability: SQS provides guaranteed delivery of messages. Even if Lambda fails to process an event, the\nmessage remains in the queue and can be retried, enhancing the application's reliability. This is crucial for\nevent-driven architectures where data loss is unacceptable.\nAsynchronous Processing: SQS allows Lambda to process events asynchronously. Lambda doesn't have to\nwait for S3 to acknowledge receipt, improving the overall performance of the application.\nFanout Pattern: SNS itself can be used to fan out events to multiple SQS queues, if needed, for different\nprocessing requirements. However, for a single Lambda function as the processor, a single SQS queue\nprovides sufficient buffering and scalability.\nAlternatives are less suitable:\nA & B (ECS/EKS): Introducing container orchestration like ECS or EKS adds unnecessary complexity for this\nscenario. Lambda is designed for event-driven, serverless execution, making it a more suitable choice.\nMoreover, adding a processing layer before Lambda is counterintuitive to the problem's intent. The goal is to\nmake Lambda event processing scalable.\nD (AWS SMS): AWS SMS is for migrating on-premises servers to AWS, completely unrelated to event\nprocessing from S3.\nIn summary, using SNS to deliver S3 events to an SQS queue and then triggering a Lambda function from the\nqueue creates a scalable, reliable, and decoupled event processing architecture.\nSupporting Links:\nAWS SQS: https://aws.amazon.com/sqs/\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon SNS: https://aws.amazon.com/sns/\nUsing AWS Lambda with Amazon SQS: https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html",
    "links": [
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/sns/",
      "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is designing a new service behind Amazon API Gateway. The request patterns for the service\nwill be unpredictable and can change suddenly from 0 requests to over 500 per second. The total size of the data\nthat needs to be persisted in a backend database is currently less than 1 GB with unpredictable future growth.\nData can be queried using simple key-value requests.\nWhich combination ofAWS services would meet these requirements? (Choose two.)",
    "options": {},
    "answer": "B",
    "explanation": "Here's a detailed justification for why AWS Lambda and Amazon DynamoDB are the best choices for the\ndescribed scenario:\nAWS Lambda: Lambda excels at handling unpredictable and sudden spikes in traffic. It's a serverless\ncompute service, meaning it automatically scales in response to incoming requests without requiring manual\nintervention or capacity provisioning. This aligns perfectly with the requirement for handling request patterns\nthat can change drastically. Lambda functions are triggered by API Gateway, processing requests as they\narrive and scaling up or down as needed. Fargate, while also scalable, requires containerization and more\noperational overhead. EC2 Auto Scaling requires provisioning and managing virtual machines, adding\nunnecessary complexity for this use case.\nSupporting Concept: Serverless computing and event-driven architecture.\nAuthoritative Link: https://aws.amazon.com/lambda/\nAmazon DynamoDB: DynamoDB is a NoSQL database service designed for key-value and document data\nmodels. It's highly scalable and can handle unpredictable growth in data size. DynamoDB scales horizontally\nand automatically, without requiring manual intervention. Its ability to handle simple key-value queries\nefficiently matches the data access pattern described. The current data size of less than 1 GB is easily\naccommodated, and its ability to scale allows it to handle unpredictable future growth. MySQL-compatible\nAurora, while a good relational database option, introduces more overhead than needed for simple key-value\nqueries. Additionally, the auto-scaling capabilities of DynamoDB are better suited for unpredictable growth\ncompared to Aurora's scaling mechanisms in this simplified case.\nSupporting Concept: NoSQL database, horizontal scaling, key-value data model.\nAuthoritative Link: https://aws.amazon.com/dynamodb/\nTherefore, the combination of AWS Lambda for processing unpredictable request volumes from API Gateway\nand Amazon DynamoDB for storing and querying data using a simple key-value approach provides the most\nefficient and scalable solution.",
    "links": [
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/dynamodb/"
    ]
  },
  {
    "question": "CertyIQ\nA company collects and shares research data with the company's employees all over the world. The company\nwants to collect and store the data in an Amazon S3 bucket and process the data in the AWS Cloud. The company\nwill share the data with the company's employees. The company needs a secure solution in the AWS Cloud that\nminimizes operational overhead.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "A",
    "explanation": "The most suitable solution is A: Use an AWS Lambda function to create an S3 presigned URL. Instruct\nemployees to use the URL.\nHere's why:\nSecurity: Presigned URLs provide time-limited, temporary access to specific S3 objects. This is much more\nsecure than long-term credentials. Each URL grants access only for a specific object and a defined duration,\nreducing the risk of unauthorized access if the URL is compromised.\nMinimal Operational Overhead: A Lambda function can be triggered to generate these URLs on demand. This\nautomated process reduces the administrative burden of managing individual IAM users and policies. It avoids\nthe complexity of managing infrastructure.\nScalability: Lambda functions scale automatically based on demand, making the solution suitable for a\nfluctuating number of employees and data access requests.\nData Sharing: Presigned URLs allow the company to share data with employees without granting them\npermanent access to the S3 bucket.\nIAM User Management Avoidance: Option B, creating IAM users for each employee, is cumbersome and\nintroduces significant operational overhead for user management, policy updates, and credential rotation.\nS3 File Gateway Limitation: Option C, S3 File Gateway, is designed to integrate on-premises applications\nwith S3 storage. While it provides file-based access, it is more complex to set up and maintain for simple data\nsharing with employees compared to presigned URLs. File Gateway requires additional infrastructure on the\non-premise side to mount the drives.\nAWS Transfer Family SFTP Overhead: Option D, Transfer Family, is best suited for secure file transfers, often\nfor business-to-business scenarios. Implementing a custom identity provider with Secrets Manager adds\ncomplexity that is unnecessary for this simple internal data sharing use case. Transfer Family also incurs\nmore operational overhead in managing users and SFTP endpoints.\nIn summary, using Lambda with S3 presigned URLs offers a balanced solution that prioritizes security,\nminimizes operational overhead, and efficiently enables data sharing with the company's employees.\nSupporting Links:\nS3 Presigned URLs: https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-presigned-urls.html\nAWS Lambda: https://aws.amazon.com/lambda/",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-presigned-urls.html",
      "https://aws.amazon.com/lambda/"
    ]
  },
  {
    "question": "CertyIQ\nA company is building a new furniture inventory application. The company has deployed the application on a fleet\nofAmazon EC2 instances across multiple Availability Zones. The EC2 instances run behind an Application Load\nBalancer (ALB) in their VP",
    "options": {
      "C": "A solutions architect has observed that incoming traffic seems to favor one EC2 instance, resulting in latency for"
    },
    "answer": "A",
    "explanation": "The issue is that traffic favors one EC2 instance, leading to latency. This suggests an uneven distribution of\nrequests across the available instances.\nOption A, disabling session affinity (sticky sessions) on the ALB, is the best solution. Session affinity, when\nenabled, directs all requests from a specific user session to the same EC2 instance. This can cause\nimbalances if some users are more active than others, leading to overloaded instances while others remain\nunderutilized. Disabling sticky sessions ensures that the ALB distributes traffic more evenly across all\nregistered instances based on its configured load-balancing algorithm (e.g., round robin). This promotes\nbetter resource utilization and reduces latency.\nOption B, replacing the ALB with a Network Load Balancer (NLB), is not appropriate. NLBs are designed for\nhigh-performance, low-latency traffic such as TCP, UDP, and TLS. They do not provide the application-level\nfeatures of ALBs, such as content-based routing or host-based routing, and would not directly address the\nsession affinity issue.\nOption C, increasing the number of EC2 instances, might alleviate the problem somewhat by increasing the\noverall capacity. However, it does not address the root cause of uneven distribution due to sticky sessions and\ncould be a less cost-effective solution than simply disabling sticky sessions.\nOption D, adjusting health check frequency, is irrelevant to the observed traffic imbalance. Health checks\ndetermine the availability of instances, but they do not control how traffic is distributed among healthy\ninstances.\nTherefore, disabling sticky sessions (Option A) is the most direct and efficient way to address the load\nimbalance observed on the EC2 instances behind the ALB, ensuring a more even distribution of traffic and\nreduced latency.\nFurther Reading:\nApplication Load Balancer Concepts:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\nALB Target Group Attributes (Sticky Sessions):\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-attributes.html",
    "links": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-attributes.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has an application workflow that uses an AWS Lambda function to download and decrypt files from\nAmazon S3. These files are encrypted using AWS Key Management Service (AWS KMS) keys. A solutions architect\nneeds to design a solution that will ensure the required permissions are set correctly.\nWhich combination of actions accomplish this? (Choose two.)",
    "options": {
      "B": "Grant the decrypt permission for the Lambda IAM role in the KMS key's policy: The KMS key policy",
      "A": "Attach the kms:decrypt permission to the Lambda functions resource policy: Lambda resource policies",
      "C": "Grant the decrypt permission for the Lambda resource policy in the KMS key's policy: As in A, the",
      "D": "Create a new IAM policy with the kms:decrypt permission and attach the policy to the Lambda function:"
    },
    "answer": "B",
    "explanation": "The correct answer is BE. Here's a detailed justification:\nThe Lambda function needs permission to decrypt the files encrypted with KMS. This involves configuring\nappropriate IAM policies and KMS key policies. Two actions are crucial:\nB. Grant the decrypt permission for the Lambda IAM role in the KMS key's policy: The KMS key policy\ncontrols who can use the key. The Lambda function needs the kms:Decrypt permission to decrypt the S3 files.\nThis permission is granted by modifying the KMS key's policy to explicitly allow the Lambda function's IAM\nrole to perform the kms:Decrypt action. This grants the Lambda function access to use the KMS key for\ndecryption.\nAWS KMS Key Policies\nE. Create a new IAM role with the kms:decrypt permission and attach the execution role to the Lambda\nfunction: The Lambda function executes with an IAM role. To ensure the Lambda function has the necessary\npermissions to perform actions on AWS services, like KMS, you create an IAM role that grants these\npermissions. In this case, a new IAM role with the kms:Decrypt permission would be created. The kms:decrypt\npermission is granted to the IAM role in an IAM policy. Then, attach this execution role to the Lambda function.\nIAM Roles for Lambda\nLet's examine why other options are incorrect:\nA. Attach the kms:decrypt permission to the Lambda functions resource policy: Lambda resource policies\nare primarily used to grant other AWS services permission to invoke the Lambda function. They are not the\nprimary mechanism for granting the Lambda function itself permissions to access other services like KMS.\nThe Lambda function needs an IAM role, not just a resource policy, to execute with permissions.\nC. Grant the decrypt permission for the Lambda resource policy in the KMS key's policy: As in A, the\nLambda resource policy focuses on invocation by other services, not the Lambda function's permissions to\naccess other AWS services. Adding the Lambda resource policy to the KMS key policy won't give the function\nthe ability to use KMS to decrypt.\nD. Create a new IAM policy with the kms:decrypt permission and attach the policy to the Lambda function:\nWhile creating an IAM policy with kms:decrypt is correct, attaching the policy to the Lambda function is less\ncommon and often confusing. It's best practice to attach the IAM policy to an IAM role, then assign the role to\nthe Lambda function. Lambda functions are typically associated with IAM roles as their execution context\nrather than attaching policies directly. Option E correctly captures this.\nIn summary, granting the Lambda function's IAM role the kms:Decrypt permission in both the IAM policy and\nthe KMS key policy ensures that the function is authorized to decrypt the files using the KMS key. This is the\nstandard method for granting permissions between services in AWS, adhering to the principle of least\nprivilege.",
    "links": []
  },
  {
    "question": "CertyIQ\nA company wants to monitor its AWS costs for financial review. The cloud operations team is designing an\narchitecture in the AWS Organizations management account to query AWS Cost and Usage Reports for all\nmember accounts. The team must run this query once a month and provide a detailed analysis of the bill.\nWhich solution is the MOST scalable and cost-effective way to meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The most scalable and cost-effective solution is to use Cost and Usage Reports (CUR) with S3 and Athena.\nHere's why:\nCost and Usage Reports (CUR): CUR provides detailed information about AWS costs and usage. Enabling it in\nthe management account consolidates data from all member accounts in AWS Organizations, fulfilling the\nrequirement to analyze costs across the organization.\nhttps://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html\nAmazon S3: Storing the CUR in S3 offers a highly scalable and cost-effective storage solution. S3 provides\ndurable and readily available storage for the monthly reports. https://aws.amazon.com/s3/\nAmazon Athena: Athena is a serverless query service that enables analyzing data directly in S3 using\nstandard SQL. It is ideal for ad-hoc querying and monthly analysis of the CUR data without the need to\nmanage any infrastructure. Athena is pay-per-query, making it cost-effective for a monthly analysis.\nhttps://aws.amazon.com/athena/\nLet's analyze the other options:\nOption A (Kinesis and EMR): Kinesis is designed for real-time data streaming, which isn't necessary for a\nmonthly cost analysis. EMR (Elastic MapReduce) is suitable for large-scale data processing, but it is an\noverkill for monthly CUR analysis and more costly than Athena for this specific use case.\nOption C (Redshift): Redshift is a data warehouse service, which is ideal for complex data warehousing\nscenarios. Using Redshift would be much more costly and complex than required for the monthly analysis of\nCUR.\nOption D (Kinesis and QuickSight): As with option A, Kinesis is not necessary here. QuickSight is a BI tool\nsuitable for visualizing data, but the raw CUR needs to be queried and prepared for analysis first, which\nAthena efficiently handles. QuickSight can complement Athena, but it does not replace Athena's role in\nefficiently querying the raw CUR data stored in S3.\nTherefore, Option B delivers the best balance of scalability, cost-effectiveness, and simplicity for monthly\ncost analysis within an AWS Organizations environment. It allows the cloud operations team to easily analyze\nthe CUR data and generate detailed billing analyses.",
    "links": [
      "https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html",
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/athena/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to run a gaming application on Amazon EC2 instances that are part of an Auto Scaling group in\nthe AWS Cloud. The application will transmit data by using UDP packets. The company wants to ensure that the\napplication can scale out and in as traffic increases and decreases.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "A": "Attach a Network Load Balancer to the Auto Scaling group."
    },
    "answer": "A",
    "explanation": "The correct answer is A. Attach a Network Load Balancer to the Auto Scaling group.\nHere's a detailed justification:\nUDP Protocol Support: Network Load Balancers (NLBs) are designed to handle UDP traffic efficiently.\nApplication Load Balancers (ALBs), on the other hand, primarily work with HTTP/HTTPS (Layer 7) protocols\nand do not support UDP. Since the gaming application uses UDP packets, an NLB is the appropriate choice.\nAuto Scaling Integration: NLBs seamlessly integrate with Auto Scaling groups. As the Auto Scaling group\nscales instances in or out, the NLB automatically updates its target group to include the new instances and\nremove the terminated ones, ensuring traffic is always routed to healthy instances. This is essential for the\napplication to scale effectively based on traffic demand.\nHigh Performance and Low Latency: NLBs offer high throughput and low latency, making them well-suited\nfor performance-sensitive applications like online gaming where responsiveness is critical. They operate at\nLayer 4 (Transport Layer), forwarding traffic based on IP addresses and ports.\nRoute 53 Weighted Policy Inadequacy: While Route 53 can distribute traffic across multiple endpoints, it\noperates at the DNS level. It doesn't provide health checks at the instance level or automatically adjust\nrouting based on the dynamic scaling of instances within an Auto Scaling group. This option does not fulfill\nthe automatic scaling requirement efficiently. It's also not UDP specific and doesn't replace a load balancer\nfor instance health management.\nNAT Instance Inefficiency: Using a NAT instance for port forwarding is a complex, single point of failure, and\nscaling bottleneck. NAT instances are typically used for enabling private instances to access the internet, not\nfor load balancing or traffic distribution in a highly available manner. Also, manually configuring port\nforwarding on a NAT instance is not a scalable solution and would require continuous adjustments as the Auto\nScaling group changes.\nIn summary, attaching a Network Load Balancer to the Auto Scaling group provides the necessary UDP\nsupport, seamless integration with Auto Scaling, high performance, and automatic target registration, making\nit the most appropriate solution for the gaming application's requirements.\nSupporting Documentation:\nAWS Network Load Balancer\nAuto Scaling Groups",
    "links": []
  },
  {
    "question": "CertyIQ\nA company runs several websites on AWS for its different brands. Each website generates tens of gigabytes of\nweb traffic logs each day. A solutions architect needs to design a scalable solution to give the company's\ndevelopers the ability to analyze traffic patterns across all the company's websites. This analysis by the\ndevelopers will occur on demand once a week over the course of several months. The solution must support\nqueries with standard SQL.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "A",
    "explanation": "The most cost-effective solution is to use Amazon S3 and Amazon Athena (Option A) for this scenario because\nit directly addresses the requirements of scalable storage, on-demand SQL analysis, and cost optimization.\nHere's why:\nAmazon S3 for Scalable Storage: S3 provides virtually unlimited storage at a low cost. It's designed for\nstoring large amounts of data like web traffic logs. S3's durability and scalability make it ideal for this\npurpose. https://aws.amazon.com/s3/\nAmazon Athena for SQL Analysis: Athena is a serverless query service that allows you to analyze data in S3\nusing standard SQL. You only pay for the queries you run, which is perfect for infrequent, on-demand analysis.\nIt avoids the costs of maintaining a dedicated database or cluster. https://aws.amazon.com/athena/\nCost-Effectiveness: Since the analysis occurs only once a week for a few months, Athena's pay-per-query\nmodel is significantly cheaper than running a persistent database or cluster. S3's storage costs are also very\nlow.\nWhy other options are less suitable:\nAmazon RDS (Option B): RDS is a managed relational database service. Storing tens of gigabytes of logs daily\nin a database and then using a database client would be far more expensive than using S3 for storage. The\nRDS instance would need to be sized appropriately to handle the volume of data and SQL queries. This is also\nless scalable for handling rapidly growing amounts of logs. The cost of keeping the RDS instance active even\nwhen not in use would not be cost-effective.\nAmazon OpenSearch Service (Option C): OpenSearch is suitable for log analytics, especially when real-time\nor near-real-time insights are needed. However, for infrequent analysis, the ongoing costs of running an\nOpenSearch cluster would be higher than using Athena, since you are billed continuously for OpenSearch\neven when it's not performing queries.\nAmazon EMR (Option D): EMR is suitable for large-scale data processing and analytics using frameworks like\nHadoop and Spark. While EMR can support SQL-based analysis, the cost of running an EMR cluster, even on-\ndemand, is generally higher than using Athena for this specific infrequent analysis scenario. The overhead of\nstarting up and managing an EMR cluster also contributes to higher operational complexity.",
    "links": [
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/athena/"
    ]
  },
  {
    "question": "CertyIQ\nAn international company has a subdomain for each country that the company operates in. The subdomains are\nformatted as example.com, country1.example.com, and country2.example.com. The company's workloads are\nbehind an Application Load Balancer. The company wants to encrypt the website data that is in transit.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is AE. Here's a detailed justification:\n*A: Use the AWS Certificate Manager (ACM) console to request a public certificate for the apex top domain\nexample.com and a wildcard certificate for .example.com.\nThis step is essential because the company wants to encrypt website data in transit, requiring SSL/TLS\ncertificates. A public certificate issued by ACM is needed for the Application Load Balancer (ALB) to establish\nsecure HTTPS connections with clients. The apex domain example.com needs a separate certificate because\nthe wildcard certificate (.example.com) only covers subdomains directly beneath example.com. The wildcard\ncertificate .example.com will cover country1.example.com, country2.example.com, etc. A private certificate is\nnot appropriate in this case because external clients need to trust the certificate authority, and private\ncertificates are not inherently trusted by public browsers.\nE: Validate domain ownership for the domain by adding the required DNS records to the DNS provider.\nACM requires validation of domain ownership before issuing a certificate. This proves that the requester has\ncontrol over the domain they're requesting a certificate for. DNS validation is the recommended method. It\ninvolves adding a CNAME record provided by ACM to the DNS configuration of the domain. This is a more\nreliable and automated method than email validation, especially when dealing with wildcard certificates and\nmultiple subdomains. With email validation, each subdomain will need to be validated separately. DNS\nvalidation only needs to be done once.\nWhy other options are incorrect:\nB: Using a private certificate is incorrect because public clients (browsers) won't trust it. Private certificates\nare suitable for internal services where you control the client environment.\nC: Requesting both public and private certificates for the apex domain is unnecessary and doesn't address the\nneed for a wildcard certificate to cover all subdomains.\nD: While validating domain ownership is necessary, switching from email validation to DNS validation is not\nstrictly required. DNS validation is simply the recommended approach due to its automation and reliability.\nEmail validation is also an acceptable alternative (though less preferred).\nSupporting concepts and links:\nSSL/TLS Certificates: Required for encrypting data in transit over\nHTTPS.https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html\nAWS Certificate Manager (ACM): AWS service for provisioning, managing, and deploying SSL/TLS\ncertificates.https://aws.amazon.com/certificate-manager/\nWildcard Certificates: Cover multiple subdomains with a single\ncertificate.https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html\nDNS Validation: Preferred method for validating domain ownership with\nACM.https://docs.aws.amazon.com/acm/latest/userguide/dns-validation.html\nApplication Load Balancer (ALB): Load balancer that supports HTTPS listeners and uses ACM certificates for\nSSL/TLS termination.https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-\nlisteners.html",
    "links": [
      "https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html",
      "https://aws.amazon.com/certificate-manager/",
      "https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html",
      "https://docs.aws.amazon.com/acm/latest/userguide/dns-validation.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-"
    ]
  },
  {
    "question": "CertyIQ\nA company is required to use cryptographic keys in its on-premises key manager. The key manager is outside of\nthe AWS Cloud because of regulatory and compliance requirements. The company wants to manage encryption\nand decryption by using cryptographic keys that are retained outside of the AWS Cloud and that support a variety\nof external key managers from different vendors.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B: Use an AWS Key Management Service (AWS KMS) external key store backed by an\nexternal key manager.\nHere's why:\nThe core requirement is to use an on-premises key manager (outside AWS) due to regulatory compliance,\nwhile minimizing operational overhead. AWS KMS external key store (XKS) is specifically designed for this\nscenario. It allows you to use KMS to encrypt and decrypt data, but the actual cryptographic operations are\nperformed by your own external key management infrastructure, which satisfies the compliance needs. KMS\nonly holds metadata about the keys.\nOption B achieves the required control and compliance with the least amount of custom development and\nmanagement effort. It's a native integration provided by AWS KMS. You don't need to build and maintain\ncustom software to interface with your on-premises key manager.\nOption A (AWS CloudHSM key store) doesn't address the specific requirement of using an existing on-\npremises key manager. CloudHSM creates HSMs within AWS, not outside of it. While CloudHSM offers high\nsecurity, it necessitates managing HSMs within AWS, adding operational overhead.\nOption C (default KMS managed key store) is incorrect because the keys are entirely managed by AWS KMS,\nwhich contradicts the requirement to retain keys outside of AWS.\nOption D (custom key store backed by CloudHSM) requires significantly more configuration and operational\noverhead. Building a custom key store involves developing the integration between KMS and CloudHSM, and\nsubsequently managing the underlying HSMs, which contradicts the goal of minimizing operational overhead.\nFurthermore, it does not address the need to use an existing on-premise key manager.\nIn summary, XKS is the AWS-provided solution that directly addresses the stated requirement of managing\nkeys outside the AWS cloud while leveraging the KMS service, thereby minimizing operational overhead\ncompared to managing CloudHSM or developing custom solutions.\nFurther Reading:\nAWS KMS external key store: https://docs.aws.amazon.com/kms/latest/developerguide/keystore-\nexternal.html",
    "links": [
      "https://docs.aws.amazon.com/kms/latest/developerguide/keystore-"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect needs to host a high performance computing (HPC) workload in the AWS Cloud. The workload\nwill run on hundreds of Amazon EC2 instances and will require parallel access to a shared file system to enable\ndistributed processing of large datasets. Datasets will be accessed across multiple instances simultaneously. The\nworkload requires access latency within 1 ms. After processing has completed, engineers will need access to the\ndataset for manual postprocessing.\nWhich solution will meet these requirements?",
    "options": {
      "C": "Use Amazon FSx for Lustre as a shared file system. Link the file system to an"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Use Amazon FSx for Lustre as a shared file system. Link the file system to an\nAmazon S3 bucket for postprocessing.\nHere's why:\nHigh-Performance Computing (HPC) needs: HPC workloads, involving hundreds of EC2 instances and\nparallel processing of large datasets, demand a high-performance, low-latency shared file system.\nAmazon FSx for Lustre: This service is specifically designed for HPC and machine learning workloads. It\nprovides sub-millisecond latency, which meets the 1 ms requirement, and delivers high throughput for parallel\ndata access. https://aws.amazon.com/fsx/lustre/\nAmazon S3 integration: FSx for Lustre can be linked to an S3 bucket. This enables the HPC workload to\nefficiently process data stored in S3 and provides a seamless way for engineers to access the processed\ndataset in S3 for postprocessing. Data can be exported easily and efficiently to S3 after the HPC job\nconcludes.\nWhy not Amazon EFS (A): Amazon EFS provides a scalable, elastic file system, but it is not optimized for the\nvery low latency requirements (under 1 ms) that are typical of HPC workloads. While EFS is excellent for\ngeneral-purpose file sharing, it doesn't offer the performance needed here.\nWhy not Amazon S3 as a file system (B & D): S3 is an object storage service, not a file system. Mounting S3\ndirectly to EC2 instances via S3 fuse will introduce high latency and poor performance for parallel file system\naccess. S3 is great for storage but does not provide the required parallel access speed for HPC workloads.\nAWS Resource Access Manager (RAM) allows sharing AWS resources, but it doesn't change the fundamental\ncharacteristics of S3, which is still not suitable for this purpose.\nIn summary, FSx for Lustre provides the necessary low latency and high throughput for the HPC workload,\nwhile the S3 integration facilitates efficient data access and postprocessing.",
    "links": [
      "https://aws.amazon.com/fsx/lustre/"
    ]
  },
  {
    "question": "CertyIQ\nA gaming company is building an application with Voice over IP capabilities. The application will serve traffic to\nusers across the world. The application needs to be highly available with an automated failover across AWS\nRegions. The company wants to minimize the latency of users without relying on IP address caching on user\ndevices.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "C": "Create an Amazon CloudFront distribution that includes multiple origins."
    },
    "answer": "C",
    "explanation": "The correct answer is C. Create an Amazon CloudFront distribution that includes multiple origins.\nHere's a detailed justification:\nCloudFront is a content delivery network (CDN) that excels at minimizing latency for users across the globe.\nBy setting up multiple origins (representing application endpoints in different AWS Regions) behind a\nCloudFront distribution, the company can leverage CloudFront's global network of edge locations. CloudFront\nautomatically routes user requests to the nearest available origin (the one with the lowest latency), thereby\nimproving the user experience. This is crucial for VoIP applications where latency is very sensitive.\nCloudFront can be configured to use origin failover. In this approach, CloudFront monitors the health of your\norigins. If the primary origin becomes unhealthy, CloudFront automatically switches traffic to a secondary\norigin located in a different region. This achieves the high availability and automated failover requirements.\nOption A is incorrect because AWS Global Accelerator is a great option for directing traffic at the regional\nlevel. It minimizes latency by routing user traffic to the optimal AWS endpoint, but requires you to configure\nfailover within the regional endpoint (e.g. through Elastic Load Balancing). For simple application servers, this\napproach is not as well suited to the needs of this application.\nOption B is incorrect because while Route 53 with geolocation routing can direct users to specific Regions, it\nrelies on DNS caching at the user's resolver. This caching can lead to users being directed to a failed region if\ntheir DNS record has not expired yet (DNS TTL). Since VoIP apps are sensitive to failures, you cannot rely on\nDNS caching.\nOption D is incorrect because Application Load Balancers (ALBs) are regional resources. Although ALBs\nsupport routing based on path, host headers etc, they do not natively provide global distribution or automated\ncross-region failover in the same way as CloudFront or Global Accelerator. They also won't route traffic to\ndifferent origins based on latency considerations. You can have multiple ALBs routing to different regions and\ncreate a regional fallback with Lambda, but this does not eliminate the need for a global CDN like CloudFront.\nIn summary, CloudFront with multiple origins and origin failover directly addresses all the requirements (low\nlatency, high availability, automated failover, and global distribution).\nRelevant Links:\nAmazon CloudFront documentation\nConfiguring Origin Failover in CloudFront",
    "links": []
  },
  {
    "question": "CertyIQ\nA weather forecasting company needs to process hundreds of gigabytes of data with sub-millisecond latency. The\ncompany has a high performance computing (HPC) environment in its data center and wants to expand its\nforecasting capabilities.\nA solutions architect must identify a highly available cloud storage solution that can handle large amounts of\nsustained throughput. Files that are stored in the solution should be accessible to thousands of compute instances\nthat will simultaneously access and process the entire dataset.\nWhat should the solutions architect do to meet these requirements?",
    "options": {
      "B": "Use Amazon FSx for Lustre persistent file systems."
    },
    "answer": "B",
    "explanation": "The correct answer is B. Use Amazon FSx for Lustre persistent file systems.\nHere's why:\nHigh Throughput and Low Latency: The core requirement is to handle \"large amounts of sustained\nthroughput\" with \"sub-millisecond latency.\" FSx for Lustre, particularly its persistent file systems, is designed\nfor high-performance workloads. It excels in providing the necessary throughput and low latency.\nParallel Access: The problem states that \"thousands of compute instances\" need to \"simultaneously access\nand process the entire dataset.\" FSx for Lustre is a parallel file system, meaning it's built to handle concurrent\naccess from numerous compute instances without significant performance degradation.\nHPC Compatibility: The company already uses an HPC environment. FSx for Lustre is a natural extension of\nthat, as it's commonly used for HPC workloads like weather forecasting, genomic research, and financial\nmodeling. Its architecture and optimizations align with the needs of these kinds of environments.\nPersistent vs. Scratch: While FSx for Lustre offers both scratch and persistent file systems, persistent is\nbetter for this scenario. Scratch file systems are optimized for short-term storage and rapid creation/deletion,\nwhile persistent file systems provide durable storage and are suitable for workloads requiring long-term data\naccess and availability. The problem doesn't mention data being short-lived, rather it emphasizes accessibility.\nPersistent FSx for Lustre instances can be deployed across multiple availability zones for fault tolerance and\nrecovery.\nEFS Limitations: Amazon EFS, even with provisioned throughput, is generally not designed to handle the\nsame extreme levels of sustained throughput and low latency as FSx for Lustre. While EFS is a good choice\nfor many file-sharing workloads, it's not the best option when sub-millisecond latency and extremely high\nthroughput are critical. It offers lower peak and sustained performance compared to FSx for Lustre. EFS\nBursting throughput is also unideal because it would likely require significant capacity and bursts to handle\n\"hundreds of gigabytes of data\", and doesn't explicitly support the high performance required in the question.\nAuthoritative Links:\nAmazon FSx for Lustre: https://aws.amazon.com/fsx/lustre/\nAmazon EFS: https://aws.amazon.com/efs/",
    "links": [
      "https://aws.amazon.com/fsx/lustre/",
      "https://aws.amazon.com/efs/"
    ]
  },
  {
    "question": "CertyIQ\nAn ecommerce company runs a PostgreSQL database on premises. The database stores data by using high IOPS\nAmazon Elastic Block Store (Amazon EBS) block storage. The daily peak I/O transactions per second do not exceed\n15,000 IOPS. The company wants to migrate the database to Amazon RDS for PostgreSQL and provision disk IOPS\nperformance independent of disk storage capacity.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "C",
    "explanation": "The company needs a cost-effective solution for migrating their PostgreSQL database to Amazon RDS for\nPostgreSQL while ensuring they can handle up to 15,000 IOPS independently of storage capacity. Let's\nanalyze why option C is the most appropriate:\nOption A: gp2: gp2 volume performance is tied to its size. Reaching 15,000 IOPS with gp2 would require\nprovisioning a large volume, potentially leading to unnecessary storage costs since the requirement is to\noptimize IOPS performance independent of storage capacity.\nOption B: io1: While io1 allows you to provision IOPS independently of storage, it is generally more expensive\nper IOPS than gp3, especially for moderate IOPS requirements.\nOption C: gp3: gp3 volumes provide a baseline performance level (3,000 IOPS) and allow you to provision\nadditional IOPS separately from storage capacity. This means the company can provision the necessary IOPS\nwithout having to over-provision storage, resulting in cost savings. In many AWS Regions gp3 supports\nprovisioned IOPS beyond its baseline without increasing the provisioned storage capacity. Also, gp3 provides\nhigher performance than gp2 for a similar price.\nOption D: EBS Magnetic: Magnetic volumes offer the lowest cost but have significantly lower IOPS\nperformance capabilities. They would be completely unsuitable for the stated performance requirement of\n15,000 IOPS. Magnetic volumes are unsuitable for databases due to their poor and inconsistent performance.\nTherefore, gp3 offers the best balance of cost and performance for the requirement. gp3 enables independent\nprovisioning of IOPS and storage and has a lower cost compared to io1.\nSupporting Documentation:\nAmazon EBS Volume Types: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-\ntypes.html\nGeneral Purpose SSD (gp3) volumes https://aws.amazon.com/ebs/general-purpose/",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-",
      "https://aws.amazon.com/ebs/general-purpose/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to migrate its on-premises Microsoft SQL Server Enterprise edition database to AWS. The\ncompany's online application uses the database to process transactions. The data analysis team uses the same\nproduction database to run reports for analytical processing. The company wants to reduce operational overhead\nby moving to managed services wherever possible.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "B": "Use DynamoDB on-demand replicas for reporting purposes"
    },
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A is the best solution, along with supporting explanations and\nlinks:\nThe key requirements are migrating an on-premises Microsoft SQL Server Enterprise edition database to AWS\nwith minimal operational overhead, supporting both transactional processing for an online application and\nanalytical reporting, and leveraging managed services where feasible.\nOption A, migrating to Amazon RDS for Microsoft SQL Server and using read replicas for reporting, is the\noptimal choice because RDS is a managed service. This significantly reduces operational overhead as AWS\nhandles patching, backups, and database management tasks. Using read replicas offloads analytical\nreporting from the primary database, preventing performance impact on the online application's transactional\nworkload. RDS for SQL Server allows you to continue to use familiar SQL Server functionality.\nOption B, using SQL Server on EC2, increases operational overhead because you are responsible for managing\nthe operating system, SQL Server installation, patching, and backups. While Always On Availability Groups\nprovide read replicas, the operational burden is significantly higher compared to using RDS.\nOption C, migrating to Amazon DynamoDB, would require a significant application rewrite since DynamoDB is\na NoSQL database, which is not compatible with the existing SQL Server database schema and SQL queries.\nDynamoDB on-demand backups are not the intended replica function for reporting. This leads to a much\ngreater effort required to convert the application.\nOption D, migrating to Amazon Aurora MySQL, also necessitates a database migration and application rewrite.\nWhile Aurora offers high performance and read replicas, the migration process is a substantial undertaking,\nwhich contrasts with the need to minimize overhead and maintain the business functionality.\nTherefore, RDS provides the least operational overhead by being a managed service. It allows the company to\ncontinue to use MS SQL Server, which avoids a costly and complex migration. The use of read replicas\nmaintains application performance.\nSupporting Links:\nAmazon RDS: https://aws.amazon.com/rds/\nAmazon RDS for SQL Server: https://aws.amazon.com/rds/sqlserver/\nRDS Read Replicas: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html",
    "links": [
      "https://aws.amazon.com/rds/",
      "https://aws.amazon.com/rds/sqlserver/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html"
    ]
  },
  {
    "question": "CertyIQ\nA company stores a large volume of image files in an Amazon S3 bucket. The images need to be readily available\nfor the first 180 days. The images are infrequently accessed for the next 180 days. After 360 days, the images\nneed to be archived but must be available instantly upon request. After 5 years, only auditors can access the\nimages. The auditors must be able to retrieve the images within 12 hours. The images cannot be lost during this\nprocess.\nA developer will use S3 Standard storage for the first 180 days. The developer needs to configure an S3 Lifecycle\nrule.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "C": "Transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after",
      "B": "360 Days to 5 Years (Instant Retrieval): The problem explicitly states that after 360 days the images must",
      "A": "S3 Glacier Flexible Retrieval does not meet this requirement since retrieval times can vary from"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after\n180 days, S3 Glacier Instant Retrieval after 360 days, and S3 Glacier Deep Archive after 5 years.\nHere's a detailed justification:\nThe problem states that the images need to be readily available for the first 180 days, and S3 Standard fulfills\nthis requirement. The subsequent stages require different access patterns and archival needs, influencing the\nchoice of storage classes for the S3 Lifecycle rule.\n180 Days to 360 Days (Infrequent Access): S3 Standard-IA is the most cost-effective choice for data that is\naccessed infrequently but requires rapid access when needed. S3 One Zone-IA has lower availability (data\nloss can occur) than S3 Standard-IA because data is stored in a single availability zone. Since data cannot be\nlost during the process this eliminates options A and B.\n360 Days to 5 Years (Instant Retrieval): The problem explicitly states that after 360 days the images must\nbe \"available instantly upon request.\" S3 Glacier Instant Retrieval delivers the lowest-cost storage for long-\nterm data that is accessed once a quarter, with the same low latency performance as S3 Standard and S3\nStandard-IA. S3 Glacier Flexible Retrieval does not meet this requirement since retrieval times can vary from\nminutes to hours.\nAfter 5 Years (Archival with 12-Hour Retrieval): S3 Glacier Deep Archive is the lowest-cost storage class\nsuitable for long-term archival. The 12-hour retrieval window aligns with the retrieval capabilities of S3 Glacier\nDeep Archive, which has retrieval times ranging from 12 hours.\nTherefore, configuring an S3 Lifecycle rule to transition objects as follows:\n1. S3 Standard for the first 180 days (already configured).\n2. S3 Standard-IA after 180 days.\n3. S3 Glacier Instant Retrieval after 360 days.\n4. S3 Glacier Deep Archive after 5 years.\nThis solution adheres to all requirements of the question while minimizing storage costs.\nHere are some authoritative links for further research:\nAmazon S3 Storage Classes\nManaging your storage lifecycle",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has a large data workload that runs for 6 hours each day. The company cannot lose any data while the\nprocess is running. A solutions architect is designing an Amazon EMR cluster configuration to support this critical\ndata workload.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "B": "Here's a detailed justification:"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Here's a detailed justification:\nThe requirement is to run a 6-hour workload daily without data loss and in a cost-effective manner using\nAmazon EMR. A transient cluster is best suited for this because it automatically provisions, executes the\nworkload, and terminates after completion, minimizing idle time and associated costs.\nOption B suggests running the primary and core nodes on On-Demand Instances and the task nodes on Spot\nInstances. On-Demand Instances for the primary and core nodes provide predictable availability during the 6-\nhour workload. The primary node is critical for cluster management, and core nodes typically store data\n(HDFS). Losing these nodes during processing could lead to data loss and job failure. Thus, utilizing On-\nDemand Instances here reduces that risk.\nTask nodes perform compute tasks and do not store persistent data. Therefore, using Spot Instances for task\nnodes can significantly reduce costs. If a Spot Instance is interrupted, the task will be rescheduled on another\navailable instance without affecting the overall data integrity as data isn't stored persistently on these nodes.\nEMR is fault-tolerant and can handle task failures due to Spot Instance revocations.\nOptions A and D propose long-running clusters. Keeping the cluster alive even when it's not processing the\nworkload results in unnecessary expenses. Option C suggests using Spot Instances for core nodes, which\nstore data. Loss of core nodes leads to potential data loss, violating the critical requirement of the workload.\nIn summary, Option B provides the optimal balance between cost and reliability by leveraging On-Demand\nInstances for data-critical nodes (primary, core) and Spot Instances for compute-intensive tasks (task nodes),\nutilizing the benefits of transient clusters for cost optimization and ephemeral workloads.\nFurther Research:\nAmazon EMR Instance Fleets and Instance Groups:\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html\nAmazon EMR Pricing: https://aws.amazon.com/emr/pricing/\nAWS Spot Instances: https://aws.amazon.com/ec2/spot/",
    "links": [
      "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html",
      "https://aws.amazon.com/emr/pricing/",
      "https://aws.amazon.com/ec2/spot/"
    ]
  },
  {
    "question": "CertyIQ\nA company maintains an Amazon RDS database that maps users to cost centers. The company has accounts in an\norganization in AWS Organizations. The company needs a solution that will tag all resources that are created in a\nspecific AWS account in the organization. The solution must tag each resource with the cost center ID of the user\nwho created the resource.\nWhich solution will meet these requirements?",
    "options": {
      "B": "Here's why:",
      "D": "The"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Here's why:\nOption B leverages a real-time, event-driven approach using Lambda and EventBridge, which aligns perfectly\nwith the requirement to tag resources immediately after creation. AWS Lambda is a serverless compute\nservice that allows you to run code without provisioning or managing servers, making it ideal for on-demand\ntasks like tagging. Amazon EventBridge, a serverless event bus, can be configured to trigger Lambda\nfunctions based on events, such as the creation of an AWS resource.\nThe solution works as follows:\n1. CloudTrail Event: AWS CloudTrail logs API calls made in the AWS account. When a resource is\ncreated, CloudTrail captures this event.\n2. EventBridge Rule: An EventBridge rule is set up to listen for CloudTrail events related to resource\ncreation (e.g., CreateVolume, CreateInstance). When such an event occurs, the rule is triggered.\n3. Lambda Function: The EventBridge rule invokes an AWS Lambda function. This function receives the\nevent data, which includes the user's identity (via the AWS IAM user or role that initiated the API call)\nand the resource ARN (Amazon Resource Name).\n4. Cost Center Lookup: The Lambda function uses the user's identity to query the RDS database\n(mapping users to cost centers) to retrieve the correct cost center ID.\n5. Resource Tagging: The Lambda function then uses the AWS SDK (e.g., boto3 for Python) to tag the\ncreated resource with the CostCenter tag, setting its value to the retrieved cost center ID. The\nfunction uses the resource ARN from the CloudTrail event data to identify the specific resource to\ntag.\nThis approach ensures that resources are tagged immediately after creation with the appropriate cost center\nID based on the user who created them. The solution is fully automated and requires no manual intervention.\nOption A is incorrect because Service Control Policies (SCPs) primarily enforce permission boundaries across\naccounts in an organization and do not directly tag resources based on user information stored in a database.\nSCPs can require tags to be present, but not dynamically set them. Furthermore, applying an SCP to an\nexisting resource won't retroactively add tags based on user information.\nOption C is inefficient. Using a scheduled EventBridge rule to invoke a CloudFormation stack that then runs a\nLambda function adds unnecessary complexity. The scheduled invocation is not event-driven or real-time,\ntherefore it is more likely to cause delay in tagging.\nOption D is insufficient because it relies on a default value and only attempts to tag resources that are\nmissing the cost center tag. This approach is reactive rather than proactive and does not guarantee that all\nresources will be correctly tagged with the appropriate cost center from the beginning. It also means that a\nresource will initially be created with the wrong tag, which is then corrected later.\nAuthoritative Links:\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon EventBridge: https://aws.amazon.com/eventbridge/\nAWS CloudTrail: https://aws.amazon.com/cloudtrail/\nAWS Organizations: https://aws.amazon.com/organizations/",
    "links": [
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/eventbridge/",
      "https://aws.amazon.com/cloudtrail/",
      "https://aws.amazon.com/organizations/"
    ]
  },
  {
    "question": "CertyIQ\nA company recently migrated its web application to the AWS Cloud. The company uses an Amazon EC2 instance to\nrun multiple processes to host the application. The processes include an Apache web server that serves static\ncontent. The Apache web server makes requests to a PHP application that uses a local Redis server for user\nsessions.\nThe company wants to redesign the architecture to be highly available and to use AWS managed solutions.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "Here's a breakdown of why option D is the best solution for achieving high availability and using AWS\nmanaged services, along with justifications for excluding the other options:\nWhy Option D is Correct:\nOption D leverages several AWS managed services to achieve high availability, scalability, and resilience.\nAmazon CloudFront with Amazon S3: Serving static content from S3 behind CloudFront provides global\ncontent delivery, caching, and reduced load on the application servers. S3 offers high availability and\ndurability for static assets. https://aws.amazon.com/cloudfront/\nApplication Load Balancer (ALB): The ALB distributes traffic across multiple ECS Fargate tasks, ensuring\nhigh availability and enabling scaling based on demand. ALBs offer built-in health checks and automatic\nfailover. https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\nAmazon ECS with AWS Fargate: ECS Fargate allows running the PHP application in containers without\nmanaging the underlying EC2 instances. Fargate provides automatic scaling and resource management.\nhttps://aws.amazon.com/fargate/\nAmazon ElastiCache for Redis (Multi-AZ): Using ElastiCache for Redis with Multi-AZ ensures high availability\nof the session data. In case of a failure in one Availability Zone, Redis automatically fails over to another zone.\nhttps://aws.amazon.com/elasticache/redis/\nWhy Other Options Are Incorrect:\nOption A (Elastic Beanstalk): While Elastic Beanstalk can simplify deployment, using a single EC2 instance in\na public subnet with a public IP address negates high availability.\nOption B (Lambda): Hosting static content with Lambda is not cost-effective and it's not suitable for serving\nstatic web assets. Lambda functions are designed for stateless, event-driven computing and have execution\ntime limitations.\nOption C (EC2 and S3 with ElastiCache): Keeping backend code on a single EC2 instance doesn't address the\nhigh availability requirement for the application logic. The backend code must also be HA to benefit the most\nfrom HA Redis.\nIn summary, option D is the best answer because it delivers a highly available, scalable, and managed solution\nusing a combination of CloudFront, S3, ALB, ECS Fargate, and ElastiCache for Redis. The other options do not\nprovide the same level of availability and/or utilize less suitable AWS services.",
    "links": [
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
      "https://aws.amazon.com/fargate/",
      "https://aws.amazon.com/elasticache/redis/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a web application on Amazon EC2 instances in an Auto Scaling group that has a target group. The\ncompany designed the application to work with session affinity (sticky sessions) for a better user experience.\nThe application must be available publicly over the internet as an endpoint. A WAF must be applied to the endpoint\nfor additional security. Session affinity (sticky sessions) must be configured on the endpoint.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": {},
    "answer": "C",
    "explanation": "Let's break down why options C and E are the correct choices to satisfy the requirements:\nWhy C is correct: Create a public Application Load Balancer. Specify the application target group.\nAn Application Load Balancer (ALB) is the appropriate choice when routing HTTP/HTTPS traffic and requiring\napplication-level features like session affinity (sticky sessions). ALBs operate at Layer 7 of the OSI model,\nallowing them to inspect the content of the traffic and make routing decisions based on it.\nALBs can be public, meaning they have a public IP address and can accept traffic directly from the internet.\nALBs have built-in support for session affinity (sticky sessions), which ensures that a user's requests are\nconsistently routed to the same EC2 instance for the duration of their session.\nBy specifying the application's target group (which contains the EC2 instances), the ALB knows where to\nforward the incoming traffic.\nReference: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\nWhy E is correct: Create a web ACL in AWS WAF. Associate the web ACL with the endpoint.\nAWS WAF (Web Application Firewall) is a service that protects web applications from common web exploits\nand bots.\nA Web ACL (Access Control List) in AWS WAF defines the rules and conditions that determine how WAF\nshould inspect and filter incoming traffic.\nCritically, AWS WAF integrates directly with Application Load Balancers. By creating a web ACL and\nassociating it with the ALB, you apply WAF's protection to the web application, fulfilling the security\nrequirement.\nReference: https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html\nWhy A is incorrect: Create a public Network Load Balancer. Specify the application target group.\nNetwork Load Balancers (NLBs) operate at Layer 4 of the OSI model (TCP/UDP). They are designed for high-\nperformance, low-latency traffic and do not have the same application-level features as ALBs, such as session\naffinity. While NLBs can be public, they are not the best choice when needing sticky sessions and integrating\nwith WAF effectively.\nWhy B is incorrect: Create a Gateway Load Balancer. Specify the application target group.\nGateway Load Balancers (GWLBs) are designed to route traffic to virtual appliances, such as firewalls or\nintrusion detection systems. They are not intended for directly serving web application traffic.\nGWLBs are not directly associated with WAF.\nGWLBs do not handle sticky sessions.\nWhy D is incorrect: Create a second target group. Add Elastic IP addresses to the EC2 instances.\nCreating a second target group doesn't address the requirements of session affinity or WAF integration.\nTarget groups are merely containers for registered instances.\nWhile adding Elastic IP addresses to EC2 instances might seem to provide fixed addresses, it is not a scalable\nor recommended approach for public-facing applications. Load balancers are specifically designed to\ndistribute traffic across multiple instances, making Elastic IPs unnecessary and harder to manage. It also does\nnot provide any WAF protection.",
    "links": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a website that stores images of historical events. Website users need the ability to search and\nview images based on the year that the event in the image occurred. On average, users request each image only\nonce or twice a year. The company wants a highly available solution to store and deliver the images to users.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "D": "Store images in Amazon S3 Standard-Infrequent Access (S3 Standard-IA). Use S3",
      "A": "Given the infrequent access pattern,"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Store images in Amazon S3 Standard-Infrequent Access (S3 Standard-IA). Use S3\nStandard-IA to directly deliver images by using a static website.\nHere's why:\nCost-Effectiveness: The scenario specifies that images are accessed infrequently (once or twice a year). S3\nStandard-IA is designed for data that is accessed less frequently but requires rapid access when needed. It\noffers lower storage costs compared to S3 Standard, making it more cost-effective for this usage pattern.\nHigh Availability: Amazon S3, including S3 Standard-IA, provides high availability and durability, fulfilling the\ncompany's requirement. S3 offers 99.999999999% (11 9's) of data durability because it automatically creates\nand stores object copies across multiple Availability Zones.\nDirect Delivery: Using S3 to directly deliver images via a static website eliminates the need for web servers\n(like EC2 instances), reducing operational overhead and costs associated with server management and\nmaintenance. S3 can be configured to host static websites, allowing users to access images directly from the\nS3 bucket.\nAlternatives are Less Suitable:\nA (EBS on EC2): EBS is block storage, typically used for operating systems or databases on EC2 instances.\nHosting images directly on EBS attached to an EC2 instance would be more expensive and require managing\nthe EC2 instance.\nB (EFS on EC2): EFS is a file system suitable for shared storage. While highly available, it's more expensive\nthan S3 and requires managing EC2 instances to serve the images.\nC (S3 Standard): S3 Standard is more expensive than S3 Standard-IA. Given the infrequent access pattern,\nS3 Standard-IA is the more cost-effective choice without sacrificing availability.\nTherefore, storing images in S3 Standard-IA and directly delivering them as a static website provides a cost-\neffective, highly available, and easily manageable solution for the company's requirements.\nSupporting Links:\nAmazon S3 Storage Classes: https://aws.amazon.com/s3/storage-classes/\nHosting a Static Website on Amazon S3:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has multiple AWS accounts in an organization in AWS Organizations that different business units use.\nThe company has multiple offices around the world. The company needs to update security group rules to allow\nnew office CIDR ranges or to remove old CIDR ranges across the organization. The company wants to centralize\nthe management of security group rules to minimize the administrative overhead that updating CIDR ranges\nrequires.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the most cost-effective solution:\nOption B leverages AWS-managed services to centralize and streamline security group rule management\nacross multiple AWS accounts within an organization. The key is the customer-managed prefix list. A prefix\nlist is a managed collection of CIDR blocks. By creating a prefix list containing the company's office CIDR\nranges, the company can define these ranges in one central location.\nAWS Resource Access Manager (RAM) facilitates sharing this prefix list across all accounts within the AWS\nOrganizations setup. This eliminates the need to manually create and maintain the same CIDR range\ninformation in each account's security groups. This substantially reduces administrative overhead.\nUsing the shared prefix list within the security groups means that instead of specifying individual CIDR\nranges, the security group rules simply reference the prefix list. When a CIDR range needs to be updated (e.g.,\na new office opens or an old one closes), the company only needs to modify the prefix list in its central\nlocation. The change automatically propagates to all security groups that use the prefix list across the\norganization.\nCost-effectiveness stems from a few factors. Prefix lists and RAM are relatively inexpensive services. The\nprimary benefit is the massive reduction in administrative effort and potential for errors associated with\nmanually managing security group rules across multiple accounts.\nOther options are less efficient. Option A lacks centralized management, increasing administrative burden.\nOption C relies on AWS-managed prefix list which can't be updated. Option D is expensive since Firewall\nManager is not cost-effective for just security group management. Therefore, option B provides the best\nbalance of centralized management, automation, and cost-effectiveness.\nAuthoritative Links:\nAWS Resource Access Manager (RAM): https://aws.amazon.com/ram/\nVPC Prefix Lists: https://docs.aws.amazon.com/vpc/latest/userguide/working-with-prefix-lists.html\nSecurity Groups: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-groups.html",
    "links": [
      "https://aws.amazon.com/ram/",
      "https://docs.aws.amazon.com/vpc/latest/userguide/working-with-prefix-lists.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-groups.html"
    ]
  },
  {
    "question": "CertyIQ\nA company uses an on-premises network-attached storage (NAS) system to provide file shares to its high\nperformance computing (HPC) workloads. The company wants to migrate its latency-sensitive HPC workloads and\nits storage to the AWS Cloud. The company must be able to provide NFS and SMB multi-protocol access from the\nfile system.\nWhich solution will meet these requirements with the LEAST latency? (Choose two.)",
    "options": {},
    "answer": "A",
    "explanation": "The question focuses on migrating latency-sensitive HPC workloads and their storage to AWS while\nmaintaining NFS and SMB multi-protocol access with the least latency.\nOption A suggests deploying compute-optimized EC2 instances into a cluster placement group. This is a good\nstarting point as cluster placement groups minimize latency by placing instances within the same Availability\nZone on the same high-bandwidth network. This reduces the network distance between the compute\ninstances, crucial for latency-sensitive applications.\nOption E proposes attaching the EC2 instances to an Amazon FSx for NetApp ONTAP file system. FSx for\nNetApp ONTAP provides high performance and supports both NFS and SMB protocols, fulfilling the multi-\nprotocol requirement. Furthermore, it offers low-latency access to data, crucial for HPC workloads. FSx for\nNetApp ONTAP also offers advanced data management features.\nWhile Option C (FSx for Lustre) is known for high-performance computing, especially with large datasets and\nparallel workloads, FSx for NetApp ONTAP can also be suitable for HPC needs, especially when the workload\nrequires multi-protocol access.\nOption D (FSx for OpenZFS) also supports NFS and SMB, but might not always provide the lowest latency\ncompared to ONTAP and isn't typically the go-to choice when the focus is explicitly on minimizing latency in a\nmulti-protocol environment.\nOption B (Partition placement group) is less relevant here. While it enhances fault tolerance, it doesn't\ninherently minimize latency like a cluster placement group. The primary goal is to reduce latency for the\nperformance sensitive workload.\nTherefore, combining cluster placement groups for the EC2 instances (A) with FSx for NetApp ONTAP (E)\nprovides the lowest latency solution that meets the multi-protocol access requirement. The placement group\nminimizes latency between compute nodes, and FSx for NetApp ONTAP delivers low-latency shared storage.\nSupporting links:\nAmazon EC2 Placement Groups: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-\ngroups.html\nAmazon FSx for NetApp ONTAP: https://aws.amazon.com/fsx/netapp-ontap/",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-",
      "https://aws.amazon.com/fsx/netapp-ontap/"
    ]
  },
  {
    "question": "CertyIQ\nA company is relocating its data center and wants to securely transfer 50 TB of data to AWS within 2 weeks. The\nexisting data center has a Site-to-Site VPN connection to AWS that is 90% utilized.\nWhich AWS service should a solutions architect use to meet these requirements?",
    "options": {
      "C": "AWS Snowball Edge Storage Optimized. Here's a detailed justification:"
    },
    "answer": "C",
    "explanation": "The correct answer is C. AWS Snowball Edge Storage Optimized. Here's a detailed justification:\nThe primary challenge is transferring a large amount of data (50 TB) quickly and securely to AWS within a\nlimited timeframe (2 weeks) while minimizing the impact on the existing, heavily utilized (90%) Site-to-Site\nVPN connection.\nAWS Snowball Edge Storage Optimized devices are designed for transferring large datasets to AWS when\nnetwork bandwidth is limited or cost-prohibitive. They provide a secure and ruggedized physical appliance\nthat can be shipped directly to the data center. This bypasses the need to transfer the data over the\ncongested VPN connection, addressing the core constraint. Once the data is loaded onto the Snowball Edge\ndevice, it's shipped back to AWS, where the data is loaded into the desired AWS service (e.g., S3).\nhttps://aws.amazon.com/snowball/\nAWS DataSync with a VPC endpoint is a good option for ongoing data synchronization, but it relies on\nnetwork bandwidth. Given the existing VPN is 90% utilized, using DataSync would be slow and potentially\ndisrupt other services. It doesn't solve the initial large data migration problem within the timeframe.\nAWS Direct Connect establishes a dedicated network connection from the data center to AWS. While ideal\nfor ongoing, high-bandwidth needs, it takes time to provision and implement. It would likely not be feasible\nwithin the 2-week timeframe for the initial data transfer. Furthermore, it's overkill for a one-time migration\nand more suited to a long-term hybrid cloud strategy. https://aws.amazon.com/directconnect/\nAWS Storage Gateway connects on-premises applications to AWS storage. While it's useful for hybrid cloud\nscenarios, it relies on the existing network connection for data transfer. Therefore, it would be constrained by\nthe already saturated VPN connection.\nTherefore, AWS Snowball Edge is the best option because it circumvents the network limitations, offers a\nsecure transfer method, and is specifically designed for large data migrations in scenarios where network\nbandwidth is a constraint. It allows for an offline data transfer.",
    "links": [
      "https://aws.amazon.com/snowball/",
      "https://aws.amazon.com/directconnect/"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts an application on Amazon EC2 On-Demand Instances in an Auto Scaling group. Application peak\nhours occur at the same time each day. Application users report slow application performance at the start of peak\nhours. The application performs normally 2-3 hours after peak hours begin. The company wants to ensure that the\napplication works properly at the start of peak hours.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D, configuring a scheduled scaling policy, is the best solution and\nwhy the other options are less suitable:\nThe core problem is predictable performance degradation during peak hours. This predictability is key. The\ngoal is to ensure proper application function at the start of these peak hours.\nOption A, configuring an Application Load Balancer (ALB), helps distribute traffic evenly across instances.\nHowever, an ALB alone won't address the underlying problem of insufficient capacity at the beginning of peak\nhours. While a properly configured ALB is essential for a well-architected application, it doesn't proactively\nincrease resources to handle the anticipated surge in demand. It's a component of the solution, but not the\nprimary fix.\nOptions B and C, using dynamic scaling policies based on memory or CPU utilization, react after the\nperformance degradation has already begun. Dynamic scaling waits for the metrics to exceed a certain\nthreshold (high CPU/memory) before launching new instances. This inherent delay means users will still\nexperience slow performance at the start of peak hours while the Auto Scaling group reacts to the increased\nload. While reactive scaling is valuable for unexpected surges, it doesn't solve a predictable, recurring issue.\nDynamic scaling policies are best used in conjunction with scheduled scaling, or for unexpected traffic spikes.\nOption D, configuring a scheduled scaling policy, is the most effective solution. Because the peak hours are\npredictable, a scheduled policy can be configured to proactively launch additional EC2 instances before the\npeak traffic arrives. This ensures that sufficient capacity is available right from the start, eliminating the\nperformance degradation reported by users. The Auto Scaling group will automatically increase the desired\nand minimum capacity at a specific time, adding instances before users experience problems. This proactive\napproach directly addresses the stated requirement.\nIn summary, scheduled scaling anticipates the demand, proactively adds capacity, and prevents performance\ndegradation at the start of peak hours. ALBs provide traffic distribution, and dynamic scaling reacts to\nunforeseen changes, but neither effectively handles a consistently predictable surge in demand as well as\nscheduled scaling.\nAuthoritative Links:\nAWS Auto Scaling  Scheduled Scaling:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\nAWS Auto Scaling  Dynamic Scaling: https://docs.aws.amazon.com/autoscaling/ec2/userguide/dynamic-\nscaling.html\nApplication Load Balancer:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
    "links": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/dynamic-",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs applications on AWS that connect to the company's Amazon RDS database. The applications scale\non weekends and at peak times of the year. The company wants to scale the database more effectively for its\napplications that connect to the database.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B: Use Amazon RDS Proxy with a target group for the database and change the\napplications to use the RDS Proxy endpoint.\nRDS Proxy is a fully managed, highly available database proxy that sits between your applications and your\nRDS databases. It manages database connections efficiently, which is crucial for applications that experience\nscaling issues due to increased load. By pooling and sharing database connections, RDS Proxy reduces the\noverhead associated with establishing new connections each time an application needs to interact with the\ndatabase. This enhances database efficiency and lowers operational costs.\nBy using RDS Proxy, the company can effectively manage database connections, minimizing connection\noverhead and improving overall performance during peak times. This solution requires minimal operational\noverhead because RDS Proxy is a managed service, meaning AWS handles the infrastructure management\nand patching. The only configuration needed is pointing applications to the RDS Proxy endpoint, making it the\nleast operationally intensive option.\nOption A is incorrect because DynamoDB is a NoSQL database, and migrating applications to use DynamoDB\nwould require significant code changes and is not a simple scaling solution for an existing RDS database.\nOption C requires managing and maintaining a custom proxy on EC2, which introduces significant operational\noverhead. Option D, using a Lambda function for connection pooling, is not a recommended solution because\nLambda functions have execution time limits and are better suited for event-driven architectures, not as\ncontinuous database connection proxies. Additionally, it adds complexity compared to a managed RDS Proxy.\nIn summary, RDS Proxy offers a straightforward, fully managed solution to address the company's database\nscaling needs with minimal operational overhead.\nhttps://aws.amazon.com/rds/proxy/https://docs.aws.amazon.com/rds/proxy/latest/userguide/what-is.html",
    "links": [
      "https://aws.amazon.com/rds/proxy/https://docs.aws.amazon.com/rds/proxy/latest/userguide/what-is.html"
    ]
  },
  {
    "question": "CertyIQ\nA company uses AWS Cost Explorer to monitor its AWS costs. The company notices that Amazon Elastic Block\nStore (Amazon EBS) storage and snapshot costs increase every month. However, the company does not purchase\nadditional EBS storage every month. The company wants to optimize monthly costs for its current storage usage.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the best solution, along with supporting concepts and links:\nThe problem highlights escalating EBS storage and snapshot costs, despite no apparent increase in storage\npurchases. This strongly indicates inefficiencies in snapshot management. The goal is to optimize costs with\nminimal operational overhead.\nOption A and B, while potentially helpful in identifying over-provisioned volumes, involve significant overhead.\nMonitoring EBS volume utilization using CloudWatch logs (A) or custom scripts (B) requires configuration,\nmaintenance, and interpretation, and they don't directly address the root cause of increasing snapshot costs.\nElastic Volumes allow you to reduce the size of the EBS volumes but doesn't solve the issue with snapshot\ncosts.\nOption C focuses on deleting expired/unused snapshots, which is a good practice, but it's a manual process.\nRegularly identifying and deleting snapshots is time-consuming and prone to error. It doesn't establish an\nautomated, policy-driven approach. It might be useful if you don't want to implement automation.\nOption D is the most effective and efficient approach. By deleting nonessential snapshots and implementing\nAmazon Data Lifecycle Manager (DLM), the company can automate snapshot creation, retention, and deletion\nbased on defined policies. DLM significantly reduces operational overhead by handling the snapshot lifecycle\nautomatically. This ensures that snapshots are created regularly for data protection but are also purged when\nthey are no longer needed, preventing unnecessary cost accumulation. DLM directly addresses the snapshot\ncost issue in a scalable and sustainable manner. It also addresses potential compliance requirements for data\nbackup and retention. Nonessential snapshots take up storage and are potentially unassociated.\nHere are some authoritative links for further research:\nAmazon Data Lifecycle Manager (DLM): https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-\nlifecycle.html\nAmazon EBS Snapshots: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html\nAmazon EBS Elastic Volumes: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-expand-\nvolume.html",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-expand-"
    ]
  },
  {
    "question": "CertyIQ\nA company is developing a new application on AWS. The application consists of an Amazon Elastic Container\nService (Amazon ECS) cluster, an Amazon S3 bucket that contains assets for the application, and an Amazon RDS\nfor MySQL database that contains the dataset for the application. The dataset contains sensitive information. The\ncompany wants to ensure that only the ECS cluster can access the data in the RDS for MySQL database and the\ndata in the S3 bucket.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D because it provides a secure and controlled access mechanism for the ECS cluster to\naccess both the RDS for MySQL database and the S3 bucket without exposing them to the public internet or\nother unauthorized entities.\nHere's a breakdown of why option D is correct:\nRDS Access Control: Creating a VPC endpoint for RDS ensures that traffic to RDS remains within the AWS\nnetwork and doesn't traverse the public internet. Restricting the RDS security group to only allow traffic from\nthe subnets where the ECS tasks are running isolates the database access solely to the authorized ECS\ncluster. This follows the principle of least privilege and prevents other resources from accessing the\ndatabase.\nS3 Access Control: Creating a VPC endpoint for S3 also keeps S3 traffic within the AWS network. By\nupdating the S3 bucket policy to only allow access from the S3 VPC endpoint, you restrict access to the\nbucket solely through the VPC and, indirectly, only from resources authorized to use that VPC endpoint (in\nthis case, the ECS cluster).\nWhy other options are incorrect:\nOption A: While KMS can encrypt data at rest, it doesn't control network access. Even with encryption, anyone\nwith network access and the necessary KMS permissions could still attempt to access the data. Relying solely\non KMS for access control is insufficient. Also, encrypting using KMS only solves data at rest.\nOption B: AWS managed keys are not customizable in terms of policies. Therefore, one cannot configure the\nS3 bucket policy to specify the ECS task execution role.\nOption C: Using only an S3 bucket policy to restrict access to the ECS task execution role is not as secure as\nusing a VPC endpoint. The bucket policy can be bypassed if someone gains access to the ECS task execution\nrole's credentials outside the controlled network environment.\nKey Concepts and Links:\nVPC Endpoints: VPC endpoints allow you to privately connect your VPC to supported AWS services without\nrequiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\nSecurity Groups: Security groups act as virtual firewalls for your EC2 instances and other AWS resources,\ncontrolling inbound and outbound traffic.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\nS3 Bucket Policies: S3 bucket policies are access control policies that you can attach to S3 buckets to grant\nor deny permissions to users or other AWS accounts.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-policies.html\nPrinciple of Least Privilege: Granting only the minimum necessary permissions to perform a task.",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-policies.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has a web application that runs on premises. The application experiences latency issues during peak\nhours. The latency issues occur twice each month. At the start of a latency issue, the application's CPU utilization\nimmediately increases to 10 times its normal amount.\nThe company wants to migrate the application to AWS to improve latency. The company also wants to scale the\napplication automatically when application demand increases. The company will use AWS Elastic Beanstalk for\napplication deployment.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the most suitable solution, considering the scenario's\nrequirements:\nThe problem describes a web application experiencing significant, but infrequent, latency spikes due to CPU\nutilization increasing tenfold during peak hours. The company needs to migrate the application to AWS,\nimprove latency, and implement automatic scaling to handle demand surges. Elastic Beanstalk is chosen as\nthe deployment platform.\nOption D is the most effective because it combines burstable performance instances in unlimited mode with\nscaling based on predictive metrics. Burstable performance instances (like those in the T family) are designed\nfor workloads with average CPU utilization interspersed with occasional bursts. Unlimited mode allows these\ninstances to burst beyond their baseline CPU for as long as required, at the cost of accruing CPU credits or\ncharges. This is ideal for handling the sudden, drastic increase in CPU utilization.\nScaling based on predictive metrics utilizes AWS's forecasting services to anticipate demand. Instead of\nreacting to immediate CPU spikes (reactive scaling), predictive scaling proactively adds or removes instances\nbefore the anticipated spike, thereby mitigating latency issues before they occur. This is crucial, as the\ncompany knows when to expect the load.\nOptions A, B, and C are less effective. While using requests as a scaling trigger (Option A & B) is valid, it's\nreactive. The application will already be experiencing latency before the scaling action takes effect. Compute\noptimized instances (Option B & C) are designed for sustained high CPU usage, not infrequent bursts.\nScheduling scaling (Option C) might work, but it requires precise prediction and is less flexible than predictive\nscaling if the spikes shift slightly in time. Also, if the spikes are actually unpredictable, scheduled scaling is\nnot a proper solution.\nTherefore, using burstable performance instances in unlimited mode combined with scaling based on\npredictive metrics allows the application to handle the bursty CPU demand efficiently and proactively,\nminimizing latency and meeting the company's requirements.\nRelevant Resources:\nAmazon EC2 Instance Types: https://aws.amazon.com/ec2/instance-types/\nElastic Beanstalk Auto Scaling: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-\nfeatures.managing.as.html\nAmazon EC2 CPU Credits and Baseline Utilization:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html\nPredictive Scaling for EC2: https://aws.amazon.com/blogs/compute/introducing-predictive-scaling-for-\namazon-ec2/",
    "links": [
      "https://aws.amazon.com/ec2/instance-types/",
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html",
      "https://aws.amazon.com/blogs/compute/introducing-predictive-scaling-for-"
    ]
  },
  {
    "question": "CertyIQ\nA company has customers located across the world. The company wants to use automation to secure its systems\nand network infrastructure. The company's security team must be able to track and audit all incremental changes\nto the infrastructure.\nWhich solution will meet these requirements?",
    "options": {
      "B": "AWS CloudFormation allows you to define and provision your infrastructure as code,"
    },
    "answer": "B",
    "explanation": "The correct answer is B. AWS CloudFormation allows you to define and provision your infrastructure as code,\nenabling automation and consistency across deployments. By defining your infrastructure in CloudFormation\ntemplates, you can version control them, ensuring all incremental changes are tracked. This addresses the\nrequirement for automated infrastructure setup and tracking of modifications.\nAWS Config continuously monitors and records the configuration of your AWS resources. It allows you to\ntrack changes to resource configurations and evaluate them against desired states, providing an audit trail of\nall modifications. This meets the requirement for the security team to track and audit all incremental changes\nto the infrastructure.\nOption A is incorrect because while AWS Organizations is useful for managing multiple AWS accounts, it\ndoesn't directly provision infrastructure or track individual configuration changes. Option C is incorrect\nbecause AWS Service Catalog allows organizations to create and manage catalogs of IT services that are\napproved for use, but it doesn't inherently track configuration changes like AWS Config does. Option D is\nincorrect because, while CloudFormation is useful for defining and provisioning resources, Service Catalog\ndoesn't provide the detailed tracking of incremental infrastructure changes provided by AWS Config.\nCloudFormation provisions the infrastructure, providing the automation component, while Config provides\ncontinuous monitoring of changes for auditing, fulfilling both requirements. This makes option B the most\ncomplete and correct answer.\nFurther Reading:\nAWS CloudFormation: https://aws.amazon.com/cloudformation/\nAWS Config: https://aws.amazon.com/config/",
    "links": [
      "https://aws.amazon.com/cloudformation/",
      "https://aws.amazon.com/config/"
    ]
  },
  {
    "question": "CertyIQ\nA startup company is hosting a website for its customers on an Amazon EC2 instance. The website consists of a\nstateless Python application and a MySQL database. The website serves only a small amount of traffic. The\ncompany is concerned about the reliability of the instance and needs to migrate to a highly available architecture.\nThe company cannot modify the application code.\nWhich combination of actions should a solutions architect take to achieve high availability for the website?\n(Choose two.)",
    "options": {
      "C": "Availability Zones do not require individual IGWs. This"
    },
    "answer": "B",
    "explanation": "The correct answer is BE. Here's a detailed justification:\nB - Migrate the database to an Amazon RDS for MySQL Multi-AZ DB instance: Moving the MySQL database\nto Amazon RDS (Relational Database Service) and enabling Multi-AZ (Multi-Availability Zone) is crucial for\nhigh availability. RDS Multi-AZ automatically provisions and maintains a synchronous standby replica of the\ndatabase in a different Availability Zone. If the primary database instance fails, RDS automatically fails over to\nthe standby instance, minimizing downtime. This eliminates the single point of failure associated with a\ndatabase running on a single EC2 instance. This ensures the database component of the website is highly\navailable.\nE - Create an Application Load Balancer to distribute traffic to an Auto Scaling group of EC2 instances that\nare distributed across two Availability Zones: Creating an Application Load Balancer (ALB) and an Auto\nScaling group (ASG) for the stateless Python application ensures high availability and scalability. The ALB\ndistributes incoming traffic across multiple EC2 instances running the application in different Availability\nZones. This prevents a single point of failure for the web application layer. The Auto Scaling group\ndynamically adjusts the number of EC2 instances based on traffic demand. If an instance fails in one\nAvailability Zone, Auto Scaling automatically launches a new instance in a healthy Availability Zone.\nWhy other options are incorrect:\nA - Provision an internet gateway in each Availability Zone in use: Internet Gateways (IGWs) are regional\nresources, not zonal. You only need one IGW per VPC. Availability Zones do not require individual IGWs. This\noption is incorrect since Internet Gateway is used to provide internet access for VPCs and this does not\nimprove the high availability of the EC2 instance.\nC - Migrate the database to Amazon DynamoDB, and enable DynamoDB auto scaling: While DynamoDB\noffers high availability and scalability, it requires application code changes to interact with a NoSQL database.\nThe question explicitly states that the company cannot modify the application code, making this option\nunsuitable. The application is currently using MySQL so it is not designed to connect to a NoSQL database\nD - Use AWS DataSync to synchronize the database data across multiple EC2 instances: While AWS\nDataSync is useful for data transfer, it is not a solution for high availability of a MySQL database. DataSync\ndoes not provide automatic failover capabilities, which are essential for achieving high availability. RDS with\nMulti-AZ handles failover automatically, ensuring minimal downtime.\nRelevant AWS documentation:\nAmazon RDS Multi-AZ: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\nApplication Load Balancer:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\nAuto Scaling: https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is moving its data and applications to AWS during a multiyear migration project. The company wants to\nsecurely access data on Amazon S3 from the company's AWS Region and from the company's on-premises\nlocation. The data must not traverse the internet. The company has established an AWS Direct Connect connection\nbetween its Region and its on-premises location.\nWhich solution will meet these requirements?",
    "options": {
      "C": "They do not provide access from on-"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Create interface endpoints for Amazon S3. Use the interface endpoints to securely\naccess the data from the Region and the on-premises location.\nHere's a detailed justification:\nRequirement Analysis: The core requirements are secure access to S3 from both an AWS Region and on-\npremises, and data must not traverse the internet, leveraging the existing AWS Direct Connect.\nInterface Endpoints and Private Connectivity: Interface endpoints, powered by AWS PrivateLink, enable\nprivate connectivity to AWS services like S3 without exposing traffic to the public internet. They create a\nnetwork interface within your VPC that acts as an entry point to S3. Traffic between your VPC (and connected\non-premises location via Direct Connect) and S3 remains within the AWS network.\nDirect Connect Integration: The AWS Direct Connect connection provides a private, dedicated network\nconnection between the on-premises environment and the AWS Region. By using interface endpoints, the on-\npremises location can access S3 through Direct Connect, maintaining the required security and avoiding the\ninternet.\nWhy not Gateway Endpoints (Option A)? Gateway endpoints are designed specifically for S3 and DynamoDB,\nbut they only support access from within the same AWS Region's VPC. They do not provide access from on-\npremises locations via Direct Connect.\nWhy not Transit Gateway (Option B)? While Transit Gateway facilitates connectivity between multiple VPCs\nand on-premises, it doesn't directly provide a secure, private path to S3 that avoids the internet. You would\nstill need a mechanism like interface endpoints for the actual S3 access. The transit gateway itself isn't\ndirectly involved in the secure S3 access.\nWhy not AWS KMS (Option D)? AWS KMS is used for encrypting data at rest and in transit. While encryption\nis essential for data security, it doesn't address the requirement of avoiding the internet for data access. KMS\nkeys protect the data itself but do not change the network path.\nIn summary: Interface endpoints offer the only solution that satisfies both the security requirement (no\ninternet access) and the connectivity requirement (access from both the AWS Region and on-premises via\nDirect Connect).\nAuthoritative Links:\nAWS PrivateLink: https://aws.amazon.com/privatelink/\nAccessing S3 Through Interface Endpoints: https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-\nendpoints-s3.html\nAWS Direct Connect: https://aws.amazon.com/directconnect/",
    "links": [
      "https://aws.amazon.com/privatelink/",
      "https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-",
      "https://aws.amazon.com/directconnect/"
    ]
  },
  {
    "question": "CertyIQ\nA company created a new organization in AWS Organizations. The organization has multiple accounts for the\ncompany's development teams. The development team members use AWS IAM Identity Center (AWS Single Sign-\nOn) to access the accounts. For each of the company's applications, the development teams must use a predefined\napplication name to tag resources that are created.\nA solutions architect needs to design a solution that gives the development team the ability to create resources\nonly if the application name tag has an approved value.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D: Create a tag policy in Organizations that has a list of allowed application names.\nHere's why:\nAWS Organizations provides centralized governance and management of multiple AWS accounts. Tag\npolicies, a feature within Organizations, allow administrators to enforce tagging standards across all accounts\nwithin an organization or specific organizational units (OUs). By creating a tag policy with a list of allowed\napplication names, you directly address the requirement of ensuring that resources are only created with\napproved application name tag values.\nWhen a tag policy is in effect, AWS will deny the creation of a resource if it lacks the required tag or if the\ntag's value doesn't match an approved value defined in the policy. This preventative approach is crucial for\nmaintaining consistent tagging, simplifying resource management, and ensuring compliance. The tag policy\napplies to all accounts governed by the organization, guaranteeing that the development teams adhere to the\ndefined standards regardless of the account they're working in.\nOption A is incorrect because while IAM policies can enforce tagging requirements, applying this directly\nthrough IAM groups is less scalable and harder to centrally manage across multiple accounts in an AWS\nOrganization. Each account would need similar, yet possibly diverging, IAM policies. Organizations' tag\npolicies provide a centralized point of control.\nOption B is incorrect because a cross-account role with a Deny policy wouldn't prevent the resource creation\ninitially. It would primarily act after resource creation which isn't ideal. It would also require complex\nimplementation across accounts and not be easily managed.\nOption C is incorrect because AWS Resource Groups helps in organizing and managing resources based on\ntags, but it doesn't prevent the creation of resources without the correct tags. Resource Groups are reactive;\nthey identify issues after resource creation, whereas the requirement calls for a proactive solution to prevent\nnon-compliant resource creation.\nIn summary, tag policies in AWS Organizations are the most suitable solution because they provide a\ncentralized, proactive, and scalable way to enforce mandatory tags with predefined allowed values across\nmultiple accounts, meeting the company's requirements.\nSupporting documentation:\nAWS Organizations Tag Policies:\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies.html\nTagging Best Practices: https://aws.amazon.com/blogs/aws/aws-tagging-strategies/",
    "links": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies.html",
      "https://aws.amazon.com/blogs/aws/aws-tagging-strategies/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs its databases on Amazon RDS for PostgreSQL. The company wants a secure solution to manage\nthe master user password by rotating the password every 30 days.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C: Integrate AWS Secrets Manager with Amazon RDS for PostgreSQL to automate\npassword rotation. Here's why:\nAWS Secrets Manager is specifically designed for securely storing and managing secrets like database\ncredentials, API keys, and other sensitive information. Its built-in rotation feature allows automatic password\nrotation for RDS databases, including PostgreSQL, with minimal operational overhead. Secrets Manager\nhandles the entire rotation process: it creates a new password, validates it, updates the database, and then\nstores the new secret.\nOption A is less desirable because it requires developing and maintaining a custom Lambda function and\nEventBridge rule. This adds operational overhead for development, testing, and maintenance. While possible,\nit's more complex than using a dedicated secrets management service.\nOption B, using the AWS CLI modify-db-instance command, only allows manual password changes. It does not\nautomate the rotation process, requiring manual intervention every 30 days, which defeats the purpose of\nautomation and increases operational overhead.\nOption D, using AWS Systems Manager Parameter Store, while capable of storing secrets, is primarily\ndesigned for storing configuration data. It lacks the built-in rotation features of Secrets Manager, requiring\ncustom code or scripting for rotation, which again increases operational overhead. Secrets Manager provides\na purpose-built solution for secrets management including rotation, auditing and access control. Parameter\nStore is more appropriate for storing non-sensitive configuration data.\nTherefore, integrating AWS Secrets Manager provides the most secure and least operationally intensive\nsolution for automating RDS PostgreSQL password rotation.\nReferences:\nAWS Secrets Manager: https://aws.amazon.com/secrets-manager/\nRotating Amazon RDS, Amazon Redshift, and Amazon DocumentDB secrets automatically with AWS Secrets\nManager: https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html",
    "links": [
      "https://aws.amazon.com/secrets-manager/",
      "https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html"
    ]
  },
  {
    "question": "CertyIQ\nA company performs tests on an application that uses an Amazon DynamoDB table. The tests run for 4 hours once\na week. The company knows how many read and write operations the application performs to the table each\nsecond during the tests. The company does not currently use DynamoDB for any other use case. A solutions\narchitect needs to optimize the costs for the table.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B: Choose provisioned mode. Update the read and write capacity units appropriately.\nHere's why:\nThe scenario describes predictable, recurring usage patterns. The company knows the read/write operations\nper second during the 4-hour weekly tests. This predictability is the key indicator that provisioned capacity is\na better fit than on-demand.\nProvisioned Capacity: Allows you to specify the read and write capacity units (RCUs and WCUs) required for\nyour application. You pay for the capacity you provision. This is cost-effective when your workload is\npredictable. Since the company knows its read and write needs during the tests, they can accurately provision\ncapacity, avoiding the higher per-request cost of on-demand.\nOn-Demand Capacity: Charges based on actual read and write requests, without the need to pre-provision\ncapacity. It's suitable for unpredictable workloads. While convenient, it's generally more expensive for\nconsistent workloads.\nReserved Capacity: Offers discounts for committing to provisioned capacity for 1 or 3 years. While attractive\nfor cost savings, it's only beneficial if the capacity utilization is consistently high. Given that the DynamoDB\ntable is only used for 4 hours a week, reserved capacity would likely lead to significant underutilization of the\nreserved capacity, making it less cost-effective.\nAnswer A is incorrect because even though you can \"update the read and write capacity units appropriately\",\non-demand capacity is not the ideal option for a predictable workload that occurs every week. Provisioned\nmode is better.\nAnswers C and D are incorrect because reserved capacity is beneficial when the capacity utilization is high,\nand the DynamoDB table is only used for a short amount of time each week. The reserved capacity would be\nunderutilized.\nBy choosing provisioned mode and setting the RCUs and WCUs based on the known requirements during the\n4-hour test, the company can optimize costs by avoiding the on-demand per-request pricing and the\nunderutilization that would occur when purchasing DynamoDB reserved capacity.\nRelevant Documentation:\nDynamoDB On-Demand vs. Provisioned Capacity: Costs & Use Cases | Chaos Genius\nChoosing Between Provisioned and On-Demand Capacity in AWS DynamoDB - Scalegrid",
    "links": []
  },
  {
    "question": "CertyIQ\nA company runs its applications on Amazon EC2 instances. The company performs periodic financial assessments\nof its AWS costs. The company recently identified unusual spending.\nThe company needs a solution to prevent unusual spending. The solution must monitor costs and notify\nresponsible stakeholders in the event of unusual spending.\nWhich solution will meet these requirements?",
    "options": {
      "B": "Create an AWS Cost Anomaly Detection monitor in the AWS Billing and Cost"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Create an AWS Cost Anomaly Detection monitor in the AWS Billing and Cost\nManagement console.\nHere's why: AWS Cost Anomaly Detection is specifically designed to identify unusual spending patterns in\nyour AWS costs. It uses machine learning algorithms to learn your typical cost behavior and automatically\ndetects anomalies. When an anomaly is detected, it sends notifications to designated stakeholders, enabling\nprompt action to investigate and mitigate unexpected costs. This directly addresses the requirement of\nmonitoring costs and notifying stakeholders of unusual spending.\nOption A, using an AWS Budgets template to create a zero spend budget, while useful for setting hard limits,\nis not designed for detecting unusual spending. A zero spend budget would simply prevent any spending,\nwhich is not the desired outcome (the company wants to monitor and be notified of unusual spending).\n[https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html]\nOption C, creating AWS Pricing Calculator estimates, is helpful for initial cost planning, but it's a static\ncalculation. It doesn't continuously monitor actual spending or detect anomalies based on historical cost\npatterns. It only provides a point-in-time estimate. [https://calculator.aws/]\nOption D, using Amazon CloudWatch to monitor costs, is not a dedicated cost monitoring tool. While\nCloudWatch can monitor various AWS metrics, including billing metrics, it requires manual configuration of\nalarms and thresholds based on cost data. It doesn't automatically learn cost patterns or detect anomalies\nlike Cost Anomaly Detection does. Manually setting thresholds is more cumbersome and less effective at\nidentifying unexpected increases in cost patterns compared to Cost Anomaly Detection.\n[https://aws.amazon.com/cloudwatch/]\nCost Anomaly Detection provides an automated and proactive approach to identify and address unusual\nspending, aligning perfectly with the company's requirements. [https://aws.amazon.com/aws-cost-\nmanagement/aws-cost-anomaly-detection/] It also provides customization for anomaly detection and\nnotification preferences.",
    "links": [
      "https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html]",
      "https://calculator.aws/]",
      "https://aws.amazon.com/cloudwatch/]",
      "https://aws.amazon.com/aws-cost-"
    ]
  },
  {
    "question": "CertyIQ\nA marketing company receives a large amount of new clickstream data in Amazon S3 from a marketing campaign.\nThe company needs to analyze the clickstream data in Amazon S3 quickly. Then the company needs to determine\nwhether to process the data further in the data pipeline.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "B": "Here's a detailed justification:"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Here's a detailed justification:\nThe scenario requires quickly analyzing clickstream data in S3 with minimal operational overhead to\ndetermine if further processing is needed. Option B, utilizing AWS Glue and Amazon Athena, offers the most\nefficient and cost-effective approach. AWS Glue crawler automatically infers the schema from the data\nstored in S3, catalogs the data, and creates metadata tables in the AWS Glue Data Catalog. This eliminates\nthe need for manual schema definition or table creation, reducing operational overhead. Athena can then\ndirectly query the data in S3 using SQL.\nOption A, using a Spark catalog, is less optimal. While AWS Glue can be used with Spark, configuring Spark\njobs specifically for this quick analysis introduces more complexity. Setting up and managing Spark jobs\ninvolves higher operational overhead than using Athena.\nOption C, using a Hive metastore with Amazon EMR, is an even heavier approach. EMR clusters require\nprovisioning and management, adding significant operational complexity and cost compared to the serverless\nAthena. It also requires maintaining and managing the Hive metastore.\nOption D, using AWS Glue crawler and Kinesis Data Analytics, is not the right choice because Kinesis Data\nAnalytics is typically used for real-time stream processing. While it supports querying data, it's designed for\ncontinuous data streams, not for analyzing a static data set residing in S3. Also, using SQL in Kinesis Data\nAnalytics is more complex and doesn't fit the use case of quickly analyzing data in S3.\nAthena is serverless, meaning there are no servers to manage, scale, or provision. You only pay for the queries\nyou run. Coupled with Glue's automatic schema discovery, this provides the lowest operational overhead for\nquickly querying the clickstream data to determine whether further processing is necessary.\nTherefore, AWS Glue Crawler + Amazon Athena strikes the best balance of speed, minimal management, and\ncost-effectiveness for quickly analyzing S3 data and determining whether it warrants further data pipeline\nprocessing.\nRelevant links:\nAWS Glue: https://aws.amazon.com/glue/\nAmazon Athena: https://aws.amazon.com/athena/",
    "links": [
      "https://aws.amazon.com/glue/",
      "https://aws.amazon.com/athena/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an SMB file server in its data center. The file server stores large files that the company frequently\naccesses for up to 7 days after the file creation date. After 7 days, the company needs to be able to access the\nfiles with a maximum retrieval time of 24 hours.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the most suitable solution, along with supporting concepts\nand links:\nThe primary requirement is to provide fast access to files for 7 days and then transition them to a slower,\ncheaper storage tier with a retrieval time of less than 24 hours. Let's analyze the options:\nOption A: Using AWS DataSync alone doesn't address the need for immediate access for the first 7 days.\nDataSync primarily focuses on data transfer, not tiered storage with lifecycle management.\nOption B: This option effectively addresses both requirements. Amazon S3 File Gateway allows on-premises\napplications (like the SMB server) to seamlessly store and access data in Amazon S3. It acts as a local cache\nfor frequently accessed files, providing low-latency access. Critically, the S3 Lifecycle policy can\nautomatically transition older data (older than 7 days) to S3 Glacier Deep Archive. Glacier Deep Archive offers\nthe lowest storage cost and meets the requirement of retrieval within 24 hours.\nOption C: While Amazon FSx File Gateway could provide file system access in AWS, it is not designed to\nextend on-premise storage capacity in the way S3 File Gateway does. Additionally, an S3 Lifecycle policy is\nnot directly compatible with FSx. It manages objects in S3 buckets.\nOption D: Requiring each user to access S3 directly bypasses the existing SMB server infrastructure,\nintroducing complexity and potential permission management issues. While the lifecycle policy to Glacier\nFlexible Retrieval is valid, it doesn't address the fast access requirement for the first 7 days.\nTherefore, S3 File Gateway coupled with a Lifecycle policy to Glacier Deep Archive offers the most integrated\nand cost-effective solution for this scenario. It extends the SMB server's storage, provides fast local access\nfor recent files, and transitions older files to a cheaper archive tier while meeting the retrieval time\nrequirement.\nKey Concepts:\nAmazon S3 File Gateway: Provides a local cache for frequently accessed files, enabling low-latency access\nfrom on-premises applications.\nAmazon S3 Lifecycle policies: Automate the transition of objects between different storage classes (e.g., S3\nStandard, S3 Glacier Deep Archive) based on defined rules.\nS3 Glacier Deep Archive: Offers the lowest cost storage for long-term archiving with infrequent access.\nStorage Tiering: Moving data to different storage classes based on access frequency and cost\nconsiderations.\nAuthoritative Links:\nAmazon S3 File Gateway: https://aws.amazon.com/storagegateway/file/\nAmazon S3 Storage Classes: https://aws.amazon.com/s3/storage-classes/\nAmazon S3 Lifecycle: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-\nconcept.html\nAmazon S3 Glacier Deep Archive: https://aws.amazon.com/glacier/deep-archive/",
    "links": [
      "https://aws.amazon.com/storagegateway/file/",
      "https://aws.amazon.com/s3/storage-classes/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-",
      "https://aws.amazon.com/glacier/deep-archive/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a web application on Amazon EC2 instances in an Auto Scaling group. The application uses a\ndatabase that runs on an Amazon RDS for PostgreSQL DB instance. The application performs slowly when traffic\nincreases. The database experiences a heavy read load during periods of high traffic.\nWhich actions should a solutions architect take to resolve these performance issues? (Choose two.)",
    "options": {
      "B": "Create a read replica for the DB instance. Configure the application to send read traffic to the read",
      "D": "Create an Amazon ElastiCache cluster. Configure the application to cache query results in the",
      "A": "Turn on auto scaling for the DB instance. While RDS does support autoscaling for storage, it does not",
      "C": "Convert the DB instance to a Multi-AZ DB instance deployment. Configure the application to send read"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for the answer BD:\nThe problem describes a web application experiencing performance issues due to a heavy read load on an\nRDS for PostgreSQL database during periods of high traffic. The core issue is database read contention,\nwhich is slowing down the application.\nB. Create a read replica for the DB instance. Configure the application to send read traffic to the read\nreplica. This is a direct and effective solution. Read replicas are designed to offload read traffic from the\nprimary database instance. By directing read queries to the read replica, the primary instance is freed up to\nhandle write operations and other critical tasks. This significantly reduces the load on the primary database,\nimproving overall application performance. RDS makes it easy to create and manage read replicas.\nD. Create an Amazon ElastiCache cluster. Configure the application to cache query results in the\nElastiCache cluster. Implementing a caching layer using Amazon ElastiCache addresses the read load\nproblem in a different but complementary way. By caching frequently accessed data or query results, the\napplication can retrieve information directly from the cache instead of querying the database every time. This\nreduces the number of read requests hitting the database, further alleviating the load and improving response\ntimes. ElastiCache offers managed in-memory caching services, allowing for fast data retrieval and efficient\nscaling.\nWhy other options are incorrect:\nA. Turn on auto scaling for the DB instance. While RDS does support autoscaling for storage, it does not\nautoscaling for compute resources directly like EC2 instances. This will not address the read load problem.\nAlso, autoscaling would not work when traffic decreases as quickly as it goes up.\nC. Convert the DB instance to a Multi-AZ DB instance deployment. Configure the application to send read\ntraffic to the standby DB instance. Multi-AZ is primarily for high availability and disaster recovery, not read\nscaling. The standby instance in a Multi-AZ deployment is not intended for serving read traffic. It's a passive\nreplica that becomes active only in case of a failure of the primary instance. Directly using the standby\ninstance for reads is not supported and can lead to data inconsistency or other issues.\nE. Configure the Auto Scaling group subnets to ensure that the EC2 instances are provisioned in the same\nAvailability Zone as the DB instance. This is a best practice for reducing latency and network costs, it does\nnothing to reduce load on the database instance.\nSupporting Links:\nAmazon RDS Read Replicas:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\nAmazon ElastiCache: https://aws.amazon.com/elasticache/\nAmazon RDS Multi-AZ Deployments:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html",
      "https://aws.amazon.com/elasticache/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html"
    ]
  },
  {
    "question": "CertyIQ\nA company uses Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) volumes to run an\napplication. The company creates one snapshot of each EBS volume every day to meet compliance requirements.\nThe company wants to implement an architecture that prevents the accidental deletion of EBS volume snapshots.\nThe solution must not change the administrative rights of the storage administrator user.\nWhich solution will meet these requirements with the LEAST administrative effort?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D: Lock the EBS snapshots to prevent deletion.\nHere's why:\nRecycle Bin (Option C) provides a temporary buffer against accidental deletion, allowing recovery within a\nconfigured retention period. However, it doesn't prevent deletion, which is a critical requirement for the\ncompany. An administrator could still permanently delete a snapshot from the Recycle Bin.\nIAM Policy (Option B) while effective at preventing a specific user (the storage administrator) from deleting\nsnapshots, it directly modifies their administrative rights, which violates the requirement that these rights\nshould not change. This approach also doesn't protect against deletion from other users or services with\nsufficient permissions.\nUsing an EC2 instance with a restricted IAM role to delete snapshots (Option A) complicates the process\nand adds unnecessary overhead. It doesn't prevent accidental deletion by the storage administrator; it simply\ndelegates the deletion task to another entity.\nEBS snapshot locking directly addresses the requirement of preventing accidental deletion without\naltering the storage administrator's fundamental rights. Snapshot locking provides a write-once-read-\nmany (WORM) capability, meaning that once a snapshot is locked, it cannot be deleted until the lock\nexpires, regardless of user permissions. This fulfills the compliance requirement and ensures data retention\nwithout modifying the existing administrative structure.\nSnapshot locking is the least administrative effort option because it's a built-in feature designed specifically\nfor this purpose. The storage administrator maintains their existing rights, but the snapshots are protected by\nthe lock.\nFurther research on EBS snapshot locking can be found in the official AWS documentation:\nAmazon EBS Snapshots - Preventing Accidental Deletion with Snapshot Locks",
    "links": []
  },
  {
    "question": "CertyIQ\nA company's application uses Network Load Balancers, Auto Scaling groups, Amazon EC2 instances, and\ndatabases that are deployed in an Amazon VP",
    "options": {
      "C": "CloudWatch"
    },
    "answer": "B",
    "explanation": "The requirement is to capture and analyze VPC traffic information in near real-time using Amazon\nOpenSearch Service.\nOption B is the correct solution because it utilizes VPC Flow Logs to capture the network traffic data, sends it\nto CloudWatch Logs, and then uses Kinesis Data Firehose to stream the logs to OpenSearch Service. VPC\nFlow Logs directly capture IP traffic information at the network interface level within the VPC. CloudWatch\nLogs acts as a central repository for these logs. Kinesis Data Firehose is specifically designed to reliably\nstream data to destinations like Amazon OpenSearch Service.\nOption A is incorrect because while it uses CloudWatch Logs and VPC Flow Logs correctly, Kinesis Data\nStreams is more suitable for custom processing of data in transit and isn't the most efficient way to deliver\ndirectly to OpenSearch in this scenario. Firehose has direct integration and built-in buffering for OpenSearch\nService.\nOptions C and D are incorrect because CloudTrail is used to log API calls made to AWS services, not network\ntraffic. VPC Flow Logs capture the network traffic directly. Therefore, sending data to CloudTrail and then\nstreaming from there is not suitable for network traffic analysis.\nIn summary, the combination of VPC Flow Logs, CloudWatch Logs, and Kinesis Data Firehose provides the\nmost efficient and appropriate solution for capturing, storing, and analyzing VPC network traffic data in near\nreal-time with Amazon OpenSearch Service.\nSupporting Documentation:\nVPC Flow Logs: https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html\nAmazon Kinesis Data Firehose: https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\nAmazon OpenSearch Service: https://aws.amazon.com/opensearch-service/",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html",
      "https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html",
      "https://aws.amazon.com/opensearch-service/"
    ]
  },
  {
    "question": "CertyIQ\nA company is developing an application that will run on a production Amazon Elastic Kubernetes Service (Amazon\nEKS) cluster. The EKS cluster has managed node groups that are provisioned with On-Demand Instances.\nThe company needs a dedicated EKS cluster for development work. The company will use the development cluster\ninfrequently to test the resiliency of the application. The EKS cluster must manage all the nodes.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "A": "Create a managed node group that contains only Spot Instances."
    },
    "answer": "A",
    "explanation": "The correct answer is A. Create a managed node group that contains only Spot Instances.\nHere's why:\nCost-Effectiveness: The primary requirement is cost-effectiveness for an infrequently used development\ncluster. Spot Instances offer significant discounts (up to 90%) compared to On-Demand Instances because\nyou bid on unused EC2 capacity. Since the development cluster will be used sporadically for resiliency\ntesting, interruptions from Spot Instance revocations are acceptable, making them the most economical\nchoice.\nManaged Node Group Convenience: Using a managed node group simplifies node management. EKS\nautomatically handles provisioning, updating, and scaling the nodes within the group, reducing operational\noverhead compared to self-managed nodes (which Auto Scaling groups would entail).\nMeeting Requirements: A managed node group with Spot Instances fulfills the requirement of a dedicated\nEKS cluster managed entirely by EKS itself. The infrequent usage aligns perfectly with the nature of Spot\nInstances, as interruptions are tolerable in a development/testing environment.\nWhy other options are not ideal:\nB: Provisioning both On-Demand and Spot Instances increases complexity and cost. On-Demand Instances are\nnot needed for infrequent development use.\nC: Using an Auto Scaling group with a launch configuration and user data requires more manual configuration\nand management, which is contrary to the simplicity offered by managed node groups. Also, it moves away\nfrom managed nodes by EKS.\nD: On-Demand Instances are too expensive for infrequent development use.\nAuthoritative Links:\nAmazon EKS Managed Node Groups: https://docs.aws.amazon.com/eks/latest/userguide/managed-node-\ngroups.html\nAmazon EC2 Spot Instances: https://aws.amazon.com/ec2/spot/",
    "links": [
      "https://docs.aws.amazon.com/eks/latest/userguide/managed-node-",
      "https://aws.amazon.com/ec2/spot/"
    ]
  },
  {
    "question": "CertyIQ\nA company stores sensitive data in Amazon S3. A solutions architect needs to create an encryption solution. The\ncompany needs to fully control the ability of users to create, rotate, and disable encryption keys with minimal\neffort for any data that must be encrypted.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Use default server-side encryption with Amazon S3 managed encryption keys (SSE-S3): SSE-S3",
      "C": "Create an AWS managed key by using AWS Key Management Service (AWS KMS): This is incorrect.",
      "D": "Download S3 objects to an Amazon EC2 instance. Encrypt the objects by using customer managed keys:"
    },
    "answer": "B",
    "explanation": "The correct answer is B because it allows the company to fully control the encryption keys. The question\nexplicitly states the need for the company to \"fully control the ability of users to create, rotate, and disable\nencryption keys.\"\nLet's analyze why the other options are incorrect:\nA. Use default server-side encryption with Amazon S3 managed encryption keys (SSE-S3): SSE-S3\nencrypts data at rest using keys managed by AWS. While easy to implement, the customer has no control over\nkey rotation or disabling. AWS manages the keys entirely, failing to meet the requirement of full control.\nC. Create an AWS managed key by using AWS Key Management Service (AWS KMS): This is incorrect.\nWhile KMS keys are used, \"AWS managed keys\" (formerly known as \"AWS managed CMKs\") in KMS are not\nfully controlled by the customer. Although you can choose to use these keys for SSE-KMS encryption in S3,\nthe customer has very limited control over key rotation policies. The customer cannot disable or delete these\nkeys.\nD. Download S3 objects to an Amazon EC2 instance. Encrypt the objects by using customer managed keys:\nThis solution is inefficient, complex, and introduces unnecessary overhead. It involves transferring data out of\nS3, performing encryption on an EC2 instance, and then re-uploading the encrypted data. This approach\ncreates a management burden and doesn't leverage the built-in encryption capabilities of S3. Moreover, this\nmethod adds complexity and is not suitable for the requirement of minimal effort.\nWhy Option B is the Best Choice:\nOption B, using a customer managed key in AWS KMS for SSE-KMS, provides the best balance of security,\ncontrol, and ease of use:\nCustomer Control: The company creates and manages the KMS key, granting them complete control over the\nkey's lifecycle, including creation, rotation, enabling, disabling, and deletion. This perfectly aligns with the\nstated requirement.\nSecurity: AWS KMS provides a secure and compliant environment for managing cryptographic keys.\nIntegration with S3: SSE-KMS is a native feature of S3, simplifying encryption and decryption processes.\nMinimal Effort: Using SSE-KMS with a customer managed key involves configuring the S3 bucket to use the\nkey, which is relatively straightforward.\nIn summary, customer managed keys in AWS KMS, used with SSE-KMS, are designed to give customers\ncomplete control over their encryption keys while leveraging the security and scalability of AWS.\nAuthoritative Links:\nAWS KMS: https://aws.amazon.com/kms/\nProtecting Data Using Server-Side Encryption:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html\nAWS KMS concepts: https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html",
    "links": [
      "https://aws.amazon.com/kms/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html",
      "https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to back up its on-premises virtual machines (VMs) to AWS. The company's backup solution\nexports on-premises backups to an Amazon S3 bucket as objects. The S3 backups must be retained for 30 days\nand must be automatically deleted after 30 days.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "options": {
      "A": "Create an S3 bucket that has S3 Object Lock enabled: S3 Object Lock is crucial to ensure that the",
      "C": "Configure a default retention period of 30 days for the objects: This ensures that all objects placed in the",
      "B": "Create an S3 bucket that has object versioning enabled: While object versioning is a good practice for",
      "D": "Configure an S3 Lifecycle policy to protect the objects for 30 days: While the intention might be correct,"
    },
    "answer": "A",
    "explanation": "Here's a detailed justification for why options A, C, and E are the correct choices for the scenario, along with\nsupporting concepts and links:\nThe requirement is to back up on-premises VMs to S3, retain the backups for 30 days, and automatically\ndelete them after 30 days.\nA. Create an S3 bucket that has S3 Object Lock enabled: S3 Object Lock is crucial to ensure that the\nbackups are protected from accidental or malicious deletion during the retention period. Once a backup is\nwritten to S3 with Object Lock enabled, it cannot be deleted until the retention period expires. This is an\nimportant protection mechanism for backups.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html\nC. Configure a default retention period of 30 days for the objects: This ensures that all objects placed in the\nS3 bucket are subject to the retention policy enforced by Object Lock. The objects cannot be deleted or\noverwritten until the 30-day period has elapsed, fulfilling the 30-day retention requirement. A retention\nperiod protects data integrity.\nE. Configure an S3 Lifecycle policy to expire the objects after 30 days: While Object Lock protects the\nobjects during the 30-day retention period, it doesn't automatically delete them after the retention period. A\nlifecycle policy configured to expire (delete) the objects after 30 days ensures automatic cleanup. If a\nretention period of 30 days is enabled using Object Lock, the expiration action of the Lifecycle rule cannot be\nfor a period less than the retention period. This ensures that the backups are automatically purged once they\nare no longer needed, optimizing storage costs.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-examples.html\nWhy the other options are incorrect:\nB. Create an S3 bucket that has object versioning enabled: While object versioning is a good practice for\ndata protection in general (allowing you to recover previous versions of objects), it doesn't automatically\ndelete objects. Versioning alone doesn't satisfy the requirement of automatic deletion after 30 days. It only\nkeeps all versions of the object, which will increase storage costs without addressing the retention and\nautomatic deletion requirement.\nD. Configure an S3 Lifecycle policy to protect the objects for 30 days: While the intention might be correct,\nthe phrase \"protect the objects\" is vague in the context of S3 Lifecycle policies. Lifecycle policies primarily\nmanage object transitions (e.g., moving to cheaper storage classes) or expirations (deletions). Protection from\ndeletion is achieved via Object Lock, not standard Lifecycle actions.\nF. Configure the backup solution to tag the objects with a 30-day retention period: Tagging objects is useful\nfor metadata and categorization, but S3 by itself does not automatically enforce retention based on tags.\nWhile the backup solution could be programmed to delete based on these tags, it introduces unnecessary\ncomplexity and reliance on the backup solution's custom logic. S3's built-in Object Lock and Lifecycle policies\nare more reliable and scalable for this purpose.\nIn summary, the combination of S3 Object Lock (A), a default retention period (C), and a Lifecycle policy for\nexpiration (E) provides a robust and automated solution for backing up on-premises VMs to S3 with the\nspecified retention and deletion requirements.",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-examples.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect needs to copy files from an Amazon S3 bucket to an Amazon Elastic File System (Amazon\nEFS) file system and another S3 bucket. The files must be copied continuously. New files are added to the original\nS3 bucket consistently. The copied files should be overwritten only if the source file changes.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "AWS DataSync with Change Detection (Best Solution)",
      "B": "Lambda Function: While feasible, Lambda requires custom code to handle file copying and change",
      "C": "DataSync with Full Transfer: Transferring all data on each run is inefficient and costly, especially as the",
      "D": "EC2 Instance: This approach requires managing an EC2 instance, configuring synchronization scripts"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's a detailed justification:\nA. AWS DataSync with Change Detection (Best Solution)\nDataSync Efficiency: AWS DataSync is purpose-built for efficient data transfer between on-premises\nstorage, S3, and EFS. It handles data transfer, synchronization, and metadata management automatically.\nChange Detection: DataSync's \"transfer only data that has changed\" setting avoids unnecessary transfers\nand reduces operational overhead. It detects changes based on file modification times and sizes. This ensures\nonly new or updated files are copied, satisfying the requirement of overwriting only changed files.\nContinuous Operation: DataSync can be scheduled to run periodically or triggered by events, enabling\ncontinuous file synchronization.\nLeast Operational Overhead: DataSync is a managed service, meaning AWS handles the underlying\ninfrastructure, scaling, and maintenance. This significantly reduces the operational burden compared to\nsolutions involving EC2 instances or Lambda functions.\nAuthoritative Link: https://aws.amazon.com/datasync/\nWhy other options are less suitable:\nB. Lambda Function: While feasible, Lambda requires custom code to handle file copying and change\ndetection. Managing the function, handling potential errors, and ensuring scalability adds operational\noverhead. Mounting EFS to Lambda functions introduces complexity and can be less performant than a\ndedicated service like DataSync.\nC. DataSync with Full Transfer: Transferring all data on each run is inefficient and costly, especially as the\ndata volume grows. It violates the requirement to overwrite only changed files.\nD. EC2 Instance: This approach requires managing an EC2 instance, configuring synchronization scripts\n(potentially using rsync or similar tools), and handling scaling and fault tolerance. This is significantly more\ncomplex and has more operational overhead than a managed service like DataSync.",
    "links": [
      "https://aws.amazon.com/datasync/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses Amazon EC2 instances and stores data on Amazon Elastic Block Store (Amazon EBS) volumes.\nThe company must ensure that all data is encrypted at rest by using AWS Key Management Service (AWS KMS).\nThe company must be able to control rotation of the encryption keys.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A because it offers the most control over key rotation while minimizing operational\noverhead. The requirement is to encrypt EBS volumes at rest using KMS, with the ability to control key\nrotation.\nOption A, using a customer-managed key (CMK), directly addresses this requirement. CMKs allow for full\ncontrol over the key lifecycle, including defining rotation policies. AWS KMS automatically rotates CMKs\ncreated within KMS every year, unless you opt-out. This meets the need for key rotation without any manual\nintervention.\nOption B, using an AWS-managed key, also provides encryption at rest. However, AWS-managed keys do not\nallow the user to control key rotation, which is a critical requirement. Although AWS will automatically rotate\nthese keys, you cannot manage the rotation schedule or bring your own rotation schedule.\nOption C, importing key material into KMS, introduces significant operational overhead. While it technically\nfulfills the requirement of using KMS and encryption, the management of external keys, including their\navailability and security, becomes the company's responsibility. This adds unnecessary complexity compared\nto using CMKs.\nOption D, using AWS-owned keys, is not suitable because these keys are managed entirely by AWS and are\nnot accessible or controllable by the user. You have no control over encryption or rotation, and are also not\nsuitable for regulatory compliance in several cases. AWS owned keys are used by default when you encrypt\nEBS volumes.\nTherefore, creating a customer-managed key is the most efficient and controllable solution for encrypting\nEBS volumes at rest with KMS and managing key rotation policies. It balances security requirements with\noperational efficiency.\nSupporting Links:\nAWS KMS Key Types: https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html\nEncrypting EBS Volumes: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\nRotating Customer Managed Keys: https://docs.aws.amazon.com/kms/latest/developerguide/rotate-\nkeys.html",
    "links": [
      "https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html",
      "https://docs.aws.amazon.com/kms/latest/developerguide/rotate-"
    ]
  },
  {
    "question": "CertyIQ\nA company needs a solution to enforce data encryption at rest on Amazon EC2 instances. The solution must\nautomatically identify noncompliant resources and enforce compliance policies on findings.\nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": {},
    "answer": "A",
    "explanation": "The best solution is A because it combines preventative and reactive measures with minimal administrative\noverhead.\nFirst, using an IAM policy to restrict users to creating only encrypted EBS volumes enforces encryption at the\npoint of creation, preventing unencrypted volumes from being provisioned in the first place. This is a proactive\napproach.\nSecond, AWS Config continuously monitors your AWS resource configurations. It can be configured to\nevaluate whether EBS volumes are encrypted, identifying non-compliant resources. This provides continuous\nauditing and drift detection.\nThird, integrating AWS Config with AWS Systems Manager (SSM) Automation allows for automated\nremediation. When AWS Config identifies a non-compliant (unencrypted) EBS volume, it can trigger an SSM\nAutomation runbook to encrypt the volume. This automates the remediation process, minimizing manual\nintervention.\nOption B is less efficient because it requires the creation and maintenance of Lambda functions and\nEventBridge rules, adding administrative overhead. While possible, it's more complex than using AWS Config\nand SSM Automation.\nOption C, while Amazon Macie is designed for data security and data privacy service and can be used to\ndiscover sensitive data stored in Amazon S3, relational databases, and data warehouses, including personally\nidentifiable information (PII), it is primarily designed for detecting and reporting on data breaches. It doesn't\ndirectly automate remediation actions on EBS volumes in the same straightforward way as AWS Config.\nOption D: Amazon Inspector is designed for vulnerability assessments and security hardening. While it might\nbe able to identify some security issues related to unencrypted volumes, its primary focus isn't on enforcing\nencryption policies.\nTherefore, the combination of preventative IAM policies and automated detection and remediation through\nAWS Config and SSM Automation in option A provides the least administrative overhead and directly\naddresses the requirement of enforcing data encryption at rest.\nAuthoritative links:\nAWS Config: https://aws.amazon.com/config/\nAWS Systems Manager: https://aws.amazon.com/systems-manager/\nAmazon EBS Encryption: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html",
    "links": [
      "https://aws.amazon.com/config/",
      "https://aws.amazon.com/systems-manager/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is migrating its multi-tier on-premises application to AWS. The application consists of a single-node\nMySQL database and a multi-node web tier. The company must minimize changes to the application during the\nmigration. The company wants to improve application resiliency after the migration.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": {
      "A": "Migrate the web tier to Amazon EC2 instances in an Auto Scaling group behind an",
      "C": "Migrate the database to an Amazon RDS Multi-AZ deployment."
    },
    "answer": "A",
    "explanation": "The correct answer is A. Migrate the web tier to Amazon EC2 instances in an Auto Scaling group behind an\nApplication Load Balancer and C. Migrate the database to an Amazon RDS Multi-AZ deployment.\nHere's a detailed justification:\nOption A: Migrating the web tier to EC2 instances in an Auto Scaling group behind an Application Load\nBalancer (ALB) provides high availability and scalability for the web tier. The Auto Scaling group ensures that\nthe desired number of EC2 instances is maintained, even if some instances fail. The ALB distributes incoming\ntraffic across the healthy EC2 instances, further enhancing availability and resilience. This addresses the\nrequirement to improve application resiliency after migration while minimizing changes to the application\nlogic itself. https://aws.amazon.com/elasticloadbalancing/application-load-balancer/ and\nhttps://aws.amazon.com/autoscaling/\nOption C: Migrating the MySQL database to an Amazon RDS Multi-AZ deployment significantly improves\ndatabase availability and durability. RDS Multi-AZ creates a synchronous, replicated standby instance in a\ndifferent Availability Zone. In the event of a failure of the primary instance, RDS automatically fails over to the\nstandby instance, minimizing downtime and improving resiliency. This meets the company's objective of\nenhancing application resiliency post-migration with minimal application changes as RDS largely handles the\nfailover process. https://aws.amazon.com/rds/features/multi-az/\nWhy other options are incorrect:\nOption B: Migrating the database to EC2 instances in an Auto Scaling group behind a Network Load Balancer\n(NLB) is not the best approach for a MySQL database. While Auto Scaling can help manage EC2 instance\nfailures, setting up and managing database replication and failover on EC2 instances would be more complex\nthan using RDS Multi-AZ, requiring more changes to the application and database setup. Also, NLB is\ndesigned for TCP traffic, not necessarily application layer awareness.\nOption D: Migrating the web tier to an AWS Lambda function might require significant refactoring of the web\ntier code, as Lambda functions are typically used for event-driven, stateless workloads. This contradicts the\nrequirement to minimize changes to the application during the migration.\nOption E: Migrating the database to an Amazon DynamoDB table would necessitate substantial changes to\nthe application's data access layer because MySQL and DynamoDB are fundamentally different database\ntypes (relational vs. NoSQL). This approach would violate the requirement of minimizing application changes.",
    "links": [
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
      "https://aws.amazon.com/autoscaling/",
      "https://aws.amazon.com/rds/features/multi-az/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to migrate its web applications from on premises to AWS. The company is located close to the\neu-central-1 Region. Because of regulations, the company cannot launch some of its applications in eu-central-1.\nThe company wants to achieve single-digit millisecond latency.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Regional Edge Caches are still part of the content"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the correct answer, along with why the other options are less\nsuitable, supported by relevant AWS concepts:\nThe key requirements are: (1) migration to AWS, (2) compliance preventing some apps in eu-central-1, (3)\nsingle-digit millisecond latency, and (4) the company's proximity to eu-central-1.\nOption B, deploying applications in AWS Local Zones by extending the VPC from eu-central-1, directly\naddresses these requirements. AWS Local Zones are designed to place compute, storage, database, and\nother select AWS services closer to large population, industry, and IT centers. This proximity is crucial for\nachieving the single-digit millisecond latency target. Because the company cannot use eu-central-1 for all\napplications, a local zone geographically separate allows for compliance without the need for global\ninfrastructure deployment. Extending the VPC allows for private, low-latency connectivity between resources\nin the eu-central-1 region and the Local Zone.\nOption A is incorrect because CloudFront Edge Locations are for caching content closer to users, not for\nrunning applications that require low latency access to the primary region. Extending a VPC to a CloudFront\nedge location is not possible. CloudFront does not run compute resources.\nOption C is incorrect for the same reason as Option A. Regional Edge Caches are still part of the content\ndelivery network (CDN) infrastructure and do not provide the compute resources needed to run the\napplications. They are for caching content. Extending a VPC into an edge location is incorrect and not\npossible.\nOption D is incorrect because AWS Wavelength Zones are designed for ultra-low latency applications at the\nedge of the 5G network, which is more pertinent to mobile and IoT scenarios, and typically necessitates\nintegrating with specific telecommunications providers. The scenario doesn't mention 5G or mobile\nconsiderations. Furthermore, the question indicates some applications cannot be run within eu-central-1, not\nall. Local Zones provide a more generalized compute environment near the region.\nTherefore, Local Zones are the most appropriate solution for fulfilling all the requirements of the scenario.\nAuthoritative Links:\nAWS Local Zones: https://aws.amazon.com/local-zones/\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nAWS Wavelength: https://aws.amazon.com/wavelength/",
    "links": [
      "https://aws.amazon.com/local-zones/",
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/wavelength/"
    ]
  },
  {
    "question": "CertyIQ\nA companys ecommerce website has unpredictable traffic and uses AWS Lambda functions to directly access a\nprivate Amazon RDS for PostgreSQL DB instance. The company wants to maintain predictable database\nperformance and ensure that the Lambda invocations do not overload the database with too many connections.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "C": "RDS Proxy: RDS Proxy acts as a fully managed database proxy service. It sits between your application"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the correct solution, along with supporting concepts and\nlinks:\nThe core problem is managing unpredictable traffic from Lambda functions directly accessing an RDS\ndatabase, potentially causing connection exhaustion and impacting database performance. The key to solving\nthis is to decouple the Lambda functions from directly managing database connections.\nWhy option B is correct: Point the client driver at an RDS proxy endpoint. Deploy the Lambda functions\ninside a VPC.\nRDS Proxy: RDS Proxy acts as a fully managed database proxy service. It sits between your application\n(Lambda functions) and your database (RDS for PostgreSQL). It pools and shares database connections,\nreducing the overhead of establishing new connections for each Lambda invocation. This is crucial for\nhandling unpredictable traffic spikes from Lambda, preventing the database from being overloaded. It also\nautomatically manages connection reuse, allowing Lambda to scale efficiently.\nDeploying Lambda inside a VPC: This is necessary for Lambda functions to access private resources like an\nRDS database within your AWS environment. When Lambda functions reside inside a VPC, they can leverage\nsecurity groups and network ACLs to establish secure, private connections to the RDS instance. If the\ndatabase is only accessible within the VPC (which is a common security practice), the Lambda functions must\nalso be within the VPC to reach it.\nWhy other options are incorrect:\nOptions A & C (RDS Custom Endpoint): RDS Custom endpoints are used for managing database instances\nwith customizations and advanced configurations. These are not appropriate for managing the connection\nmanagement issues associated with high concurrency from Lambda. Custom endpoints won't solve the\ndatabase connection overload problem.\nOption D (Deploy Lambda functions outside a VPC): If the RDS instance resides within a VPC (which is\ngenerally recommended for security), Lambda functions outside the VPC will not be able to access the\ndatabase unless configured with public accessibility, which introduces security vulnerabilities. This is\ngenerally discouraged. Also, using an RDS proxy outside the VPC would require the database to be publicly\naccessible, which is a bad security practice.\nIn Summary: RDS Proxy handles connection management, preventing database overload, and deploying\nLambda inside the VPC ensures secure, private access to the database. The combination addresses both\nrequirements effectively.\nAuthoritative Links:\nAWS RDS Proxy: https://aws.amazon.com/rds/proxy/\nConfiguring Lambda to access resources in a VPC:\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html\nBest Practices for Lambda functions and RDS: https://aws.amazon.com/blogs/compute/using-amazon-rds-\nproxy-with-aws-lambda/",
    "links": [
      "https://aws.amazon.com/rds/proxy/",
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html",
      "https://aws.amazon.com/blogs/compute/using-amazon-rds-"
    ]
  },
  {
    "question": "CertyIQ\nA company is creating an application. The company stores data from tests of the application in multiple on-\npremises locations.\nThe company needs to connect the on-premises locations to VPCs in an AWS Region in the AWS Cloud. The\nnumber of accounts and VPCs will increase during the next year. The network architecture must simplify the\nadministration of new connections and must provide the ability to scale.\nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": {
      "C": "Connect"
    },
    "answer": "C",
    "explanation": "The correct answer is C: Create a transit gateway. Create VPC attachments for the VPC connections. Create\nVPN attachments for the on-premises connections. This solution offers the least administrative overhead and\ngreatest scalability for connecting multiple VPCs and on-premises locations.\nHere's why:\nTransit Gateway Simplifies Network Management: AWS Transit Gateway acts as a central hub, simplifying\nthe routing between multiple VPCs and on-premises networks. Instead of managing numerous point-to-point\nconnections (like VPC peering), you manage connections to the Transit Gateway.\nScalability: As the number of VPCs and on-premises locations grows, the Transit Gateway scales easily. You\nsimply create new attachments without having to reconfigure existing connections. This eliminates the\ncomplexity of managing a mesh network created by VPC peering or VPN connections.\nCentralized Routing Policies: Transit Gateway provides a central place to manage routing policies. You can\ncontrol the traffic flow between different VPCs and on-premises locations using route tables associated with\nthe Transit Gateway.\nVPN Attachments: It supports VPN attachments for on-premises connections, integrating seamlessly with\nexisting VPN infrastructure.\nLet's analyze why the other options are not optimal:\nA: VPC Peering and VPNs: This solution is not scalable and involves high administrative overhead. VPC\npeering is a one-to-one connection, meaning you'd need numerous peering connections as your VPCs\nincrease. Managing all these connections becomes complex and time-consuming.\nB: EC2 VPN Instance: While it's a potential solution, it introduces a single point of failure. The EC2 instance\nbecomes a bottleneck. It also requires more hands-on management for patching, scaling, and high availability,\nadding to the administrative overhead.\nD: Direct Connect and VPC Peering: Direct Connect provides a dedicated network connection from on-\npremises to AWS, but using a central VPC and peering to others becomes complex as the number of VPCs\ngrows. Similar to option A, it results in a mesh network of peering connections, which is difficult to manage.\nDirect Connect is also more expensive and is more suitable for continuous large data transfers. For this\nscenario where a company will increase the number of accounts and VPCs during the next year, transit\ngateway will provide a scalable solution.\nIn summary, Transit Gateway centralizes network management, offers scalability, and integrates well with\nboth VPCs and VPN connections, making it the most suitable solution for the company's needs.\nSupporting Documentation:\nAWS Transit Gateway: https://aws.amazon.com/transit-gateway/\nAWS VPN Connections: https://aws.amazon.com/vpn/\nAWS Direct Connect: https://aws.amazon.com/directconnect/",
    "links": [
      "https://aws.amazon.com/transit-gateway/",
      "https://aws.amazon.com/vpn/",
      "https://aws.amazon.com/directconnect/"
    ]
  },
  {
    "question": "CertyIQ\nA company that uses AWS needs a solution to predict the resources needed for manufacturing processes each\nmonth. The solution must use historical values that are currently stored in an Amazon S3 bucket. The company has\nno machine learning (ML) experience and wants to use a managed service for the training and predictions.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": {
      "B": "Use Amazon SageMaker to train a model by using the historical data in the S3 bucket. SageMaker offers",
      "D": "Configure an AWS Lambda function with a function URL that uses an Amazon Forecast predictor to",
      "A": "Deploy an Amazon SageMaker model. Create a SageMaker endpoint for inference. While SageMaker is a",
      "C": "Configure an AWS Lambda function with a function URL that uses Amazon SageMaker endpoints to"
    },
    "answer": "B",
    "explanation": "The correct answer is BD because it leverages the most suitable managed service for time-series forecasting\nwhen the company lacks ML expertise. Here's a detailed breakdown:\nB. Use Amazon SageMaker to train a model by using the historical data in the S3 bucket. SageMaker offers\nmanaged model training capabilities. It allows users to bring their data stored in S3 and select a suitable\nalgorithm or even implement their own. SageMaker handles the infrastructure provisioning, scaling, and\noptimization for training the model. This is a valid step to create a prediction model using the historical data.\nD. Configure an AWS Lambda function with a function URL that uses an Amazon Forecast predictor to\ncreate a prediction based on the inputs. Amazon Forecast is a managed service specifically designed for\ntime-series forecasting. Once the model is trained in Forecast, you can trigger it using a Lambda function.\nFunction URLs offer a simple way to invoke your Lambda functions over HTTP(S), meaning you can easily\nintegrate Forecast predictions into your applications.\nWhy other options are incorrect:\nA. Deploy an Amazon SageMaker model. Create a SageMaker endpoint for inference. While SageMaker is a\nvaluable tool, this option skips the crucial training stage, which the scenario requires. Deploying a model\nbefore training is not possible or makes sense.\nC. Configure an AWS Lambda function with a function URL that uses Amazon SageMaker endpoints to\ncreate predictions based on the inputs. This option is viable, but it should follow training the model first. In\naddition, using Amazon Forecast, which the scenario is looking for, would be a much simpler and fully\nmanaged approach.\nE. Train an Amazon Forecast predictor by using the historical data in the S3 bucket. While this option is valid\nas the data is used for training a model, it is paired with choice A which isn't valid.\nAmazon Forecast is the superior choice compared to SageMaker in this particular scenario because the\ncompany explicitly states that it has no ML experience and wants to use a managed service. Forecast\nabstracts away the complexities of choosing algorithms, feature engineering, and model tuning, allowing the\ncompany to focus on their manufacturing predictions.\nSupporting Links:\nAmazon SageMaker: https://aws.amazon.com/sagemaker/\nAmazon Forecast: https://aws.amazon.com/forecast/\nAWS Lambda Function URLs: https://aws.amazon.com/blogs/compute/introducing-aws-lambda-function-\nurls-built-in-https-endpoints-for-single-function-microservices/",
    "links": [
      "https://aws.amazon.com/sagemaker/",
      "https://aws.amazon.com/forecast/",
      "https://aws.amazon.com/blogs/compute/introducing-aws-lambda-function-"
    ]
  },
  {
    "question": "CertyIQ\nA company manages AWS accounts in AWS Organizations. AWS IAM Identity Center (AWS Single Sign-On) and\nAWS Control Tower are configured for the accounts. The company wants to manage multiple user permissions\nacross all the accounts.\nThe permissions will be used by multiple IAM users and must be split between the developer and administrator\nteams. Each team requires different permissions. The company wants a solution that includes new users that are\nhired on both teams.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C because it leverages AWS IAM Identity Center's permission sets for centralized\npermission management, minimizing operational overhead. Here's a detailed justification:\nCentralized Management with IAM Identity Center: AWS IAM Identity Center (successor to AWS Single\nSign-On) is designed for managing user access across multiple AWS accounts within an AWS Organization.\nThis centralizes identity and access management, reducing the complexity of managing individual IAM users\nin each account.\nPermission Sets for Role-Based Access: Permission sets define a collection of IAM policies that grant\nspecific permissions. By creating developer and administrator permission sets, the company can define\ngranular permissions for each role.\nGroup-Based Assignment: Assigning permission sets to IAM Identity Center groups (developer and\nadministrator) simplifies user management. When a new user joins a team, they are added to the appropriate\ngroup, and they automatically inherit the corresponding permissions. This approach avoids managing\npermissions on a per-user basis.\nIAM Policies within Permission Sets: Permission sets contain IAM policies, which are JSON documents that\ndefine permissions. This enables administrators to define precise access controls for each team.\nLeast Operational Overhead: This solution offers the least operational overhead because new users are\nadded to the appropriate group, rather than creating new policies/users for each account individually. This\nmethod significantly reduces the manual work required to manage permissions, especially as the number of\nusers and accounts grows.\nAWS Control Tower Integration: The question mentions AWS Control Tower, which works well with IAM\nIdentity Center to enforce governance and compliance across AWS accounts. Control Tower helps establish\nguardrails and policies, while IAM Identity Center manages user access.\nIncorrect Alternatives:\nA & B: Creating individual users in IAM Identity Center for each account is not scalable or optimal, as it would\ncreate an admin overhead, and defeat the goal of a single managed solution.\nD: Assigning different permissions to different accounts for the same user is a high-overhead solution and a\ncumbersome method.\nAlternatives B and D are not optimal because they do not correctly apply IAM policies to the appropriate\ngroups.\nSupporting Documentation:\nAWS IAM Identity Center (successor to AWS Single Sign-On) Documentation\nAWS Control Tower Documentation\nIAM Policies",
    "links": []
  },
  {
    "question": "CertyIQ\nA company wants to standardize its Amazon Elastic Block Store (Amazon EBS) volume encryption strategy. The\ncompany also wants to minimize the cost and configuration effort required to operate the volume encryption\ncheck.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D because it leverages AWS Config, a service specifically designed for evaluating the\nconfiguration of AWS resources. AWS Config rules can automatically check whether EBS volumes are\nencrypted and flag those that are not, ensuring continuous compliance with the desired encryption strategy.\nThis automated approach minimizes both configuration effort and ongoing operational costs. Option A and B\ninvolve custom scripting and scheduling (Lambda/Fargate), which introduce unnecessary complexity and\nmanagement overhead compared to using AWS Config's built-in capabilities. While these options are\ntechnically feasible, they don't represent the most cost-effective and streamlined solution. Option C, relying\non IAM policies and tagging, only prevents unencrypted volumes from being created if the policies are\ncorrectly configured. It also relies on Cost Explorer which is not designed to alert on configuration\ncompliance. It doesn't automatically detect or flag existing unencrypted volumes, nor does it provide\ncontinuous monitoring of encryption status. Thus, AWS Config provides the most suitable solution for\nstandardizing and enforcing EBS volume encryption with minimal operational overhead.\nSupporting documentation:\nAWS Config: https://aws.amazon.com/config/\nAWS Config managed rules for EBS encryption:\nhttps://docs.aws.amazon.com/config/latest/developerguide/managed-rules-by-aws-config.html",
    "links": [
      "https://aws.amazon.com/config/",
      "https://docs.aws.amazon.com/config/latest/developerguide/managed-rules-by-aws-config.html"
    ]
  },
  {
    "question": "CertyIQ\nA company regularly uploads GB-sized files to Amazon S3. After the company uploads the files, the company uses\na fleet of Amazon EC2 Spot Instances to transcode the file format. The company needs to scale throughput when\nthe company uploads data from the on-premises data center to Amazon S3 and when the company downloads\ndata from Amazon S3 to the EC2 instances.\nWhich solutions will meet these requirements? (Choose two.)",
    "options": {},
    "answer": "C",
    "explanation": "The question focuses on improving the throughput of data uploads from on-premises to S3 and downloads\nfrom S3 to EC2 instances for large files, especially considering EC2 Spot Instances and transcoding needs.\nOption C, using S3 multipart uploads, directly addresses the need for increased throughput when uploading\nlarge files to S3. Multipart upload allows you to upload a single object as a set of parts. Each part can be\nuploaded independently and in parallel, improving upload speed and resilience to network interruptions.\n[https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html]\nOption D, fetching multiple byte-ranges of an object in parallel, improves the download throughput from S3 to\nthe EC2 instances. By requesting different byte ranges concurrently, the EC2 instances can assemble the\ncomplete file much faster than downloading it sequentially. This is particularly beneficial given the\ntranscoding workload on the EC2 Spot Instances.\n[https://docs.aws.amazon.com/AmazonS3/latest/userguide/range-get-http.html]\nOption A, using S3 bucket access points, primarily controls access and permissions, not throughput. While\naccess points can simplify management, they don't inherently boost upload or download speeds.\nOption B, uploading files into multiple S3 buckets, might offer some partitioning benefits, but doesn't directly\naddress the individual large file's upload/download speed like multipart upload and byte-range fetching.\nOption E, adding a random prefix to each object, is primarily used to avoid hot partitions or uneven distribution\nof keys within S3. This helps improve performance and scalability at the S3 level, especially with a high\nvolume of uploads, but it's not the primary solution for optimizing the throughput of individual file transfers. It\nmainly helps prevent throttling at the partition level within S3.\nTherefore, options C and D directly enhance the throughput of large file uploads to S3 and downloads to EC2,\nmeeting the company's requirements. Multipart upload accelerates uploads, and parallel byte-range fetching\naccelerates downloads.",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html]",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/range-get-http.html]"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is designing a shared storage solution for a web application that is deployed across multiple\nAvailability Zones. The web application runs on Amazon EC2 instances that are in an Auto Scaling group. The\ncompany plans to make frequent changes to the content. The solution must have strong consistency in returning\nthe new content as soon as the changes occur.\nWhich solutions meet these requirements? (Choose two.)",
    "options": {
      "B": "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on the",
      "A": "Use AWS Storage Gateway Volume Gateway Internet Small Computer Systems Interface (iSCSI) block",
      "C": "Create a shared Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on the",
      "D": "Use AWS DataSync to perform continuous synchronization of data between EC2 hosts in the Auto"
    },
    "answer": "B",
    "explanation": "The requirement for strong consistency in returning new content immediately after changes rules out\neventually consistent solutions like Amazon S3.\nB. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on the\nindividual EC2 instances. This is a suitable option because Amazon EFS provides a shared file system that\ncan be mounted concurrently by multiple EC2 instances across different Availability Zones. EFS offers strong\nconsistency for file system metadata operations (like renaming files) and close-to-open consistency for data\nreads and writes. This ensures that once a write operation completes, subsequent reads from any EC2\ninstance will reflect the new content, fulfilling the strong consistency requirement.\nhttps://aws.amazon.com/efs/\nE. Create an Amazon S3 bucket to store the web content. Set the metadata for the Cache-Control header to\nno-cache. Use Amazon CloudFront to deliver the content. While S3 is great for storage, it's only eventually\nconsistent. However, using CloudFront with Cache-Control: no-cache helps mitigate this. Cache-Control: no-\ncache instructs CloudFront to always validate with the origin (S3) before serving content. This effectively\nforces CloudFront to retrieve the latest version from S3 frequently, thus satisfying the requirement for\nreturning the new content as soon as the changes occur. Despite the presence of a cache, using no-cache\ntransforms it into a validation proxy rather than a classic cache where stale content could be served. This\nensures that updates are swiftly reflected to users.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html\nWhy other options are not suitable:\nA. Use AWS Storage Gateway Volume Gateway Internet Small Computer Systems Interface (iSCSI) block\nstorage that is mounted to the individual EC2 instances. Volume Gateway doesn't inherently provide shared\naccess between multiple EC2 instances across different Availability Zones in a way that guarantees\nconsistency for rapidly changing web content without complex application-level coordination and\nmanagement.\nC. Create a shared Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on the\nindividual EC2 instances. EBS volumes cannot be simultaneously attached to multiple EC2 instances except\nin certain limited scenarios with specific Nitro-based instances and clustered applications. This option is\ntherefore fundamentally unsuitable for a web application deployed across multiple Availability Zones and\nrelying on Auto Scaling.\nD. Use AWS DataSync to perform continuous synchronization of data between EC2 hosts in the Auto\nScaling group. While DataSync excels at data transfer, it does not provide the strong consistency needed.\nDataSync performs asynchronous transfers, meaning changes will not be immediately reflected across all\ninstances. The application would experience inconsistencies during the synchronization window. Furthermore,\nusing DataSync between multiple EC2 instances in an Auto Scaling group is an anti-pattern and extremely\ncomplex to manage reliably.",
    "links": [
      "https://aws.amazon.com/efs/",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is deploying an application in three AWS Regions using an Application Load Balancer. Amazon Route\n53 will be used to distribute traffic between these Regions.\nWhich Route 53 configuration should a solutions architect use to provide the MOST high-performing experience?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B: Create an A record with a geolocation policy.\nHere's why:\nGeolocation Routing: Geolocation routing in Route 53 directs traffic based on the geographic location of the\nuser making the request. This means users are sent to the AWS Region that is geographically closest to them.\nThis minimizes latency, providing a faster and more responsive experience for the user. This directly\naddresses the \"MOST high-performing experience\" requirement.\nLatency Routing (Option A): While latency routing also aims to minimize latency, it relies on measuring the\nlatency from Route 53's DNS servers to the application endpoints. This measurement might not perfectly\nreflect the actual latency experienced by end users, especially users far from the DNS servers. Geolocation is\nmore accurate.\nCNAME Records (Options C & D): CNAME records map a domain name to another domain name, not directly\nto IP addresses. Using CNAME records would add an extra DNS lookup, which could increase latency. A\nrecords map a domain name directly to an IP address.\nFailover Policy (Option C): A failover policy is primarily used for high availability and disaster recovery, not for\noptimizing performance based on location. It directs traffic to a secondary region only when the primary\nregion is unavailable.\nGeoproximity Routing (Option D): Geoproximity routing routes traffic based on geographic proximity with the\noption of biasing traffic to specific locations. While useful, it's more complex to configure and may not provide\nas consistent a performance improvement as simple geolocation-based routing when the goal is merely to\ndirect users to the nearest region. It also typically requires Route 53 Traffic Flow which adds extra cost.\nIn summary, geolocation routing provides the most direct and effective way to minimize latency and improve\nperformance for users across multiple AWS Regions, aligning perfectly with the prompt's requirement for the\n\"MOST high-performing experience.\" It maps user location to the closest AWS region, unlike latency routing\nthat depends on less precise measurements, and avoids the additional DNS lookup associated with CNAME\nrecords.\nSupporting Documentation:\nRoute 53 Geolocation Routing: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-\npolicy.html#routing-policy-geo\nRoute 53 Routing Policies: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html",
    "links": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has a web application that includes an embedded NoSQL database. The application runs on Amazon\nEC2 instances behind an Application Load Balancer (ALB). The instances run in an Amazon EC2 Auto Scaling group\nin a single Availability Zone.\nA recent increase in traffic requires the application to be highly available and for the database to be eventually\nconsistent.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the most appropriate solution:\nThe core requirements are high availability and eventual consistency for the application and its database, all\nwhile minimizing operational overhead. The existing architecture has several limitations: running in a single\nAvailability Zone makes it vulnerable to AZ failures, and relying on an embedded NoSQL database within EC2\ninstances complicates replication and scaling, increasing operational burden.\nOption D addresses these issues directly. Spanning the EC2 Auto Scaling group across three Availability\nZones (AZs) inherently improves high availability. If one AZ experiences an outage, the application continues\nto run in the remaining AZs. [https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-\navailability-zones.html]\nMigrating the embedded NoSQL database to Amazon DynamoDB fulfills the high availability and eventual\nconsistency requirements while significantly reducing operational overhead. DynamoDB is a fully managed\nNoSQL database service that automatically handles replication, scaling, and patching. This eliminates the\nneed for manual database administration, patching, and scaling efforts. DynamoDB also offers built-in\nfeatures for high availability and eventual consistency, aligning perfectly with the requirements.\n[https://aws.amazon.com/dynamodb/] AWS DMS can efficiently handle the database migration.\nOption A is incorrect because Network Load Balancers do not offer significant advantages over Application\nLoad Balancers in this specific scenario for web applications and do not address the database's availability or\nscalability issues. It maintains the complexities of managing an embedded NoSQL database.\nOption B suffers from the same database management issue as Option A, while also utilizing Network Load\nBalancers which aren't optimal for this specific web application. It solves the DB portion but not the AZ part.\nOption C addresses the Availability Zone issue, but keeps the complexity of managing the embedded NoSQL\ndatabase. This significantly increases the operational overhead. Maintaining replication within the EC2\ninstances requires manual configuration, monitoring, and troubleshooting.Therefore, only Option D addresses\nboth the availability of the application (through multi-AZ deployment) and the availability and manageability\nof the database (through DynamoDB), while minimizing operational overhead through a fully managed\ndatabase service.",
    "links": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-",
      "https://aws.amazon.com/dynamodb/]"
    ]
  },
  {
    "question": "CertyIQ\nA company is building a shopping application on AWS. The application offers a catalog that changes once each\nmonth and needs to scale with traffic volume. The company wants the lowest possible latency from the\napplication. Data from each user's shopping cart needs to be highly available. User session data must be available\neven if the user is disconnected and reconnects.\nWhat should a solutions architect do to ensure that the shopping cart data is preserved at all times?",
    "options": {},
    "answer": "B",
    "explanation": "The correct solution leverages Amazon ElastiCache for Redis to ensure highly available shopping cart data\npreservation. Here's why:\nOption A is incorrect because sticky sessions provided by an Application Load Balancer only ensure that a\nuser's requests are routed to the same backend server. While it can improve performance by keeping session\ndata in memory on that server, it doesn't guarantee data persistence if that server fails. Session affinity is not\na reliable method for highly available data storage.\nOption C is incorrect because while OpenSearch Service is suitable for searching and analyzing large volumes\nof data, it's not the optimal choice for caching frequently accessed data like shopping carts. OpenSearch is\nnot designed for low-latency, key-value retrieval in the same way that a caching service like Redis is.\nAdditionally, using OpenSearch for session state management is not a standard practice.\nOption D is not ideal as EC2 instances with EBS volumes lack the high availability and scalability required for\nshopping cart data. Relying on a single EC2 instance creates a single point of failure. Automated snapshots\nare a good backup strategy, but they don't provide real-time data availability during instance failures.\nRecovery from snapshots also involves downtime.\nOption B utilizes Amazon ElastiCache for Redis, a highly available, in-memory data store, is ideal for caching\nshopping cart data and user session data. Redis offers very low latency read and write operations, satisfying\nthe requirement for low latency. Crucially, ElastiCache provides managed Redis clusters with automatic\nfailover, ensuring high availability. If a Redis node fails, ElastiCache automatically promotes a replica to the\nprimary role, minimizing downtime. By caching catalog data from DynamoDB in ElastiCache, the solution also\nimproves the performance of catalog access. Furthermore, Redis's persistence options (RDB snapshots and\nAOF) can provide durability for the shopping cart data, although, for shopping carts, eventual consistency can\nusually be tolerated in the event of a loss in the cache.\nRelevant documentation:\nAmazon ElastiCache for Redis: https://aws.amazon.com/elasticache/redis/\nRedis Data Persistence: https://redis.io/docs/management/persistence/\nApplication Load Balancer sticky sessions:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/sticky-sessions.html",
    "links": [
      "https://aws.amazon.com/elasticache/redis/",
      "https://redis.io/docs/management/persistence/",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/sticky-sessions.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is building a microservices-based application that will be deployed on Amazon Elastic Kubernetes\nService (Amazon EKS). The microservices will interact with each other. The company wants to ensure that the\napplication is observable to identify performance issues in the future.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "Option B is the correct solution because it directly addresses the observability requirements for a\nmicroservices application deployed on Amazon EKS.\nAmazon CloudWatch Container Insights: This service is specifically designed to collect, aggregate, and\nsummarize container logs and metrics from containerized applications and microservices. It provides insights\ninto the performance of EKS clusters, including CPU utilization, memory usage, network traffic, and disk I/O at\nthe cluster, node, pod, and container levels. This detailed visibility is crucial for identifying performance\nbottlenecks within the EKS environment.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights.html\nAWS X-Ray: X-Ray is a distributed tracing system that helps developers analyze and debug production,\ndistributed applications, such as those built using a microservices architecture. It tracks requests as they\ntravel through the different microservices, providing a trace map that shows the latency and dependencies\nbetween services. This allows developers to pinpoint which microservice is causing performance issues or\nerrors. https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html\nTogether, CloudWatch Container Insights and X-Ray offer a comprehensive observability solution. Container\nInsights monitors the performance of the EKS infrastructure, while X-Ray traces requests across\nmicroservices, enabling the company to identify and resolve performance issues in their microservices-based\napplication.\nLet's examine why the other options are incorrect:\nOption A: ElastiCache is a caching service that can improve application performance by reducing database\nload, but it does not provide observability into the microservices themselves. Caching can be part of an overall\nsolution, but doesn't fulfill the observability requirement.\nOption C: CloudTrail tracks API calls made to AWS services. While useful for auditing and security, it doesn't\nprovide insight into the performance or interactions of the microservices themselves. QuickSight can visualize\ndata, but it relies on data sources; CloudTrail isn't the right source for application performance.\nOption D: Trusted Advisor provides recommendations on cost optimization, security, fault tolerance, and\nperformance, but it doesn't offer detailed observability data into the internal workings of the microservices\napplication. It's a high-level check, not a monitoring tool.",
    "links": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights.html",
      "https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to provide customers with secure access to its data. The company processes customer data and\nstores the results in an Amazon S3 bucket.\nAll the data is subject to strong regulations and security requirements. The data must be encrypted at rest. Each\ncustomer must be able to access only their data from their AWS account. Company employees must not be able to\naccess the data.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The correct solution is C, which involves using separate AWS KMS keys for each customer and controlling\naccess through KMS key policies. Let's break down why:\nData Encryption at Rest: The requirement of data encryption at rest is crucial. Both client-side and server-\nside encryption address this, but server-side encryption is generally preferred for S3 as it simplifies key\nmanagement on the client side. AWS KMS (Key Management Service) is the go-to service for managing\nencryption keys within AWS.\nCustomer-Specific Keys: Having a dedicated KMS key per customer ensures strong isolation. If one key is\ncompromised (theoretically), it only affects data encrypted with that specific key, limiting the blast radius.\nThis approach aligns well with strong regulatory and security requirements.\nAccess Control: Each customer should only be able to access their data. This requires fine-grained access\ncontrol at the key level. This is precisely what KMS key policies are for.\nKey Policy vs. Bucket Policy: S3 bucket policies can control access to objects within the bucket, but they\ncannot control access to the KMS key itself. To prevent unauthorized decryption of the data, you need to\ncontrol who can use the KMS key to decrypt the data. Key policies are specifically designed to manage the\nuse of the KMS key itself, including controlling who can encrypt or decrypt data with it.\nDenying Access to Company Employees: The key policies can explicitly deny decryption permissions to\ncompany employees. This is a critical aspect of the solution to meet the requirement that company employees\nmust not be able to access customer data.\nUsing IAM Roles: The solution mentions using IAM roles that the customers provide. Customers can create an\nIAM role within their AWS account and grant it permission to decrypt the data using the appropriate KMS key\nin the company's account. This allows for secure cross-account access.\nWhy other options are incorrect:\nOptions A and D (Using ACM Certificates): AWS Certificate Manager (ACM) is used for managing SSL/TLS\ncertificates for securing network traffic (HTTPS). It's not designed for encrypting data at rest. ACM\ncertificates are for encryption in transit, not at rest. Although you could technically perform client-side\nencryption using the certificate, it's not the intended use case, and KMS is a more appropriate solution.\nAdditionally, ACM policies don't offer the same level of granular control over decryption as KMS key policies.\nOption B (Using S3 Bucket Policy): While S3 bucket policies are important for overall bucket access control,\nthey can't directly control access to the KMS key used for server-side encryption. The bucket policy can\ndefine who can access the encrypted objects, but the KMS key policy determines who can decrypt those\nobjects.\nAuthoritative Links:\nAWS KMS Key Policies: https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html\nAmazon S3 Encryption: https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html\nAWS IAM Roles: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html",
    "links": [
      "https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect creates a VPC that includes two public subnets and two private subnets. A corporate security\nmandate requires the solutions architect to launch all Amazon EC2 instances in a private subnet. However, when\nthe solutions architect launches an EC2 instance that runs a web server on ports 80 and 443 in a private subnet, no\nexternal internet traffic can connect to the server.\nWhat should the solutions architect do to resolve this issue?",
    "options": {
      "B": "The ALB can handle SSL/TLS termination, protect against common web exploits (e.g., using"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the correct answer, and why the other options are incorrect:\nJustification for Option B (Correct)\nOption B, provisioning an internet-facing Application Load Balancer (ALB) in a public subnet, and associating\nthe EC2 instance in the private subnet as a target, is the standard and recommended approach for exposing\nweb applications securely and scalably in AWS.\n1. Public Subnet ALB: An ALB placed in a public subnet is reachable from the internet. It receives\nincoming HTTP/HTTPS requests on ports 80 and 443.\n2. Private Subnet EC2: The EC2 instance running the web server resides securely in a private subnet,\nshielded from direct internet access. This aligns with the security mandate.\n3. ALB Target Group: The EC2 instance is registered as a target within a target group associated with\nthe ALB. The ALB forwards requests to the EC2 instance in the private subnet.\n4. DNS Resolution: The DNS record for the website resolves to the ALB's DNS name. Therefore, all\ninternet traffic directed to the website first goes to the ALB.\n5. Security Benefits: This architecture provides enhanced security by hiding the EC2 instance behind\nthe ALB. The ALB can handle SSL/TLS termination, protect against common web exploits (e.g., using\nAWS WAF), and provide health checks.\n6. Scalability and Availability: ALBs are highly scalable and can distribute traffic across multiple EC2\ninstances in the private subnet, improving application availability and performance. The ALB can\nautomatically scale resources based on traffic demand.\nWhy other options are incorrect:\nOption A: While Auto Scaling helps with scaling and availability, it doesn't inherently solve the issue of\nexposing the web server to the internet from a private subnet. DNS resolution to an Auto Scaling group\nidentifier is not standard practice and doesn't provide direct internet access to the instances.\nOption C: NAT Gateways allow instances in a private subnet to initiate outbound traffic to the internet (e.g., to\ndownload updates or access external services). However, they do not allow unsolicited inbound traffic from\nthe internet to reach the instances. A NAT Gateway is for outbound communication only.\nOption D: Even if the security group allows inbound HTTP/HTTPS traffic, the EC2 instance is in a private\nsubnet. Private subnets by definition do not have direct internet connectivity. A DNS record pointing to the\nEC2 instance's public IP address (if it had one, which it ideally shouldn't in this scenario) would not work\nbecause the instance is not directly accessible from the internet.\nAuthoritative Links:\nApplication Load Balancers:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\nVPC with Public and Private Subnets (NAT):\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html\nSecurity Groups: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html",
    "links": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is deploying a new application to Amazon Elastic Kubernetes Service (Amazon EKS) with an AWS\nFargate cluster. The application needs a storage solution for data persistence. The solution must be highly\navailable and fault tolerant. The solution also must be shared between multiple application containers.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the correct answer and why the other options are not\nsuitable, focusing on minimizing operational overhead, high availability, fault tolerance, and shared access\nwithin an EKS cluster using Fargate.\nOption B (Create an Amazon Elastic File System (Amazon EFS) file system. Register the file system in a\nStorageClass object on an EKS cluster. Use the same file system for all containers.) is the best solution\nbecause it directly addresses the requirements with the least operational overhead. Amazon EFS is a fully\nmanaged, scalable, and highly available network file system ideal for sharing data between multiple\ncontainers in an EKS cluster. Registering the EFS file system with a StorageClass object simplifies\nprovisioning and dynamic mounting of the file system to pods within the EKS cluster. Since EFS is a managed\nservice, AWS handles the underlying infrastructure, including backups, replication, and scaling, minimizing\noperational tasks for the company. EFS is inherently designed for shared access, high availability across\nmultiple Availability Zones, and durability. This eliminates the need for manual configuration of replication or\nsynchronization mechanisms. Fargate doesn't manage underlying EC2 instances; therefore, storage solutions\nneed to be network-based for them to attach.\nOption A (Create Amazon Elastic Block Store (Amazon EBS) volumes in the same Availability Zones where\nEKS worker nodes are placed. Register the volumes in a StorageClass object on an EKS cluster. Use EBS\nMulti-Attach to share the data between containers.) is incorrect. While EBS Multi-Attach allows a single EBS\nvolume to be attached to multiple instances, its application with Fargate presents challenges. Fargate\nmanages the underlying compute infrastructure abstractly, making AZ placement of EBS volumes based on\nFargate nodes less predictable and manageable. Also, EBS Multi-Attach is more complex to set up than EFS\nand might not perfectly align with the shared filesystem needs of multiple containers. The availability zones\nof the Fargate compute cannot be chosen.\nOption C (Create an Amazon Elastic Block Store (Amazon EBS) volume. Register the volume in a StorageClass\nobject on an EKS cluster. Use the same volume for all containers.) is incorrect. EBS volumes are block storage\nand are not inherently designed for shared access between multiple instances or containers concurrently.\nAttempting to share a single EBS volume directly among multiple containers typically leads to data corruption\nand is not a recommended practice without advanced clustering or locking mechanisms, which increases\noperational complexity. Moreover, EBS volumes are zonal resources, so they would require additional steps\nfor high availability and fault tolerance. Fargate compute nodes live independently across different\nAvailability Zones, making this solution prone to errors due to zonal affinity.\nOption D (Create Amazon Elastic File System (Amazon EFS) file systems in the same Availability Zones where\nEKS worker nodes are placed. Register the file systems in a StorageClass object on an EKS cluster. Create an\nAWS Lambda function to synchronize the data between file systems.) is incorrect because it introduces\nunnecessary complexity. Creating multiple EFS file systems and synchronizing data between them using a\nLambda function adds significant operational overhead. The Lambda function requires maintenance,\nmonitoring, and configuration, increasing the management burden. A single EFS file system is already highly\navailable and durable, making data synchronization redundant and less efficient. Fargate doesn't grant you\ncontrol over underlying Availability Zone placements of compute nodes; therefore, choosing Availability\nZones for different EFS instances is problematic.\nIn summary, EFS provides a native, highly available, and shared filesystem solution that tightly integrates with\nEKS and Fargate, requiring minimal configuration and management. It aligns perfectly with the requirement\nfor data persistence and shared access with the least operational overhead.\nRelevant links:\nAmazon EFS: https://aws.amazon.com/efs/\nAmazon EKS: https://aws.amazon.com/eks/\nAWS Fargate: https://aws.amazon.com/fargate/",
    "links": [
      "https://aws.amazon.com/efs/",
      "https://aws.amazon.com/eks/",
      "https://aws.amazon.com/fargate/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an application that uses Docker containers in its local data center. The application runs on a\ncontainer host that stores persistent data in a volume on the host. The container instances use the stored\npersistent data.\nThe company wants to move the application to a fully managed service because the company does not want to\nmanage any servers or storage infrastructure.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "Here's a breakdown of why option B is the correct solution and why the others are not:\nThe requirement specifies a fully managed service where the company doesn't want to manage servers or\nstorage infrastructure. This immediately points us toward using a serverless container orchestration solution\nwith a fully managed storage service.\nOption B: Use Amazon Elastic Container Service (Amazon ECS) with an AWS Fargate launch type. Create an\nAmazon Elastic File System (Amazon EFS) volume. Add the EFS volume as a persistent storage volume\nmounted in the containers.\nAmazon ECS with Fargate: Fargate is a serverless compute engine for ECS that eliminates the need to\nmanage EC2 instances. It's a fully managed solution.\nAmazon EFS: EFS is a fully managed, scalable, elastic network file system. It can be mounted across multiple\nEC2 instances and, importantly, with Fargate tasks, providing persistent storage accessible to the containers\nwithout requiring server or storage management. This addresses the need for persistent data storage.\nMounting EFS Volume: The containers can be configured to mount the EFS volume, allowing them to read\nand write persistent data.\nWhy the other options are incorrect:\nOption A: Amazon EKS with self-managed nodes and EBS: EKS can be used for container orchestration, but\nself-managed nodes involve managing EC2 instances. EBS is also tied to a specific EC2 instance, making it\nless suitable for shared, persistent storage in a dynamic container environment. This violates the \"fully\nmanaged\" requirement because you must maintain the underlying EC2 instances.\nOption C: Amazon ECS with Fargate and S3: While Fargate is suitable, S3 is not designed as a file system for\ndirect mounting into containers and accessing files like a typical file system. S3 is object storage, and you'd\nneed additional layers or custom code to treat it as a persistent volume in this scenario. This adds\nunnecessary complexity and potentially impacts application performance compared to EFS. S3 is also\noptimized for storing large amounts of data that is typically not frequently modified within a running\napplication.\nOption D: Amazon ECS with EC2 launch type and EFS: The EC2 launch type requires you to manage the EC2\ninstances, directly contradicting the requirement for a fully managed service.\nIn summary, the correct answer provides a completely serverless and fully managed approach to container\norchestration and persistent storage, meeting all the stated requirements.\nSupporting Links:\nAWS Fargate: https://aws.amazon.com/fargate/\nAmazon EFS: https://aws.amazon.com/efs/\nECS Storage Configuration: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-storage-\nconfiguration.html",
    "links": [
      "https://aws.amazon.com/fargate/",
      "https://aws.amazon.com/efs/",
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-storage-"
    ]
  },
  {
    "question": "CertyIQ\nA gaming company wants to launch a new internet-facing application in multiple AWS Regions. The application\nwill use the TCP and UDP protocols for communication. The company needs to provide high availability and\nminimum latency for global users.\nWhich combination of actions should a solutions architect take to meet these requirements? (Choose two.)",
    "options": {
      "A": "Create internal Network Load Balancers in front of the application in each Region. This is a good starting",
      "C": "Create an AWS Global Accelerator accelerator to route traffic to the load balancers in each Region. AWS",
      "B": "Create external Application Load Balancers in front of the application in each Region. Application Load",
      "D": "Configure Amazon Route 53 to use a geolocation routing policy to distribute the traffic. While Route 53's"
    },
    "answer": "A",
    "explanation": "Here's a breakdown of why options A and C are correct and why the others are not:\nA. Create internal Network Load Balancers in front of the application in each Region. This is a good starting\npoint. Network Load Balancers (NLBs) are designed for handling TCP and UDP traffic efficiently and at high\nscale. Because the question states the application is internet-facing, it may be confusing to suggest internal\nNLBs. However, the NLBs are a necessary component of the overall solution because they manage traffic\ndistribution within each region. They distribute traffic across multiple instances of the application for high\navailability within that region.\nC. Create an AWS Global Accelerator accelerator to route traffic to the load balancers in each Region. AWS\nGlobal Accelerator is designed to improve the availability and performance of applications for global users. It\nuses the AWS global network to route user traffic to the optimal endpoint based on geographic location,\nnetwork health, and application health. Using Global Accelerator in front of the NLBs in each region ensures\nusers are directed to the closest and healthiest endpoint, reducing latency and improving the overall user\nexperience. Global Accelerator works with both TCP and UDP.\nLet's analyze why the other options are incorrect:\nB. Create external Application Load Balancers in front of the application in each Region. Application Load\nBalancers (ALBs) primarily handle HTTP/HTTPS traffic. While ALBs could handle some of the traffic if the\ngaming application also has a web-based component, they do not support UDP, which is a key requirement.\nTherefore, ALBs alone are insufficient.\nD. Configure Amazon Route 53 to use a geolocation routing policy to distribute the traffic. While Route 53's\ngeolocation routing is useful for directing traffic based on user location, it relies on DNS resolution. This\nmakes it slower to react to changes in network conditions or application health than AWS Global Accelerator.\nIt doesn't offer the same performance advantages. Route 53 also involves client-side DNS caching, which can\ndelay failover.\nE. Configure Amazon CloudFront to handle the traffic and route requests to the application in each Region.\nAmazon CloudFront is a content delivery network (CDN) optimized for caching and delivering static and\ndynamic content. While CloudFront can accelerate web applications, it is not primarily designed for handling\nreal-time, low-latency TCP and UDP traffic like a gaming application. While CloudFront can now support UDP\nthrough WebSockets, it doesn't provide the global routing and optimization benefits that Global Accelerator\noffers. Using CloudFront also might not fit with the architectural design of this application as other aspects of\nthis question point towards Global Accelerator.\nIn summary, the combination of internal NLBs for regional high availability and AWS Global Accelerator for\nglobal routing and low latency is the best solution.\nRelevant Documentation:\nAWS Global Accelerator: https://aws.amazon.com/global-accelerator/\nNetwork Load Balancer: https://aws.amazon.com/elasticloadbalancing/network-load-balancer/\nAmazon Route 53: https://aws.amazon.com/route53/\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
    "links": [
      "https://aws.amazon.com/global-accelerator/",
      "https://aws.amazon.com/elasticloadbalancing/network-load-balancer/",
      "https://aws.amazon.com/route53/",
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/"
    ]
  },
  {
    "question": "CertyIQ\nA city has deployed a web application running on Amazon EC2 instances behind an Application Load Balancer\n(ALB). The application's users have reported sporadic performance, which appears to be related to DDoS attacks\noriginating from random IP addresses. The city needs a solution that requires minimal configuration changes and\nprovides an audit trail for the DDoS sources.\nWhich solution meets these requirements?",
    "options": {
      "B": "Therefore, AWS Shield Advanced and engagement of the DRT offer the most effective solution for mitigating"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it offers a comprehensive solution to the described problem, addressing both\nmitigation and reporting of DDoS attacks with minimal configuration changes.\nHere's a breakdown of why:\nAWS Shield Advanced: This service provides enhanced DDoS protection beyond the standard DDoS\nprotections included with AWS Shield Standard (which is automatically enabled for all AWS customers).\nSpecifically, it provides deeper inspection of traffic patterns and offers advanced detection and mitigation\ntechniques tailored to the specific application and attack vectors. It is designed for applications running on\nservices like EC2, ELB, CloudFront, Global Accelerator, and Route 53. https://aws.amazon.com/shield/\nAWS DDoS Response Team (DRT): A crucial advantage of Shield Advanced is direct access to the AWS DDoS\nResponse Team. This team of experts can assist in understanding attack patterns and integrating custom\nmitigation controls tailored to the unique characteristics of the detected DDoS attacks. This hands-on expert\nsupport is vital for effectively responding to complex or rapidly evolving attacks.\nAudit Trail and Reporting: Shield Advanced offers detailed reports and dashboards that provide visibility into\nDDoS events, including the source IP addresses and attack vectors used. This is a key requirement for the city,\nas they need an audit trail to understand the origins of the attacks.\nLet's examine why the other options are less suitable:\nOption A (AWS WAF on ALB): While AWS WAF can block traffic based on rules, it requires significant\nconfiguration and maintenance to identify and block unknown sources effectively, especially with rapidly\nchanging IP addresses in a DDoS attack. It doesn't provide the deep inspection and automated mitigation\ncapabilities of Shield Advanced. Furthermore, manually creating and maintaining rules under a DDoS attack's\npressure can be cumbersome.\nOption B (Amazon Inspector and AWS DRT): Amazon Inspector is a vulnerability management service, not a\nDDoS protection service. While DRT can provide mitigation support, subscribing to Inspector is unrelated to\nDDoS mitigation and won't solve the city's problem efficiently. Inspector focuses on security assessments and\nvulnerability detection within EC2 instances and container images, whereas the city needs immediate\nprotection against external network attacks.\nOption D (CloudFront with WAF): While using CloudFront as a CDN with WAF is a good security practice, it\ndoesn't offer the same level of DDoS protection as Shield Advanced, especially against sophisticated attacks.\nAdditionally, introducing CloudFront involves architectural changes to the application, which goes against the\nrequirement of minimal configuration changes. Furthermore, CloudFront primarily protects the origin (ALB in\nthis case) by caching content and absorbing some of the traffic, but a determined DDoS attack can still\noverload the ALB.\nTherefore, AWS Shield Advanced and engagement of the DRT offer the most effective solution for mitigating\nDDoS attacks, providing a comprehensive audit trail, and requiring minimal architectural changes to the\nexisting application. This makes it the best answer.",
    "links": [
      "https://aws.amazon.com/shield/"
    ]
  },
  {
    "question": "CertyIQ\nA company copies 200 TB of data from a recent ocean survey onto AWS Snowball Edge Storage Optimized\ndevices. The company has a high performance computing (HPC) cluster that is hosted on AWS to look for oil and\ngas deposits. A solutions architect must provide the cluster with consistent sub-millisecond latency and high-\nthroughput access to the data on the Snowball Edge Storage Optimized devices. The company is sending the\ndevices back to AWS.\nWhich solution will meet these requirements?",
    "options": {
      "C": "Its architecture"
    },
    "answer": "D",
    "explanation": "The requirement is to provide the HPC cluster with sub-millisecond latency and high-throughput access to\n200 TB of data after importing it from Snowball Edge devices. Option D, creating an Amazon FSx for Lustre\nfile system and directly importing the data into it, best meets these stringent requirements.\nFSx for Lustre is specifically designed for high-performance workloads, including HPC. Its architecture\nprovides the necessary low latency and high throughput. The data is directly imported into the Lustre file\nsystem, eliminating the need for intermediate storage or gateways, and reducing potential bottlenecks.\nOptions A, B, and C involve S3, which, while scalable and durable, isn't optimized for the sub-millisecond\nlatency demanded by the HPC cluster. AWS Storage Gateway File Gateway (option A) introduces latency.\nWhile FSx for Lustre can be integrated with S3 (option B), directly importing the data circumvents S3\naltogether, leading to lower latency and complexity. EFS (option C) is suitable for shared file storage but\ntypically doesn't match the performance of FSx for Lustre for demanding HPC workloads. Furthermore,\ncopying data from S3 to EFS adds unnecessary time and complexity. Importing directly to FSx for Lustre is\nthe streamlined and performant choice, making it the most suitable solution.\nhttps://aws.amazon.com/fsx/lustre/https://aws.amazon.com/hpc/",
    "links": [
      "https://aws.amazon.com/fsx/lustre/https://aws.amazon.com/hpc/"
    ]
  },
  {
    "question": "CertyIQ\nA company has NFS servers in an on-premises data center that need to periodically back up small amounts of data\nto Amazon S3.\nWhich solution meets these requirements and is MOST cost-effective?",
    "options": {
      "A": "AWS Glue: AWS Glue is a fully managed extract, transform, and load (ETL) service. While it could be used",
      "C": "AWS Transfer for SFTP: AWS Transfer for SFTP (formerly AWS SFTP) is a managed SFTP service. While it",
      "D": "AWS Direct Connect: AWS Direct Connect establishes a dedicated network connection between the on-"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B (AWS DataSync) is the most cost-effective solution for backing\nup small amounts of data from on-premises NFS servers to Amazon S3, along with explanations of why the\nother options are less suitable:\nWhy AWS DataSync is the Best Choice:\nAWS DataSync is specifically designed for efficient and automated data transfer between on-premises\nstorage and AWS services like S3. It leverages a purpose-built agent installed on-premises to handle data\ntransfer optimization, including compression, encryption, and incremental transfers. For small amounts of\ndata, the ease of setup and the efficiency of DataSync's transfer mechanisms offer a distinct cost advantage.\nDataSync minimizes the amount of data transferred by only transferring changed blocks, which significantly\nreduces network bandwidth usage and S3 storage costs. The agent's management of the connection helps\nensure reliable and secure data transfer without needing constant intervention. DataSync provides detailed\nmonitoring and reporting, giving visibility into the transfer process. The pay-as-you-go pricing model of\nDataSync, which charges only for the data transferred, is ideal for periodic backups of small datasets,\navoiding the fixed costs associated with other solutions.\nWhy Other Options are Less Suitable:\nA. AWS Glue: AWS Glue is a fully managed extract, transform, and load (ETL) service. While it could be used\nto move data, it's fundamentally designed for data processing and transformation at scale, not simple file\nreplication. The overhead of setting up Glue jobs and the associated compute resources make it overkill and\nmore expensive for this use case. Furthermore, Glue is geared towards structured and semi-structured data,\nnot necessarily unstructured files on an NFS share.\nC. AWS Transfer for SFTP: AWS Transfer for SFTP (formerly AWS SFTP) is a managed SFTP service. While it\nfacilitates secure file transfer, it requires setting up and managing SFTP clients and users on the on-premises\nside. This adds complexity and overhead compared to DataSync's agent-based approach. Additionally,\nTransfer for SFTP typically involves transferring entire files, potentially leading to higher bandwidth\nconsumption compared to DataSync's incremental transfer capability.\nD. AWS Direct Connect: AWS Direct Connect establishes a dedicated network connection between the on-\npremises data center and AWS. While providing high bandwidth and potentially lower latency, it is\nsignificantly more expensive than other options, especially for small data transfers. Direct Connect involves\nupfront costs (port fees, router configuration) and monthly recurring charges, making it unsuitable for\ninfrequent backups of small datasets. This option is an overkill solution as this connection is not needed for a\nsmall amount of data.\nAuthoritative Links:\nAWS DataSync: https://aws.amazon.com/datasync/\nAWS Glue: https://aws.amazon.com/glue/\nAWS Transfer Family: https://aws.amazon.com/aws-transfer-family/\nAWS Direct Connect: https://aws.amazon.com/directconnect/",
    "links": [
      "https://aws.amazon.com/datasync/",
      "https://aws.amazon.com/glue/",
      "https://aws.amazon.com/aws-transfer-family/",
      "https://aws.amazon.com/directconnect/"
    ]
  },
  {
    "question": "CertyIQ\nAn online video game company must maintain ultra-low latency for its game servers. The game servers run on\nAmazon EC2 instances. The company needs a solution that can handle millions of UDP internet traffic requests\neach second.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C, using a Network Load Balancer (NLB), is the most cost-\neffective solution for the online video game company's requirements:\nThe primary requirements are ultra-low latency and the ability to handle millions of UDP traffic requests per\nsecond for game servers running on EC2 instances.\nNetwork Load Balancer (NLB): NLBs are designed for high performance and low latency, operating at the\ntransport layer (Layer 4) of the OSI model. This makes them well-suited for UDP traffic. They can handle\nmillions of requests per second and are optimized for TCP, UDP, and TLS traffic. NLBs provide static IP\naddresses per Availability Zone, which can be beneficial for game clients needing consistent endpoints.\nCrucially, NLBs offer the lowest latency compared to other load balancer types.\nApplication Load Balancer (ALB): ALBs operate at the application layer (Layer 7), offering features like\ncontent-based routing. However, these features add overhead, increasing latency compared to NLBs. While\nALBs support UDP, they are not optimized for this protocol as much as NLBs are.\nGateway Load Balancer (GWLB): GWLBs are designed for integrating third-party network virtual appliances\nlike firewalls and intrusion detection systems. They are not the best choice for directly load balancing game\nserver traffic as their main purpose lies in routing traffic through security appliances. They also introduce\nmore latency than NLBs.\nMulti-Region Deployment: Launching identical game servers in multiple AWS Regions would increase cost\nsignificantly due to the duplication of infrastructure, data replication costs, and the complexity of managing\ndeployments and data consistency across regions. While it improves availability, it is not cost-effective for the\nstated requirements, especially when a load balancer can handle the traffic within a single region.\nThe justification emphasizes the NLB's low latency and ability to handle UDP traffic efficiently, fitting the\nrequirements for online games. Furthermore, compared to launching a separate set of EC2 instances in\ndifferent AWS Regions, the NLB provides a cost-effective solution for load balancing the incoming UDP\nrequests to the game servers. By comparison, the Gateway Load Balancer is for routing network traffic and\nthe Application Load Balancer operates at a higher layer, resulting in higher latency, making them less ideal\nfor the gaming use case.\nAuthoritative Links:\nAWS Load Balancing Documentation: https://docs.aws.amazon.com/elasticloadbalancing/index.html\nNetwork Load Balancer: https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\nApplication Load Balancer:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\nGateway Load Balancer:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/gateway/introduction.html",
    "links": [
      "https://docs.aws.amazon.com/elasticloadbalancing/index.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/gateway/introduction.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a three-tier application in a VP",
    "options": {
      "C": "The database tier uses an Amazon RDS for MySQL DB instance."
    },
    "answer": "A",
    "explanation": "The correct answer is A and D because these choices offer the most effective approach for migrating data\nfrom RDS for MySQL to Aurora PostgreSQL with minimal downtime and data loss.\nOption A is correct because AWS DMS Schema Conversion is specifically designed to translate database\nschemas from one engine to another. MySQL and PostgreSQL have different syntaxes and data types, so\nconverting the schema is a crucial first step before migrating the data. This ensures that the Aurora\nPostgreSQL database is ready to receive the data in the correct format.\nOption D is correct because AWS DMS with Change Data Capture (CDC) is the recommended way to\ncontinuously replicate data changes from the source database to the target database. CDC captures all the\nupdates, inserts, and deletes happening on the RDS for MySQL database and applies them to the Aurora\nPostgreSQL DB cluster. This ensures that the Aurora PostgreSQL database stays synchronized with the RDS\nfor MySQL database during the migration process, minimizing downtime when switching over.\nOption B is incorrect because AWS DMS Schema Conversion doesn't create read replicas. It focuses on\nschema transformation only.\nOption C is incorrect because Aurora MySQL read replicas cannot be created for RDS for MySQL databases.\nRead replicas must be of the same database engine.\nOption E is incorrect because Aurora PostgreSQL read replicas cannot be created directly from RDS for\nMySQL DB instance.\nIn summary, the combination of AWS DMS Schema Conversion and AWS DMS with CDC provides a robust\nsolution for migrating data from RDS for MySQL to Aurora PostgreSQL with minimal downtime and data loss.\nDMS Schema Conversion handles the necessary schema translations, while DMS with CDC ensures\ncontinuous data replication, keeping the target database in sync with the source database.\nRelevant links:\nAWS Database Migration Service: https://aws.amazon.com/dms/\nAWS DMS Schema Conversion:\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_SchemaConversion.html\nAWS DMS Change Data Capture (CDC):\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Task.CDC.html",
    "links": [
      "https://aws.amazon.com/dms/",
      "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_SchemaConversion.html",
      "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Task.CDC.html"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts a database that runs on an Amazon RDS instance that is deployed to multiple Availability Zones.\nThe company periodically runs a script against the database to report new entries that are added to the database.\nThe script that runs against the database negatively affects the performance of a critical application. The company\nneeds to improve application performance with minimal costs.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "C": "Cost-Effective: Read replicas generally have lower costs than scaling the primary instance itself or"
    },
    "answer": "B",
    "explanation": "The correct answer is B: Create a read replica of the database. Configure the script to query only the read\nreplica to report the total new entries.\nHere's why this is the best solution:\nOffloading Read Workload: Read replicas are designed specifically to offload read-heavy workloads from the\nprimary database instance. This is exactly the problem the company is facing with its reporting script\nimpacting application performance.\nMinimal Operational Overhead: Creating a read replica is a straightforward process in RDS. Once created, the\nscript can be reconfigured to point to the read replica. RDS handles replication automatically, minimizing\noperational overhead. There are no extra steps needed to create this solution like in option C.\nCost-Effective: Read replicas generally have lower costs than scaling the primary instance itself or\nimplementing more complex solutions. This aligns with the requirement of minimizing costs.\nNo Application Code Changes (Mostly): The primary application should not need any modification, as it\ncontinues to read/write to the primary instance. Only the reporting script needs to be reconfigured to target\nthe read replica.\nImproved Application Performance: By directing the reporting script's queries to the read replica, the primary\ndatabase instance is freed from the performance impact, allowing the critical application to perform\noptimally.\nReplication Latency Consideration: While generally low, replication latency should be considered. Depending\non the acceptable delay for reporting data, this might influence the choice of read replica (e.g., same\nAvailability Zone for minimal latency).\nHere's why the other options are not as suitable:\nA: Adding connection-checking logic to the script is unnecessarily complex and doesn't fundamentally solve\nthe problem of the script impacting the primary database. It only attempts to mitigate the impact.\nC: Manual exporting is a tedious and error-prone process. It does not scale well and has high operational\noverhead. Also, it's not automated, contradicting the need for efficiency.\nD: ElastiCache is suitable for caching frequently accessed data to improve read performance, but it doesn't\ndirectly address the problem of offloading the reporting workload. Furthermore, setting up and managing\ncaching invalidation can introduce unnecessary complexity for this particular scenario.\nAuthoritative Links:\nAmazon RDS Read Replicas:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Aurora.Managing.Replication.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Aurora.Managing.Replication.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is using an Application Load Balancer (ALB) to present its application to the internet. The company\nfinds abnormal traffic access patterns across the application. A solutions architect needs to improve visibility into\nthe infrastructure to help the company understand these abnormalities better.\nWhat is the MOST operationally efficient solution that meets these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The most operationally efficient solution to analyze abnormal traffic patterns accessing an application via an\nApplication Load Balancer (ALB) is to enable ALB access logging to Amazon S3 and then query the logs using\nAmazon Athena. This approach combines the benefits of comprehensive logging and cost-effective,\nserverless querying.\nOption B is the best because ALB access logs provide detailed information about requests received by the\nALB, including the source IP address, request path, latency, and response codes. Storing these logs in\nAmazon S3 provides a durable and scalable storage solution. Amazon Athena allows you to analyze the S3-\nbased log data using standard SQL queries without the need for infrastructure management. This combination\nenables efficient analysis of traffic patterns and identification of anomalies.\nOption A is less ideal because AWS CloudTrail logs are primarily for auditing API calls made to AWS services,\nnot for capturing detailed request-level data for the application itself. While CloudTrail provides valuable\ninsights, it does not provide the granularity needed to analyze abnormal traffic patterns.\nOption C is inefficient because manually opening and searching through individual log files in a text editor is\ntime-consuming, error-prone, and not scalable for large volumes of log data. This method doesn't leverage the\nadvantages of automated analysis.\nOption D is unnecessarily complex and expensive. Amazon EMR is designed for large-scale data processing\nand analysis, but using it on a dedicated EC2 instance solely to query the ALB directly is overkill for this use\ncase. The overhead of managing and configuring EMR is significant compared to the simplicity of Athena.\nAdditionally, querying the ALB directly is not a standard practice for retrieving access logs.\nIn conclusion, Option B offers a balance of comprehensive logging, scalable storage, and cost-effective\nanalysis. This allows for better visibility into the infrastructure, thus empowering the company to efficiently\nunderstand traffic abnormalities.\nRelevant Links:\nApplication Load Balancer Access Logs:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\nAmazon Athena: https://aws.amazon.com/athena/",
    "links": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html",
      "https://aws.amazon.com/athena/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to use NAT gateways in its AWS environment. The company's Amazon EC2 instances in private\nsubnets must be able to connect to the public internet through the NAT gateways.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The correct solution is to create public NAT gateways in public subnets within the same VPC as the EC2\ninstances residing in private subnets. NAT gateways enable instances in private subnets to initiate outbound\nconnections to the internet without allowing inbound connections from the internet.\nOption C is correct because public NAT gateways require placement in public subnets. These subnets have a\nroute to an internet gateway, allowing the NAT gateway to communicate with the internet. EC2 instances in\nprivate subnets then route their internet-bound traffic to the NAT gateway, which performs network address\ntranslation, replacing the private IP address of the instance with its own public IP address. The internet sees\nthe traffic originating from the NAT gateway's public IP. The NAT gateway remembers the source and\ndestination and forwards return traffic to the appropriate EC2 instance.\nOption A is incorrect because NAT gateways should not be created in the same private subnets as the EC2\ninstances. While this technically could work in a custom setup, it does not utilize the intended functionality of\na NAT gateway and introduces complexities. More importantly, NAT Gateways in private subnets are not\ndesigned to directly facilitate internet access.\nOption B is incorrect because \"private NAT gateways\" are not a recognized AWS service or feature. NAT\ngateways are designed for outbound internet access, a function that inherently requires a route to the\ninternet.\nOption D is incorrect. While placing something within a public subnet is the first step, creating a \"private NAT\ngateway\" implies that it is internal to the VPC only. A NAT Gateway must be \"public\" to give instances a route\nto the internet.\nIn summary, creating a public NAT gateway in a public subnet provides a secure and managed way for\ninstances in private subnets to connect to the internet.\nRelevant resources:\nAWS NAT Gateway Documentation: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has an organization in AWS Organizations. The company runs Amazon EC2 instances across four AWS\naccounts in the root organizational unit (OU). There are three nonproduction accounts and one production account.\nThe company wants to prohibit users from launching EC2 instances of a certain size in the nonproduction accounts.\nThe company has created a service control policy (SCP) to deny access to launch instances that use the prohibited\ntypes.\nWhich solutions to deploy the SCP will meet these requirements? (Choose two.)",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B and E. Let's break down why:\nWhy B is correct: Attaching the SCP directly to the three non-production member accounts is a\nstraightforward way to achieve the requirement. It directly targets the accounts where you want to restrict\nEC2 instance types. SCPs, when applied to an account, limit what identities (users, roles) within that account\ncan do. This is a precise application of the policy, ensuring that only the desired non-production accounts are\naffected.\nWhy E is correct: Creating a dedicated OU for the non-production accounts and then attaching the SCP to\nthat OU provides a more organized and scalable solution, especially if the number of non-production accounts\nis expected to grow. When an SCP is attached to an OU, it applies to all accounts contained within that OU.\nThis approach simplifies management because you only need to maintain the policy at the OU level, rather\nthan individually on each account.\nWhy A is incorrect: Attaching the SCP to the root OU would restrict instance types in all accounts within the\norganization, including the production account, which violates the requirement. SCPs applied at the root level\nact as the outermost limit.\nWhy C is incorrect: Attaching the SCP to the Organizations management account has no effect on the\nmember accounts' permissions. The management account manages the organization but doesn't inherently\nrestrict activities within its member accounts through SCPs.\nWhy D is incorrect: Creating an OU for the production account and attaching the SCP to that OU would\nessentially have no effect as the policy limits instance types, and it's being applied only to the production\naccount which they want to exclude from the restriction.\nTherefore, options B and E directly address the stated requirements by applying the restriction to the specific\nnon-production environments, either individually or as a grouped OU.\nHere are some authoritative links for further research:\nAWS Organizations documentation on Service Control Policies (SCPs)\nAWS Organizations concepts",
    "links": []
  },
  {
    "question": "CertyIQ\nA companys website hosted on Amazon EC2 instances processes classified data stored in Amazon S3. Due to\nsecurity concerns, the company requires a private and secure connection between its EC2 resources and Amazon\nS3.\nWhich solution meets these requirements?",
    "options": {},
    "answer": "A",
    "explanation": "The correct solution is to set up S3 bucket policies to allow access from a VPC endpoint. Here's why:\nThe core requirement is a private and secure connection between EC2 instances and S3, without traversing\nthe public internet. VPC endpoints for S3 address this directly by creating a secure, private connection within\nthe AWS network.\nOption A leverages VPC endpoints, specifically Gateway Endpoints for S3. These endpoints allow EC2\ninstances in a VPC to access S3 without requiring internet access or NAT gateways. Traffic destined for S3\nfrom the EC2 instances remains within the AWS network, enhancing security and reducing data transfer\ncosts. The S3 bucket policy is then configured to only allow access from the specified VPC endpoint, further\nrestricting access and enforcing the private connection requirement. This creates a secure channel, as traffic\nnever leaves the AWS network and is isolated from external access.\nOption B, setting up an IAM policy, while necessary for granting permissions, does not guarantee a private\nconnection. IAM policies control what actions an EC2 instance can perform on S3, but the traffic might still\ntraverse the internet if there isn't a mechanism to keep it within AWS.\nOption C, setting up a NAT gateway, is the opposite of what is required. A NAT gateway allows instances in a\nprivate subnet to initiate outbound traffic to the internet, which violates the security requirement of a private\nconnection to S3. It is used when the connection is internet-based.\nOption D, setting up an access key ID and a secret access key, also doesn't address the private connection\nrequirement. These keys are used for authentication, but the traffic to S3 can still go over the internet.\nMoreover, hardcoding access keys in EC2 instances is a security risk. Using IAM roles is a best practice.\nIn summary, VPC endpoints in conjunction with restrictive S3 bucket policies provide the private and secure\nconnection required, fulfilling the security concerns outlined in the problem. The traffic from EC2 to S3 will\nnot go through the internet, and the bucket policy will restrict access to the specific VPC endpoint.\nFor further research, refer to these AWS documentation links:\nVPC Endpoints: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\nS3 Bucket Policies: https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-policies.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-policies.html"
    ]
  },
  {
    "question": "CertyIQ\nAn ecommerce company runs its application on AWS. The application uses an Amazon Aurora PostgreSQL cluster\nin Multi-AZ mode for the underlying database. During a recent promotional campaign, the application experienced\nheavy read load and write load. Users experienced timeout issues when they attempted to access the application.\nA solutions architect needs to make the application architecture more scalable and highly available.\nWhich solution will meet these requirements with the LEAST downtime?",
    "options": {},
    "answer": "C",
    "explanation": "The best solution to improve scalability and availability of the Aurora PostgreSQL cluster with the least\ndowntime is option C: adding read replicas and implementing RDS Proxy.\nAdding Aurora read replicas (reader instances) allows offloading read traffic from the primary Aurora\ninstance. This immediately alleviates the read load on the primary, improving application performance and\nreducing timeouts. Aurora automatically replicates data to these read replicas, ensuring data consistency.\n[https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.ReadReplicas.html]\nRDS Proxy acts as a connection pooler in front of the database. By pooling database connections, it reduces\nthe overhead of establishing new connections for each request. This improves application response times and\nprotects the database from being overwhelmed by a large number of concurrent connections, especially\nduring peak periods. It enhances availability by allowing the application to continue operating even if the\nprimary database instance experiences temporary issues. [https://aws.amazon.com/rds/proxy/]\nOption A is incorrect because EventBridge and Lambda are not directly involved in scaling or improving the\navailability of the database itself. They are used for monitoring cluster state changes, not for addressing\nperformance issues. Adding read nodes to fail over to does not solve the load issue as read queries still go to\nthe primary node.\nOption B is incorrect because Zero-Downtime Restart (ZDR) primarily addresses planned maintenance events\nand does not help with scaling for heavy read/write loads. Database Activity Streams is for auditing purposes,\nnot performance optimization.\nOption D involves setting up ElastiCache with DMS. This requires significant time to set up and synchronize\ndata. Implementing a write-around cache can also introduce complexities with data consistency and is\ngenerally more suitable for scenarios with very high read-to-write ratios. It's also more complex and involves\nmore downtime compared to simply adding read replicas. It would be better if the company had the cache\nimplemented and tested before the promotional campaign. Therefore, this option takes time and has a\ndowntime risk.",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.ReadReplicas.html]",
      "https://aws.amazon.com/rds/proxy/]"
    ]
  },
  {
    "question": "CertyIQ\nA company is designing a web application on AWS. The application will use a VPN connection between the\ncompanys existing data centers and the company's VPCs.\nThe company uses Amazon Route 53 as its DNS service. The application must use private DNS records to\ncommunicate with the on-premises services from a VP",
    "options": {
      "C": "This approach is the most secure because it allows fine-grained control over which DNS queries are",
      "A": "Create a Route 53 Resolver outbound endpoint. Create a resolver rule. Associate"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Create a Route 53 Resolver outbound endpoint. Create a resolver rule. Associate\nthe resolver rule with the VPC.\nHere's a detailed justification:\nThe scenario requires resolving private DNS records of on-premises services from within an AWS VPC. This\nmeans that DNS queries originating from the VPC need to be forwarded to the on-premises DNS servers for\nresolution.\nRoute 53 Resolver Outbound Endpoint: An outbound endpoint allows DNS queries from the VPC to be\nforwarded to on-premises DNS servers. This is essential for resolving private DNS records hosted on-\npremises. It acts as a forwarder.\nResolver Rule: A resolver rule specifies the conditions under which DNS queries should be forwarded to the\non-premises DNS servers via the outbound endpoint. For instance, a rule can specify that any query for a\ndomain like \"internal.example.com\" should be forwarded to the on-premises DNS servers' IP addresses.\nAssociating the Rule with the VPC: This ensures that the resolver rule applies to all DNS queries originating\nfrom the specified VPC.\nThis approach is the most secure because it allows fine-grained control over which DNS queries are\nforwarded to the on-premises environment. You only forward queries that match your specific internal\ndomain, minimizing the exposure of your VPC's DNS traffic.\nOption B is incorrect because an inbound endpoint is used to resolve DNS records hosted in Route 53 from on-\npremises networks.\nOption C is incorrect because a private hosted zone is used to host private DNS records within AWS, not for\nresolving on-premises records from AWS. While useful for internal AWS services, it does not facilitate\ncommunication with on-premises resources in this scenario.\nOption D is incorrect because a public hosted zone is for publically accessible DNS records and is not\napplicable for resolving private on-premises records.\nSupporting Links:\nAWS Documentation on Route 53 Resolver:\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver.html\nAWS Documentation on Resolver Endpoints:\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver-endpoints.html\nAWS Knowledge Center: How do I configure Route 53 to resolve domain names in my private network from\nmy Amazon VPC?: https://aws.amazon.com/premiumsupport/knowledge-center/route-53-resolve-private-\ndomains/",
    "links": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver.html",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver-endpoints.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/route-53-resolve-private-"
    ]
  },
  {
    "question": "CertyIQ\nA company is running a photo hosting service in the us-east-1 Region. The service enables users across multiple\ncountries to upload and view photos. Some photos are heavily viewed for months, and others are viewed for less\nthan a week. The application allows uploads of up to 20 MB for each photo. The service uses the photo metadata to\ndetermine which photos to display to each user.\nWhich solution provides the appropriate user access MOST cost-effectively?",
    "options": {
      "B": "DAX can help with caching but doesn't"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the most cost-effective solution for the described photo\nhosting service:\nThe core requirement is cost-effective storage and retrieval of photos with varying access patterns. Option B\nproposes storing photos in Amazon S3 Intelligent-Tiering. This is ideal because Intelligent-Tiering\nautomatically moves data between frequent and infrequent access tiers based on usage patterns, optimizing\ncost without operational overhead. Heavily viewed photos remain in the more expensive, readily accessible\ntier, while rarely viewed photos are automatically moved to a cheaper tier. This automated tiering directly\naddresses the variable access pattern described in the question.\nFurthermore, storing photo metadata and S3 location in DynamoDB is a sound strategy. DynamoDB provides\nfast, low-latency access to this metadata, which is essential for determining which photos to display to users.\nUnlike storing the entire photo in DynamoDB (option A), this approach avoids the high cost and performance\nlimitations of storing large binary objects in a NoSQL database.\nOption A is inefficient because storing photos directly in DynamoDB is generally not recommended due to the\ncost associated with storing large binary objects in DynamoDB. DAX can help with caching but doesn't\naddress the fundamental inefficiency.\nOption C is less optimal because it manually transitions photos to S3 Standard-IA after a fixed period (30\ndays). This might not be the most cost-effective strategy as it doesn't react dynamically to actual access\npatterns. Some photos viewed heavily for longer than 30 days will be prematurely moved to S3 Standard-IA,\nincurring retrieval costs if still accessed. Object tags are also less efficient than a dedicated database like\nDynamoDB for complex querying and filtering of metadata.\nOption D utilizes Amazon S3 Glacier, which is primarily for archiving data that is infrequently accessed.\nRetrieval from Glacier is slow and costly, making it unsuitable for a photo hosting service where users need\nrelatively quick access to photos, even if they aren't accessed frequently. OpenSearch Service is overkill for\nsimply storing metadata and object locations and adds unnecessary complexity and cost.\nIn summary, using S3 Intelligent-Tiering for the photos and DynamoDB for the metadata offers the best\nbalance of cost-effectiveness, performance, and manageability for the given scenario.\nSupporting Links:\nAmazon S3 Intelligent-Tiering: https://aws.amazon.com/s3/storage-classes/intelligent-tiering/\nAmazon DynamoDB: https://aws.amazon.com/dynamodb/\nS3 Lifecycle: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-management-\noverview.html",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/intelligent-tiering/",
      "https://aws.amazon.com/dynamodb/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-management-"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a highly available web application on Amazon EC2 instances behind an Application Load Balancer.\nThe company uses Amazon CloudWatch metrics.\nAs the traffic to the web application increases, some EC2 instances become overloaded with many outstanding\nrequests. The CloudWatch metrics show that the number of requests processed and the time to receive the\nresponses from some EC2 instances are both higher compared to other EC2 instances. The company does not want\nnew requests to be forwarded to the EC2 instances that are already overloaded.\nWhich solution will meet these requirements?",
    "options": {
      "B": "However, the best option is B. Use the least outstanding"
    },
    "answer": "B",
    "explanation": "The problem describes a scenario where some EC2 instances behind an Application Load Balancer (ALB) are\noverloaded, leading to higher processing times and request counts. The goal is to prevent new requests from\nbeing routed to these overloaded instances. The correct solution utilizes the \"least outstanding requests\"\nalgorithm, which is not natively supported by ALB. However, the best option is B. Use the least outstanding\nrequests algorithm based on the RequestCountPerTarget and ActiveConnectionCount CloudWatch\nmetrics.\nHere's why:\nUnderstanding the Need: Overloaded instances have a higher number of requests in flight. The ALB should\nideally route new requests to instances that are less burdened, improving overall application performance and\nresponse times.\nLeast Outstanding Requests: The core principle is to direct traffic to instances with fewer pending requests.\nWhile the ALB doesn't directly offer a \"least outstanding requests\" routing algorithm, these values can be\ncalculated for a custom solution.\nRequestCountPerTarget and ActiveConnectionCount Metrics: These metrics provide insights into the\ncurrent load on each instance:\nRequestCountPerTarget: Represents the number of requests that were sent to each target.\nActiveConnectionCount: Represents the number of established connections from clients to the target.\nWhy other options are incorrect:\nRound Robin: Round robin distributes requests evenly across all instances, regardless of their current load.\nThis is unsuitable for handling overloaded instances because it will continue to send requests to them,\nexacerbating the problem.\nTargetResponseTime: The TargetResponseTime metric is primarily focused on measuring the time it takes for\nthe ALB to receive a response from the target. While helpful for monitoring, it's not directly used by the ALB\nfor its native routing algorithms. It can be used for creating custom algorithms.\nImplementation with CloudWatch metrics: You cannot directly configure ALB to use a least outstanding\nrequest algorithm based on CloudWatch metrics. The described solution would necessitate implementing a\ncustom solution which leverages CloudWatch metrics to monitor instances and dynamically modify the target\ngroup weights associated with your instances. AWS services like Lambda could automate this.\nAuthoritative Links:\nApplication Load Balancer Listener Rules:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html\nCloudWatch Metrics for ALB: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-\nbalancer-cloudwatch-metrics.html\nIn conclusion, while option B isn't a built-in ALB feature, it presents the most suitable approach by leveraging\nmetrics that reflect instance load and creating a custom routing mechanism or auto-scaling policy based on\nthose metrics.",
    "links": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-"
    ]
  },
  {
    "question": "CertyIQ\nA company uses Amazon EC2, AWS Fargate, and AWS Lambda to run multiple workloads in the company's AWS\naccount. The company wants to fully make use of its Compute Savings Plans. The company wants to receive\nnotification when coverage of the Compute Savings Plans drops.\nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": {},
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A is the best solution, focusing on operational efficiency and\nAWS best practices:\nOption A leverages AWS Budgets' built-in capabilities for monitoring Savings Plans coverage. AWS Budgets\nallows defining a budget for Savings Plans and setting coverage thresholds. When the actual coverage falls\nbelow the defined threshold, AWS Budgets automatically sends notifications to specified email addresses.\nThis approach provides a proactive and automated way to track coverage without requiring custom coding or\ninfrastructure management. The daily budget frequency ensures timely alerts, allowing the company to\naddress any coverage drops quickly.\nOption B involves creating a Lambda function and using Amazon SES, requiring custom code development,\ndeployment, and maintenance. While it provides flexibility, it increases operational overhead compared to\nusing the readily available AWS Budgets feature.\nOption C creates a budget report but lacks the critical component of notification. A report alone won't actively\nalert the company when coverage drops below the desired level, necessitating manual monitoring.\nOption D, while seemingly simple, oversimplifies the requirement. Savings Plans alert subscriptions are often\ngeared towards payment failures or other account-related issues, not specifically coverage thresholds\nagainst your compute usage. While it might provide some alerts, it won't directly and efficiently address the\nspecific need to monitor coverage percentage against their actual EC2, Fargate, and Lambda spend. This is\nthe primary goal outlined in the prompt.\nTherefore, option A offers the most operationally efficient solution by utilizing AWS Budgets' integrated\ncapabilities for Savings Plans monitoring and automated notifications, eliminating the need for custom code\nor manual intervention.\nHere are some authoritative links for further research:\nAWS Budgets: https://aws.amazon.com/aws-cost-management/aws-budgets/\nCreating a Savings Plans Coverage Budget: https://docs.aws.amazon.com/cost-\nmanagement/latest/userguide/sp-coverage.html\nCompute Savings Plans: https://aws.amazon.com/savingsplans/compute-savings-plans/",
    "links": [
      "https://aws.amazon.com/aws-cost-management/aws-budgets/",
      "https://docs.aws.amazon.com/cost-",
      "https://aws.amazon.com/savingsplans/compute-savings-plans/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a real-time data ingestion solution on AWS. The solution consists of the most recent version of\nAmazon Managed Streaming for Apache Kafka (Amazon MSK). The solution is deployed in a VPC in private subnets\nacross three Availability Zones.\nA solutions architect needs to redesign the data ingestion solution to be publicly available over the internet. The\ndata in transit must also be encrypted.\nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": {
      "C": "Creating new infrastructure introduces operational overhead. Using the existing VPC leverages existing"
    },
    "answer": "A",
    "explanation": "The most operationally efficient solution for making an Amazon MSK cluster publicly available over the\ninternet with encryption is option A: configure public subnets in the existing VPC, deploy an MSK cluster in\nthose subnets, and enable mutual TLS authentication.\nHere's why:\nOperational Efficiency: Reusing the existing VPC avoids the complexity of creating and managing a new VPC.\nCreating new infrastructure introduces operational overhead. Using the existing VPC leverages existing\nsecurity groups and network configurations.\nPublic Subnets: Deploying the MSK cluster in public subnets allows the instances to have public IP addresses\n(or be associated with a NAT Gateway for outbound access), making them reachable from the internet.\nMutual TLS Authentication: Mutual TLS (mTLS) provides robust encryption and authentication. It ensures\nthat both the client and the server (MSK cluster) authenticate each other before establishing a connection.\nThis provides a secure, encrypted channel for data ingestion.\nWhy the other options are less optimal:\nOption B (New VPC): Creating a new VPC adds unnecessary complexity. It requires configuring new\nnetworking (subnets, route tables, security groups) and potentially peering it with the existing VPC if\ncommunication between internal and external applications is needed.\nOption C (ALB): An ALB is designed for HTTP/HTTPS traffic and operates at Layer 7 of the OSI model. MSK\nuses a binary protocol, making ALB unsuitable.\nOption D (NLB): While an NLB can handle TCP traffic, it doesn't provide the application-level security and\nmutual authentication that mTLS does. NLBs operate at Layer 4 and do not offer the same deep inspection or\nsecurity features as mTLS. NLB HTTPS listeners do not operate in the same manner as with standard web\napplications.\nTherefore, placing the MSK cluster in public subnets within the existing VPC and enabling mTLS offers the\nbest balance of security, functionality, and operational simplicity.\nRelevant Links:\nAmazon MSK Security: https://docs.aws.amazon.com/msk/latest/developerguide/security.html\nAmazon VPC: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html\nMutual TLS Authentication: https://aws.amazon.com/blogs/security/how-to-enable-mutual-tls-\nauthentication-on-your-applications-using-aws-certificate-manager-private-ca/",
    "links": [
      "https://docs.aws.amazon.com/msk/latest/developerguide/security.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html",
      "https://aws.amazon.com/blogs/security/how-to-enable-mutual-tls-"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to migrate an on-premises legacy application to AWS. The application ingests customer order\nfiles from an on-premises enterprise resource planning (ERP) system. The application then uploads the files to an\nSFTP server. The application uses a scheduled job that checks for order files every hour.\nThe company already has an AWS account that has connectivity to the on-premises network. The new application\non AWS must support integration with the existing ERP system. The new application must be secure and resilient\nand must use the SFTP protocol to process orders from the ERP system immediately.\nWhich solution will meet these requirements?",
    "options": {
      "D": "Here's a detailed justification:"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Here's a detailed justification:\nRequirement for SFTP: The application must use the SFTP protocol for processing orders. AWS Transfer\nFamily directly addresses this requirement by providing managed SFTP servers.\nInternal Access: The application integrates with an on-premises ERP system through existing network\nconnectivity to AWS. Therefore, the SFTP server should be internal, not internet-facing, to maintain security\nand utilize existing private network connections.\nResilience: Deploying the SFTP server in two Availability Zones (AZs) enhances resilience by ensuring that\nthe application remains available even if one AZ experiences an outage.\nStorage: Amazon S3 is a suitable storage option for order files due to its scalability, durability, and cost-\neffectiveness.\nImmediate Processing: Using AWS Transfer Family managed workflows allows for immediate processing of\nuploaded files. When a file is uploaded to the S3 bucket via SFTP, the Transfer Family workflow can\nautomatically trigger a Lambda function.\nLambda Function: The Lambda function processes the order files. Because it's triggered by the Transfer\nFamily workflow after each upload, the processing happens immediately, meeting the \"process orders from\nthe ERP system immediately\" requirement.\nEFS vs. S3: While Amazon EFS could be used for storage, S3 is generally preferred for file storage due to its\ncost-effectiveness, scalability, and integration with serverless architectures. EFS is more suitable for\napplications requiring shared file system access across multiple EC2 instances. Also, Transfer Family\nworkflows are natively integrated with S3.\nEventBridge Scheduler vs. Transfer Family Workflows: Using EventBridge Scheduler to periodically check\nEFS for order files would introduce latency and miss the requirement of immediate processing. Transfer\nFamily workflows directly integrate with the file transfer process.\nStep Functions: Step Functions manage complex workflows. While it could be used in conjunction with\nLambda, it isn't necessary for this scenario, which is a simple file processing trigger.\nAuthoritative Links:\nAWS Transfer Family: https://aws.amazon.com/transfer/\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon S3: https://aws.amazon.com/s3/\nTransfer Family Managed Workflows:\nhttps://docs.aws.amazon.com/transfer/latest/userguide/workflows.html",
    "links": [
      "https://aws.amazon.com/transfer/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/s3/",
      "https://docs.aws.amazon.com/transfer/latest/userguide/workflows.html"
    ]
  },
  {
    "question": "CertyIQ\nA companys applications use Apache Hadoop and Apache Spark to process data on premises. The existing\ninfrastructure is not scalable and is complex to manage.\nA solutions architect must design a scalable solution that reduces operational complexity. The solution must keep\nthe data processing on premises.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C because it directly addresses the requirements of scalability, reduced operational\ncomplexity, and on-premises data processing. AWS Outposts brings AWS services, including Amazon EMR,\ndirectly to the on-premises environment. By running Amazon EMR on AWS Outposts, the company can\nleverage the scalability and managed services of EMR for their Hadoop and Spark workloads without\nmigrating the data off-premises. This fulfills the requirement of keeping data processing on premises while\nsimplifying management and improving scalability.\nOption A is incorrect because it involves using AWS Site-to-Site VPN and Amazon EMR in the AWS cloud. This\nrequires moving the data across the VPN, which doesn't satisfy the on-premises processing requirement.\nOption B is also incorrect because it involves moving data to the AWS cloud using AWS DataSync and\nprocessing it there with Amazon EMR, which again violates the requirement of keeping data processing on\npremises.\nOption D is incorrect because it involves moving the data to Amazon S3 in the AWS cloud using AWS\nSnowball, and then processing the data there, violating the on-premises processing requirement. Snowball is\nfor data migration to the cloud, not for on-premises processing.\nIn summary, only option C maintains data processing on-premises using Amazon EMR on AWS Outposts,\ndirectly addressing the stated requirements of scalability, reduced operational complexity, and data\nresidency.AWS Outposts: https://aws.amazon.com/outposts/Amazon EMR: https://aws.amazon.com/emr/",
    "links": [
      "https://aws.amazon.com/outposts/Amazon",
      "https://aws.amazon.com/emr/"
    ]
  },
  {
    "question": "CertyIQ\nA company is migrating a large amount of data from on-premises storage to AWS. Windows, Mac, and Linux based\nAmazon EC2 instances in the same AWS Region will access the data by using SMB and NFS storage protocols. The\ncompany will access a portion of the data routinely. The company will access the remaining data infrequently.\nThe company needs to design a solution to host the data.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "C": "Create an Amazon S3 bucket that uses S3 Intelligent-Tiering. Migrate the data to",
      "B": "FSx for ONTAP, while supporting both protocols, is more"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Create an Amazon S3 bucket that uses S3 Intelligent-Tiering. Migrate the data to\nthe S3 bucket by using an AWS Storage Gateway Amazon S3 File Gateway.\nHere's a detailed justification:\nThe scenario requires a solution to host a large amount of data accessible via SMB and NFS protocols, with\nboth frequently and infrequently accessed portions, and aims for minimal operational overhead.\nOption C provides the most efficient and cost-effective solution. S3 Intelligent-Tiering automatically moves\ndata between frequent and infrequent access tiers based on access patterns, optimizing costs without\nmanual intervention. AWS Storage Gateway's File Gateway presents the S3 bucket as a network file share\naccessible via SMB and NFS, fulfilling the protocol requirements. This eliminates the need for managing file\nsystems directly, greatly reducing operational overhead.\nOptions A and B, involving Amazon EFS and Amazon FSx for ONTAP, respectively, require managing file\nsystems and involve more administrative effort compared to S3. While EFS Intelligent-Tiering offers cost\noptimization, EFS doesn't natively support SMB. FSx for ONTAP, while supporting both protocols, is more\ncomplex and expensive than S3 for this specific use case.\nOption D, utilizing Amazon FSx for OpenZFS, is suitable for high-performance file systems. It doesn't have\nbuilt-in intelligent tiering, leading to less cost optimization for infrequently accessed data and might have a\nhigher operational burden than the S3 solution.\nS3's scalability and durability are well-suited for storing large datasets. Using Storage Gateway's File\nGateway simplifies the integration with existing Windows, Mac, and Linux environments, providing a seamless\nuser experience without requiring significant application changes. By utilizing S3 Intelligent-Tiering, the\nsolution minimizes storage costs by automatically transitioning data to the most cost-effective tier based on\naccess patterns, without any operational overhead. Therefore, option C strikes the best balance between\nfunctionality, cost, and ease of management.\nSupporting links:\nAmazon S3 Intelligent-Tiering: https://aws.amazon.com/s3/storage-classes/intelligent-tiering/\nAWS Storage Gateway: https://aws.amazon.com/storagegateway/\nAWS Storage Gateway File Gateway: https://aws.amazon.com/storagegateway/file-gateway/",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/intelligent-tiering/",
      "https://aws.amazon.com/storagegateway/",
      "https://aws.amazon.com/storagegateway/file-gateway/"
    ]
  },
  {
    "question": "CertyIQ\nA manufacturing company runs its report generation application on AWS. The application generates each report in\nabout 20 minutes. The application is built as a monolith that runs on a single Amazon EC2 instance. The application\nrequires frequent updates to its tightly coupled modules. The application becomes complex to maintain as the\ncompany adds new features.\nEach time the company patches a software module, the application experiences downtime. Report generation\nmust restart from the beginning after any interruptions. The company wants to redesign the application so that the\napplication can be flexible, scalable, and gradually improved. The company wants to minimize application\ndowntime.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The best solution for the manufacturing company is to run the application on Amazon Elastic Container\nService (Amazon ECS) as microservices with service auto scaling. This approach addresses the issues of\nmonolithic architecture, frequent updates causing downtime, and the need for scalability and flexibility.\nHere's why:\nMicroservices Architecture: Breaking down the monolithic application into smaller, independent\nmicroservices allows for independent deployment and scaling of individual components. This means updates\nto one microservice don't require redeploying the entire application, reducing downtime and complexity.\nAmazon ECS: ECS is a fully managed container orchestration service. It allows you to run, scale, and manage\ncontainerized applications, simplifying deployment and management of microservices.\nService Auto Scaling: ECS service auto scaling automatically adjusts the number of running tasks\n(containers) based on demand. This ensures the application can handle peak loads efficiently and cost-\neffectively, providing scalability.\nFlexibility and Gradual Improvement: Microservices enable the company to introduce new features and\nimprovements incrementally without disrupting the entire application. Each microservice can be updated and\ndeployed independently.\nMinimized Downtime: ECS facilitates rolling deployments, where new versions of microservices are deployed\ngradually while old versions remain active. This significantly reduces or eliminates downtime during updates.\nResilience: In case a container fails, ECS can automatically restart it or launch a new one, enhancing the\napplication's resilience and availability.\nOptions A, B, and D are less suitable:\nA (Lambda as a single function): While Lambda provides scalability, running the entire monolithic application\nas a single Lambda function is not recommended. Lambda functions have execution time limits and are better\nsuited for smaller, event-driven tasks.\nB (EC2 Spot Instances): Using Spot Instances can be cost-effective, but relying solely on them for a critical\napplication can lead to unpredictable interruptions if Spot Instances are reclaimed. Although Spot Fleets\nallow you to specify fallback on-demand instances, this choice still isn't as robust or scalable as ECS for the\nscenario described. Furthermore, running the application as microservices on Spot Instances without a\ncontainer orchestrator introduces additional complexity in managing deployments, scaling, and service\ndiscovery.\nD (Elastic Beanstalk with all-at-once deployment): Elastic Beanstalk provides a platform for deploying and\nmanaging web applications, but an all-at-once deployment strategy causes significant downtime. The all-at-\nonce deployment strategy isn't appropriate for minimizing downtime. Furthermore, deploying a monolith into\nBeanstalk does not resolve the complexity of deployments, scaling, and service discovery.\nAuthoritative Links:\nAmazon ECS: https://aws.amazon.com/ecs/\nMicroservices Architecture on AWS: https://aws.amazon.com/microservices/\nService Auto Scaling: https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-integrations.html",
    "links": [
      "https://aws.amazon.com/ecs/",
      "https://aws.amazon.com/microservices/",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-integrations.html"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to rearchitect a large-scale web application to a serverless microservices architecture. The\napplication uses Amazon EC2 instances and is written in Python.\nThe company selected one component of the web application to test as a microservice. The component supports\nhundreds of requests each second. The company wants to create and test the microservice on an AWS solution\nthat supports Python. The solution must also scale automatically and require minimal infrastructure and minimal\noperational support.\nWhich solution will meet these requirements?",
    "options": {
      "D": "Use an AWS Lambda function that runs custom developed code.",
      "A": "Spot Fleet with Auto Scaling: Although Spot Fleets offer cost savings, they are not ideal for a highly",
      "B": "AWS Elastic Beanstalk: While Elastic Beanstalk simplifies deployment and management, it's still",
      "C": "Amazon Elastic Kubernetes Service (Amazon EKS): EKS offers a managed Kubernetes service, allowing"
    },
    "answer": "C",
    "explanation": "The incorrect answer is D. Use an AWS Lambda function that runs custom developed code.\nHere's why the correct answer is a better choice and why option D is not:\nThe primary reason option D (AWS Lambda) is unsuitable is the statement that the component supports\n\"hundreds of requests each second.\" While Lambda can handle concurrent invocations, managing the scaling\nand dependencies for a component processing hundreds of requests per second can quickly become complex\nand lead to cold start issues or function timeouts if not appropriately provisioned and optimized. Lambda is\nbest suited for event-driven, short-lived, and more granular microservices. The sheer volume of requests\nmakes orchestrating multiple Lambda functions and managing their state challenging.\nNow, let's address why the other options are not suitable and expand on why Amazon Elastic Kubernetes\nService (Amazon EKS) with Auto Scaling is a good option:\nA. Spot Fleet with Auto Scaling: Although Spot Fleets offer cost savings, they are not ideal for a highly\navailable production environment due to the possibility of instances being terminated with short notice.\nManaging the disruption caused by Spot instance terminations would add operational overhead and\ncomplexity, conflicting with the requirement for minimal operational support.\nB. AWS Elastic Beanstalk: While Elastic Beanstalk simplifies deployment and management, it's still\nmanaging instances. Transitioning to microservices generally involves containerization. Although Beanstalk\nsupports Docker, it does not provide the native orchestration capabilities of Kubernetes.\nC. Amazon Elastic Kubernetes Service (Amazon EKS): EKS offers a managed Kubernetes service, allowing\nthe company to deploy their microservice as a containerized application. Kubernetes excels at managing\ncontainerized workloads, providing automatic scaling, self-healing, and simplified deployment and\nmanagement. Auto Scaling groups for the underlying EC2 worker nodes further ensure the cluster can handle\nthe required load. With Python support available through containers, this approach meets all the\nrequirements.\nIn Summary:\nEKS with Auto Scaling provides the best solution for managing a high-volume microservice written in Python.\nIt offers automated scaling, minimal operational support through a managed Kubernetes service, and a\nflexible containerized environment suitable for microservices. The high volume of requests is more naturally\nhandled by container orchestration than individual Lambda functions.\nSupporting Links:\nAmazon EKS: https://aws.amazon.com/eks/\nKubernetes Auto Scaling: https://kubernetes.io/docs/tasks/configure-pod-container/horizontal-pod-\nautoscale/\nAWS Lambda Limitations: https://docs.aws.amazon.com/lambda/latest/dg/limits.html",
    "links": [
      "https://aws.amazon.com/eks/",
      "https://kubernetes.io/docs/tasks/configure-pod-container/horizontal-pod-",
      "https://docs.aws.amazon.com/lambda/latest/dg/limits.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has an AWS Direct Connect connection from its on-premises location to an AWS account. The AWS\naccount has 30 different VPCs in the same AWS Region. The VPCs use private virtual interfaces (VIFs). Each VPC\nhas a CIDR block that does not overlap with other networks under the company's control.\nThe company wants to centrally manage the networking architecture while still allowing each VPC to\ncommunicate with all other VPCs and on-premises networks.\nWhich solution will meet these requirements with the LEAST amount of operational overhead?",
    "options": {
      "C": "Ensure that both VPN tunnels are"
    },
    "answer": "A",
    "explanation": "The correct answer is A because it leverages AWS Transit Gateway, the recommended solution for\ninterconnecting multiple VPCs and on-premises networks with minimal operational overhead. Transit Gateway\nacts as a central hub, simplifying the routing configuration between numerous VPCs and Direct Connect.\nHere's a breakdown:\nOption A: Transit Gateway: Creates a transit gateway and associates the Direct Connect connection using a\nnew transit VIF. Enables route propagation, automatically learning and distributing routes between connected\nVPCs and the Direct Connect connection. This eliminates the need for manual route table updates,\nsignificantly reducing operational burden. Transit Gateway's route propagation simplifies routing\nmanagement by dynamically adjusting routes as networks change. https://aws.amazon.com/transit-gateway/\nOption B: Direct Connect Gateway: Direct Connect Gateway provides global access for resources connected\nthrough Direct Connect. However, integrating it with numerous VPCs still requires managing separate virtual\nprivate gateways (VGWs) for each VPC, adding complexity compared to Transit Gateway. This option requires\nrecreating existing VIFs, increasing disruption and effort.\nOption C: Transit VPC and Peering: Transit VPCs, while a viable older solution, are more complex to manage\nat scale than Transit Gateways, especially with 30 VPCs. Peering all VPCs together creates a full mesh\nnetwork, leading to complex route table management and scaling issues. The number of peering connections\ngrows quadratically with the number of VPCs, becoming unwieldy and difficult to manage. This approach\ndoesn't effectively address the need for centralized management and low operational overhead.\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/transit-vpc.html\nOption D: Site-to-Site VPN: Creating VPN connections to each VPC is highly inefficient and costly.\nMaintaining 30 VPN connections would introduce significant operational overhead, including managing\nindividual VPN configurations, certificates, and monitoring. VPN connections don't provide the centralized\nmanagement and efficient routing capabilities of Transit Gateway.\nIn summary, Transit Gateway offers the most streamlined and scalable solution for connecting multiple VPCs\nand on-premises networks via Direct Connect, minimizing operational overhead through centralized routing\nand automatic route propagation.",
    "links": [
      "https://aws.amazon.com/transit-gateway/",
      "https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/transit-vpc.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has applications that run on Amazon EC2 instances. The EC2 instances connect to Amazon RDS\ndatabases by using an IAM role that has associated policies. The company wants to use AWS Systems Manager to\npatch the EC2 instances without disrupting the running applications.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A: Create a new IAM role. Attach the AmazonSSMManagedInstanceCore policy to the\nnew IAM role. Attach the new IAM role to the EC2 instances and the existing IAM role.\nHere's a detailed justification:\nThe requirement is to use AWS Systems Manager to patch EC2 instances without disrupting the running\napplications. The applications are already using an IAM role with specific policies to access RDS. If we modify\nthis existing role, it could potentially interrupt the application's ability to access RDS.\nOption A addresses this perfectly by creating a separate IAM role specifically for Systems Manager. The\nAmazonSSMManagedInstanceCore policy grants Systems Manager the necessary permissions to manage the\nEC2 instance (e.g., install updates, run commands). By attaching both the existing application IAM role and the\nnew Systems Manager IAM role to the EC2 instances, we ensure that:\n1. The applications retain their original permissions for RDS access, preventing disruption.\n2. Systems Manager gains the necessary permissions to patch the instances.\nOption B is incorrect because IAM users are not directly associated with EC2 instances for granting\npermissions to the instance itself. IAM users are used for programmatic access (e.g., via the CLI or SDK), not\nfor granting instances permissions to access AWS resources. Systems Manager needs instance-level\npermissions.\nOption C, enabling Default Host Configuration Management in Systems Manager, doesn't address the IAM\npermissions needed by Systems Manager to manage the instances. While it simplifies initial configuration in\nsome cases, it doesn't automatically grant the necessary permissions. Systems Manager still needs an IAM\nrole on the instance to act on it.\nOption D is problematic because removing existing policies from the original IAM role will definitely disrupt\nthe running applications' access to RDS. The current IAM role provides the applications with the necessary\npermissions to access the database. Modifying it directly is explicitly against the requirement to avoid\ndisruption.\nTherefore, Option A is the only solution that provides the necessary permissions for Systems Manager without\ninterfering with the existing application functionality. It follows the principle of least privilege by granting\nSystems Manager only the permissions it needs and avoids modifying a role that is already in use by a critical\napplication.\nHere are some authoritative links for further research:\nIAM Roles for Amazon EC2: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-\nec2.html\nAmazonSSMManagedInstanceCore Policy: https://docs.aws.amazon.com/systems-\nmanager/latest/userguide/security-iam-awsmanpol.html\nAWS Systems Manager Patch Manager: https://docs.aws.amazon.com/systems-\nmanager/latest/userguide/patch-manager.html",
    "links": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-",
      "https://docs.aws.amazon.com/systems-",
      "https://docs.aws.amazon.com/systems-"
    ]
  },
  {
    "question": "CertyIQ\nA company runs container applications by using Amazon Elastic Kubernetes Service (Amazon EKS) and the\nKubernetes Horizontal Pod Autoscaler. The workload is not consistent throughout the day. A solutions architect\nnotices that the number of nodes does not automatically scale out when the existing nodes have reached\nmaximum capacity in the cluster, which causes performance issues.\nWhich solution will resolve this issue with the LEAST administrative overhead?",
    "options": {
      "B": "Use the Kubernetes Cluster Autoscaler to manage the number of nodes in the"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Use the Kubernetes Cluster Autoscaler to manage the number of nodes in the\ncluster.\nThe problem describes a scenario where the Kubernetes Horizontal Pod Autoscaler (HPA) is successfully\nscaling the pods within the cluster, but the underlying compute capacity (the nodes themselves) is not scaling\nto accommodate the increased pod count, leading to performance bottlenecks. This indicates a need for\ncluster-level autoscaling, not just pod-level autoscaling.\nThe Kubernetes Cluster Autoscaler is specifically designed to automatically adjust the size of the Kubernetes\ncluster by adding or removing nodes based on resource demands. It monitors the Kubernetes scheduler for\npending pods that cannot be scheduled due to insufficient resources. When it finds such pods, it triggers the\nunderlying infrastructure (in this case, the EC2 Auto Scaling group behind the EKS cluster) to add more\nnodes. Conversely, if nodes are underutilized, the Cluster Autoscaler can remove nodes.\nOption A is incorrect because scaling nodes based on memory usage alone might not be the most effective\nstrategy. CPU utilization and pod scheduling constraints are equally important factors. The Cluster Autoscaler\nconsiders all these factors.\nOption C, using a Lambda function, would require custom logic to monitor the cluster state, make scaling\ndecisions, and interact with the EC2 Auto Scaling group. This introduces significant administrative overhead\nand complexity compared to using the purpose-built Cluster Autoscaler.\nOption D, using an EC2 Auto Scaling group to distribute the workload, doesn't directly address the\nKubernetes cluster's scaling needs. While an Auto Scaling group is essential for the underlying infrastructure,\nit needs to be integrated with Kubernetes awareness to function correctly in this scenario. The Cluster\nAutoscaler utilizes an EC2 Auto Scaling group but provides the necessary Kubernetes integration.\nThe Cluster Autoscaler provides a native, integrated, and automated solution for scaling the EKS cluster,\nminimizing administrative overhead and ensuring that the cluster capacity matches the application's resource\nrequirements. It's the simplest and most efficient way to address the described problem.\nSupporting documentation:\nKubernetes Cluster Autoscaler: https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler\nAWS EKS Cluster Autoscaler: https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html",
    "links": [
      "https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler",
      "https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html"
    ]
  },
  {
    "question": "CertyIQ\nA company maintains about 300 TB in Amazon S3 Standard storage month after month. The S3 objects are each\ntypically around 50 GB in size and are frequently replaced with multipart uploads by their global application. The\nnumber and size of S3 objects remain constant, but the company's S3 storage costs are increasing each month.\nHow should a solutions architect reduce costs in this situation?",
    "options": {
      "B": "Enable an S3 Lifecycle policy that deletes incomplete multipart uploads.",
      "A": "Switch from multipart uploads to Amazon S3 Transfer Acceleration: S3 Transfer Acceleration optimizes",
      "C": "Configure S3 inventory to prevent objects from being archived too quickly: S3 Inventory provides a listing",
      "D": "Configure Amazon CloudFront to reduce the number of objects stored in Amazon S3: CloudFront is a"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Enable an S3 Lifecycle policy that deletes incomplete multipart uploads.\nHere's why:\nThe problem statement indicates the company's S3 storage costs are increasing despite the number and size\nof objects remaining relatively constant. This suggests a buildup of something that's not meant to be there\nand is consuming storage space. Multipart uploads are a prime suspect.\nMultipart uploads are used to upload large objects to S3 in parts. If an upload is interrupted or fails before all\nparts are committed, these incomplete parts remain in S3 storage, consuming space and incurring costs.\nSince the company frequently uses multipart uploads, especially for large 50GB objects, the likelihood of\nincomplete uploads is relatively high. Over time, these fragments accumulate, leading to increased storage\ncosts.\nAn S3 Lifecycle policy can be configured to automatically delete incomplete multipart uploads after a\nspecified number of days. This action reclaims storage space occupied by the orphaned parts and reduces\noverall storage costs. It addresses the root cause of the increasing costs by cleaning up unfinished processes.\nLet's analyze the other options:\nA. Switch from multipart uploads to Amazon S3 Transfer Acceleration: S3 Transfer Acceleration optimizes\ndata transfer speeds over long distances using Amazon's globally distributed edge locations. This improves\nupload performance but doesn't directly address the issue of increasing storage costs due to incomplete\nuploads. While Transfer Acceleration can be beneficial, it won't fix the underlying storage waste problem.\nC. Configure S3 inventory to prevent objects from being archived too quickly: S3 Inventory provides a listing\nof your objects and their metadata. While useful for auditing and reporting, it doesn't prevent incomplete\nmultipart uploads or address the cost issue directly. Furthermore, the scenario states they use S3 Standard\nwhich isn't an archive storage class.\nD. Configure Amazon CloudFront to reduce the number of objects stored in Amazon S3: CloudFront is a\ncontent delivery network (CDN) that caches content at edge locations to improve content delivery\nperformance. CloudFront caches objects, so this would reduce the number of requests to S3, not the objects\nstored in S3. It might indirectly reduce egress costs but doesn't address the primary problem of increasing\nstorage costs. Objects remain in S3.\nTherefore, configuring an S3 Lifecycle policy to delete incomplete multipart uploads is the most effective\nsolution to reduce the company's increasing S3 storage costs in this scenario.\nFurther Research:\nAmazon S3 Lifecycle: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-\nexamples.html\nMultipart Upload Overview: https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html\nS3 Transfer Acceleration: https://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-\nacceleration.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-"
    ]
  },
  {
    "question": "CertyIQ\nA company has deployed a multiplayer game for mobile devices. The game requires live location tracking of\nplayers based on latitude and longitude. The data store for the game must support rapid updates and retrieval of\nlocations.\nThe game uses an Amazon RDS for PostgreSQL DB instance with read replicas to store the location data. During\npeak usage periods, the database is unable to maintain the performance that is needed for reading and writing\nupdates. The game's user base is increasing rapidly.\nWhat should a solutions architect do to improve the performance of the data tier?",
    "options": {
      "B": "Since the application currently uses RDS PostgreSQL, DAX is not applicable. Moreover, migrating"
    },
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the best solution, along with why the other options are less\nsuitable:\nWhy Option D (Amazon ElastiCache for Redis) is the Best Choice:\nOption D, deploying an Amazon ElastiCache for Redis cluster in front of the RDS instance and modifying the\ngame to utilize Redis, addresses the core performance issues effectively. The question highlights rapid\nupdates and retrieval of location data as crucial requirements. Redis is an in-memory data store, known for its\nextremely low latency and high throughput, which is perfect for caching frequently accessed data. By placing\nRedis in front of the RDS PostgreSQL database, the game application can read and write location data to\nRedis first. Most read requests can then be served directly from the Redis cache, drastically reducing the load\non the RDS instance and improving read performance. Furthermore, Redis's fast write capabilities allow for\nquick updates to the location data, and these updates can be asynchronously persisted to the RDS instance in\nthe background, further offloading the database. This caching strategy is especially useful given the read-\nheavy nature of location tracking in a multiplayer game. ElastiCache simplifies the management and scaling\nof the Redis cluster. Redis is well-suited for geospatial data as well.\nWhy the other options are less suitable:\nOption A (Multi-AZ for RDS): Enabling Multi-AZ on the RDS instance primarily provides high availability and\nfault tolerance, not a significant performance boost for read and write operations. While Multi-AZ can help\nwith failover, it doesn't address the issue of database overload during peak usage.\nOption B (Amazon OpenSearch Service): While OpenSearch is suitable for search and analytics, it's not the\nideal solution for the described use case. OpenSearch is designed for indexing and searching large volumes of\ndata, but it's not optimized for the rapid, low-latency updates required for real-time location tracking. It's also\nmore complex to set up and maintain than Redis in this scenario.\nOption C (DynamoDB Accelerator (DAX)): DAX is a caching service specifically designed for Amazon\nDynamoDB. Since the application currently uses RDS PostgreSQL, DAX is not applicable. Moreover, migrating\nto DynamoDB would necessitate major application code changes which makes this approach less efficient.\nAuthoritative Links:\nAmazon ElastiCache: https://aws.amazon.com/elasticache/\nRedis: https://redis.io/\nCaching Strategies: https://aws.amazon.com/caching/",
    "links": [
      "https://aws.amazon.com/elasticache/",
      "https://redis.io/",
      "https://aws.amazon.com/caching/"
    ]
  },
  {
    "question": "CertyIQ\nA company stores critical data in Amazon DynamoDB tables in the company's AWS account. An IT administrator\naccidentally deleted a DynamoDB table. The deletion caused a significant loss of data and disrupted the\ncompany's operations. The company wants to prevent this type of disruption in the future.\nWhich solution will meet this requirement with the LEAST operational overhead?",
    "options": {
      "C": "Configure deletion protection on the DynamoDB tables. This option directly",
      "A": "CloudTrail, EventBridge, and Lambda: While this approach allows for automated restoration, it's far more",
      "B": "Backup and Restore Plan: While creating backups is a good practice for disaster recovery, it doesn't",
      "D": "Point-in-Time Recovery (PITR): PITR allows you to restore a table to any point in time within the past 35"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Configure deletion protection on the DynamoDB tables. This option directly\naddresses the problem of accidental table deletion with the least operational overhead.\nHere's why:\nDeletion protection is a feature specifically designed to prevent accidental deletion of DynamoDB tables.\nOnce enabled, a table cannot be deleted without first disabling the protection. This introduces a necessary\nstep that mitigates unintended deletions.\nOperational Overhead: This solution requires only enabling a setting on each DynamoDB table. This minimal\nconfiguration translates to the least amount of operational overhead compared to the other options.\nLet's analyze why the other options are less suitable:\nA. CloudTrail, EventBridge, and Lambda: While this approach allows for automated restoration, it's far more\ncomplex. It requires configuring multiple services, writing code, and testing the automated restoration\nprocess. The operational overhead is significant. Furthermore, restoration isn't prevention; the table is still\nmomentarily deleted, and the recovery process might have its own issues.\nB. Backup and Restore Plan: While creating backups is a good practice for disaster recovery, it doesn't\nprevent accidental deletion. Restoring from a backup is a manual process that takes time and introduces\npotential data loss (data written after the backup). This option addresses data recovery, not prevention.\nD. Point-in-Time Recovery (PITR): PITR allows you to restore a table to any point in time within the past 35\ndays. While useful for recovering from data corruption or unintended updates, it doesn't prevent deletion. Like\nbackups, it addresses data recovery, not prevention. The table is still deleted initially, which causes downtime\nuntil the point-in-time recovery completes. Also, enabling PITR introduces some cost overhead.\nIn summary, deletion protection is the most straightforward, least operationally intensive, and most effective\nway to prevent accidental DynamoDB table deletions. It's a preventative measure, unlike the other options\nthat focus on recovery after the event has already occurred.Here are some authoritative links for further\nresearch:\nDynamoDB Deletion Protection:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAXDeletionProtection.html\nDynamoDB Point-in-Time Recovery:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html",
    "links": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAXDeletionProtection.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has an on-premises data center that is running out of storage capacity. The company wants to migrate\nits storage infrastructure to AWS while minimizing bandwidth costs. The solution must allow for immediate\nretrieval of data at no additional cost.\nHow can these requirements be met?",
    "options": {
      "C": "Deploy AWS Storage Gateway using stored volumes to store data locally. Use",
      "A": "Amazon S3 Glacier Vault and expedited retrieval: S3 Glacier is designed for long-term archiving and",
      "B": "AWS Storage Gateway using cached volumes: Cached volumes store frequently accessed data locally and",
      "D": "AWS Direct Connect and Storage Gateway with stored volumes: While AWS Direct Connect provides a"
    },
    "answer": "C",
    "explanation": "The correct solution is C. Deploy AWS Storage Gateway using stored volumes to store data locally. Use\nStorage Gateway to asynchronously back up point-in-time snapshots of the data to Amazon S3.\nHere's a detailed justification:\nThe primary goal is to migrate on-premises storage to AWS while minimizing bandwidth costs and ensuring\nimmediate data retrieval at no extra cost. AWS Storage Gateway helps bridge the gap between on-premises\nenvironments and AWS storage services. Different Storage Gateway types offer varying strategies for data\nmanagement.\nStored Volumes: In this configuration, the entire dataset is stored locally on the on-premises storage\nappliance. Storage Gateway then asynchronously backs up point-in-time snapshots of this local data to\nAmazon S3. This directly addresses the requirements:\nImmediate Retrieval: Since the complete dataset is stored locally, data retrieval is fast and doesn't incur\nbandwidth costs associated with pulling data from AWS every time.\nMinimized Bandwidth Costs: Only snapshots are transferred to S3, reducing the overall bandwidth usage\ncompared to constantly synchronizing the entire dataset.\nData Migration: Provides a path for migrating data by storing it in the cloud while retaining a local copy.\nLet's analyze why the other options are less suitable:\nA. Amazon S3 Glacier Vault and expedited retrieval: S3 Glacier is designed for long-term archiving and\ninfrequent access. Expedited retrievals come at a cost, which violates the \"no additional cost\" requirement for\nimmediate retrievals. Also, Glacier is not a storage gateway.\nB. AWS Storage Gateway using cached volumes: Cached volumes store frequently accessed data locally and\nstore the entire dataset in S3. While this reduces latency for frequently accessed data, it still requires\ntransferring the entire dataset to S3 initially, increasing bandwidth costs. Furthermore, infrequently accessed\ndata still requires retrieval from S3.\nD. AWS Direct Connect and Storage Gateway with stored volumes: While AWS Direct Connect provides a\ndedicated network connection between the on-premises environment and AWS, it is primarily focused on the\nnetwork link. It reduces bandwidth costs compared to internet transit, but still represents a cost. The Storage\nGateway part is the same as option C, but the Direct Connect component adds unnecessary complexity and\ncost without directly addressing the core requirement of immediate retrieval at no additional cost. Option C\nalready provides the method of data backup through snapshots. Direct Connect is only needed when there is a\nhigher bandwidth needed for the backup and the network connection is critical.\nIn summary, Stored Volumes provide the optimal balance between local accessibility, reduced bandwidth\nconsumption, and gradual migration to the cloud.Authoritative Links:\nAWS Storage Gateway: https://aws.amazon.com/storagegateway/\nUnderstanding Storage Gateway Volume Types:\nhttps://docs.aws.amazon.com/storagegateway/latest/userguide/HowStorageGatewayWorks.html",
    "links": [
      "https://aws.amazon.com/storagegateway/",
      "https://docs.aws.amazon.com/storagegateway/latest/userguide/HowStorageGatewayWorks.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a three-tier web application in a VPC across multiple Availability Zones. Amazon EC2 instances\nrun in an Auto Scaling group for the application tier.\nThe company needs to make an automated scaling plan that will analyze each resource's daily and weekly\nhistorical workload trends. The configuration must scale resources appropriately according to both the forecast\nand live changes in utilization.\nWhich scaling strategy should a solutions architect recommend to meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the best choice, addressing the requirements of analyzing\nhistorical workload trends, forecasting, and reacting to live utilization changes:\nOption B, \"Enable predictive scaling to forecast and scale. Configure dynamic scaling with target tracking,\" is\nthe most comprehensive solution for the stated problem. It leverages the strengths of both predictive and\ndynamic scaling approaches.\nPredictive scaling uses machine learning to analyze historical workload patterns (both daily and weekly\ntrends, as required) and forecasts future demand. This proactive approach allows the system to pre-emptively\nscale resources before the actual increase in traffic occurs. This is superior to purely reactive approaches as it\navoids potential performance bottlenecks during the initial surge.\nDynamic scaling with target tracking provides real-time responsiveness. It continuously monitors a chosen\nmetric (like CPU utilization or request latency) and adjusts resources to maintain that metric at a defined\ntarget. This complements predictive scaling by handling unexpected traffic spikes or deviations from the\nforecast that predictive scaling might miss. This allows the system to adapt to live changes in utilization.\nThe combination ensures the application is scaled both proactively based on trends and reactively based on\ncurrent demand.\nOption A (step scaling based on CPU utilization) only addresses dynamic scaling, not historical trend analysis\nor predictive scaling. Its reactive and doesn't prepare for anticipated load increases.\nOption C (scheduled scaling) might work for predictable traffic patterns, but it doesn't adapt to live changes\nor leverage historical trend analysis effectively. It's also less flexible than a predictive and dynamic scaling\ncombination. It would require manual updating of the schedule.\nOption D (simple scaling with cooldown) is a basic scaling strategy, not suitable for complex workloads. It\nlacks trend analysis, forecasting, and the precision of target tracking. Cooldown periods are intended to\nprevent excessive scaling actions, but don't help with the underlying problem of matching resources to\ndemand.\nTherefore, option B, combining predictive scaling with dynamic target tracking, provides the best solution to\nmeet the requirements of historical analysis, forecasting, and real-time responsiveness in the three-tier web\napplication.\nAuthoritative Links for further research:\nAWS Auto Scaling Predictive Scaling: https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-\nscaling-predictive-scaling.html\nAWS Auto Scaling Dynamic Scaling: https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-\nscaling-scaling-policies.html\nAWS Auto Scaling - Choose a Scaling Strategy: https://aws.amazon.com/autoscaling/scaling-strategies/",
    "links": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-",
      "https://aws.amazon.com/autoscaling/scaling-strategies/"
    ]
  },
  {
    "question": "CertyIQ\nA package delivery company has an application that uses Amazon EC2 instances and an Amazon Aurora MySQL\nDB cluster. As the application becomes more popular, EC2 instance usage increases only slightly. DB cluster usage\nincreases at a much faster rate.\nThe company adds a read replica, which reduces the DB cluster usage for a short period of time. However, the load\ncontinues to increase. The operations that cause the increase in DB cluster usage are all repeated read statements\nthat are related to delivery details. The company needs to alleviate the effect of repeated reads on the DB cluster.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "A",
    "explanation": "The best solution for alleviating the effect of repeated read operations on the Aurora MySQL DB cluster cost-\neffectively is to implement an Amazon ElastiCache for Redis cluster between the application and the DB\ncluster.\nHere's why:\nThe problem focuses on repeated read operations causing a bottleneck in the DB cluster. While read replicas\noffload some read traffic, the load continues to grow. Adding more read replicas (option B) might offer\ntemporary relief but doesn't address the underlying issue of repeatedly fetching the same data. Aurora Auto\nScaling (option C) scales read replicas, but if the root cause is repetitive queries, it will just scale more servers\nperforming the same inefficient operation, incurring higher costs. Option D, multiple writer instances, is not\napplicable for read scaling and is used for handling write-heavy workloads and is not related to the scenario.\nElastiCache for Redis acts as an in-memory data store (cache) situated between the application and the\ndatabase. This caching mechanism stores frequently accessed delivery details. When the application requests\nthis data, ElastiCache retrieves it from memory, drastically reducing the load on the DB cluster. Redis's in-\nmemory nature makes it significantly faster than accessing the database directly for read operations.\nBecause it avoids querying the DB cluster for repetitive data, it's the most cost-effective solution. It avoids\nscaling the DB cluster while directly addressing the performance bottleneck.\nHere are some resources for further research:\nAmazon ElastiCache: https://aws.amazon.com/elasticache/\nCaching Strategies: https://aws.amazon.com/caching/\nAurora Read Replicas: https://aws.amazon.com/rds/aurora/features/",
    "links": [
      "https://aws.amazon.com/elasticache/",
      "https://aws.amazon.com/caching/",
      "https://aws.amazon.com/rds/aurora/features/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an application that uses an Amazon DynamoDB table for storage. A solutions architect discovers\nthat many requests to the table are not returning the latest data. The company's users have not reported any other\nissues with database performance. Latency is in an acceptable range.\nWhich design change should the solutions architect recommend?",
    "options": {},
    "answer": "C",
    "explanation": "The problem indicates that the application is retrieving stale data from DynamoDB, even though overall\nlatency is acceptable. This points directly to the read consistency level being used. DynamoDB offers both\neventually consistent reads and strongly consistent reads.\nOption A, adding read replicas, doesn't apply to DynamoDB's internal architecture. DynamoDB automatically\nreplicates data across multiple Availability Zones for high availability and durability; users do not manually\ncreate read replicas.\nOption B, using a Global Secondary Index (GSI), improves query performance based on different attributes but\ndoes not directly address data consistency issues. While a GSI could improve query speed, it wouldn't\nguarantee the latest data is returned unless it uses strongly consistent reads.\nOption D, requesting eventually consistent reads, is actually the default behavior in DynamoDB and explains\nwhy the application isn't getting the latest data. Eventually consistent reads provide the best read\nperformance (lowest latency) because they may read from any replica, but the trade-off is a small delay\nbefore the latest updates become visible across all replicas. This is what is causing the stale data issue.\nOption C, requesting strongly consistent reads, will directly solve the problem. Strongly consistent reads\nguarantee that the read operation will return the most up-to-date version of an item, reflecting all previous\nwrite operations that completed successfully. While strongly consistent reads may have slightly higher\nlatency than eventually consistent reads, it's crucial in this scenario where data accuracy is paramount. The\nquestion states that latency is within an acceptable range, so the slightly higher latency associated with\nstrongly consistent reads is permissible in the context of solving the consistency problem. Therefore,\nrequesting strongly consistent reads is the most appropriate design change.\nHere are links for further research:\nDynamoDB Read Consistency:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html\nDynamoDB Global Secondary Indexes:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html",
    "links": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has deployed its application on Amazon EC2 instances with an Amazon RDS database. The company\nused the principle of least privilege to configure the database access credentials. The company's security team\nwants to protect the application and the database from SQL injection and other web-based attacks.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the best solution for protecting an application and RDS\ndatabase from SQL injection and web-based attacks with the least operational overhead, adhering to the\nprinciple of least privilege:\nThe problem requires mitigating SQL injection and web-based attacks with minimal operational effort,\nleveraging the principle of least privilege. AWS WAF (Web Application Firewall) directly addresses this\nrequirement by inspecting HTTP(S) traffic and blocking malicious requests before they reach the application.\nIt offers pre-configured rules for common web exploits, including SQL injection, cross-site scripting (XSS),\nand others, reducing the need for custom security rules and maintenance.\nUsing RDS parameter groups to configure security settings helps in hardening the database. Although\nparameter groups primarily manage database configurations, they can indirectly contribute to security. For\nexample, you can use parameter groups to enforce stricter password policies or disable certain features that\nmight be susceptible to vulnerabilities.\nOption A (Security Groups and Network ACLs) focuses on network-level security, controlling traffic based on\nIP addresses and ports. While crucial, they don't provide deep inspection of application layer traffic necessary\nto prevent SQL injection and other web-based attacks. They are more concerned with allowing or denying\nconnections, not analyzing the content within those connections.\nOption C (AWS Network Firewall) is a powerful network security service, but it is generally more complex and\noperationally heavy than AWS WAF for protecting web applications. While it offers broader protection at the\nnetwork layer, its configuration and maintenance would be higher than that of WAF, adding unnecessary\ncomplexity.\nOption D (Different database accounts with limited privileges) aligns with the principle of least privilege.\nHowever, by itself, this doesn't protect against SQL injection; it only limits the damage if an attack succeeds.\nIt also requires more careful management of database credentials within the application code. While a good\npractice, it doesn't replace the need for web application firewall protection.\nTherefore, using AWS WAF to filter malicious requests and employing RDS parameter groups to indirectly\nenhance database security is the most effective and operationally efficient solution. It provides specific\nprotection against web-based attacks while keeping the operational overhead manageable, and complements\nthe least privilege principle. WAF's managed rulesets and easy deployment make it ideal for addressing the\ncore security concerns with minimal manual intervention.\nSupporting links:\nAWS WAF: https://aws.amazon.com/waf/\nAmazon RDS Parameter Groups:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html",
    "links": [
      "https://aws.amazon.com/waf/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html"
    ]
  },
  {
    "question": "CertyIQ\nAn ecommerce company runs applications in AWS accounts that are part of an organization in AWS Organizations.\nThe applications run on Amazon Aurora PostgreSQL databases across all the accounts. The company needs to\nprevent malicious activity and must identify abnormal failed and incomplete login attempts to the databases.\nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "options": {},
    "answer": "B",
    "explanation": "The best approach to detect abnormal database login attempts across an AWS Organization involves\nleveraging specialized security services designed for threat detection. Option B, enabling Amazon RDS\nProtection in Amazon GuardDuty for member accounts, directly addresses this requirement efficiently.\nGuardDuty RDS Protection monitors Aurora PostgreSQL logs and activity to identify suspicious behavior\nrelated to login attempts, password failures, and potential brute-force attacks. This solution is operationally\nefficient because GuardDuty is a managed threat detection service that automates log analysis, threat\nintelligence correlation, and anomaly detection, removing the need to manually parse logs. GuardDuty\nautomatically integrates with AWS Organizations and can be enabled centrally, applying to all member\naccounts. This reduces administrative overhead compared to setting up and maintaining custom logging\nsolutions.\nOption A, using SCPs, can restrict actions but isn't designed to analyze and identify failed login attempts.\nOption C, which involves publishing logs to CloudWatch and exporting them to S3, requires a custom analysis\npipeline to detect anomalies, adding significant operational complexity. Option D, publishing database events\nto CloudTrail, while helpful for auditing API calls, doesn't directly monitor database login activity.\nGuardDuty's RDS Protection offers a purpose-built, managed service that provides pre-built detectors for\ncommon database security threats and integrates seamlessly with AWS Organizations, providing the most\noperationally efficient\nsolution.https://aws.amazon.com/guardduty/features/https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_rds_protection.html",
    "links": [
      "https://aws.amazon.com/guardduty/features/https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_rds_protection.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has an AWS Direct Connect connection from its corporate data center to its VPC in the us-east-1\nRegion. The company recently acquired a corporation that has several VPCs and a Direct Connect connection\nbetween its on-premises data center and the eu-west-2 Region. The CIDR blocks for the VPCs of the company and\nthe corporation do not overlap. The company requires connectivity between two Regions and the data centers. The\ncompany needs a solution that is scalable while reducing operational overhead.\nWhat should a solutions architect do to meet these requirements?",
    "options": {
      "C": "D.Connect the existing Direct Connect connection to a Direct Connect gateway. Route traffic from the virtual",
      "D": "Connect the existing Direct Connect connection to a Direct Connect gateway. Route"
    },
    "answer": "D",
    "explanation": "The best solution is D. Connect the existing Direct Connect connection to a Direct Connect gateway. Route\ntraffic from the virtual private gateways of the VPCs in each Region to the Direct Connect gateway.\nHere's why:\nScalability and Reduced Overhead: Direct Connect Gateway (DXGW) is designed for connecting multiple\nVPCs across different AWS Regions. It simplifies the management of multiple connections and routing\nconfigurations, reducing operational overhead as the company's AWS footprint grows.\nCentralized Management: DXGW acts as a central point for routing traffic between on-premises networks\n(via Direct Connect) and multiple VPCs across different Regions.\nNo Overlapping CIDR Blocks: Since the CIDR blocks of the VPCs don't overlap, DXGW can seamlessly route\ntraffic between them without any address translation issues.\nInter-Region Connectivity: DXGW supports inter-Region connectivity, fulfilling the requirement to connect\nVPCs in us-east-1 and eu-west-2.\nDirect Connect Utilization: This solution leverages the existing Direct Connect connections, ensuring cost-\neffectiveness and minimizing the need for new infrastructure.\nAlternatives Considered & Why They Are Less Suitable:\nA (Inter-Region VPC Peering): While possible, VPC peering is point-to-point. Connecting multiple VPCs across\nRegions requires establishing and managing numerous peering connections, leading to increased complexity\nand administrative burden. It doesn't scale well.\nB (Direct Connect Private VIFs to Each VPC): This is not recommended because creating multiple private VIFs\nfrom one Direct Connect connection to VPCs in different Regions is complex and less efficient than using\nDXGW. Also, some organizations may have Direct Connect port limits.\nC (VPN Appliances in EC2 with AWS VPN CloudHub): VPN appliances in EC2 incur higher operational\noverhead compared to DXGW. Managing VPN appliances, routing, and security can become complex, and the\nperformance may be less reliable than using a Direct Connect Gateway. Also, it doesn't fully utilize the\nexisting Direct Connect connections. EC2 based VPNs will also be significantly more costly at high\nthroughputs.\nIn summary, Direct Connect Gateway provides a scalable, manageable, and cost-effective solution for\nconnecting multiple VPCs across different AWS Regions using existing Direct Connect connections, fulfilling\nall the company's requirements.\nAuthoritative Links:\nAWS Direct Connect Gateway: https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-\ngateways-intro.html\nAWS VPN CloudHub: https://docs.aws.amazon.com/vpn/latest/s2svpn/VPN_CloudHub.html\nInter-Region VPC Peering: https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html",
    "links": [
      "https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-",
      "https://docs.aws.amazon.com/vpn/latest/s2svpn/VPN_CloudHub.html",
      "https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is developing a mobile game that streams score updates to a backend processor and then posts results\non a leaderboard. A solutions architect needs to design a solution that can handle large traffic spikes, process the\nmobile game updates in order of receipt, and store the processed updates in a highly available database. The\ncompany also wants to minimize the management overhead required to maintain the solution.\nWhat should the solutions architect do to meet these requirements?",
    "options": {
      "B": "Option D: Amazon SQS provides a message queueing service, which isn't designed for preserving the order of"
    },
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A is the most suitable solution, along with supporting cloud\ncomputing concepts and links for further research:\nOption A leverages several AWS services optimized for high-throughput data streaming, processing, and\nstorage, aligning perfectly with the problem's requirements. Amazon Kinesis Data Streams is designed for\nreal-time streaming of large volumes of data, making it ideal for handling score updates from the mobile\ngame. Crucially, Kinesis Data Streams guarantees data ordering within each shard, fulfilling the requirement\nto process updates in the order they are received. AWS Lambda provides a serverless compute environment,\nminimizing management overhead, as it automatically scales in response to the volume of updates from\nKinesis. Lambda's function will consume the Kinesis stream and process the updates. Finally, Amazon\nDynamoDB is a highly available and scalable NoSQL database, well-suited for storing the processed score\nupdates. Its managed nature further reduces management overhead. DynamoDB also offers fast read and\nwrite performance.\nWhy other options are less suitable:\nOption B: While Kinesis Data Streams is appropriate, using EC2 instances with Auto Scaling for processing\nintroduces more management overhead compared to the serverless Lambda option. Also, Amazon Redshift is\ndesigned for analytical workloads and isn't the best choice for transactional storage of individual score\nupdates.\nOption C: Amazon SNS is designed for publish/subscribe messaging and doesn't guarantee message\nordering. The mobile game needs to receive updates in order. Storing the processed updates in a SQL\ndatabase running on EC2 increases management overhead and doesn't offer the same scalability as\nDynamoDB.\nOption D: Amazon SQS provides a message queueing service, which isn't designed for preserving the order of\nmessage within queue ( unless configured using FIFO). It's ideal for decoupling components but might not be\nthe best choice when order is important. Also, using EC2 instances with Auto Scaling for processing is good,\nbut adds overhead. Although Amazon RDS Multi-AZ DB instance offers high availability, managing the\ndatabase and EC2 instances leads to increased management overhead compared to serverless alternatives.\nIn summary: Option A provides the best balance of scalability, managed services, guaranteed data ordering,\nand minimal management overhead, making it the optimal solution for handling the mobile game's score\nupdates.\nAuthoritative Links for Further Research:\nAmazon Kinesis Data Streams: https://aws.amazon.com/kinesis/data-streams/\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon DynamoDB: https://aws.amazon.com/dynamodb/\nAmazon SNS: https://aws.amazon.com/sns/\nAmazon SQS: https://aws.amazon.com/sqs/\nAmazon Redshift: https://aws.amazon.com/redshift/\nAmazon RDS: https://aws.amazon.com/rds/",
    "links": [
      "https://aws.amazon.com/kinesis/data-streams/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/dynamodb/",
      "https://aws.amazon.com/sns/",
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/redshift/",
      "https://aws.amazon.com/rds/"
    ]
  },
  {
    "question": "CertyIQ\nA company has multiple AWS accounts with applications deployed in the us-west-2 Region. Application logs are\nstored within Amazon S3 buckets in each account. The company wants to build a centralized log analysis solution\nthat uses a single S3 bucket. Logs must not leave us-west-2, and the company wants to incur minimal operational\noverhead.\nWhich solution meets these requirements and is MOST cost-effective?",
    "options": {},
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the most suitable solution:\nOption B, using S3 Same-Region Replication (SRR), is the most cost-effective and operationally efficient\nsolution for centralizing logs from multiple AWS accounts into a single S3 bucket within the us-west-2 region.\nS3 SRR automatically and asynchronously replicates objects between S3 buckets in the same AWS Region.\nThis aligns perfectly with the requirement of keeping the logs within us-west-2.\nThe main advantage of SRR is its ease of configuration and management. Once enabled between source and\ndestination buckets, the replication process is fully managed by AWS S3, requiring minimal operational\noverhead. There's no need to write custom scripts or functions. This simplifies the log centralization process\nsignificantly.\nOption A, using an S3 Lifecycle policy for copying, is less suitable because Lifecycle policies are primarily\ndesigned for object transition to cheaper storage classes (like Glacier) or deletion, not general cross-bucket\ncopying for analysis. While it could be configured, SRR is the more direct and intended mechanism for\nreplication.\nOption C, using a script with PutObject API calls, is the least efficient. It involves significant operational\noverhead in developing, deploying, and maintaining the script. Additionally, manually copying data is prone to\nerrors and can lead to data inconsistencies if not implemented correctly. It also lacks the real-time aspect\nthat is usually expected with log centralization.\nOption D, employing Lambda functions triggered by s3:ObjectCreated:* events, is a viable option but more\ncomplex and costly than SRR. While it offers near real-time log transfer, it introduces the overhead of\nmanaging Lambda functions, including code development, deployment, monitoring, and potential scaling\nissues, along with associated Lambda execution costs. The cost of invoking Lambda per log write is likely to\noutweigh the S3 replication costs.\nCompared to the other options, SRR leverages a built-in S3 feature designed specifically for data replication,\nreducing operational complexity and potential for errors. It's cost-effective due to its managed nature, and\navoids the overhead associated with custom scripting or Lambda function management. This makes it the\nmost suitable approach for the company's centralized log analysis needs.\nFor further research, consult these AWS documentation pages:\nS3 Same-Region Replication (SRR):\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html\nS3 Lifecycle Policies: https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-\nmanagement.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-"
    ]
  },
  {
    "question": "CertyIQ\nA company has an application that delivers on-demand training videos to students around the world. The\napplication also allows authorized content developers to upload videos. The data is stored in an Amazon S3 bucket\nin the us-east-2 Region.\nThe company has created an S3 bucket in the eu-west-2 Region and an S3 bucket in the ap-southeast-1 Region.\nThe company wants to replicate the data to the new S3 buckets. The company needs to minimize latency for\ndevelopers who upload videos and students who stream videos near eu-west-2 and ap-southeast-1.\nWhich combination of steps will meet these requirements with the FEWEST changes to the application? (Choose\ntwo.)",
    "options": {},
    "answer": "C",
    "explanation": "Here's a detailed justification for the answer choices C and E, and why the other options are incorrect:\nWhy C and E are correct:\nE: Create an S3 Multi-Region Access Point. Modify the application to use the Amazon Resource Name\n(ARN) of the Multi-Region Access Point for video streaming and uploads. This is a core element of the\nsolution because S3 Multi-Region Access Points (MRAP) are designed to simplify accessing data stored\nacross multiple S3 buckets in different AWS Regions. By using a MRAP, the application interacts with a single\nendpoint (the ARN), and S3 intelligently routes requests to the appropriate bucket based on proximity,\navailability, and configured routing policies. This minimizes latency for both uploads and downloads because\nusers are directed to the nearest S3 bucket. Modifying the application to use the MRAP ARN for both\nstreaming and uploading is crucial for ensuring all requests benefit from the multi-region setup.\nS3 Multi-Region Access Points documentation\nC: Configure two-way (bidirectional) replication among the S3 buckets that are in all three Regions.\nBidirectional replication is essential when content developers are uploading videos not just to us-east-2, but\nalso to eu-west-2 and ap-southeast-1, or if there are developers in all these regions who upload content.\nReplicating data bi-directionally between all three regions ensures that any changes made in one region are\nautomatically propagated to the other two regions. This keeps the data consistent and up-to-date across all\nlocations, which is vital for content delivery and user experience. If uploads only occurred in the us-east-2\nbucket, then bi-directional replication would not be required and would not be cost effective.\nS3 Replication documentation\nWhy other options are incorrect:\nA: Configure one-way replication from the us-east-2 S3 bucket to the eu-west-2 S3 bucket. Configure one-\nway replication from the us-east-2 S3 bucket to the ap-southeast-1 S3 bucket. This option addresses the\nreplication requirement but doesn't minimize latency for developers uploading videos in the eu-west-2 and ap-\nsoutheast-1 regions. They would still be uploading to the us-east-2 bucket, which defeats the purpose of\nminimizing latency.\nB: Configure one-way replication from the us-east-2 S3 bucket to the eu-west-2 S3 bucket. Configure one-\nway replication from the eu-west-2 S3 bucket to the ap-southeast-1 S3 bucket. Similar to option A, this\nsetup doesn't address the latency issue for developers uploading videos in the ap-southeast-1 region. Uploads\nwill be required from eu-west-2 to ap-southeast-1, which will not improve latency for uploads coming from the\nap-southeast-1 region.\nD: Create an S3 Multi-Region Access Point. Modify the application to use the Amazon Resource Name\n(ARN) of the Multi-Region Access Point for video streaming. Do not modify the application for video\nuploads. While using a Multi-Region Access Point for streaming is a good step to minimize latency for\nstudents, this option doesn't address the latency issue for developers uploading videos. They would still be\nuploading to the original us-east-2 bucket, negating the benefits of multi-region access.\nIn summary, the combination of creating a Multi-Region Access Point and configuring bidirectional replication\nensures both low-latency video delivery for students and low-latency video uploads for content developers,\nregardless of their location.",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has a new mobile app. Anywhere in the world, users can see local news on topics they choose. Users\nalso can post photos and videos from inside the app.\nUsers access content often in the first minutes after the content is posted. New content quickly replaces older\ncontent, and then the older content disappears. The local nature of the news means that users consume 90% of\nthe content within the AWS Region where it is uploaded.\nWhich solution will optimize the user experience by providing the LOWEST latency for content uploads?",
    "options": {
      "B": "Upload and store content in Amazon S3. Use S3 Transfer",
      "A": "Upload and store content in Amazon S3. Use Amazon CloudFront for the uploads. While CloudFront",
      "C": "Upload content to Amazon EC2 instances in the Region that is closest to the user. Copy the data to",
      "D": "Upload and store content in Amazon S3 in the Region that is closest to the user. Use multiple"
    },
    "answer": "B",
    "explanation": "The optimal solution for minimizing content upload latency for a globally distributed mobile app with\ngeographically localized content consumption is B. Upload and store content in Amazon S3. Use S3 Transfer\nAcceleration for the uploads.\nHere's why:\nS3 as the core storage: Amazon S3 provides highly durable, scalable, and available storage for the media\nassets (photos and videos). It's a natural fit for this use case because the content is largely consumed soon\nafter upload, fitting the object storage paradigm well.\nS3 Transfer Acceleration: This feature leverages the globally distributed AWS edge locations (same network\nused by CloudFront) to accelerate uploads to S3. When a user uploads content, it's routed to the closest edge\nlocation. Then, data is transferred to the destination S3 bucket over an optimized network path, potentially\nreducing latency. The edge location handles the TCP handshake and optimizes the transfer protocol.\nLet's analyze why other options are not ideal:\nA. Upload and store content in Amazon S3. Use Amazon CloudFront for the uploads. While CloudFront\nexcels at content delivery, it primarily caches content that is already stored in an origin (like S3). CloudFront\ncan accept uploads, but this functionality (using CloudFront for PUT requests) is generally less optimized for\nupload performance than S3 Transfer Acceleration, especially considering the use case involves a large\nvolume of uploads. The primary function of CloudFront is CDN, and it is optimal for content distribution rather\nthan ingestion.\nC. Upload content to Amazon EC2 instances in the Region that is closest to the user. Copy the data to\nAmazon S3. Introducing EC2 instances adds unnecessary complexity and operational overhead. You'd need to\nmanage these instances, ensure their availability, and handle scaling. The transfer from EC2 to S3 would still\nincur latency. Moreover, the data transfer out costs from EC2 could be higher.\nD. Upload and store content in Amazon S3 in the Region that is closest to the user. Use multiple\ndistributions of Amazon CloudFront. This would be a valid solution for content delivery but does not directly\naddress the upload latency concern. You'd still need a fast mechanism to get the content into S3. The\ncomplexity of managing multiple distributions is also unnecessary for simply improving upload speeds. It\ndoesn't help to resolve the initial upload latency to S3.\nIn summary, S3 Transfer Acceleration is specifically designed to speed up uploads to S3, making it the best\noption for this scenario where low latency uploads are crucial for user experience.\nAuthoritative Links:\nAmazon S3 Transfer Acceleration: https://aws.amazon.com/s3/transfer-acceleration/\nCloudFront Documentation: https://aws.amazon.com/cloudfront/",
    "links": [
      "https://aws.amazon.com/s3/transfer-acceleration/",
      "https://aws.amazon.com/cloudfront/"
    ]
  },
  {
    "question": "CertyIQ\nA company is building a new application that uses serverless architecture. The architecture will consist of an\nAmazon API Gateway REST API and AWS Lambda functions to manage incoming requests.\nThe company wants to add a service that can send messages received from the API Gateway REST API to multiple\ntarget Lambda functions for processing. The service must offer message filtering that gives the target Lambda\nfunctions the ability to receive only the messages the functions need.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Send the requests from the API Gateway REST API to an Amazon Simple Notification"
    },
    "answer": "A",
    "explanation": "The best solution is A. Send the requests from the API Gateway REST API to an Amazon Simple Notification\nService (Amazon SNS) topic. Subscribe Amazon Simple Queue Service (Amazon SQS) queues to the SNS\ntopic. Configure the target Lambda functions to poll the different SQS queues. This approach provides\nmessage filtering and fan-out capability with the least operational overhead.\nHere's why:\nSNS for Fan-out: Amazon SNS excels at distributing messages to multiple subscribers. It natively supports a\npublish-subscribe (pub/sub) pattern, which is perfect for sending the same message to multiple Lambda\nfunctions.\nSQS for Decoupling and Reliability: By placing SQS queues between SNS and Lambda functions, you\ndecouple the services. This adds resilience, allowing Lambda functions to process messages even if they are\ntemporarily unavailable or experiencing errors. SQS also provides buffering, preventing message loss during\ntraffic spikes.\nSQS Filtering through SNS Message Filtering: SQS queues can subscribe to the SNS topic with filter\npolicies. These policies allow each queue (and thus the corresponding Lambda function) to receive only\nmessages with specific attributes. This satisfies the requirement for message filtering.\nReduced Operational Overhead: This solution minimizes the amount of custom code and infrastructure\nmanagement. SNS and SQS are managed services, meaning AWS handles the underlying infrastructure,\nscaling, and maintenance.\nAlternatives Considered:\nB (EventBridge): While EventBridge can invoke Lambda functions, it is generally better suited for event-driven\narchitectures that integrate various AWS services. Using it solely for API Gateway to Lambda fan-out adds\nunnecessary complexity compared to SNS+SQS.\nC (Amazon MSK): MSK is designed for high-throughput, real-time streaming data, which is overkill for this use\ncase. Managing an MSK cluster requires significantly more operational effort than SNS and SQS. It is also not\ndirectly integrated with API Gateway for message pushing.\nD (Multiple SQS Queues): Sending the request directly from API Gateway to multiple SQS queues would\nrequire the API Gateway configuration to know about each target Lambda function. This creates tight\ncoupling and doesn't support filtering without custom code within API Gateway to evaluate the messages and\ndirect them to the appropriate queues. This approach violates the \"least operational overhead\" requirement.\nIn summary, SNS with SQS offers a cost-effective, scalable, and easily manageable solution for distributing\nmessages to multiple Lambda functions with filtering capabilities, making it the best choice for this serverless\narchitecture.\nRelevant Documentation:\nAmazon SNS: https://aws.amazon.com/sns/\nAmazon SQS: https://aws.amazon.com/sqs/\nSNS Message Filtering: https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html\nAPI Gateway Lambda Integration: https://docs.aws.amazon.com/apigateway/latest/developerguide/services-\nlambda-integrations.html",
    "links": [
      "https://aws.amazon.com/sns/",
      "https://aws.amazon.com/sqs/",
      "https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/services-"
    ]
  },
  {
    "question": "CertyIQ\nA company migrated millions of archival files to Amazon S3. A solutions architect needs to implement a solution\nthat will encrypt all the archival data by using a customer-provided key. The solution must encrypt existing\nunencrypted objects and future objects.\nWhich solution will meet these requirements?",
    "options": {
      "C": "Encrypting existing objects: S3 Inventory is the efficient way to identify unencrypted objects within an S3"
    },
    "answer": "A",
    "explanation": "The correct answer is A because it directly addresses both requirements: encrypting existing unencrypted\nobjects and ensuring future objects are encrypted with SSE-C.\nEncrypting existing objects: S3 Inventory is the efficient way to identify unencrypted objects within an S3\nbucket. Filtering the Inventory report provides a list of unencrypted objects. S3 Batch Operations is then used\nto process this list and encrypt each object using SSE-C, ensuring the archival data is encrypted with the\ncustomer-provided key.\nEncrypting future objects: Configuring the S3 default encryption feature to use SSE-C guarantees that all\nnew objects uploaded to the bucket will be automatically encrypted with the customer-provided key. This\nensures all future archival data is encrypted as required.\nOther options are incorrect because:\nB: S3 Storage Lens is focused on bucket-level metrics and doesn't identify individual unencrypted objects.\nSSE-KMS is not SSE-C as required.\nC: The AWS usage report is not designed to list unencrypted objects and AWS Batch can't be directly used to\nencrypt S3 objects. SSE-KMS is not SSE-C as required.\nD: The AWS usage report is not designed to list unencrypted objects. Setting the S3 default encryption\nhandles future objects but doesn't encrypt existing ones.\nAuthoritative Links:\nAmazon S3 Inventory: https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory.html\nAmazon S3 Batch Operations: https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-\nmanage.html\nAmazon S3 Default Encryption: https://docs.aws.amazon.com/AmazonS3/latest/userguide/default-bucket-\nencryption.html\nServer-Side Encryption with Customer-Provided Keys (SSE-C):\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/default-bucket-",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html"
    ]
  },
  {
    "question": "CertyIQ\nThe DNS provider that hosts a company's domain name records is experiencing outages that cause service\ndisruption for a website running on AWS. The company needs to migrate to a more resilient managed DNS service\nand wants the service to run on AWS.\nWhat should a solutions architect do to rapidly migrate the DNS hosting service?",
    "options": {
      "C": "Specify the IP addresses that the"
    },
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A is the correct answer:\nThe primary goal is to migrate a publicly accessible website's DNS hosting to a more resilient AWS service,\nspecifically in response to current outages. Amazon Route 53 is AWS's highly available and scalable DNS\nservice. To host a public website's DNS records, a public hosted zone in Route 53 is required. This is because\nthe public hosted zone contains records that are accessible to the internet, allowing users to resolve the\nwebsite's domain name to the correct IP address.\nThe fastest way to migrate the existing DNS configuration is to import the zone file from the previous\nprovider. A zone file is a standard text file that contains all the DNS records (e.g., A, CNAME, MX) for a domain.\nRoute 53 provides a mechanism to import zone files, significantly reducing the manual effort required to\nrecreate the records. This ensures minimal disruption during the migration.\nOption B is incorrect because a private hosted zone is used for internal DNS resolution within a VPC, not for\npublic websites. Public users won't be able to resolve the website's domain through a private hosted zone.\nOption C is incorrect. Simple AD is used to create a directory service integrated with AWS. While it can be\nused for DNS, it is more complicated than Route 53 for public DNS migration. Also, the question is more about\nrapidly migrating a DNS service, not setting up an entirely new directory service solution. While AWS\nDirectory Service for Microsoft Active Directory can handle DNS forwarding, it is an overkill to use it to simply\nmigrate to a more resilient DNS service.\nOption D is incorrect because Route 53 Resolver inbound endpoints are used for hybrid cloud scenarios\nwhere you want to forward DNS queries from your on-premises network to AWS. This is the reverse of what's\nneeded  the question is about moving to AWS for DNS hosting. Route 53 resolver can handle the DNS\nqueries if they are hosted there; the scenario is to use a third party and forward those queries to\nAWS.Therefore, creating a public hosted zone and importing the existing records is the most efficient and\nappropriate solution for rapidly migrating a public website's DNS hosting to AWS.\nHere are authoritative links for further research:\nAmazon Route 53: https://aws.amazon.com/route53/\nWorking with Public Hosted Zones: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-\nzones-working-with.html\nImporting a Zone File: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/migrate-dns-zone-\nfile.html",
    "links": [
      "https://aws.amazon.com/route53/",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/migrate-dns-zone-"
    ]
  },
  {
    "question": "CertyIQ\nA company is building an application on AWS that connects to an Amazon RDS database. The company wants to\nmanage the application configuration and to securely store and retrieve credentials for the database and other\nservices.\nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A because it offers the most suitable and streamlined approach for managing\napplication configurations and securely handling credentials with minimal administrative burden.\nAWS AppConfig is specifically designed for managing application configurations. It allows you to create,\nmanage, and deploy application configuration updates in a controlled and validated manner. AppConfig\nintegrates with other AWS services, enabling you to validate configuration data before deploying it to your\napplications, thereby reducing the risk of configuration errors and application downtime. It offers features like\nvalidation schemas, controlled rollouts, and rollback capabilities. https://aws.amazon.com/appconfig/\nAWS Secrets Manager is the AWS service dedicated to securely storing and retrieving sensitive information\nlike database credentials, API keys, and other secrets. It enables you to rotate, manage, and retrieve secrets\nthroughout their lifecycle. Secrets Manager also offers automatic rotation of database credentials, enhancing\nsecurity without requiring manual intervention. https://aws.amazon.com/secrets-manager/\nOption B is less suitable. While AWS Systems Manager Parameter Store can store credentials, it's primarily\nintended for configuration data and operational parameters rather than sensitive secrets. Lambda functions\nare not designed to manage application configuration.\nOption C is not recommended due to security vulnerabilities. Storing credentials in an encrypted file within S3\nintroduces risks of unauthorized access if the file is compromised, or the encryption key is leaked. This\nmethod requires manual management and rotation of encryption keys and credentials.\nOption D is incorrect because using Amazon RDS to store application configuration is not its intended\npurpose, and it complicates the database schema and overall architecture. Furthermore, it does not provide\nthe same level of features and benefits that AppConfig provides for managing configuration changes. RDS is\ndesigned for storing structured data within a relational database, not for storing configurations or secrets.\nUsing RDS to store secrets exposes security vulnerabilities.\nTherefore, the combination of AWS AppConfig for configuration management and AWS Secrets Manager for\nsecure credential storage offers the most secure, scalable, and manageable solution with the least\nadministrative overhead.",
    "links": [
      "https://aws.amazon.com/appconfig/",
      "https://aws.amazon.com/secrets-manager/"
    ]
  },
  {
    "question": "CertyIQ\nTo meet security requirements, a company needs to encrypt all of its application data in transit while\ncommunicating with an Amazon RDS MySQL DB instance. A recent security audit revealed that encryption at rest\nis enabled using AWS Key Management Service (AWS KMS), but data in transit is not enabled.\nWhat should a solutions architect do to satisfy the security requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The correct solution is to download AWS-provided root certificates and use them in all connections to the RDS\ninstance because it enables SSL/TLS encryption for data in transit, securing communication between the\napplication and the RDS MySQL database.\nOption A, enabling IAM database authentication, primarily manages authentication and authorization, not\nencryption of data in transit. While it enhances security by controlling access, it doesn't address the\nrequirement of encrypting data while it's being transferred.\nOption B, providing self-signed certificates, is not a best practice in production environments due to security\nconcerns. Self-signed certificates are not trusted by default and can lead to man-in-the-middle attacks. Using\nAWS-provided certificates ensures trust and validity.\nOption C, taking a snapshot and restoring it to a new instance with encryption enabled, addresses encryption\nat rest, which is already configured. The security audit revealed that data in transit encryption is the missing\npiece, so this option doesn't solve the core issue.\nBy using AWS-provided root certificates, the application can establish an SSL/TLS connection with the RDS\ninstance. SSL/TLS encrypts the data stream between the application and the database, ensuring that\nsensitive information is protected during transmission. These certificates are pre-trusted and automatically\nupdated by AWS, simplifying management and improving security posture. This approach directly fulfills the\nsecurity requirement of encrypting all application data in transit while communicating with the RDS MySQL\nDB instance.\nReference:\nUsing SSL/TLS to encrypt a connection to a DB instance",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is designing a new web service that will run on Amazon EC2 instances behind an Elastic Load Balancing\n(ELB) load balancer. However, many of the web service clients can only reach IP addresses authorized on their\nfirewalls.\nWhat should a solutions architect recommend to meet the clients needs?",
    "options": {
      "C": "An A record in an Amazon Route 53 hosted zone pointing to an Elastic IP address.",
      "B": "This would also accomplish the task but can introduce additional cost and complexity."
    },
    "answer": "C",
    "explanation": "The correct answer is C. An A record in an Amazon Route 53 hosted zone pointing to an Elastic IP address.\nHere's why:\nThe primary requirement is that clients, with firewall restrictions allowing only specific IP addresses, need to\naccess the web service. Using Elastic IPs directly tied to the load balancer (options A and B) is generally\nproblematic and doesn't align with best practices. Elastic Load Balancers (both Application and Network Load\nBalancers) are dynamic. While you can technically associate Elastic IPs with Network Load Balancers, it's\ngenerally discouraged because the whole point of a load balancer is to distribute traffic across multiple\ninstances which may change, and fixing a static IP to it defeats this purpose and can introduce availability\nrisks. Application Load Balancers cannot directly associate with Elastic IPs. This makes options A and B\ntechnically impractical, and certainly not the best solution.\nOption D, using a proxy EC2 instance with a public IP, is also suboptimal. It introduces a single point of failure\nand adds unnecessary operational overhead for managing the proxy server. It also complicates the\narchitecture when a load balancer is already in place.\nOption C provides a cleaner and more scalable solution. By creating an A record in Route 53 that resolves to\nan Elastic IP address, clients can use the IP address in their firewall rules. Here's how it works:\n1. Elastic IP Address: You create and associate an Elastic IP address. This IP address remains constant\nand can be moved between resources (although not while it's assigned to the load balancer as we will\nresolve it to the load balancer's DNS name).\n2. Route 53 A Record: In Route 53, you create an A record for a domain name (e.g., api.example.com)\nand point it to the Elastic IP address. Important: Instead of directly assigning the Elastic IP to the\nELB, the DNS record initially points to the Elastic IP, and the Elastic IP is configured to point to the\nELB's DNS name (CNAME) internally within Route 53 or via a custom resource. This means that even\nif the underlying EC2 instances behind the ELB change (due to scaling or failures), the clients'\nfirewalls remain unaffected because they only allow traffic to the static Elastic IP. However, we never\nexpose the ELB's IP addresses directly to the internet. This is a critical difference.\nThis approach offers several advantages:\nStatic IP for Clients: Clients can use the Elastic IP address in their firewall rules, ensuring connectivity.\nDynamic Backend: The load balancer can still dynamically distribute traffic across multiple EC2 instances.\nThe Elastic IP acts as a consistent entry point, while the load balancer handles the backend infrastructure\nchanges.\nScalability and High Availability: The load balancer provides scalability and high availability for the web\nservice.\nManaged DNS: Route 53 is a highly available and scalable DNS service.\nCost-Effective: Compared to managing a proxy server, using Route 53 is generally more cost-effective.\nIn summary, using an A record in Route 53 pointing to an Elastic IP address provides a stable and\nmanageable entry point for clients with firewall restrictions while leveraging the benefits of a load\nbalancer for scalability and high availability.\nAuthoritative Links:\nElastic IP Addresses: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html\nAmazon Route 53: https://aws.amazon.com/route53/\nElastic Load Balancing: https://aws.amazon.com/elasticloadbalancing/\nClarification: To be completely clear, the A record (api.example.com) points to the Elastic IP, and then some\nmechanism, likely a custom AWS Lambda, or manual intervention would be required to ensure the Elastic IP\ncontinues to point to the load balancer's DNS name. This indirection makes it possible to use the ELB's load\nbalancing functionality without exposing the ELB's dynamic IP addresses to the internet. An alternative\nstrategy would be to utilize AWS Global Accelerator with its static IP addresses and direct traffic through it to\nthe ALB. This would also accomplish the task but can introduce additional cost and complexity.",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html",
      "https://aws.amazon.com/route53/",
      "https://aws.amazon.com/elasticloadbalancing/"
    ]
  },
  {
    "question": "CertyIQ\nA company has established a new AWS account. The account is newly provisioned and no changes have been\nmade to the default settings. The company is concerned about the security of the AWS account root user.\nWhat should be done to secure the root user?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B: Create IAM users for daily administrative tasks. Enable multi-factor authentication on\nthe root user. Here's why:\nThe AWS account root user has unrestricted access to all AWS resources in the account. It should only be\nused for initial setup tasks or when root access is explicitly required, such as changing account settings. Daily\nadministrative tasks should never be performed with the root user due to the high risk of accidental or\nmalicious misuse. Option A suggests disabling the root user, which is not recommended. The root user needs\nto be available for essential account management activities like changing the AWS support plan or closing the\naccount.\nCreating IAM users (option B) follows the principle of least privilege. IAM users can be granted specific\npermissions needed for their roles, limiting the potential impact of a security breach or accidental\nmisconfiguration. This greatly reduces the attack surface.\nFurthermore, enabling multi-factor authentication (MFA) on the root user account adds a crucial layer of\nsecurity. Even if the root user's password is compromised, an attacker would still need the MFA device to gain\naccess. MFA makes it significantly harder to compromise the root user.\nOption C suggests generating an access key for the root user. This is a dangerous practice. Access keys\nshould be used carefully, and not for day-to-day use. Using an access key on the root account creates a\nsignificant security risk. If the key is compromised, the entire account is compromised. IAM users should\nalways be utilized with appropriate policies attached.\nOption D is fundamentally incorrect. Giving the root user credentials to a single individual for daily tasks\ncompletely defeats the purpose of least privilege and shared responsibility. It centralizes a critical security\nrisk and violates best practices.\nIn summary, using IAM users for daily tasks with MFA on the root account is the most secure approach. It\nprotects the root account, promotes least privilege, and is a fundamental security practice in AWS. Disabling\nthe root account is discouraged because it's needed for specific administrative tasks. Giving the root user key\nor credentials for daily tasks makes the account vulnerable.\nRelevant links:\nAWS Identity and Access Management (IAM): https://aws.amazon.com/iam/\nSecuring Your AWS Account Root User: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-\nuser.html\nIAM Best Practices: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html",
    "links": [
      "https://aws.amazon.com/iam/",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is deploying an application that processes streaming data in near-real time. The company plans to use\nAmazon EC2 instances for the workload. The network architecture must be configurable to provide the lowest\npossible latency between nodes.\nWhich combination of network solutions will meet these requirements? (Choose two.)",
    "options": {},
    "answer": "A",
    "explanation": "The optimal solution for minimizing latency between EC2 instances processing streaming data in near real-\ntime necessitates reducing network hops and maximizing network throughput.\nOption A, enabling and configuring enhanced networking, is crucial. Enhanced networking utilizes Single Root\nI/O Virtualization (SR-IOV) to provide high performance networking capabilities on supported EC2 instance\ntypes. This bypasses the virtual switch layer, enabling lower latency, lower jitter, and higher packets per\nsecond (PPS) performance. By leveraging SR-IOV, data transmission between instances benefits from a more\ndirect path, minimizing delays. [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-\nnetworking.html]\nOption C, running the EC2 instances in a cluster placement group, is also vital. A cluster placement group\nplaces instances within a single Availability Zone, grouping them as close as possible to each other. This\nproximity reduces network latency because traffic doesn't have to traverse across different zones, thereby\nproviding high network throughput. This minimizes the physical distance and network hops, crucial for low-\nlatency communication. [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html]\nOption B is incorrect as separating instances into different accounts introduces greater network overhead and\nlikely increases latency due to cross-account communication complexities. Option D, attaching multiple ENIs,\ndoesn't inherently lower latency; it primarily increases network bandwidth and provides redundancy, and the\nincreased complexity can sometimes introduce latency. Option E, using EBS-optimized instance types,\noptimizes the throughput between the EC2 instance and EBS volumes, impacting storage I/O performance,\nnot network latency between instances. Therefore, only A and C directly address the low latency requirement.",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html]"
    ]
  },
  {
    "question": "CertyIQ\nA financial services company wants to shut down two data centers and migrate more than 100 TB of data to AWS.\nThe data has an intricate directory structure with millions of small files stored in deep hierarchies of subfolders.\nMost of the data is unstructured, and the companys file storage consists of SMB-based storage types from\nmultiple vendors. The company does not want to change its applications to access the data after migration.\nWhat should a solutions architect do to meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the best solution, considering the requirements of a financial\nservices company migrating a large amount of unstructured data with complex directory structure from SMB-\nbased file storage to AWS, while minimizing operational overhead and application changes:\nOption C, using AWS DataSync to migrate the data to Amazon FSx for Windows File Server, is the most\nsuitable choice for several key reasons. The company's reliance on SMB-based storage types points towards\nWindows-centric applications and workflows. Amazon FSx for Windows File Server provides a fully managed,\nnative Windows file system compatible with the SMB protocol. This compatibility allows the company to\nmigrate its data without needing to refactor existing applications, as they can continue to access files using\nthe same SMB paths. AWS DataSync is designed to efficiently move large datasets between on-premises\nstorage and AWS storage services. It optimizes data transfer using features like parallel data streams, in-\ntransit encryption, and automatic handling of file metadata. This helps to accelerate the migration process\nand ensures data integrity.\nOption A, using AWS Direct Connect to migrate data to Amazon S3, requires application changes. S3 is an\nobject storage service, which doesn't directly support the hierarchical file system structure that the company\ncurrently utilizes. Adapting applications to use the S3 API would involve significant development effort and\noperational overhead.\nOption B, using AWS DataSync to migrate the data to Amazon FSx for Lustre, isn't the best fit because FSx\nfor Lustre is optimized for high-performance computing and doesn't offer native SMB protocol support. While\nit could handle large datasets, it would likely require application changes to interact with Lustre's parallel file\nsystem.\nOption D, using AWS Direct Connect to migrate the on-premises file storage to an AWS Storage Gateway\nvolume gateway, introduces operational overhead related to managing the Storage Gateway appliance or VM\nand the underlying storage it fronts. Furthermore, it doesn't inherently solve the application compatibility\nissue as it mostly provides block-level storage.\nIn summary, option C provides the best balance between ease of migration (DataSync), application\ncompatibility (FSx for Windows File Server's SMB support), and minimizing operational overhead through a\nfully managed AWS service. FSx for Windows File Server addresses the key requirements of maintaining the\nexisting file structure and application compatibility.\nAuthoritative links:\nAWS DataSync: https://aws.amazon.com/datasync/\nAmazon FSx for Windows File Server: https://aws.amazon.com/fsx/windows/",
    "links": [
      "https://aws.amazon.com/datasync/",
      "https://aws.amazon.com/fsx/windows/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses an organization in AWS Organizations to manage AWS accounts that contain applications. The\ncompany sets up a dedicated monitoring member account in the organization. The company wants to query and\nvisualize observability data across the accounts by using Amazon CloudWatch.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A because it leverages the built-in, purpose-built features of CloudWatch cross-account\nobservability, designed specifically for centralizing monitoring data across multiple AWS accounts within an\norganization.\nHere's a detailed justification:\n1. CloudWatch Cross-Account Observability: AWS provides a feature called \"CloudWatch cross-\naccount observability\" that allows you to monitor and troubleshoot applications across multiple AWS\naccounts from a central monitoring account. This feature eliminates the need to manually configure\ncomplex IAM roles and policies in each account.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Unified-Cross-\nAccount.html\n2. Monitoring Account as Central Hub: The question explicitly states the desire for a dedicated\nmonitoring member account. CloudWatch cross-account observability is designed with this use case\nin mind, allowing you to designate one account as the monitoring hub.\n3. Simplified Data Sharing: To share data with the monitoring account, a CloudFormation template\nprovided by the monitoring account needs to be deployed in each source account. This template sets\nup the necessary permissions and configurations for securely transmitting observability data to the\ncentral monitoring account. This automation is essential for managing many accounts within an\norganization.\n4. Avoiding Complex IAM Management: Option C and D suggest creating IAM users and cross-account\nroles/policies manually. This approach becomes increasingly complex and difficult to manage as the\nnumber of accounts grows, contradicting the need for a scalable solution in a multi-account AWS\nOrganizations setup. Option A simplifies this by using pre-built CloudFormation templates.\n5. SCPs Are Not Intended for Data Access: Service Control Policies (SCPs), as suggested in option B,\nare primarily used to set guardrails and enforce policies at the organization level. While SCPs can\nrestrict actions, they are not designed for granting granular access to CloudWatch data within the\nmonitoring account. They operate at a higher level of abstraction, controlling which services and\nactions are allowed, rather than facilitating data sharing.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html\n6. Scalability and Manageability: CloudWatch cross-account observability provides a scalable and\nmanageable solution for monitoring across multiple accounts. By utilizing CloudFormation templates,\nthe deployment and configuration process can be automated and easily replicated across all\naccounts in the organization.\nIn conclusion, option A provides the most efficient and scalable solution for querying and visualizing\nobservability data across multiple AWS accounts by using Amazon CloudWatch, aligned with the design\nprinciples of AWS Organizations and the intended use of CloudWatch cross-account observability. The other\noptions involve more manual configuration and less efficient use of AWS features.",
    "links": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Unified-Cross-",
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html"
    ]
  },
  {
    "question": "CertyIQ\nA companys website is used to sell products to the public. The site runs on Amazon EC2 instances in an Auto\nScaling group behind an Application Load Balancer (ALB). There is also an Amazon CloudFront distribution, and\nAWS WAF is being used to protect against SQL injection attacks. The ALB is the origin for the CloudFront\ndistribution. A recent review of security logs revealed an external malicious IP that needs to be blocked from\naccessing the website.\nWhat should a solutions architect do to protect the application?",
    "options": {
      "B": "Again, this is also less efficient than blocking"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Modify the configuration of AWS WAF to add an IP match condition to block the\nmalicious IP address. Here's a detailed justification:\nAWS WAF (Web Application Firewall) is specifically designed to protect web applications from common web\nexploits and bots. It operates at Layer 7 of the OSI model, allowing it to inspect HTTP/HTTPS traffic and apply\nrules based on various criteria, including IP addresses.\nSince the requirement is to block a specific malicious IP address, WAF provides the most suitable and direct\nmethod. An IP match condition within WAF can be configured to block requests originating from that IP\naddress. This rule will be evaluated for every incoming request, preventing the malicious IP from reaching the\nALB and the EC2 instances. WAF is integrated directly with CloudFront, and protects CloudFront traffic.\nOption A is incorrect because Network ACLs (NACLs) operate at Layer 3 and 4 of the OSI model and are\nprimarily designed for controlling network traffic at the subnet level. While NACLs can block IP addresses,\nusing them at the CloudFront distribution would be less efficient than using WAF, because WAF integrates\ndirectly with CloudFront.\nOption C is not optimal because modifying the NACL for the EC2 instances would require the traffic to\ntraverse CloudFront, the ALB, and then be blocked by the NACL. This approach wastes resources and does\nnot prevent the malicious traffic from reaching the ALB. This is also less efficient than blocking traffic at the\nWAF level before it reaches those resources.\nOption D is not the ideal approach, as security groups are instance-level firewalls that primarily control traffic\nbased on source and destination IP addresses, ports, and protocols. Modifying the security group would\nrequire the malicious traffic to traverse CloudFront and the ALB. Again, this is also less efficient than blocking\ntraffic at the WAF level before it reaches those resources.\nHere are some authoritative links for further research:\nAWS WAF: https://aws.amazon.com/waf/\nAWS WAF IP Address Condition: https://docs.aws.amazon.com/waf/latest/developerguide/waf-ip-\ncondition.html\nAWS Network ACLs: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\nAWS Security Groups: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-groups.html\nAmazon CloudFront: https://aws.amazon.com/cloudfront/",
    "links": [
      "https://aws.amazon.com/waf/",
      "https://docs.aws.amazon.com/waf/latest/developerguide/waf-ip-",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-groups.html",
      "https://aws.amazon.com/cloudfront/"
    ]
  },
  {
    "question": "CertyIQ\nA company sets up an organization in AWS Organizations that contains 10 AWS accounts. A solutions architect\nmust design a solution to provide access to the accounts for several thousand employees. The company has an\nexisting identity provider (IdP). The company wants to use the existing IdP for authentication to AWS.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C because it leverages AWS IAM Identity Center (successor to AWS Single Sign-On),\nwhich is specifically designed for centralized management of access to multiple AWS accounts using an\nexisting identity provider (IdP). IAM Identity Center directly integrates with popular IdPs like Okta, Azure AD,\nand Ping Identity, enabling users to authenticate using their existing credentials. After successful\nauthentication with the IdP, IAM Identity Center provides users with temporary AWS credentials, allowing\nthem to access the AWS accounts and resources assigned to them based on predefined permissions.\nOption A is incorrect because creating individual IAM users in each of the 10 AWS accounts for thousands of\nemployees would be incredibly cumbersome and difficult to manage. While IAM federation is possible, it\ndoesn't provide the centralized management and single sign-on capabilities needed for this scenario.\nManually synchronizing thousands of users across multiple accounts is also prone to errors.\nOption B is incorrect and a highly discouraged practice. AWS account root users should never be used for\neveryday access. Sharing root user credentials or synchronizing them with an IdP is a significant security risk,\nas the root user has unrestricted access to the entire AWS account.\nOption D is incorrect because AWS Resource Access Manager (RAM) is designed for sharing AWS resources\n(like VPCs, subnets, and transit gateways) between AWS accounts, not for managing user access across\nmultiple accounts. RAM doesn't handle authentication or integration with external identity providers. It only\nallows you to grant resource-level permissions to accounts.\nTherefore, using IAM Identity Center to connect to the existing IdP is the most efficient, secure, and scalable\nsolution for managing access to multiple AWS accounts for a large number of employees using their existing\ncredentials. It simplifies user management, improves security, and provides a better user experience.\nSupporting links:\nAWS IAM Identity Center (successor to AWS Single Sign-On)\nAWS Organizations\nIdentity Providers and Federation",
    "links": []
  },
  {
    "question": "CertyIQ\nA solutions architect is designing an AWS Identity and Access Management (IAM) authorization model for a\ncompany's AWS account. The company has designated five specific employees to have full access to AWS\nservices and resources in the AWS account.\nThe solutions architect has created an IAM user for each of the five designated employees and has created an IAM\nuser group.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C, attaching the AdministratorAccess identity-based policy to the IAM user group and\nplacing the five employee IAM users in the group. Here's why:\nIAM Policies: IAM policies define permissions that determine what actions an identity (user, group, or role) can\nperform on AWS resources.\nIdentity-Based Policies: These policies are attached directly to IAM users, groups, or roles. They grant\npermissions to the identity to perform actions on resources. This is the standard and recommended way to\ngrant permissions to users.\nResource-Based Policies: These policies are attached to AWS resources (e.g., S3 buckets, KMS keys). They\ndefine who can access the resource and what actions they can perform. Resource-based policies are typically\nused to grant permissions to other AWS accounts or services to access the resource. Applying a resource-\nbased policy to a user group wouldn't achieve the goal of giving the users themselves admin access across\nthe account.\nAdministratorAccess Policy: This is a pre-defined AWS managed policy that grants full access to all AWS\nservices and resources within an account.\nSystemAdministrator Policy: There is no AWS managed policy named \"SystemAdministrator\". This is likely a\ncustom policy, but it is not a standard AWS policy.\nTherefore, to provide five employees with full administrative access, attaching the AdministratorAccess\nidentity-based policy to an IAM user group and adding those five users to that group is the correct approach.\nThis grants the group (and thus its members) the necessary permissions efficiently. Using a user group also\nsimplifies permission management as future admin users can simply be added to the group.\nWhy other options are incorrect:\nA & D: Resource-based policies are not applicable here. They control access to a resource, not permissions of\na user or group.\nB: The policy SystemAdministrator is not a standard AWS managed policy, which could grant insufficient or\nincorrect permissions.\nAuthoritative Links:\nIAM Policies Overview: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\nAWS Managed Policies: https://docs.aws.amazon.com/IAM/latest/UserGuide/aws-managed-policies.html\nIdentity-Based Policies vs. Resource-Based Policies:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/security-identity-policy-evaluations.html",
    "links": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/aws-managed-policies.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/security-identity-policy-evaluations.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has a multi-tier payment processing application that is based on virtual machines (VMs). The\ncommunication between the tiers occurs asynchronously through a third-party middleware solution that\nguarantees exactly-once delivery.\nThe company needs a solution that requires the least amount of infrastructure management. The solution must\nguarantee exactly-once delivery for application messaging.\nWhich combination of actions will meet these requirements? (Choose two.)",
    "options": {
      "D": "Use Amazon Simple Queue Service (Amazon SQS) FIFO queues as the messaging component between",
      "A": "Use AWS Lambda for the compute layers in the architecture: AWS Lambda is a serverless compute",
      "B": "Use Amazon EC2 instances for the compute layers in the architecture: While EC2 provides flexibility, it",
      "C": "Use Amazon Simple Notification Service (Amazon SNS) as the messaging component between the"
    },
    "answer": "A",
    "explanation": "The correct answer is AD. Here's a detailed justification:\nA. Use AWS Lambda for the compute layers in the architecture: AWS Lambda is a serverless compute\nservice. This aligns with the requirement of minimizing infrastructure management because Lambda abstracts\naway the need to provision and manage servers. The company can focus solely on writing the code for the\npayment processing tiers. This reduces operational overhead.\nD. Use Amazon Simple Queue Service (Amazon SQS) FIFO queues as the messaging component between\nthe compute layers: SQS FIFO (First-In, First-Out) queues provide exactly-once processing and preserve the\norder of messages. The application requirement mandates exactly-once delivery, which SQS FIFO directly\naddresses. Standard SQS queues offer at-least-once delivery, which could result in duplicate processing, so\nusing FIFO is crucial.\nLet's examine why the other options are not ideal:\nB. Use Amazon EC2 instances for the compute layers in the architecture: While EC2 provides flexibility, it\ncontradicts the requirement of minimizing infrastructure management. EC2 instances require patching,\nscaling, and other management tasks, adding operational complexity.\nC. Use Amazon Simple Notification Service (Amazon SNS) as the messaging component between the\ncompute layers: SNS is a publish-subscribe service, not a queue. It doesn't guarantee exactly-once delivery\nor message ordering. SNS typically provides at-least-once delivery. Therefore, it doesn't satisfy the\napplication's requirements.\nE. Use containers that are based on Amazon Elastic Kubernetes Service (Amazon EKS) for the compute\nlayers in the architecture: EKS offers container orchestration, which can be beneficial for managing complex\napplications. However, it requires significant infrastructure management related to the Kubernetes control\nplane, worker nodes, and associated networking and security configurations. This contradicts the \"least\namount of infrastructure management\" requirement. Serverless compute is more appropriate here.\nIn summary, Lambda eliminates server management for compute, and SQS FIFO queues guarantee exactly-\nonce delivery for messaging, making them the most suitable combination to fulfill the specified requirements.\nAuthoritative Links:\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon SQS FIFO queues:\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-fifo-queues.html",
    "links": [
      "https://aws.amazon.com/lambda/",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-fifo-queues.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has a nightly batch processing routine that analyzes report files that an on-premises file system\nreceives daily through SFTP. The company wants to move the solution to the AWS Cloud. The solution must be\nhighly available and resilient. The solution also must minimize operational effort.\nWhich solution meets these requirements?",
    "options": {
      "C": "Amazon S3 offers"
    },
    "answer": "D",
    "explanation": "The best solution is D because it leverages fully managed services like AWS Transfer for SFTP and Amazon\nS3, minimizing operational overhead. AWS Transfer for SFTP handles the secure file transfer aspects,\nremoving the need to manage SFTP servers on EC2 instances as suggested in B and C. Amazon S3 offers\nexcellent durability, scalability, and availability for storing the report files. Instead of relying on EFS as the\nprimary storage, S3 presents a better choice due to its cost-effectiveness, stronger data protection, and\nsimpler management compared to EFS in this scenario.\nThe EC2 instance in an Auto Scaling group with a scheduled scaling policy provides the compute power to run\nthe batch operation. Scheduling ensures the EC2 instance is only running when needed, which optimizes\ncosts. This approach also improves resilience because Auto Scaling can replace unhealthy instances\nautomatically. Modifying the application to pull files from S3 gives it the flexibility to adapt to the cloud\nenvironment, aligning with best practices for decoupling application components. Options A, B, and C require\nmanaging infrastructure elements like file systems or SFTP servers on EC2, increasing operational burden.\nThe direct S3 integration in option D simplifies the overall architecture and minimizes manual intervention.\nFurther Reading:\nAWS Transfer Family: https://aws.amazon.com/aws-transfer-family/\nAmazon S3: https://aws.amazon.com/s3/\nAmazon EC2 Auto Scaling: https://aws.amazon.com/autoscaling/",
    "links": [
      "https://aws.amazon.com/aws-transfer-family/",
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/autoscaling/"
    ]
  },
  {
    "question": "CertyIQ\nA company has users all around the world accessing its HTTP-based application deployed on Amazon EC2\ninstances in multiple AWS Regions. The company wants to improve the availability and performance of the\napplication. The company also wants to protect the application against common web exploits that may affect\navailability, compromise security, or consume excessive resources. Static IP addresses are required.\nWhat should a solutions architect recommend to accomplish this?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D because it provides a comprehensive solution addressing the requirements for global\navailability, performance, security, and static IP addresses.\nHere's a breakdown of why option D is superior:\nGlobal Availability and Performance: Amazon CloudFront, a content delivery network (CDN), caches content\nat edge locations worldwide, reducing latency for users accessing the application from different geographical\nlocations. Route 53 latency-based routing directs users to the Application Load Balancer (ALB) in the region\nwith the lowest network latency, ensuring optimal performance. This also contributes to high availability as\ntraffic is automatically routed away from unhealthy regions.\nSecurity: AWS WAF, deployed on the CloudFront distribution, protects the application against common web\nexploits like SQL injection and cross-site scripting (XSS). Placing WAF at the edge (CloudFront) provides\nprotection closest to the source of malicious traffic.\nStatic IP Addresses: CloudFront provides static IP addresses. This fulfils the given requirement in the\nquestion.\nLet's examine why the other options are less suitable:\nOption A & B: AWS Global Accelerator provides static IP addresses and improves performance, but it does not\ninherently provide protection against web exploits. Using it without WAF leaves the application vulnerable.\nAlso, Global Accelerator is suited more to TCP or UDP traffic and isn't as efficient at caching content as\nCloudFront.\nOption C: While this option uses NLBs, WAF at the NLB level is not ideal. WAF is better suited at the CDN level\n(CloudFront) for protection against web exploits. Additionally, NLBs are designed for high-performance, low-\nlatency traffic and don't offer the content caching benefits of CloudFront.\nIn summary, deploying ALBs behind CloudFront with WAF ensures optimal performance through caching and\nlatency-based routing, provides static IP addresses, and protects the application against web exploits at the\nedge, fulfilling all requirements in the most efficient and secure manner.\nHere are some helpful links for further research:\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nAWS WAF: https://aws.amazon.com/waf/\nAmazon Route 53: https://aws.amazon.com/route53/\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\nAWS Global Accelerator: https://aws.amazon.com/global-accelerator/",
    "links": [
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/waf/",
      "https://aws.amazon.com/route53/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
      "https://aws.amazon.com/global-accelerator/"
    ]
  },
  {
    "question": "CertyIQ\nA companys data platform uses an Amazon Aurora MySQL database. The database has multiple read replicas and\nmultiple DB instances across different Availability Zones. Users have recently reported errors from the database\nthat indicate that there are too many connections. The company wants to reduce the failover time by 20% when a\nread replica is promoted to primary writer.\nWhich solution will meet this requirement?",
    "options": {},
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the correct answer, along with supporting concepts and\nresources:\nThe problem presents a scenario with an Aurora MySQL database experiencing \"too many connections\" errors\nand a need to reduce failover time when a read replica becomes the primary writer. The goal is to address the\nconnection management issue while also improving failover performance.\nOption B, using Amazon RDS Proxy in front of the Aurora database, is the most appropriate solution for\nseveral key reasons:\n1. Connection Pooling and Multiplexing: RDS Proxy sits between your application and the database. It\npools database connections and reuses them efficiently. Instead of each application connection\nholding a direct database connection open, RDS Proxy manages a smaller pool of connections to the\nAurora database. When an application needs to execute a query, RDS Proxy checks out a connection\nfrom the pool, executes the query, and then returns the connection to the pool. This reduces the\nnumber of active connections to the database and alleviates the \"too many connections\" errors.\n2. Connection Management During Failover: During a failover, RDS Proxy automatically reconnects to\nthe new primary instance. It maintains application connections without interruption, minimizing\ndowntime. It accomplishes this by detecting the failover event and rerouting traffic to the new\nprimary. It can reduce the failover time because the application does not need to establish new\nconnections from scratch. This is how the 20% failover reduction is achieved.\n3. Improved Application Performance: By reducing connection overhead, RDS Proxy can also improve\napplication performance, especially for applications that make frequent short-lived connections to\nthe database.\nOption A (Switching to Amazon RDS with Multi-AZ) addresses high availability but does not inherently solve\nthe \"too many connections\" issue. Multi-AZ provides failover capabilities, but it doesn't manage connection\npooling. It's a foundational HA strategy but not the specific fix required here.\nOption C (Switching to DynamoDB with DAX) represents a significant architectural change. DynamoDB is a\nNoSQL database, and Aurora MySQL is relational. A complete migration would be required, and it would not\nbe suitable unless the application is significantly refactored. While DAX caches DynamoDB reads, it addresses\nlatency but isn't related to reducing failover time of a Relational Database, nor is it related to connection\nmanagement in relational databases.\nOption D (Switching to Amazon Redshift) is designed for data warehousing and analytics, not transactional\nworkloads like the one described in the problem. Relocation capability does not address the connection\nmanagement issues nor improve the read replica promotion speed significantly enough to meet the 20%\nreduction requirement. The switch would also imply significant application rewriting.\nIn summary, RDS Proxy directly addresses the connection management bottleneck, reducing the number of\nconnections to the database and providing connection persistence during failover. This directly leads to a\nfaster failover time because the application connections are maintained automatically.\nAuthoritative Links:\nAmazon RDS Proxy: https://aws.amazon.com/rds/proxy/\nAWS Documentation - Using Amazon RDS Proxy for High Availability:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy.html (Specifically look at sections\nrelated to Failover)",
    "links": [
      "https://aws.amazon.com/rds/proxy/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy.html"
    ]
  },
  {
    "question": "CertyIQ\nA company stores text files in Amazon S3. The text files include customer chat messages, date and time\ninformation, and customer personally identifiable information (PII).\nThe company needs a solution to provide samples of the conversations to an external service provider for quality\ncontrol. The external service provider needs to randomly pick sample conversations up to the most recent\nconversation. The company must not share the customer PII with the external service provider. The solution must\nscale when the number of customer conversations increases.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "B": "Scalability: Object Lambda scales automatically with S3 requests, handling increases in data volume and"
    },
    "answer": "A",
    "explanation": "Option A, using S3 Object Lambda, is the most efficient and scalable solution for redacting PII from S3\nobjects on-the-fly for an external service provider with minimal operational overhead. Object Lambda lets you\nadd custom code to S3 to process data as it's being retrieved, transforming the content before returning it to\nthe requesting application. This eliminates the need for separate data processing pipelines or copies of the\ndata.\nHere's why Option A is superior to the other options:\nLeast Operational Overhead: Object Lambda automates the redaction process upon data access. No need to\nmanage EC2 instances, batch jobs, or DynamoDB.\nScalability: Object Lambda scales automatically with S3 requests, handling increases in data volume and\naccess frequency seamlessly.\nReal-time Redaction: PII is redacted at the time of retrieval, ensuring the external service provider never sees\nthe raw, unredacted data.\nSecurity: Redaction logic is contained within a Lambda function, minimizing the risk of accidental PII\nexposure. Access is controlled via the Object Lambda Access Point.\nCost-Effectiveness: Pay only for the Lambda execution time and the data processed by S3. Eliminates costs\nassociated with maintaining infrastructure.\nIn contrast, Option B involves a batch process on EC2, which requires infrastructure management, scheduling,\nand can introduce latency between data arrival and redaction. It also requires duplicate storage. Option C\nrequires you to maintain a custom web application, involving significant operational complexity. Option D,\nwhich stores data into DynamoDB, is also an inferior choice, because it duplicates data into a DynamoDB\ndatabase unnecessarily, adding complexity and cost. S3 Object Lambda is the leanest, most scalable, most\ncost effective, and most secure solution.\nAuthoritative Links:\nAWS S3 Object Lambda: https://aws.amazon.com/s3/features/object-lambda/\nTransforming objects with S3 Object Lambda: https://aws.amazon.com/blogs/aws/introducing-amazon-s3-\nobject-lambda-transform-objects-as-they-are-being-retrieved/",
    "links": [
      "https://aws.amazon.com/s3/features/object-lambda/",
      "https://aws.amazon.com/blogs/aws/introducing-amazon-s3-"
    ]
  },
  {
    "question": "CertyIQ\nA company is running a legacy system on an Amazon EC2 instance. The application code cannot be modified, and\nthe system cannot run on more than one instance. A solutions architect must design a resilient solution that can\nimprove the recovery time for the system.\nWhat should the solutions architect recommend to meet these requirements?",
    "options": {
      "C": "Create an Amazon CloudWatch alarm to recover the EC2 instance in case of failure."
    },
    "answer": "C",
    "explanation": "The correct answer is C. Create an Amazon CloudWatch alarm to recover the EC2 instance in case of failure.\nHere's why this is the best solution and why the other options are not suitable:\nWhy C is correct: The goal is to improve recovery time for a single, non-modifiable legacy system running on\nEC2. A CloudWatch alarm with a \"Recover\" action provides an automated mechanism to reboot the EC2\ninstance if it becomes impaired due to underlying hardware issues. This is faster than manual intervention and\nrequires no application changes, aligning with the requirements. The \"Recover\" action is designed for\nscenarios where the instance itself is unhealthy, rather than an issue with the application running on it.\nCloudWatch continuously monitors the instance's health and triggers the recovery action based on pre-\ndefined thresholds (e.g., system status checks failing). This automated recovery minimizes downtime,\nimproving resilience without application modification.\nWhy A is incorrect: Enabling termination protection prevents accidental termination of the EC2 instance, but\nit doesn't help in recovering from underlying hardware or system issues that cause the instance to become\nimpaired. Termination protection ensures the instance isn't deleted, but doesn't ensure its availability.\nWhy B is incorrect: Multi-AZ deployment is typically used for applications that can be scaled across multiple\ninstances. The requirement states the application cannot run on more than one instance, making Multi-AZ\ndeployment unsuitable. Multi-AZ relies on application-level clustering and data replication, which are not\npossible with the specified constraints.\nWhy D is incorrect: RAID configurations on EBS volumes primarily provide data redundancy within the EC2\ninstance. While data loss is mitigated, RAID does not automatically recover the instance if it becomes\nimpaired. The EC2 instance would still need to be replaced or restarted, and RAID is not a substitute for\ninstance-level recovery. Moreover, RAID configurations can add complexity and might not be suitable for the\nlegacy application without thorough testing. The focus should be on recovering the instance, not just the data.\nAuthoritative Links:\nAmazon CloudWatch Recover an EC2 Instance:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html\nAmazon EC2 Termination Protection: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-\ninstances.html#Using_ChangingDisableAPITermination",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to deploy its containerized application workloads to a VPC across three Availability Zones. The\ncompany needs a solution that is highly available across Availability Zones. The solution must require minimal\nchanges to the application.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "C": "Configure Application"
    },
    "answer": "A",
    "explanation": "The best solution for deploying containerized applications across three Availability Zones with high\navailability and minimal operational overhead is using Amazon ECS with Service Auto Scaling.\nOption A is correct because ECS inherently simplifies container management and orchestration. Service Auto\nScaling with target tracking scaling automatically adjusts the number of ECS tasks (containers) based on a\nmetric like CPU utilization or memory consumption. Setting the minimum capacity to 3 ensures that at least\none task runs in each of the three AZs, providing high availability. The 'spread' task placement strategy with\nan Availability Zone attribute ensures ECS distributes tasks evenly across AZs. This approach requires\nminimal application changes, as the application remains containerized and is deployed via ECS's standard\nmechanisms.\nOption B, using EKS with self-managed nodes, introduces significantly higher operational overhead. Managing\nthe Kubernetes cluster and nodes (EC2 instances) requires substantial effort, including patching, scaling, and\nsecurity configurations. While EKS provides container orchestration, self-managing nodes undermines the\ngoal of minimal operational overhead.\nOption C, utilizing EC2 Reserved Instances, lacks the built-in container orchestration capabilities that are\ncrucial for managing containerized applications effectively. Configuring an Auto Scaling group to manage\nEC2 instances only addresses the scaling of VMs and does not inherently handle container deployment,\nhealth checks, or networking as effectively as container orchestration platforms. This approach requires\nmanual configuration of container deployment and management within the EC2 instances, which is more\ncomplex. A spread placement group offers distribution across AZs, but it doesn't offer the container focus\ndesired.\nOption D, using AWS Lambda, is unsuitable for running typical containerized application workloads. Lambda\nfunctions are designed for event-driven, short-lived computations. While Lambda can be configured within a\nVPC, it's not meant to handle persistent, complex container applications. Additionally, the container\nlimitations of Lambda make it unrealistic for this scenario.\nTherefore, ECS with Service Auto Scaling and spread placement strategy delivers the desired high availability\nacross AZs with the least operational burden and requires minimal modifications to the containerized\napplication.\nSupporting Links:\nAmazon ECS: https://aws.amazon.com/ecs/\nAmazon ECS Service Auto Scaling:\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html\nAmazon EKS: https://aws.amazon.com/eks/",
    "links": [
      "https://aws.amazon.com/ecs/",
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html",
      "https://aws.amazon.com/eks/"
    ]
  },
  {
    "question": "CertyIQ\nA media company stores movies in Amazon S3. Each movie is stored in a single video file that ranges from 1 GB to\n10 GB in size.\nThe company must be able to provide the streaming content of a movie within 5 minutes of a user purchase. There\nis higher demand for movies that are less than 20 years old than for movies that are more than 20 years old. The\ncompany wants to minimize hosting service costs based on demand.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the most suitable solution, along with supporting concepts\nand links:\nThe primary requirements are fast content delivery (within 5 minutes) after purchase and cost minimization\nbased on demand, considering varying demand for newer vs. older movies.\nOption C, storing newer movies in S3 Intelligent-Tiering and older movies in S3 Glacier Flexible Retrieval,\nbalances performance and cost effectively.\nS3 Intelligent-Tiering automatically moves data between frequent, infrequent, and archive access tiers based\non changing access patterns. This is ideal for newer movies with potentially fluctuating demand, ensuring low\nlatency while optimizing storage costs. https://aws.amazon.com/s3/storage-classes/intelligent-tiering/\nS3 Glacier Flexible Retrieval (formerly Glacier) is designed for archiving data that is infrequently accessed.\nThe key is using \"expedited retrieval\" which guarantees access within 1-5 minutes. This meets the 5-minute\nrequirement even for older, rarely accessed movies. Although expedited retrieval has a higher cost, it's\nacceptable because these are infrequent requests. https://aws.amazon.com/s3/storage-classes/glacier/\nOption A is less efficient because S3 Lifecycle policies react more slowly than Intelligent-Tiering to shifting\naccess patterns. Using only S3 Lifecycle policies may also require custom scripting to manage tiers, adding\ncomplexity. Moreover, S3 Infrequent Access (S3 IA) doesnt necessarily deliver within the guaranteed 5\nminutes.\nOption B, storing older movies in S3 Standard-IA, is also suboptimal. S3 Standard-IA provides a lower storage\ncost than S3 Standard but is still more expensive than Glacier Flexible Retrieval. It doesn't optimize cost as\neffectively as Glacier Flexible Retrieval for infrequently accessed data while lacking a guaranteed retrieval\ntime.\nOption D uses S3 Glacier Flexible Retrieval, but suggests \"bulk retrieval.\" Bulk retrieval can take 5-12 hours,\nviolating the 5-minute requirement. Therefore, despite the lower cost, it is inappropriate for time-sensitive\npost-purchase streaming.\nTherefore, the combination of S3 Intelligent-Tiering for recent movies (automatically handling variable access\npatterns with low latency) and S3 Glacier Flexible Retrieval with expedited retrieval for older movies (meeting\nthe retrieval time requirement at a reasonable price for infrequent access) makes option C the best solution. It\nis a robust and cost-effective solution that addresses both performance and cost optimization needs.",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/intelligent-tiering/",
      "https://aws.amazon.com/s3/storage-classes/glacier/"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect needs to design the architecture for an application that a vendor provides as a Docker\ncontainer image. The container needs 50 GB of storage available for temporary files. The infrastructure must be\nserverless.\nWhich solution meets these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the best answer, along with supporting information:\nThe question prioritizes a serverless architecture with minimal operational overhead for a Docker container\nrequiring 50 GB of temporary storage.\nOption C (Amazon ECS with Fargate and EFS): This solution leverages the strengths of both Amazon ECS\nwith Fargate and Amazon EFS. Fargate provides a serverless compute environment, eliminating the need to\nmanage EC2 instances, thus reducing operational overhead. Amazon EFS offers a fully managed, scalable,\nand elastic file system. It integrates seamlessly with ECS and can provide the necessary 50 GB of storage for\ntemporary files. Mounting an EFS volume to an ECS task using Fargate is straightforward and requires\nminimal configuration.\nOption A (Lambda with S3 mounted volume): While Lambda functions can run Docker images, Lambda does\nnot support directly mounting S3 buckets as volumes within the container. S3 is object storage and not\ndesigned to be used as a file system for temporary file operations, violating the original requirement. This\noption also involves significant complexity in managing data transfer between Lambda and S3 for temporary\nfile usage.\nOption B (Lambda with EBS volume): Lambda functions do not support direct attachment of EBS volumes.\nEBS volumes are designed to be attached to EC2 instances and cannot be used with the serverless Lambda\nenvironment. Thus it violates the \"serverless\" and \"LEAST operational overhead\" requirements.\nOption D (ECS with EC2 and EBS): While this option provides the required storage, it does not meet the\nrequirement for a serverless architecture. Using EC2 instances introduces significant operational overhead,\nincluding patching, scaling, and monitoring the EC2 instances themselves. This directly contradicts the\nprinciple of minimizing operational effort.\nTherefore, option C is the best choice as it combines serverless compute with managed file storage,\nminimizing operational overhead while fulfilling the storage requirements of the Docker container.\nSupporting Documentation:\nAmazon ECS with Fargate: https://aws.amazon.com/fargate/\nAmazon EFS: https://aws.amazon.com/efs/\nUsing Amazon EFS with Amazon ECS: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/efs-\nvolumes.html\nLambda Storage Options: https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html",
    "links": [
      "https://aws.amazon.com/fargate/",
      "https://aws.amazon.com/efs/",
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/efs-",
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to use its on-premises LDAP directory service to authenticate its users to the AWS Management\nConsole. The directory service is not compatible with Security Assertion Markup Language (SAML).\nWhich solution meets these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the best solution for authenticating users from an on-\npremises LDAP directory to the AWS Management Console when SAML isn't an option:\nThe core challenge is to bridge the on-premises LDAP directory (incompatible with SAML) with AWS\nauthentication. Options A, B, and C have significant drawbacks:\nOption A (IAM Identity Center): AWS IAM Identity Center primarily relies on SAML for federation. Since the\nLDAP is not SAML compatible, this option is not viable.\nOption B (IAM Policy Integration): Directly embedding AWS credentials into LDAP or linking IAM policies to\nLDAP authentication is a major security risk. AWS credentials should never be stored or managed outside of\nAWS Identity and Access Management (IAM) to prevent credential leakage and unauthorized access.\nOption C (Rotating IAM Credentials): Rotating IAM credentials based on LDAP updates is complex, error-\nprone, and still requires storing AWS credentials somewhere outside of AWS, which is a security vulnerability.\nOption D (Custom Identity Broker): A custom identity broker is the most suitable approach for LDAP\nintegration when SAML is unavailable.\nHere's why it works:\n1. Authentication Proxy: The custom application acts as an intermediary between the LDAP directory\nand AWS. Users authenticate with their existing LDAP credentials.\n2. STS Integration: After successful LDAP authentication, the broker application uses the AWS\nSecurity Token Service (STS) to request temporary, short-lived AWS credentials. The custom\napplication will assume an IAM role configured with permissions to use STS.\n3. Secure Credential Management: The application securely communicates with AWS STS using an\nAWS IAM user with only permissions to access the STS service, retrieves the temporary credentials.\n4. Console Access: These temporary credentials allow the user to access the AWS Management\nConsole or AWS resources based on the permissions defined in the IAM role that the broker assumes.\n5. Security Best Practices: This approach keeps AWS credentials secure within AWS and avoids\nexposing them to the on-premises environment. The temporary credentials expire automatically,\nfurther limiting the potential impact of any compromise.\n6. Flexibility: Provides fine-grained control over access based on the user's LDAP group memberships.\nIn summary, the identity broker model using STS provides a secure and manageable solution for integrating\nnon-SAML LDAP directories with AWS, addressing the company's authentication needs without compromising\nsecurity.\nAuthoritative Links:\nAWS Security Token Service (STS): https://aws.amazon.com/iam/features/security-token-service/\nIAM Roles: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html",
    "links": [
      "https://aws.amazon.com/iam/features/security-token-service/",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html"
    ]
  },
  {
    "question": "CertyIQ\nA company stores multiple Amazon Machine Images (AMIs) in an AWS account to launch its Amazon EC2\ninstances. The AMIs contain critical data and configurations that are necessary for the companys operations. The\ncompany wants to implement a solution that will recover accidentally deleted AMIs quickly and efficiently.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "C",
    "explanation": "The optimal solution is to utilize Recycle Bin for AMI retention. Here's why:\nOption C, using Recycle Bin, offers the lowest operational overhead because it directly addresses accidental\ndeletion with a built-in AWS feature. Recycle Bin allows you to create retention rules that specify a period\nduring which deleted AMIs (and other resources) are retained before permanent deletion. This provides a\nbuffer for recovery without requiring manual intervention or scripting.\nOption A, EBS snapshots, while useful for backup, adds complexity. Managing snapshots across accounts\nrequires cross-account IAM roles and potentially scripts for automating the snapshot creation and replication.\nSnapshots alone don't inherently prevent deletion; they just provide a point-in-time copy.\nOption B, copying AMIs, creates operational overhead in terms of management, storage costs, and transfer\nbandwidth. Periodic copying requires automation scripts and introduces complexity in synchronization and\nversion control.\nOption D, uploading AMIs to S3, is not the standard way to back up AMIs. AMI backup methods usually utilize\nsnapshots or cross-account AMI copy, which are designed specifically for the purpose. Additionally, restoring\nAMIs from S3 wouldn't be as direct or manageable as the original AMIs.\nRecycle Bin is designed specifically to prevent accidental deletion and enable easy recovery within a defined\nretention period, making it the most efficient solution.\nFor more information on Recycle Bin, refer to the AWS documentation:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recycle-bin.html",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recycle-bin.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has 150 TB of archived image data stored on-premises that needs to be moved to the AWS Cloud within\nthe next month. The companys current network connection allows up to 100 Mbps uploads for this purpose during\nthe night only.\nWhat is the MOST cost-effective mechanism to move this data and meet the migration deadline?",
    "options": {
      "B": "Authoritative Links:",
      "A": "AWS Snowmobile: Snowmobile is for exabyte-scale data transfer. It is overkill and significantly more",
      "C": "However, this solution still relies on the limited 100 Mbps network connection, making it infeasible for",
      "D": "Amazon S3 VPC endpoint and VPN: Creating an S3 VPC endpoint provides secure access to S3 from within"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B, \"Order multiple AWS Snowball devices to ship the data to\nAWS,\" is the most cost-effective solution for migrating 150 TB of data within a month, given a limited 100\nMbps upload speed available only at night:\nLet's analyze the limitations first. A 100 Mbps connection, even if fully utilized, is extremely slow for\ntransferring 150 TB. To calculate the transfer time:\n150 TB = 150 1024 1024 1024 8 bits = 1,288,490,188,800 bits\nUpload speed = 100 Mbps = 100,000,000 bits per second\nTotal time = 1,288,490,188,800 bits / 100,000,000 bits/second = 12,884,901.888 seconds\nTotal time = 12,884,901.888 seconds / (60 60 24) days = 148.8 days\nSince the data can only be transferred at night, the actual transfer time will be substantially longer than 148.8\ndays, making it infeasible within the one-month deadline.\nNow, let's evaluate the options:\nA. AWS Snowmobile: Snowmobile is for exabyte-scale data transfer. It is overkill and significantly more\nexpensive than Snowball for 150 TB.\nB. AWS Snowball: Snowball is specifically designed for transferring large amounts of data in and out of AWS\nwhen network bandwidth is limited. Ordering multiple Snowball devices allows for parallel data transfer,\nsignificantly reducing the overall migration time. The cost of Snowball is based on the device usage and the\njob performed, and is typically far more cost-effective than upgrading network infrastructure or dealing with\nprolonged transfer times.\nC. Amazon S3 Transfer Acceleration: Transfer Acceleration utilizes CloudFront edge locations to improve\ntransfer speeds to S3. However, the improvement is highly dependent on the distance between the client and\nthe S3 bucket, and the network conditions. Given the already slow 100 Mbps connection, Transfer\nAcceleration is unlikely to provide a sufficient speed increase to meet the deadline and comes with additional\ncosts. Furthermore, the bottleneck is the on-premises network speed, which Transfer Acceleration cannot\novercome.\nD. Amazon S3 VPC endpoint and VPN: Creating an S3 VPC endpoint provides secure access to S3 from within\na VPC. Establishing a VPN connection provides a secure tunnel between the on-premises network and the\nVPC. However, this solution still relies on the limited 100 Mbps network connection, making it infeasible for\ntransferring 150 TB within the one-month deadline. This setup also adds complexity and cost without\naddressing the core problem: slow network speed.\nSnowball offers the most practical combination of speed and cost-effectiveness. By using multiple devices\nconcurrently, the company can significantly accelerate the data transfer without incurring the higher costs\nassociated with Snowmobile or the limitations of the existing network connection.\nTherefore, the answer is B.\nAuthoritative Links:\nAWS Snowball: https://aws.amazon.com/snowball/\nAWS Snowmobile: https://aws.amazon.com/snowmobile/\nAmazon S3 Transfer Acceleration: https://aws.amazon.com/s3/transfer-acceleration/",
    "links": [
      "https://aws.amazon.com/snowball/",
      "https://aws.amazon.com/snowmobile/",
      "https://aws.amazon.com/s3/transfer-acceleration/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to migrate its three-tier application from on premises to AWS. The web tier and the application\ntier are running on third-party virtual machines (VMs). The database tier is running on MySQL.\nThe company needs to migrate the application by making the fewest possible changes to the architecture. The\ncompany also needs a database solution that can restore data to a specific point in time.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "A",
    "explanation": "The best solution is A because it minimizes changes and operational overhead while fulfilling all requirements.\nMinimizing Changes: Migrating the web and application tiers to EC2 instances is a straightforward \"lift and\nshift\" approach, requiring minimal code modification. Keeping both tiers in private subnets enhances security.\nSimilarly, migrating the MySQL database to Amazon RDS for MySQL requires minimal changes compared to\nother database options.\nDatabase Point-in-Time Recovery: Both Amazon RDS for MySQL and Amazon Aurora MySQL offer point-in-\ntime recovery.\nLeast Operational Overhead: Amazon RDS manages database administration tasks like patching, backups,\nand recovery, significantly reducing operational overhead compared to managing a MySQL instance on EC2.\nUsing the same MySQL engine reduces the risk of compatibility issues during the database migration.\nChoosing RDS over Aurora also minimizes changes to database configuration.\nSecurity: Placing the web and application tiers, as well as the database, in private subnets adds a layer of\nsecurity, preventing direct internet access. This is essential for protecting the application and data.\nLet's analyze why the other options are less suitable:\nOption B & D: Migrating to Aurora MySQL introduces a change in the database engine, potentially requiring\ncode modifications and increased testing, thus more changes and overhead. Option D is also insecure by\nplacing the database in a public subnet.\nOption C: Migrating to RDS for MySQL is a good move. However, the answer states the web-tier needs to be in\na public subnet which might not be needed if the application is using a load balancer.\nTherefore, option A provides the most efficient and secure migration path with the least operational overhead\nand minimal changes to the existing architecture.\nSupporting Links:\nAmazon EC2: https://aws.amazon.com/ec2/\nAmazon RDS: https://aws.amazon.com/rds/\nAmazon Aurora: https://aws.amazon.com/rds/aurora/\nAWS VPC: https://aws.amazon.com/vpc/",
    "links": [
      "https://aws.amazon.com/ec2/",
      "https://aws.amazon.com/rds/",
      "https://aws.amazon.com/rds/aurora/",
      "https://aws.amazon.com/vpc/"
    ]
  },
  {
    "question": "CertyIQ\nA development team is collaborating with another company to create an integrated product. The other company\nneeds to access an Amazon Simple Queue Service (Amazon SQS) queue that is contained in the development\nteam's account. The other company wants to poll the queue without giving up its own account permissions to do\nso.\nHow should a solutions architect provide access to the SQS queue?",
    "options": {
      "C": "Create an SQS access policy that provides the other company access to the SQS",
      "A": "Create an instance profile that provides the other company access to the SQS queue. Instance profiles",
      "B": "Create an IAM policy that provides the other company access to the SQS queue. While you could create",
      "D": "Create an Amazon Simple Notification Service (Amazon SNS) access policy that provides the other"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Create an SQS access policy that provides the other company access to the SQS\nqueue.\nHere's why:\nResource-Based Policies: SQS queues support resource-based policies (also known as access policies).\nThese policies directly attach to the SQS queue itself and specify who (which AWS accounts or IAM users)\ncan access the queue and what actions they can perform.\nCross-Account Access: SQS access policies enable cross-account access. This means you can grant\npermissions to principals (users, roles, or accounts) in another AWS account to access your SQS queue. The\nother company retains control of its own account and identities.\nGranular Permissions: SQS access policies allow you to specify precisely which SQS actions the other\ncompany can perform on the queue (e.g., sqs:ReceiveMessage, sqs:SendMessage, sqs:DeleteMessage).\nLet's examine why the other options are incorrect:\nA. Create an instance profile that provides the other company access to the SQS queue. Instance profiles\nare used to grant permissions to EC2 instances. The other company does not need an EC2 instance in the dev\nteam account.\nB. Create an IAM policy that provides the other company access to the SQS queue. While you could create\nan IAM policy, the other company would still need to assume a role in the development team's account,\nnegating their request to not use their account.\nD. Create an Amazon Simple Notification Service (Amazon SNS) access policy that provides the other\ncompany access to the SQS queue. SNS is a different service for pub/sub messaging. An SNS policy would\nnot grant access to an SQS queue.\nBy using an SQS access policy, the development team maintains ownership and control of the SQS queue, and\nthe other company can access it using its own AWS account credentials and identities, without needing to\nshare or assume roles within the development team's account.\nFor more information, refer to these AWS documentation links:\nAmazon SQS Access Control:\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-security-\noverview.html\nGranting Cross-Account Permissions:\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-grant-cross-\naccount-permissions-to-role.html",
    "links": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-security-",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-grant-cross-"
    ]
  },
  {
    "question": "CertyIQ\nA companys developers want a secure way to gain SSH access on the company's Amazon EC2 instances that run\nthe latest version of Amazon Linux. The developers work remotely and in the corporate office.\nThe company wants to use AWS services as a part of the solution. The EC2 instances are hosted in a VPC private\nsubnet and access the internet through a NAT gateway that is deployed in a public subnet.\nWhat should a solutions architect do to meet these requirements MOST cost-effectively?",
    "options": {
      "C": "Instruct the"
    },
    "answer": "D",
    "explanation": "The most cost-effective and secure solution is to use AWS Systems Manager (SSM) Session Manager. Here's\nwhy:\nSecurity: Session Manager provides secure and auditable instance management without needing to open\ninbound SSH ports or manage SSH keys, reducing the attack surface. It uses IAM policies to control access to\ninstances, ensuring proper authorization.\nCost-Effectiveness: Session Manager is included in the AWS Free Tier and has minimal costs beyond that,\nprimarily related to CloudTrail logging (if enabled), making it more cost-effective than setting up and\nmaintaining VPN connections or bastion hosts.\nEase of Use: Developers can connect to EC2 instances directly from the AWS Management Console, AWS CLI,\nor AWS SDKs, without needing to manage SSH keys or configure SSH clients.\nAuditing: Session Manager integrates with CloudTrail to log session activity for auditing and compliance\npurposes.\nNo Public IPs: Since the EC2 instances are in a private subnet and can access the internet through a NAT\nGateway, Session Manager can function effectively, as the instances need outbound access to SSM\nendpoints, not inbound access.\nIAM Role: Attaching the AmazonSSMManagedInstanceCore IAM policy to the EC2 instances' IAM role grants\nSSM the necessary permissions to manage the instances.\nWhy other options are less suitable:\nA: Bastion Host in Private Subnet: Placing a bastion host in the same private subnet defeats the purpose of a\nbastion host, as it should be in a public subnet. Additionally, granting ec2:CreateVpnConnection permission is\nunnecessary and overly permissive.\nB: Site-to-Site VPN: Setting up a Site-to-Site VPN for access from the corporate network is a viable option\nbut less cost-effective than SSM, especially if the company only needs SSH access. Furthermore, requiring\ndevelopers to set up another VPN for remote access adds complexity.\nC: Bastion Host in Public Subnet: While a valid approach, maintaining a bastion host involves operational\noverhead (patching, scaling, monitoring) and costs associated with the instance and its network traffic. SSM\noffers a managed and more cost-effective alternative.\nSupporting Links:\nAWS Systems Manager Session Manager: https://docs.aws.amazon.com/systems-\nmanager/latest/userguide/session-manager.html\nAmazonSSMManagedInstanceCore IAM Policy: https://docs.aws.amazon.com/systems-\nmanager/latest/userguide/security-iam-id-based-policy-examples.html",
    "links": [
      "https://docs.aws.amazon.com/systems-",
      "https://docs.aws.amazon.com/systems-"
    ]
  },
  {
    "question": "CertyIQ\nA pharmaceutical company is developing a new drug. The volume of data that the company generates has grown\nexponentially over the past few months. The company's researchers regularly require a subset of the entire\ndataset to be immediately available with minimal lag. However, the entire dataset does not need to be accessed on\na daily basis. All the data currently resides in on-premises storage arrays, and the company wants to reduce\nongoing capital expenses.\nWhich storage solution should a solutions architect recommend to meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The best storage solution is option C, deploying an AWS Storage Gateway volume gateway with cached\nvolumes backed by Amazon S3. Here's why:\nReduced Capital Expenses: Migrating data to S3 significantly reduces the need for expensive on-premises\nstorage arrays.\nImmediate Availability of Subset: The cached volumes feature ensures that frequently accessed subsets of\ndata are stored locally on the Storage Gateway appliance. This provides low-latency access for the\nresearchers.\nInfrequent Access of Full Dataset: The entire dataset resides in S3, providing cost-effective storage for data\nthat is not accessed daily.\nVolume Gateway and Cached Volumes: Volume Gateway provides block-based access, which can be more\nsuitable for certain types of data or applications. The cached volumes mode keeps frequently accessed data\non-premises, while storing the full dataset in S3. This balances cost and performance.\nWhy other options are less ideal:\nOption A (AWS DataSync to S3): While DataSync migrates data to S3, it doesn't address the immediate\navailability requirement. Researchers would still have to download data from S3, which introduces lag.\nOption B (Storage Gateway file gateway): File Gateway stores the entire dataset in S3 and caches recently\nused files locally. It might not be optimized for block-level access if the researchers are working with\napplications requiring it.\nOption D (Site-to-Site VPN and EFS): EFS is suitable for shared file storage but generally more expensive\nthan S3 for large, infrequently accessed datasets. Maintaining a VPN connection and transferring large\namounts of data over the VPN may also be less performant and more complex.\nAWS Storage Gateway allows you to seamlessly integrate on-premises application environments with AWS\ncloud storage. The Cached Volumes mode is ideal for organizations that want to reduce on-premises storage\nfootprint while maintaining performance for frequently accessed data.\nAuthoritative Links:\nAWS Storage Gateway: https://aws.amazon.com/storagegateway/\nAWS Storage Gateway Cached Volumes:\nhttps://docs.aws.amazon.com/storagegateway/latest/userguide/HowStorageGatewayWorks.html\nAmazon S3: https://aws.amazon.com/s3/",
    "links": [
      "https://aws.amazon.com/storagegateway/",
      "https://docs.aws.amazon.com/storagegateway/latest/userguide/HowStorageGatewayWorks.html",
      "https://aws.amazon.com/s3/"
    ]
  },
  {
    "question": "CertyIQ\nA company has a business-critical application that runs on Amazon EC2 instances. The application stores data in\nan Amazon DynamoDB table. The company must be able to revert the table to any point within the last 24 hours.\nWhich solution meets these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A: Configure point-in-time recovery (PITR) for the table. Here's why:\nOption A directly addresses the requirement of reverting the DynamoDB table to any point within the last 24\nhours with minimal operational overhead. DynamoDB PITR provides automated, continuous backups that allow\nyou to restore your table to any point in time during the specified recovery window. This is the simplest and\nmost efficient way to meet the requirement. You enable it once, and it handles the rest.\nOption B, using AWS Backup, is a viable solution but has a slightly higher operational overhead than PITR.\nWhile AWS Backup simplifies managing backups across multiple AWS services, it typically involves\nconfiguring backup plans with specific schedules. Although effective, this requires more initial configuration\ncompared to simply enabling PITR on the table. While AWS Backup is excellent for long-term retention, the\nrequirement specifically asks for restoration within 24 hours.\nOption C, creating hourly backups with a Lambda function, is not recommended due to its significant\noperational overhead and potential for inconsistencies. Developing, deploying, and maintaining a Lambda\nfunction to perform backups introduces complexity. Furthermore, relying on a Lambda function for regular\nbackups can be less reliable than DynamoDB's built-in PITR feature, as the Lambda function could fail,\nleading to missed backups.\nOption D, using DynamoDB Streams and storing them in S3, is not designed for full table restoration and\nwould be complex to implement. Streams capture individual changes to the table, not the entire table state.\nReconstructing the table from the stream data would be computationally expensive and time-consuming,\npotentially leading to inconsistencies and significantly more operational effort. Also, the question is to revert\nthe table within the last 24 hours, meaning the stream would have to be processed every time a restore is\nneeded, which could be problematic.\nTherefore, enabling point-in-time recovery (PITR) for the DynamoDB table is the best solution as it offers a\nmanaged, automated, and efficient way to restore the table to any point within the last 24 hours with the least\noperational overhead.\nHere are links for more research:\nDynamoDB Point-in-Time Recovery:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html\nAWS Backup: https://aws.amazon.com/backup/\nDynamoDB Streams: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html",
    "links": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html",
      "https://aws.amazon.com/backup/",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts an application used to upload files to an Amazon S3 bucket. Once uploaded, the files are\nprocessed to extract metadata, which takes less than 5 seconds. The volume and frequency of the uploads varies\nfrom a few files each hour to hundreds of concurrent uploads. The company has asked a solutions architect to\ndesign a cost-effective architecture that will meet these requirements.\nWhat should the solutions architect recommend?",
    "options": {},
    "answer": "B",
    "explanation": "The correct solution is B: \"Configure an object-created event notification within the S3 bucket to invoke an\nAWS Lambda function to process the files.\" This approach directly addresses the requirement of processing\nfiles after they are uploaded to S3 in a cost-effective manner.\nHere's why this solution is optimal:\nEvent-driven architecture: S3 event notifications allow for real-time responses to object creation, ensuring\nimmediate processing.\nLambda for processing: AWS Lambda is a serverless compute service, ideal for short-duration, event-\ntriggered tasks. The metadata extraction process, taking less than 5 seconds, perfectly fits Lambda's use\ncase. Lambda's pricing model (pay-per-execution) ensures cost-effectiveness, especially with variable upload\nfrequency.\nScalability: Both S3 event notifications and Lambda scale automatically to handle varying upload volumes,\nfrom a few files per hour to hundreds of concurrent uploads.\nWhy other options are less suitable:\nA: CloudTrail and AppSync: CloudTrail is for auditing API calls, not for triggering file processing. AWS\nAppSync is for building GraphQL APIs, which is unnecessary overhead for simple metadata extraction.\nC: Kinesis Data Streams and Lambda: Kinesis Data Streams are designed for real-time streaming data, not\nfor processing individual files uploaded to S3. Using Kinesis would be an overkill, add unnecessary\ncomplexity, and increase costs.\nD: SNS and Lambda: While SNS can be used to trigger Lambda functions, it adds an unnecessary\nintermediary. S3 can directly trigger Lambda functions through event notifications, simplifying the\narchitecture.\nIn summary, S3 event notifications coupled with AWS Lambda offer a direct, cost-effective, and scalable\nsolution for processing files uploaded to S3. This combination leverages the strengths of each service to meet\nthe given requirements efficiently.\nRelevant links:\nAmazon S3 Event Notifications:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html\nAWS Lambda: https://aws.amazon.com/lambda/",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html",
      "https://aws.amazon.com/lambda/"
    ]
  },
  {
    "question": "CertyIQ\nA companys application is deployed on Amazon EC2 instances and uses AWS Lambda functions for an event-\ndriven architecture. The company uses nonproduction development environments in a different AWS account to\ntest new features before the company deploys the features to production.\nThe production instances show constant usage because of customers in different time zones. The company uses\nnonproduction instances only during business hours on weekdays. The company does not use the nonproduction\ninstances on the weekends. The company wants to optimize the costs to run its application on AWS.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "C": "Here's a detailed justification:"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Here's a detailed justification:\nProduction Instances: Production instances require consistent uptime due to global users across time zones.\nCompute Savings Plans offer significant cost savings for consistent, long-term EC2 usage compared to On-\nDemand, Reserved Instances, or Dedicated Hosts. They provide discounts on EC2 usage regardless of\ninstance type, size, or operating system within a region.\nReference: https://aws.amazon.com/savingsplans/compute-savings-plans/\nNon-Production Instances: Non-production instances are only needed during business hours on weekdays,\nand not on weekends. Therefore, shutting them down when not in use is crucial for cost optimization. Using\nOn-Demand Instances for non-production ensures you only pay for the compute time actually consumed.\nReference: https://aws.amazon.com/ec2/pricing/on-demand/\nWhy other options are less optimal:\nA: Dedicated Hosts are expensive and only provide cost benefits with specific licensing constraints which\naren't stated in the prompt. They do not provide cost benefits for instances that are only needed for limited\ntimeframes. Using On-Demand for production does not allow you to take advantage of discounts offered by\nsustained usage.\nB: Reserved Instances require commitment for a specific instance type and region, and while they save\nmoney, they are less flexible than Savings Plans. Also, RI for non-production instances is still less cost-\neffective if they are not being used for large periods of time. Shutting them down reduces the cost even more\nwhen used with On-Demand instances.\nD: Dedicated Hosts are expensive as mentioned before. While EC2 Instance Savings Plans are cost-effective,\nshutting down the non-production instances when unused provides more savings for a variable workload that\nonly operates during business hours.",
    "links": [
      "https://aws.amazon.com/savingsplans/compute-savings-plans/",
      "https://aws.amazon.com/ec2/pricing/on-demand/"
    ]
  },
  {
    "question": "CertyIQ\nA company stores data in an on-premises Oracle relational database. The company needs to make the data\navailable in Amazon Aurora PostgreSQL for analysis. The company uses an AWS Site-to-Site VPN connection to\nconnect its on-premises network to AWS.\nThe company must capture the changes that occur to the source database during the migration to Aurora\nPostgreSQL.\nWhich solution will meet these requirements?",
    "options": {
      "C": "Also, using Snowball (option D) for initial data transfer is only relevant if network"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it provides a comprehensive solution for migrating an on-premises Oracle\ndatabase to Aurora PostgreSQL while capturing ongoing changes during the migration.\nHere's a breakdown of the justification:\nAWS Schema Conversion Tool (AWS SCT): SCT is specifically designed to convert database schemas from\none database engine to another. In this case, it efficiently converts the Oracle schema to an Aurora\nPostgreSQL-compatible schema, addressing the initial schema incompatibility.\nhttps://aws.amazon.com/dms/schema-conversion-tool/\nAWS Database Migration Service (AWS DMS): DMS is a managed service that facilitates database\nmigrations. It supports both one-time full-load migrations and continuous data replication. DMS can handle\nthe initial data load and, critically, capture ongoing changes (change data capture or CDC) from the Oracle\ndatabase and apply them to the Aurora PostgreSQL database in near real-time. This ensures minimal\ndowntime and data loss during the migration. The Site-to-Site VPN provides the necessary connectivity for\nDMS to access the on-premises Oracle database.\nhttps://aws.amazon.com/dms/\nOption A is incomplete because it only addresses the initial data load and misses the crucial requirement of\ncapturing ongoing changes during the migration.\nOptions B and D involve using S3 as an intermediary. While S3 is a good storage option, it doesn't directly\naddress the schema conversion or the need to capture real-time changes. Using S3 for the initial data load is\nless efficient than DMS. The aws_s3 extension can import data from S3 but doesn't handle the initial schema\nconversion or CDC. Also, using Snowball (option D) for initial data transfer is only relevant if network\nbandwidth is severely limited, which is not stated in the problem; the company has a Site-to-Site VPN.",
    "links": [
      "https://aws.amazon.com/dms/schema-conversion-tool/",
      "https://aws.amazon.com/dms/"
    ]
  },
  {
    "question": "CertyIQ\nA company built an application with Docker containers and needs to run the application in the AWS Cloud. The\ncompany wants to use a managed service to host the application.\nThe solution must scale in and out appropriately according to demand on the individual container services. The\nsolution also must not result in additional operational overhead or infrastructure to manage.\nWhich solutions will meet these requirements? (Choose two.)",
    "options": {
      "A": "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate. ECS is a managed container",
      "B": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate. EKS is a managed Kubernetes",
      "C": "Provision an Amazon API Gateway API. Connect the API to AWS Lambda to run the containers. API",
      "D": "Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes. While ECS itself is"
    },
    "answer": "A",
    "explanation": "The question asks for container orchestration solutions on AWS that scale automatically and minimize\noperational overhead.\nA. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate. ECS is a managed container\norchestration service. Fargate is a serverless compute engine for ECS that allows you to run containers\nwithout managing EC2 instances. This combination directly addresses the requirements by automatically\nscaling containers based on demand and eliminating the need to manage underlying infrastructure.\nhttps://aws.amazon.com/ecs/fargate/\nB. Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate. EKS is a managed Kubernetes\nservice, and Kubernetes is a powerful container orchestration platform. Using EKS with Fargate provides\nsimilar benefits to ECS with Fargate: automatic scaling and reduced operational overhead, as Fargate\nmanages the underlying compute resources. EKS offers greater flexibility and richer features for complex\ncontainer deployments compared to ECS, which might be relevant depending on the application's complexity,\nthough ECS will still generally be a better solution. https://aws.amazon.com/eks/fargate/\nC. Provision an Amazon API Gateway API. Connect the API to AWS Lambda to run the containers. API\nGateway and Lambda are not directly designed for running persistent containerized applications at scale.\nWhile you could conceivably run containers inside Lambda (though this is not their intended use, and would\ninvolve a lot of orchestration code and limitations like cold starts), it is an inefficient and unnecessarily\ncomplex approach compared to ECS or EKS. Lambda is intended for event-driven, short-lived functions.\nD. Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes. While ECS itself is\na managed service, using EC2 worker nodes negates the requirement of minimizing operational overhead. You\nare then responsible for provisioning, managing, patching, and scaling the EC2 instances hosting the\ncontainers.\nE. Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes. Similar to option\nD, using EC2 worker nodes with EKS introduces significant operational overhead that the question aims to\navoid. You have to manage the EC2 instances backing the Kubernetes cluster.",
    "links": [
      "https://aws.amazon.com/ecs/fargate/",
      "https://aws.amazon.com/eks/fargate/"
    ]
  },
  {
    "question": "CertyIQ\nAn ecommerce company is running a seasonal online sale. The company hosts its website on Amazon EC2\ninstances spanning multiple Availability Zones. The company wants its website to manage sudden traffic increases\nduring the sale.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the most cost-effective solution for handling sudden traffic\nincreases during a seasonal sale for an e-commerce website hosted on EC2 instances across multiple\nAvailability Zones, along with why the other options are less ideal:\nJustification for Option D:\nOption D, configuring an Auto Scaling group (ASG) to scale out as traffic increases and using a launch\ntemplate with a preconfigured Amazon Machine Image (AMI), is the most cost-effective approach because it\nadheres to the principle of elasticity inherent in cloud computing. An ASG dynamically adjusts the number of\nEC2 instances based on real-time demand.\n1. Cost Optimization: You only pay for the EC2 instances you're actively using to serve traffic. When\ntraffic is low, the ASG scales in, reducing costs. This aligns with the principle of paying for what you\nuse.\n2. Scalability: An ASG can rapidly provision new instances to meet sudden spikes in demand. The\nlaunch template simplifies and speeds up the instance provisioning process because the AMI already\ncontains the application code and configurations.\n3. High Availability: By spanning multiple Availability Zones (AZs), the ASG ensures that the application\nremains available even if one AZ experiences an outage.\n4. Automated Response: The ASG automatically detects and replaces unhealthy instances, further\nenhancing the application's reliability.\nWhy other options are less suitable:\nOption A: Stopping instances and then starting them to scale is much slower than launching new instances\nfrom an AMI. Additionally, keeping half of the instances stopped still incurs costs for the storage associated\nwith the stopped instances. It doesn't fully leverage the \"pay-as-you-go\" model as efficiently as dynamically\nscaling.\nOption B: Setting a fixed, high minimum size for the ASG wastes resources during periods of low traffic.\nYou're essentially paying for unused capacity. This is not cost-effective.\nOption C: While using CloudFront and ElastiCache is good practice for improving performance and reducing\nload on the origin (EC2 instances), it doesn't eliminate the need for scaling. Also, scaling in the ASG after\npopulating the cache might be premature; the cache hit ratio will determine the actual load on the origin.\nFurthermore, treating CloudFront and ElastiCache as the sole scalability solution without properly configured\norigin scaling leaves the website vulnerable to overload if the cache is insufficient.\nIn summary, option D provides the best balance between cost efficiency, scalability, and high availability by\ndynamically adjusting EC2 instance capacity based on actual demand using Auto Scaling and preconfigured\nAMIs.\nAuthoritative Links:\nAWS Auto Scaling: https://aws.amazon.com/autoscaling/\nAmazon Machine Images (AMIs): https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\nAWS Launch Templates: https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-templates.html",
    "links": [
      "https://aws.amazon.com/autoscaling/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-templates.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect must provide an automated solution for a company's compliance policy that states security\ngroups cannot include a rule that allows SSH from 0.0.0.0/0. The company needs to be notified if there is any\nbreach in the policy. A solution is needed as soon as possible.\nWhat should the solutions architect do to meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B because it leverages AWS Config, a fully managed service designed for configuration\nmanagement and compliance. AWS Config's restricted-ssh managed rule directly addresses the requirement\nof monitoring security groups for SSH access from 0.0.0.0/0. This managed rule automates the detection of\npolicy violations, minimizing operational overhead compared to a custom Lambda function.\nOption A, writing a custom Lambda function, requires development, deployment, and ongoing maintenance,\nwhich increases operational overhead. Although effective, it's not the least overhead option due to the manual\neffort involved.\nOption C introduces an unnecessary and risky security practice. Creating an IAM role that globally opens\nsecurity groups and network ACLs elevates privilege escalation and potential misuse. Monitoring role\nassumption doesn't directly address the specific security group rule violation, and the operational overhead of\nmanaging and monitoring this role is high.\nOption D, using SCPs, prevents non-administrative users from creating or editing security groups but doesn't\ndetect existing violations or notify when non-compliant rules are already in place. This approach requires\nadministrators to manually review and remediate existing security groups, increasing operational overhead.\nSCPs also don't directly integrate with notification mechanisms.\nTherefore, using the AWS Config restricted-ssh managed rule and its built-in integration with Amazon SNS\noffers the fastest and least operationally intensive solution. AWS Config handles the continuous monitoring,\nevaluation, and notification of compliance status automatically, fulfilling the company's requirements with\nminimal effort.\nAuthoritative Links:\nAWS Config Managed Rules: https://docs.aws.amazon.com/config/latest/developerguide/evaluate-\nconfig.html\nAWS Config restricted-ssh Managed Rule:\nhttps://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html\nAmazon SNS: https://aws.amazon.com/sns/",
    "links": [
      "https://docs.aws.amazon.com/config/latest/developerguide/evaluate-",
      "https://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html",
      "https://aws.amazon.com/sns/"
    ]
  },
  {
    "question": "CertyIQ\nUse Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes.\nA company has deployed an application in an AWS account. The application consists of microservices that run on\nAWS Lambda and Amazon Elastic Kubernetes Service (Amazon EKS). A separate team supports each microservice.\nThe company has multiple AWS accounts and wants to give each team its own account for its microservices.\nA solutions architect needs to design a solution that will provide service-to-service communication over HTTPS\n(port 443). The solution also must provide a service registry for service discovery.\nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": {
      "C": "Create",
      "B": "Create a VPC Lattice service network. Associate the microservices with the"
    },
    "answer": "B",
    "explanation": "The correct solution is B. Create a VPC Lattice service network. Associate the microservices with the\nservice network. Define HTTPS listeners for each service. Register microservice compute resources as\ntargets. Identify VPCs that need to communicate with the services. Associate those VPCs with the service\nnetwork.\nHere's a detailed justification:\nVPC Lattice is designed specifically for service-to-service communication in a multi-account and multi-VPC\nenvironment, making it the most suitable and least administratively overhead solution. It provides service\ndiscovery, traffic management (including HTTPS routing), and security policies at the application layer (Layer\n7). By associating the Lambda functions and EKS-based microservices with the VPC Lattice service network,\nyou create a unified platform for inter-service communication. Defining HTTPS listeners for each service\nensures all communication is encrypted and occurs over port 443. Registering the microservices (Lambda and\nEKS) as targets enables VPC Lattice to route traffic to the appropriate endpoints. Associating VPCs that need\ncommunication with the lattice provides connectivity without complex routing or peering configurations.\nOptions A, C, and D have significant drawbacks in terms of administrative complexity and suitability for the\ngiven requirements. Option A involves creating and managing a central inspection VPC with Network Firewall,\nwhich is more suited for network-level security rather than application-level routing and service discovery. It\ndoes not directly address the service registry requirement. Option C, using NLBs and PrivateLink, is a valid\nsolution for service exposure, but it requires managing individual NLBs for each service and configuring\nPrivateLink endpoints in multiple VPCs, leading to higher administrative overhead than VPC Lattice.\nFurthermore, PrivateLink doesn't inherently provide service discovery capabilities. Option D, VPC peering,\nprefix lists, route tables, and security groups is the most complex and least scalable solution, as it requires\nmanaging a large number of peering connections and security rules across multiple accounts and VPCs,\nleading to significant administrative overhead and potential security risks. VPC peering does not provide built-\nin service discovery, which is a critical requirement.\nTherefore, VPC Lattice provides the most streamlined and efficient solution for service-to-service\ncommunication with service discovery, HTTPS support, and minimal administrative overhead in a multi-\naccount AWS environment.\nAuthoritative links:\nVPC Lattice: https://aws.amazon.com/vpc/lattice/\nAWS PrivateLink: https://aws.amazon.com/privatelink/\nAmazon EKS: https://aws.amazon.com/eks/",
    "links": [
      "https://aws.amazon.com/vpc/lattice/",
      "https://aws.amazon.com/privatelink/",
      "https://aws.amazon.com/eks/"
    ]
  },
  {
    "question": "CertyIQ\nA company has a mobile game that reads most of its metadata from an Amazon RDS DB instance. As the game\nincreased in popularity, developers noticed slowdowns related to the game's metadata load times. Performance\nmetrics indicate that simply scaling the database will not help. A solutions architect must explore all options that\ninclude capabilities for snapshots, replication, and sub-millisecond response times.\nWhat should the solutions architect recommend to solve these issues?",
    "options": {},
    "answer": "C",
    "explanation": "The optimal solution is adding an Amazon ElastiCache for Redis layer in front of the database.\nHere's why: The problem highlights read performance bottlenecks when retrieving metadata from RDS.\nScaling the database doesn't solve this, implying the issue is high read load rather than database capacity.\nThe requirements are snapshots, replication, and sub-millisecond response times. ElastiCache for Redis\nexcels at caching frequently accessed data, reducing the load on the RDS database. This significantly speeds\nup metadata retrieval, providing sub-millisecond response times.\nRedis supports snapshotting (RDB) for point-in-time recovery and replication (master-slave or cluster mode)\nfor high availability and read scaling. Aurora Replicas (Option A) are beneficial but primarily improve read\nscalability within the database layer, not providing the ultra-fast caching ElastiCache offers. DynamoDB\n(Option B) with global tables is a NoSQL solution. Migrating the entire RDS database to DynamoDB for\nmetadata, while viable, would be a significant architectural change with associated complexities and may not\nfully leverage the relational nature of the metadata. ElastiCache for Memcached (Option D) is also a caching\nsolution, but it does not offer native snapshotting. Redis is therefore the most suitable choice considering all\nthe requirements (snapshots, replication, and sub-millisecond response times). Adding a cache layer\n(ElastiCache) is a common and efficient strategy for handling read-heavy workloads.\nAuthoritative Links:\nAmazon ElastiCache: https://aws.amazon.com/elasticache/\nElastiCache for Redis: https://aws.amazon.com/elasticache/redis/",
    "links": [
      "https://aws.amazon.com/elasticache/",
      "https://aws.amazon.com/elasticache/redis/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses AWS Organizations for its multi-account AWS setup. The security organizational unit (OU) of the\ncompany needs to share approved Amazon Machine Images (AMIs) with the development OU. The AMIs are\ncreated by using AWS Key Management Service (AWS KMS) encrypted snapshots.\nWhich solution will meet these requirements? (Choose two.)",
    "options": {},
    "answer": "A",
    "explanation": "Here's a breakdown of why options A and C are the correct choices for sharing KMS-encrypted AMI snapshots\nacross OUs in AWS Organizations:\nOption A: Add the development team's OU Amazon Resource Name (ARN) to the launch permission list for\nthe AMIs. This allows members within the Development OU to launch instances using those specific AMIs.\nAMIs have launch permissions which control who can use them to create instances. By adding the\nDevelopment OU ARN, you're granting launch access to anyone in that OU without needing to individually\nmanage permissions for each account within. This is a key aspect of leveraging AWS Organizations for\ncentralized management.\nOption C: Update the key policy to allow the development team's OU to use the AWS KMS keys that are\nused to decrypt the snapshots. Since the AMI's snapshots are KMS-encrypted, the Development OU needs\npermissions to use those keys. KMS key policies control who can use a KMS key. Without the necessary KMS\npermissions, the Development OU won't be able to decrypt the snapshots and, consequently, won't be able to\nlaunch instances from the AMIs. Adding the Development OU ARN to the KMS key policy grants the OU the\nability to decrypt the snapshots.\nWhy the other options are incorrect:\nOption B: Add the Organizations root Amazon Resource Name (ARN) to the launch permission list for the\nAMIs. While this would technically work, it grants access to every account within the entire AWS\nOrganizations hierarchy, which is a violation of the principle of least privilege. It's overly broad and not secure.\nOption D: Add the development teams account Amazon Resource Name (ARN) to the launch permission list\nfor the AMIs. This is not scalable. OUs can contain multiple accounts. Adding individual account ARNs means\nyou need to update the AMI permissions every time an account is added/removed from the Development OU,\nincreasing administrative overhead.\nOption E: Recreate the AWS KMS key. Add a key policy to allow the Organizations root Amazon Resource\nName (ARN) to use the AWS KMS key. This is unnecessary and disruptive. Recreating a KMS key requires re-\nencrypting all data protected by the original key. Also, like Option B, using the Organizations root ARN grants\nexcessively broad access to the KMS key.\nSupporting Concepts:\nAWS Organizations: Enables you to centrally manage and govern multiple AWS accounts.\nhttps://aws.amazon.com/organizations/\nAmazon Machine Images (AMIs): Provide the information required to launch an instance, which is a virtual\nserver in the cloud. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\nAWS Key Management Service (AWS KMS): Enables you to create and manage cryptographic keys and\ncontrol their use across a wide range of AWS services. https://aws.amazon.com/kms/\nKMS Key Policies: Control who can use a KMS key and the actions they can perform.\nhttps://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html\nPrinciple of Least Privilege: Granting only the minimum necessary permissions to perform a task.\nBy combining launch permissions on the AMI itself (Option A) with KMS key access permissions (Option C), the\nDevelopment OU can launch instances from the secure, KMS-encrypted AMIs while adhering to the security\nprinciple of least privilege.",
    "links": [
      "https://aws.amazon.com/organizations/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html",
      "https://aws.amazon.com/kms/",
      "https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html"
    ]
  },
  {
    "question": "CertyIQ\nA data analytics company has 80 offices that are distributed globally. Each office hosts 1 PB of data and has\nbetween 1 and 2 Gbps of internet bandwidth.\nThe company needs to perform a one-time migration of a large amount of data from its offices to Amazon S3. The\ncompany must complete the migration within 4 weeks.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "B": "Snowball Edge Storage Optimized: Snowball Edge Storage Optimized devices are specifically designed for"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the most cost-effective solution for migrating the data to\nAmazon S3 within the given constraints:\nData Volume: The company has a substantial amount of data to migrate (80 offices * 1 PB/office = 80 PB). This\nvolume necessitates a solution designed for large-scale data transfers.\nTime Constraint: The 4-week deadline is a critical factor. The chosen method must be able to move a\nsignificant amount of data within this timeframe.\nBandwidth Limitation: While each office has 1-2 Gbps internet, it's still a limiting factor for transferring 1PB\nper office over the internet within the 4-week deadline. Relying solely on the internet may be too slow.\nCost-Effectiveness: This is the primary concern. Solutions like AWS Direct Connect involve significant\nupfront and recurring costs. Storage Gateway relies on existing internet bandwidth and incurs data transfer\ncharges. AWS Snowmobile, designed for exabyte-scale migrations, may be overkill (and more expensive) for\n80 PB.\nSnowball Edge Storage Optimized: Snowball Edge Storage Optimized devices are specifically designed for\nlarge-scale data transfers where network bandwidth is limited. They offer substantial storage capacity\n(around 80 TB per device, but can vary based on the AWS region), allowing for physical data shipping to AWS.\nEstimating number of devices: With 80PB, and approximately 80TB (0.08PB) per device, you'd need around\n1000 Snowball Edge devices.\nSnowball Edge devices support local processing, but for this use case the value is in the transfer.\nParallel Transfers: Multiple devices can operate in parallel, speeding up the overall migration process.\nAWS handles the security and integrity of the data during transport.\nWhy other options are less suitable:\nDirect Connect: While providing dedicated bandwidth, establishing Direct Connect at 80 global offices would\nbe prohibitively expensive and time-consuming. It would be more beneficial for ongoing operations.\nSnowmobile: Intended for exabyte scale migrations, meaning Snowmobile has a higher cost, and it's likely an\noverkill for 80 PB of data.\nStorage Gateway: Suitable for hybrid cloud scenarios and continuous data replication, not ideal for a one-\ntime, large-scale migration when bandwidth is constrained. The transfer would take very long over a 1-2 Gbps\nlink.\nConclusion: AWS Snowball Edge offers the best balance of speed, cost-effectiveness, and suitability for the\nnetwork constraints. It avoids the high costs of Direct Connect and the potential overkill of Snowmobile, while\novercoming the bandwidth limitations hindering Storage Gateway.\nAuthoritative Links:\nAWS Snowball Edge: https://aws.amazon.com/snowball/\nAWS Direct Connect: https://aws.amazon.com/directconnect/\nAWS Storage Gateway: https://aws.amazon.com/storagegateway/\nAWS Snowmobile: https://aws.amazon.com/snowmobile/",
    "links": [
      "https://aws.amazon.com/snowball/",
      "https://aws.amazon.com/directconnect/",
      "https://aws.amazon.com/storagegateway/",
      "https://aws.amazon.com/snowmobile/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an Amazon Elastic File System (Amazon EFS) file system that contains a reference dataset. The\ncompany has applications on Amazon EC2 instances that need to read the dataset. However, the applications must\nnot be able to change the dataset. The company wants to use IAM access control to prevent the applications from\nbeing able to modify or delete the dataset.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Mount the EFS file system in read-only mode from within the EC2 instances: This is insufficient because",
      "C": "Create an identity policy for the EFS file system that denies the elasticfilesystem:ClientWrite action on",
      "D": "Create an EFS access point for each application. Use Portable Operating System Interface (POSIX) file"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the correct solution for providing read-only access to an\nAmazon EFS file system for EC2 instances, while preventing modifications or deletion via IAM access control:\nThe core requirement is to ensure that EC2 instances can read the EFS dataset but cannot modify it. This\nnecessitates a mechanism to restrict write access at the EFS level. IAM policies offer granular control over\naccess to AWS resources. Specifically, resource-based policies attach directly to the EFS file system and\ndefine permissions for who can access the resource and what actions they can perform.\nOption B leverages a resource policy attached to the EFS file system. By explicitly denying the\nelasticfilesystem:ClientWrite action (which encompasses all actions that would modify the data) to the IAM roles\nassociated with the EC2 instances, the company effectively prevents those instances from writing to or\ndeleting anything on the EFS volume. This directly addresses the security requirement of ensuring that the\ndataset remains unchanged.\nHere's why the other options are less suitable:\nA. Mount the EFS file system in read-only mode from within the EC2 instances: This is insufficient because\nmounting in read-only mode on the client (EC2 instance) side doesn't prevent a malicious actor who has\ngained access to the EC2 instance with the correct permissions from remounting it with write access or\nmodifying the mount configuration. This approach relies on the EC2 instance configuration which could be\nchanged. IAM policies provide centralized, authoritative enforcement, making it more secure.\nC. Create an identity policy for the EFS file system that denies the elasticfilesystem:ClientWrite action on\nthe EFS file system: While technically possible, identity policies are attached to IAM roles or users, not\ndirectly to the EFS resource. This means you would have to apply this policy to every IAM entity that should\nnot have write access. It's much easier and less error-prone to control access from the EFS resource itself\nusing resource-based policies. Resource based policies give you a centralized location to manage these\ncontrols.\nD. Create an EFS access point for each application. Use Portable Operating System Interface (POSIX) file\npermissions to allow read-only access to files in the root directory: EFS access points primarily focus on\ncontrolling the user identity and file system path for access. While POSIX permissions can restrict access to\nspecific files and directories within the EFS file system, they don't fundamentally prevent an authorized user\n(authorized via IAM) from modifying the permissions themselves or other files in the file system. This adds\ncomplexity without a direct security benefit in this scenario. Access points are better suited for situations\nwhere you need different applications to access the same data under different user contexts or paths. They\ndon't replace the need for IAM-based authorization. Access points on their own do not prevent against\nauthorized users from modifying or deleting objects in the FS.\nIn summary, option B directly implements the desired access control by denying write access to the EC2\ninstances via the resource policy attached to the EFS. This method enforces centralized, declarative access\ncontrol, which is more robust and maintainable.\nAuthoritative Links:\nAmazon EFS Identity and Access Management: https://docs.aws.amazon.com/efs/latest/ug/iam.html\nUsing IAM to Control EFS Access: https://docs.aws.amazon.com/efs/latest/ug/using-iam-efs.html\nResource-Based Policies: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-\nresource.html",
    "links": [
      "https://docs.aws.amazon.com/efs/latest/ug/iam.html",
      "https://docs.aws.amazon.com/efs/latest/ug/using-iam-efs.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-"
    ]
  },
  {
    "question": "CertyIQ\nA company has hired an external vendor to perform work in the companys AWS account. The vendor uses an\nautomated tool that is hosted in an AWS account that the vendor owns. The vendor does not have IAM access to\nthe companys AWS account. The company needs to grant the vendor access to the companys AWS account.\nWhich solution will meet these requirements MOST securely?",
    "options": {},
    "answer": "A",
    "explanation": "The most secure solution is to use IAM roles for cross-account access. Option A proposes creating an IAM role\nin the company's AWS account that the vendor's IAM role can assume. This is the best approach because it\navoids sharing long-term credentials like passwords or access keys.\nIAM roles enable you to delegate access to users or services that don't normally have access to your AWS\nresources. In this case, the vendor's tool, authenticated by its own IAM role in the vendor's account, assumes\nthe role in the company's account. This temporary access is governed by the policies attached to the assumed\nrole.\nOption B is insecure because creating an IAM user with a password shares long-term credentials, violating\nsecurity best practices. Option C is also insecure because adding the vendor's IAM user directly to a group\nrequires managing individual user permissions in the company's account. Option D's permission boundary\napproach is also suboptimal as it involves creating an IAM user and managing permissions using a boundary\nwhen role assumption is the intended workflow.\nCross-account IAM roles using the principle of least privilege are the gold standard for this scenario. The\npolicies attached to the company's role precisely define what the vendor's tool can do. By using a trust policy\non the role, you specify which AWS accounts (in this case, the vendors) are allowed to assume it. This ensures\nthat only authorized accounts and users can access\nresources.https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-\nroles.htmlhttps://aws.amazon.com/blogs/security/how-to-delegate-access-across-aws-accounts-using-iam-\nroles/",
    "links": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-",
      "https://aws.amazon.com/blogs/security/how-to-delegate-access-across-aws-accounts-using-iam-"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to run its experimental workloads in the AWS Cloud. The company has a budget for cloud\nspending. The company's CFO is concerned about cloud spending accountability for each department. The CFO\nwants to receive notification when the spending threshold reaches 60% of the budget.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "A",
    "explanation": "Option A is the correct solution because it leverages AWS Budgets, which is specifically designed for setting\nspending thresholds and receiving alerts when those thresholds are approached or exceeded.\nHere's why:\n1. Cost Allocation Tags: Cost allocation tags (See\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html) enable you to\ncategorize and track your AWS resource costs. By tagging resources with department-specific tags,\nyou can attribute spending to the correct department, which addresses the CFO's concern for\naccountability.\n2. AWS Budgets: AWS Budgets (See https://aws.amazon.com/aws/tools/aws-budgets/) allows you to\nset custom budgets and track your AWS usage and costs against those budgets. You can define\nbudget periods (e.g., monthly, quarterly) and set thresholds for alerts.\n3. Alert Thresholds: The key requirement is to receive a notification when spending reaches 60% of the\nbudget. AWS Budgets lets you define custom alert thresholds (e.g., 60%, 80%, 100%) and configure\nemail notifications when these thresholds are breached. This directly satisfies the CFO's notification\nrequirement.\nWhy the other options are incorrect:\nOption B: While AWS Cost Explorer forecasts can help with anticipating future costs, it does not\nautomatically assign resource ownership. AWS Cost Anomaly Detection is designed to identify unusual\nspending patterns, but is less effective than the straight forward AWS budget approach for simple budget\nmonitoring.\nOption C: While cost allocation tags are useful, using the AWS Support API on AWS Trusted Advisor for\nbudget alerting is not the intended function of these services. AWS Trusted Advisor mainly focuses on best\npractices, cost optimization, security, and performance, not detailed budget threshold notifications.\nOption D: Similar to option B, AWS Cost Explorer forecasts cannot determine resource owners, and using it to\nset alert thresholds is not it's intended function, use AWS Budgets instead.",
    "links": [
      "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html)",
      "https://aws.amazon.com/aws/tools/aws-budgets/)"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to deploy an internal web application on AWS. The web application must be accessible only from\nthe company's office. The company needs to download security patches for the web application from the internet.\nThe company has created a VPC and has configured an AWS Site-to-Site VPN connection to the company's office.\nA solutions architect must design a secure architecture for the web application.\nWhich solution will meet these requirements?",
    "options": {
      "C": "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html"
    },
    "answer": "B",
    "explanation": "The correct solution is B because it aligns with the requirement of internal accessibility and outbound internet\naccess for patching.\nHere's why:\nPrivate Subnets for Security: Placing the EC2 instances hosting the web application in private subnets\nisolates them from direct internet exposure, fulfilling the requirement that the application be accessible only\nfrom the company's office. https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html\nInternal ALB for Load Balancing: An internal Application Load Balancer (ALB) ensures traffic is distributed\nefficiently across the EC2 instances within the private subnets. It is accessible only within the VPC.\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\nNAT Gateway for Outbound Internet Access: NAT gateways in public subnets allow the EC2 instances in the\nprivate subnets to initiate outbound connections to the internet (for downloading security patches) without\nexposing them to inbound traffic from the internet. https://docs.aws.amazon.com/vpc/latest/userguide/vpc-\nnat-gateway.html\nInternet Gateway for NAT Gateway: The internet gateway is necessary for the NAT gateway to function. The\nNAT gateway resides in the public subnet and uses the internet gateway to access the internet.\nSecurity Group Inbound Rule: Restricting the ALB's security group inbound traffic to the company's office\nnetwork CIDR block, via the Site-to-Site VPN connection, ensures only authorized traffic from the company's\nnetwork can access the web application. Security groups act as virtual firewalls, controlling traffic at the\ninstance level. https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\nOption A is incorrect because deploying instances in public subnets and setting the ALB's inbound source to\n0.0.0.0/0 makes the application accessible to the entire internet, violating the security requirement.\nOption C is wrong because the placement of NAT gateways and the intended direction of outbound security\ngroup rules are incorrect. NAT Gateways need to reside in public subnets to forward requests to the internet,\nand security group rules filter inbound traffic, not outbound traffic.\nOption D is incorrect because it uses a public ALB with EC2 instances in private subnets. While the EC2\ninstances are secured, the ALB would be exposed to the public internet, contrary to the requirement.\nFurthermore, outbound destination rules are uncommon in security groups.",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-",
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html"
    ]
  },
  {
    "question": "CertyIQ\nA company maintains its accounting records in a custom application that runs on Amazon EC2 instances. The\ncompany needs to migrate the data to an AWS managed service for development and maintenance of the\napplication data. The solution must require minimal operational support and provide immutable, cryptographically\nverifiable logs of data changes.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "B": "In conclusion, QLDB is the best option because it inherently provides the required immutability, cryptographic"
    },
    "answer": "D",
    "explanation": "The most cost-effective solution for migrating accounting records to an AWS managed service that requires\nminimal operational support and provides immutable, cryptographically verifiable logs is Amazon Quantum\nLedger Database (QLDB).\nHere's why:\nImmutability and Cryptographic Verification: QLDB is designed specifically for systems of record that\nrequire a high degree of trust and transparency. It provides an immutable transaction log that is\ncryptographically verifiable. Each transaction is chained together using a cryptographic hash, ensuring that\nany tampering is detectable. https://aws.amazon.com/qldb/\nMinimal Operational Support: As a fully managed database, QLDB handles patching, backups, and other\noperational tasks, reducing the operational burden on the company.\nCost-Effectiveness: For accounting records, QLDB offers a pay-per-use pricing model based on storage, I/O,\nand journal storage, which can be more cost-effective than other options for this specific use case.\nNow let's analyze why the other options are less suitable:\nAmazon Redshift: While Redshift is a powerful data warehouse, it's not designed for maintaining immutable\nledgers or providing cryptographic verification of data changes. It's more suitable for analytical workloads,\nwhich is not the primary need here. Also, managing a Redshift cluster requires more operational overhead\nthan QLDB.\nAmazon Neptune: Neptune is a graph database service and is designed for storing and querying relationships\nbetween data. It does not inherently provide immutability or cryptographic verification of changes, nor is it the\nbest choice for structured accounting data.\nAmazon Timestream: Timestream is a time-series database service optimized for storing and querying time-\nseries data. While it can store data over time, it does not provide the immutability and cryptographic\nverification features required for accounting records that are natively present in QLDB.\nIn conclusion, QLDB is the best option because it inherently provides the required immutability, cryptographic\nverification, and minimal operational overhead at a potentially lower cost for this particular use case.",
    "links": [
      "https://aws.amazon.com/qldb/"
    ]
  },
  {
    "question": "CertyIQ\nA company's marketing data is uploaded from multiple sources to an Amazon S3 bucket. A series of data\npreparation jobs aggregate the data for reporting. The data preparation jobs need to run at regular intervals in\nparallel. A few jobs need to run in a specific order later.\nThe company wants to remove the operational overhead of job error handling, retry logic, and state management.\nWhich solution will meet these requirements?",
    "options": {
      "A": "AWS Lambda: While Lambda can process data upon S3 upload, managing complex dependencies,",
      "B": "Amazon Athena: Athena is a query service to analyze data in S3, not a data preparation tool. It focuses on",
      "D": "AWS Data Pipeline: Data Pipeline is an older service, largely superseded by more modern alternatives like"
    },
    "answer": "C",
    "explanation": "The correct answer is C: Use AWS Glue DataBrew to process the data. Use an AWS Step Functions state\nmachine to run the DataBrew data preparation jobs.\nHere's why:\nAWS Glue DataBrew excels at data preparation tasks like cleaning, normalizing, and enriching data without\nrequiring you to write code. It provides a visual interface for data transformation. This aligns with the\nrequirement of aggregating data for reporting.\nAWS Step Functions is a serverless orchestration service that lets you define workflows (state machines) to\ncoordinate multiple AWS services. It addresses the needs for running data preparation jobs at regular\nintervals, in parallel where needed, and in a specific order later. The visual workflow simplifies orchestration,\nerror handling, retry logic, and state management.\nStep Functions' built-in error handling and retry logic significantly reduce the operational overhead by\nautomating these critical aspects.\nStep Functions can orchestrate DataBrew jobs, ensuring they run in the specified order with the needed\nparallel executions at the desired intervals.\nLet's examine why the other options are less suitable:\nA. AWS Lambda: While Lambda can process data upon S3 upload, managing complex dependencies,\nparallelism, and a series of jobs in a specific order becomes unwieldy. Regularly scheduled intervals with\nother Lambda invocations would require custom scheduling and intricate error handling, which increases\noperational overhead. Lambda has execution time limits that might hinder lengthy data preparation jobs.\nB. Amazon Athena: Athena is a query service to analyze data in S3, not a data preparation tool. It focuses on\nquerying and analysis after the data is prepared. While EventBridge Scheduler can schedule Athena queries,\nAthena itself doesn't handle data preparation tasks or orchestration of multiple jobs.\nD. AWS Data Pipeline: Data Pipeline is an older service, largely superseded by more modern alternatives like\nGlue and Step Functions. While it can schedule and orchestrate data processing tasks, it's more complex to\nmanage, less flexible than Step Functions, and requires more manual configuration for error handling and\nstate management. Choosing a legacy service when better options exist is not ideal.\nTherefore, the combination of AWS Glue DataBrew for data preparation and AWS Step Functions for\norchestration delivers the best solution by simplifying the data preparation process and reducing operational\noverhead through serverless orchestration, built-in error handling, and retry logic.\nSupporting Documentation:\nAWS Glue DataBrew: https://aws.amazon.com/glue/databrew/\nAWS Step Functions: https://aws.amazon.com/step-functions/",
    "links": [
      "https://aws.amazon.com/glue/databrew/",
      "https://aws.amazon.com/step-functions/"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is designing a payment processing application that runs on AWS Lambda in private subnets\nacross multiple Availability Zones. The application uses multiple Lambda functions and processes millions of\ntransactions each day.\nThe architecture must ensure that the application does not process duplicate payments.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the best solution for ensuring no duplicate payment\nprocessing in the described architecture, along with supporting information:\nThe primary requirement is to prevent duplicate payment processing. In a distributed system like this,\ntransient errors or retries can inadvertently cause the same payment to be processed multiple times. The key\nto solving this problem is to enforce ordered processing and deduplication.\nOption C utilizes Amazon SQS FIFO (First-In, First-Out) queues. FIFO queues guarantee that messages are\nprocessed in the exact order they are sent and provide exactly-once processing semantics when combined\nwith deduplication. This is crucial for payment processing, where the order of transactions may be significant,\nand preventing duplicates is paramount. The first Lambda function retrieves payments and publishes them to\nthe FIFO queue. The second Lambda function polls the queue and processes the payments in the order they\nwere received. The FIFO queue's built-in deduplication feature prevents duplicate messages from being\nprocessed, even if they are sent multiple times.\nOption A is incorrect because using Amazon S3 for queuing introduces complexities and doesn't guarantee\norder or prevent duplicate processing without significant custom coding. S3 event notifications are not\ndesigned for reliable message queuing.\nOption B is incorrect because standard SQS queues do not guarantee message order or prevent duplicates.\nWhile you could implement deduplication in the second Lambda function, the FIFO queue approach in Option\nC is a cleaner, more reliable, and AWS-managed solution for preventing duplicates.\nOption D is incorrect because while DynamoDB streams can provide near real-time data propagation, they do\nnot inherently guarantee ordered delivery or deduplication. It also introduces an additional dependency on\nDynamoDB, which isn't necessary given the capabilities of SQS FIFO queues. Implementing deduplication with\nDynamoDB streams would require complex logic and potentially introduce latency. The scaling\ncharacteristics of SQS are also better suited for a high-volume, asynchronous payment processing system.\nIn summary, using an Amazon SQS FIFO queue with a Lambda function consumer provides a robust and\nscalable solution for ordered, exactly-once processing, which directly addresses the requirement of\npreventing duplicate payment processing in a multi-AZ, Lambda-based payment application. The other\noptions lack the built-in capabilities for ensuring both message order and deduplication.\nSupporting Documentation:\nAmazon SQS FIFO Queues:\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\nLambda Best Practices: https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html",
    "links": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs multiple workloads in its on-premises data center. The company's data center cannot scale fast\nenough to meet the company's expanding business needs. The company wants to collect usage and configuration\ndata about the on-premises servers and workloads to plan a migration to AWS.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the correct answer, and why the other options are incorrect:\nThe core requirement is to collect usage and configuration data from on-premises servers to facilitate\nmigration planning. This necessitates a discovery and assessment process.\nOption B: Setting the home AWS Region in AWS Migration Hub and utilizing AWS Application Discovery\nService (ADS) accurately addresses this need. Migration Hub acts as a central location to track migrations\nand ADS is specifically designed to gather information about on-premises servers, including their\nconfiguration, utilization, and dependencies. https://aws.amazon.com/application-discovery/ and\nhttps://aws.amazon.com/migration-hub/\nADS provides a comprehensive view of the on-premises environment by discovering servers, their operating\nsystems, applications, and network dependencies. This data helps organizations plan the migration by\nidentifying compatible AWS services and estimating the required resources. Setting the home AWS Region\nwithin Migration Hub allows for centralized tracking of migration progress.\nOption A: While AWS Systems Manager can collect data from servers, it typically requires the Systems\nManager agent to be installed on the servers. This might present challenges in an on-premises environment\nwhere automated agent deployment and management could be difficult. Furthermore, Systems Manager is\nnot primarily designed for in-depth migration discovery and assessment. While useful for managed\ninstances, its strength isn't the initial discovery of servers in an unknown environment.\nOption C & D: AWS Schema Conversion Tool (SCT) and AWS Database Migration Service (DMS) are focused\non database migrations, not general server discovery and assessment. SCT helps convert database schemas\nbetween different database engines. DMS is used to migrate databases. Trusted Advisor provides cost\noptimization, security, fault tolerance, and performance recommendations for already running AWS\nresources, not for on-premises environments during a migration assessment phase.\nhttps://aws.amazon.com/dms/ and https://aws.amazon.com/schema-conversion-tool/ and\nhttps://aws.amazon.com/premiumsupport/technology/trusted-advisor/\nIn summary, ADS provides the required discovery capabilities for understanding the on-premises environment\nprior to migration, and Migration Hub helps organize and track the overall migration process. Therefore,\noption B is the most appropriate solution for the scenario.",
    "links": [
      "https://aws.amazon.com/application-discovery/",
      "https://aws.amazon.com/migration-hub/",
      "https://aws.amazon.com/dms/",
      "https://aws.amazon.com/schema-conversion-tool/",
      "https://aws.amazon.com/premiumsupport/technology/trusted-advisor/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an organization in AWS Organizations that has all features enabled. The company requires that all\nAPI calls and logins in any existing or new AWS account must be audited. The company needs a managed solution\nto prevent additional work and to minimize costs. The company also needs to know when any AWS account is not\ncompliant with the AWS Foundational Security Best Practices (FSBP) standard.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "A",
    "explanation": "The best solution is to deploy AWS Control Tower in the organization's management account and enable AWS\nSecurity Hub and Account Factory. Control Tower simplifies multi-account management within AWS\nOrganizations by automating the setup of a well-architected, secure, and compliant landing zone. Deploying in\nthe management account leverages Control Tower's organizational-wide governance capabilities.\nEnabling AWS Security Hub provides a centralized view of security alerts and compliance status across all\nAWS accounts in the organization. It automatically assesses resource configurations against security\nstandards like the AWS Foundational Security Best Practices (FSBP) and generates findings. Security Hub\nalso aggregates findings from other AWS security services like GuardDuty and Inspector.\nAWS Control Tower Account Factory automates the provisioning of new accounts pre-configured with\nbaseline security and compliance settings. This ensures that all new accounts adhere to the organization's\nsecurity policies from the start. By enabling Security Hub at the organization level, all API calls and logins are\nautomatically audited, and compliance with FSBP is continuously monitored, minimizing operational overhead.\nOption B is incorrect because deploying Control Tower in a member account limits its scope and requires\nadditional configuration to extend governance across the entire organization. Options C and D using AWS\nManaged Services (AMS) Accelerate involve more complexity and operational overhead compared to Control\nTower. AMS requires submitting RFCs for provisioning services and involves a more hands-on approach,\nmaking it less managed than Control Tower for the stated requirements. Furthermore, focusing solely on\nGuardDuty (option C) doesn't address the FSBP compliance requirement directly as Security Hub does.\nHere are some authoritative links for further research:\nAWS Control Tower: https://aws.amazon.com/controltower/\nAWS Security Hub: https://aws.amazon.com/security-hub/\nAWS Organizations: https://aws.amazon.com/organizations/\nAWS Foundational Security Best Practices (FSBP):\nhttps://docs.aws.amazon.com/securityhub/latest/userguide/fsbp-standards-cis.html",
    "links": [
      "https://aws.amazon.com/controltower/",
      "https://aws.amazon.com/security-hub/",
      "https://aws.amazon.com/organizations/",
      "https://docs.aws.amazon.com/securityhub/latest/userguide/fsbp-standards-cis.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has stored 10 TB of log files in Apache Parquet format in an Amazon S3 bucket. The company\noccasionally needs to use SQL to analyze the log files.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "C",
    "explanation": "The most cost-effective solution for analyzing Parquet-formatted log files in S3 with occasional SQL queries\nis using AWS Glue and Amazon Athena.\nExplanation:\nCost Considerations: Athena and Glue follow a pay-per-query model. This is significantly more cost-effective\nfor infrequent analysis compared to running and maintaining a persistent cluster like Redshift or EMR. Aurora\nwould incur costs for storage, compute, and data migration, making it less suitable for occasional queries\nagainst log files.\nFunctionality: Athena directly queries data in S3 using standard SQL. It requires metadata about the data's\nstructure. AWS Glue crawler automatically discovers the schema of the Parquet files and creates a metadata\ncatalog that Athena can use.\nAlternatives Breakdown:\nAurora MySQL with DMS: Overkill and expensive. Requires database management, data loading, and ongoing\nAurora costs, even when not actively querying.\nRedshift Spectrum: While Redshift Spectrum allows querying S3 data, Redshift itself is a data warehouse\ndesigned for more intensive analytical workloads and would be more expensive for occasional use.\nEMR with Spark SQL: EMR involves spinning up and managing a cluster, which is not cost-effective for\ninfrequent queries. It also adds operational overhead. Spark SQL is suitable for more complex data processing\nand transformations. Athena offers a simpler SQL interface directly on S3.\nWorkflow:\n1. AWS Glue Crawler: Configured to crawl the S3 bucket containing the Parquet log files.\n2. AWS Glue Data Catalog: The crawler creates a table definition in the AWS Glue Data Catalog,\ndefining the schema of the Parquet data.\n3. Amazon Athena: Users can then use the Athena console or API to run SQL queries against the table\ndefined in the Glue Data Catalog. Athena uses the table metadata to understand the structure of the\nParquet data in S3 and execute the queries.\nAuthoritative Links:\nAmazon Athena: https://aws.amazon.com/athena/\nAWS Glue: https://aws.amazon.com/glue/\nRedshift Spectrum: https://aws.amazon.com/redshift/spectrum/",
    "links": [
      "https://aws.amazon.com/athena/",
      "https://aws.amazon.com/glue/",
      "https://aws.amazon.com/redshift/spectrum/"
    ]
  },
  {
    "question": "CertyIQ\nA company needs a solution to prevent AWS CloudFormation stacks from deploying AWS Identity and Access\nManagement (IAM) resources that include an inline policy or * in the statement. The solution must also prohibit\ndeployment of Amazon EC2 instances with public IP addresses. The company has AWS Control Tower enabled in\nits organization in AWS Organizations.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the correct answer, along with explanations of why the other\noptions are incorrect, and supporting documentation:\nThe requirement is to prevent the deployment of specific non-compliant resources within an AWS\nOrganization governed by AWS Control Tower. The solution needs to block the deployment of EC2 instances\nwith public IPs and IAM resources with overly permissive inline policies.\nOption D: Use a service control policy (SCP) to block actions for the EC2 instances and IAM resources if the\nactions lead to noncompliance. This is the most suitable solution. SCPs are designed for preventative\ngovernance within AWS Organizations. They allow you to define guardrails at the organizational unit (OU) or\naccount level, preventing actions that violate the policy. In this scenario, an SCP can be written to deny the\ncreation of EC2 instances with public IP addresses and deny the creation of IAM roles or users that include\ninline policies containing \"*\". SCPs evaluate before the request is made, preventing the action from ever\noccurring. This fulfills the requirement of preventing the deployment of these resources.\nOption A: Use AWS Control Tower proactive controls to block deployment of EC2 instances with public IP\naddresses and inline policies with elevated access or *. While AWS Control Tower proactively guides\nresource provisioning, it primarily relies on enforcing guardrails using SCPs and AWS Config rules behind the\nscenes. Proactive controls, particularly with AWS Control Tower, often initiate guardrails based on best\npractices to prevent deployment of non-compliant resources. Although, these controls also end up being an\nSCP under the hood, so the answer to this question is D because it is the actual service being utilized.\nOption B: Use AWS Control Tower detective controls to block deployment of EC2 instances with public IP\naddresses and inline policies with elevated access or *. Detective controls, like AWS Config rules, identify\nand report non-compliant resources after they have been deployed. They do not prevent the initial\ndeployment. The requirement is to prevent deployment, so this option fails. Detective controls provide\nauditing and reporting, not preventative measures.\nOption C: Use AWS Config to create rules for EC2 and IAM compliance. Configure the rules to run an AWS\nSystems Manager Session Manager automation to delete a resource when it is not compliant. AWS Config\ndetects non-compliant resources. While you can use remediation actions (like a Systems Manager automation\nto delete a resource), this reacts to the deployment after it happens. The requirement is to prevent\ndeployment in the first place. This option does not provide a preventative solution, and attempting to\nautomatically delete newly created non-compliant resources can lead to operational complexities and\npotential data loss.\nIn summary: SCPs are the only option that provides the preventative control required to block the deployment\nof non-compliant EC2 instances and IAM resources. AWS Config provides detective controls. AWS Control\nTower simplifies the management of governance at the OU level but typically relies on underlying services\nsuch as SCPs and Config.\nAuthoritative Links:\nAWS Organizations Service Control Policies (SCPs):\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html\nAWS Control Tower: https://aws.amazon.com/controltower/\nAWS Config: https://aws.amazon.com/config/",
    "links": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html",
      "https://aws.amazon.com/controltower/",
      "https://aws.amazon.com/config/"
    ]
  },
  {
    "question": "CertyIQ\nA company's web application that is hosted in the AWS Cloud recently increased in popularity. The web application\ncurrently exists on a single Amazon EC2 instance in a single public subnet. The web application has not been able\nto meet the demand of the increased web traffic.\nThe company needs a solution that will provide high availability and scalability to meet the increased user demand\nwithout rewriting the web application.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": {
      "B": "Configure Amazon EC2 Auto Scaling with multiple Availability Zones in private subnets: This is a crucial",
      "A": "Replace the EC2 instance with a larger compute optimized instance: While this may provide some",
      "C": "Configure a NAT gateway in a public subnet to handle web requests: A NAT Gateway is used to allow",
      "D": "Replace the EC2 instance with a larger memory optimized instance: Similar to option A, this is vertical"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Configure Amazon EC2 Auto Scaling with multiple Availability Zones in private\nsubnets and E. Configure an Application Load Balancer in a public subnet to distribute web traffic.\nJustification:\nThe problem requires a solution that provides high availability and scalability without rewriting the web\napplication. Let's analyze why each option is or isn't suitable:\nA. Replace the EC2 instance with a larger compute optimized instance: While this may provide some\ntemporary relief, it doesn't address high availability (single point of failure) or automatic scalability as demand\nfluctuates. It's a vertical scaling approach, which has limitations.\nB. Configure Amazon EC2 Auto Scaling with multiple Availability Zones in private subnets: This is a crucial\ncomponent of the solution. Auto Scaling groups automatically launch and terminate EC2 instances based on\ndemand, ensuring scalability. Deploying instances across multiple Availability Zones (AZs) provides high\navailability; if one AZ fails, the application continues to run in others. Placing the instances in private subnets\nis important for security, shielding them from direct internet access.\nC. Configure a NAT gateway in a public subnet to handle web requests: A NAT Gateway is used to allow\ninstances in private subnets to connect to the internet, but not to handle incoming web traffic. This option is\nrelated to outbound connectivity, not the inbound traffic the application needs to handle.\nD. Replace the EC2 instance with a larger memory optimized instance: Similar to option A, this is vertical\nscaling, which doesn't provide high availability or dynamic scalability. It might not even be the correct\noptimization if the application is CPU-bound rather than memory-bound.\nE. Configure an Application Load Balancer in a public subnet to distribute web traffic: This is the second\ncrucial component. An Application Load Balancer (ALB) distributes incoming web traffic across multiple EC2\ninstances within the Auto Scaling group. It provides a single point of entry for users and intelligently routes\nrequests to healthy instances. Placing the ALB in a public subnet makes it accessible to internet users. The\nALB will be able to reach the private instances through the VPC network.\nTherefore, the combination of Auto Scaling (B) and an Application Load Balancer (E) delivers the required high\navailability and scalability to meet increased user demand without rewriting the application. The ALB\ndistributes traffic, while Auto Scaling ensures enough instances are running across multiple Availability\nZones.\nAuthoritative Links:\nAmazon EC2 Auto Scaling: https://aws.amazon.com/autoscaling/\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
    "links": [
      "https://aws.amazon.com/autoscaling/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/"
    ]
  },
  {
    "question": "CertyIQ\nA company has AWS Lambda functions that use environment variables. The company does not want its developers\nto see environment variables in plaintext.\nWhich solution will meet these requirements?",
    "options": {
      "D": "Create an AWS Key Management Service (AWS KMS) key. Enable encryption"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Create an AWS Key Management Service (AWS KMS) key. Enable encryption\nhelpers on the Lambda functions to use the KMS key to store and encrypt the environment variables.\nHere's why:\nThe core requirement is to protect Lambda environment variables from being seen in plaintext by developers.\nAWS KMS provides a managed service for creating and controlling the encryption keys used to encrypt data.\nBy using a KMS key, the environment variables can be encrypted at rest, ensuring that developers only see\nthe encrypted values, not the actual secrets.\nAWS Lambda integrates with AWS KMS to natively support encryption of environment variables. Lambda\nprovides \"encryption helpers,\" which are libraries or utilities that simplify the process of encrypting and\ndecrypting environment variables using KMS. This allows you to encrypt sensitive information before storing it\nas environment variables. The Lambda function can then decrypt the variables at runtime.\nOption A is incorrect because deploying to EC2 instances doesn't solve the problem of securing environment\nvariables. You'd still need a mechanism to protect the secrets stored on the EC2 instances. It also moves away\nfrom serverless, which may be an undesired architectural change.\nOption B is incorrect because configuring SSL encryption on the Lambda functions relates to securing\ncommunication to the function, not encrypting the environment variables themselves. While CloudHSM could\nstore keys, integrating it directly for encrypting environment variables within Lambda isn't the standard or\nmost efficient approach compared to KMS. CloudHSM is more geared towards applications requiring FIPS\n140-2 Level 3 compliance and dedicated hardware security modules.\nOption C is incorrect because ACM certificates are primarily used for securing network traffic (TLS/SSL). They\nare not designed for encrypting data at rest, such as environment variables. ACM manages SSL/TLS\ncertificates, not general-purpose encryption keys.\nUsing KMS offers several advantages:\nCentralized key management: KMS simplifies the management and rotation of encryption keys.\nAccess control: KMS allows fine-grained control over who can use the key, ensuring only authorized users\nand services can access the encrypted environment variables.\nAuditing: KMS provides audit trails of key usage, allowing you to track who accessed the encrypted\nenvironment variables.\nIntegration with Lambda: Direct integration and readily available encryption helpers streamline the\nimplementation.\nHere are some relevant links for further research:\nAWS Lambda Environment Variables: https://docs.aws.amazon.com/lambda/latest/dg/configuration-\nenvvars.html\nAWS KMS: https://aws.amazon.com/kms/\nEncrypting Lambda Environment Variables: https://aws.amazon.com/blogs/security/encrypting-aws-lambda-\nenvironment-variables-using-aws-kms/",
    "links": [
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-",
      "https://aws.amazon.com/kms/",
      "https://aws.amazon.com/blogs/security/encrypting-aws-lambda-"
    ]
  },
  {
    "question": "CertyIQ\nAn analytics company uses Amazon VPC to run its multi-tier services. The company wants to use RESTful APIs to\noffer a web analytics service to millions of users. Users must be verified by using an authentication service to\naccess the APIs.\nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A because it offers the most operationally efficient solution for user authentication and\nAPI access.\nHere's a detailed justification:\nAmazon Cognito User Pools: Cognito User Pools are designed specifically for managing user directories and\nhandling authentication. They provide a managed and scalable solution for user registration, login, and\npassword management. They handle the complexities of user management, including security best practices,\nreducing operational overhead.\nhttps://aws.amazon.com/cognito/user-pools/\nAmazon API Gateway REST APIs: REST APIs offer more features and flexibility, suitable for complex API\nrequirements, including request validation, transformation, and caching, if required for the analytics service.\nAlthough HTTP APIs are cheaper and have lower latency, REST APIs provide more control for future\nextensions and complex data processing.\nCognito Authorizer: API Gateway seamlessly integrates with Cognito User Pools via Cognito authorizers. This\nallows API Gateway to validate the JSON Web Tokens (JWTs) issued by Cognito upon successful user\nauthentication. This removes the need for custom authentication logic within your backend services,\nsimplifying the architecture.\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html\nWhy other options are less efficient:\nB: While HTTP APIs are cost-effective, Cognito Identity Pools are used to grant users access to AWS\nresources directly (like S3) and are not the primary mechanism for API authentication like User Pools.\nC: Using a Lambda function for authentication requires managing custom authentication logic, which is less\nefficient than leveraging Cognito. This adds operational burden regarding security and maintenance.\nD: IAM users should not be used directly for end-user authentication. It's difficult to manage and scale IAM\nusers for millions of users, and exposes AWS account keys.\nTherefore, using Amazon Cognito User Pools for authentication and integrating with API Gateway REST APIs\nthrough a Cognito authorizer is the most operationally efficient and secure solution for offering a web\nanalytics service to millions of users. It leverages managed services to handle user authentication, offloading\nthis responsibility from the backend and reducing operational overhead.",
    "links": [
      "https://aws.amazon.com/cognito/user-pools/",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has a mobile app for customers. The apps data is sensitive and must be encrypted at rest. The\ncompany uses AWS Key Management Service (AWS KMS).\nThe company needs a solution that prevents the accidental deletion of KMS keys. The solution must use Amazon\nSimple Notification Service (Amazon SNS) to send an email notification to administrators when a user attempts to\ndelete a KMS key.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the best solution for preventing accidental KMS key deletion\nwith the least operational overhead, incorporating relevant cloud computing concepts and links to AWS\ndocumentation:\nOption C leverages the power of EventBridge to detect the DeleteKey operation. When EventBridge identifies\nthis event, it triggers an AWS Systems Manager Automation runbook. This runbook is designed to\nimmediately cancel the KMS key deletion process, acting as a preventative measure. Concurrently,\nEventBridge publishes a message to an SNS topic, which then notifies administrators via email about the\nattempted deletion. This provides immediate awareness and allows for further investigation if needed.\nThe reason this is the preferred solution lies in its minimal operational burden. EventBridge simplifies event-\ndriven architectures, allowing for streamlined event detection and routing. AWS Systems Manager\nAutomation offers pre-built runbooks or the ability to create custom ones, enabling quick implementation of\nautomated responses like stopping a deletion. The SNS integration provides a simple, scalable, and cost-\neffective way to deliver notifications.\nOption A, while similar, utilizes AWS Config. Config's primary role is for compliance and auditing, providing a\nhistorical view of resource configurations. While Config can enforce rules, its strength isn't immediate real-\ntime action. It's better suited for detecting configuration drift after it occurs. Trying to use Config to cancel a\ndeletion is less straightforward and potentially less responsive than a System Manager Automation runbook\ndirectly triggered by EventBridge.\nOption B introduces a Lambda function and a CloudWatch alarm. While effective, this approach adds\ncomplexity. You would need to write, deploy, and maintain the custom Lambda function, increasing\noperational overhead. The CloudWatch alarm also creates an additional dependency, as the Lambda would\nneed to be triggered by the Alarm to prevent the KMS deletion\nOption D uses CloudTrail logs and CloudWatch alarms. CloudTrail captures API activity, so this setup relies on\ndetecting the DeleteKey operation after it has been logged. This means there's a delay between the attempted\ndeletion and the notification, and the solution doesn't actively prevent the deletion from occurring. This makes\nit less effective than option C, which prevents the deletion directly.\nIn summary, Option C offers a robust and streamlined solution that effectively prevents KMS key deletion\nwith minimal operational overhead by using EventBridge to trigger an AWS Systems Manager Automation\nrunbook that cancels the deletion process and delivers immediate notifications via SNS. The focus on\nprevention rather than simple detection is the key differentiator.\nAuthoritative Links:\nAmazon EventBridge: https://aws.amazon.com/eventbridge/\nAWS Systems Manager Automation: https://aws.amazon.com/systems-manager/automation/\nAmazon SNS: https://aws.amazon.com/sns/\nAWS KMS: https://aws.amazon.com/kms/",
    "links": [
      "https://aws.amazon.com/eventbridge/",
      "https://aws.amazon.com/systems-manager/automation/",
      "https://aws.amazon.com/sns/",
      "https://aws.amazon.com/kms/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to analyze and generate reports to track the usage of its mobile app. The app is popular and has\na global user base. The company uses a custom report building program to analyze application usage.\nThe program generates multiple reports during the last week of each month. The program takes less than 10\nminutes to produce each report. The company rarely uses the program to generate reports outside of the last\nweek of each month The company wants to generate reports in the least amount of time when the reports are\nrequested.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B, running the program in AWS Lambda and triggering it with an Amazon EventBridge\nrule. Here's why:\nCost-Effectiveness: Lambda functions are cost-effective for infrequent and short-running tasks. You only pay\nfor the compute time consumed when the Lambda function is executing. Since the report generation program\nruns mostly during the last week of the month and each report takes less than 10 minutes to produce,\nLambda's pay-per-use model aligns perfectly with the usage pattern, minimizing costs.\nEvent-Driven Architecture: EventBridge is ideal for creating event-driven architectures. Using an EventBridge\nrule to trigger the Lambda function ensures that the report generation process is initiated only when a report\nis requested, eliminating the need for continuous resource provisioning and reducing costs.\nScalability and Management: Lambda automatically scales to handle concurrent requests. AWS takes care\nof the underlying infrastructure management, allowing the company to focus on the report generation logic\nrather than server maintenance.\nEC2 On-Demand (A): While On-Demand EC2 instances provide predictable performance, running them\ncontinuously for a week each month would incur significant costs, even if the program is not actively\ngenerating reports.\nECS (C): ECS offers more control over the underlying infrastructure, but it also involves more operational\noverhead. It might be suitable for more complex or long-running tasks, which is not needed in the use-case.\nEC2 Spot Instances (D): Spot instances offer cost savings, but they are subject to interruption if the spot\nprice exceeds the bid price. Running the instances continuously during the last week of the month is also\ninefficient.\nIn contrast, Lambda provides a serverless, cost-optimized, and scalable solution for this specific report\ngeneration use case. It offers the lowest total cost of ownership (TCO) due to its pay-per-use model, minimal\noperational overhead, and seamless integration with EventBridge for event-driven execution.\nSupporting Links:\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon EventBridge: https://aws.amazon.com/eventbridge/\nAWS Pricing: https://aws.amazon.com/pricing/",
    "links": [
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/eventbridge/",
      "https://aws.amazon.com/pricing/"
    ]
  },
  {
    "question": "CertyIQ\nA company is designing a tightly coupled high performance computing (HPC) environment in the AWS Cloud. The\ncompany needs to include features that will optimize the HPC environment for networking and storage.\nWhich combination of solutions will meet these requirements? (Choose two.)",
    "options": {
      "C": "Lustre is a parallel"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why options B and D are the correct solutions for optimizing a tightly\ncoupled HPC environment on AWS for networking and storage:\nOption B: Create an Amazon FSx for Lustre file system. Configure the file system with scratch storage.\nAmazon FSx for Lustre is specifically designed for high-performance workloads like HPC. Lustre is a parallel\ndistributed file system that provides sub-millisecond latencies and high throughput, essential for HPC\napplications that require rapid access to large datasets. Configuring it with scratch storage is appropriate\nbecause HPC environments often generate large amounts of temporary data during computations. Scratch\nstorage on FSx for Lustre is ideal for this type of transient data, offering performance without the cost of\nlong-term storage.\nOption D: Launch Amazon EC2 instances. Attach an Elastic Fabric Adapter (EFA) to the instances. Elastic\nFabric Adapter (EFA) is a network interface designed to accelerate HPC and machine learning applications. It\nenables EC2 instances to achieve lower latency and higher throughput network communication, which is\ncritical for tightly coupled HPC workloads. EFA utilizes a custom transport protocol that bypasses the\noperating system kernel to directly communicate with other EFAs, significantly reducing latency and\nimproving scalability for inter-node communication. EFA supports Scalable Reliable Datagram (SRD) protocol\nto improve performance of inter-instance communication.\nNow, let's address why the other options are incorrect:\nOption A: Create an accelerator in AWS Global Accelerator. Configure custom routing for the accelerator.\nAWS Global Accelerator is designed to improve the performance of applications for a global user base by\nrouting traffic to the optimal endpoint. While it improves latency and availability, it doesn't directly address\nthe specific networking requirements within a tightly coupled HPC cluster itself, which needs low-latency\ncommunication between compute nodes.\nOption C: Create an Amazon CloudFront distribution. Configure the viewer protocol policy to be HTTP and\nHTTPS. Amazon CloudFront is a content delivery network (CDN) service. It is used for caching and distributing\ncontent to users worldwide. While it's valuable for making data available to users after HPC processing, it\ndoesn't contribute to the performance of the HPC computations themselves.\nOption E: Create an AWS Elastic Beanstalk deployment to manage the environment. AWS Elastic Beanstalk\nis a platform-as-a-service (PaaS) designed for deploying and managing web applications. While it simplifies\ndeployment, it doesn't offer specific networking or storage optimizations tailored for HPC environments. It is\nnot designed to handle the specific needs of tightly coupled applications or data-intensive HPC workloads.\nAuthoritative Links for Further Research:\nAmazon FSx for Lustre: https://aws.amazon.com/fsx/lustre/\nElastic Fabric Adapter (EFA): https://aws.amazon.com/hpc/efa/\nAWS Global Accelerator: https://aws.amazon.com/global-accelerator/\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nAWS Elastic Beanstalk: https://aws.amazon.com/elasticbeanstalk/",
    "links": [
      "https://aws.amazon.com/fsx/lustre/",
      "https://aws.amazon.com/hpc/efa/",
      "https://aws.amazon.com/global-accelerator/",
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/elasticbeanstalk/"
    ]
  },
  {
    "question": "CertyIQ\nA company needs a solution to prevent photos with unwanted content from being uploaded to the company's web\napplication. The solution must not involve training a machine learning (ML) model.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B because it leverages Amazon Rekognition, a pre-trained AWS service, for image\nanalysis without requiring custom ML model training. This aligns directly with the requirement of avoiding ML\nmodel training. Here's a detailed breakdown:\nOption B proposes an AWS Lambda function that uses Amazon Rekognition to detect unwanted content in\nuploaded photos. Rekognition offers pre-built models for tasks like object and scene detection, facial\nanalysis, and content moderation. The Lambda function acts as an intermediary, receiving the image, sending\nit to Rekognition for analysis, and then acting upon the results (e.g., rejecting the upload if unwanted content\nis detected). Creating a Lambda function URL enables a simple and direct way for the web application to\ninvoke the function whenever a new photo is uploaded. This architecture promotes a serverless and event-\ndriven design.\nOption A is incorrect because Amazon SageMaker Autopilot specifically automates the process of building,\ntraining, and deploying machine learning models. This directly contradicts the requirement to avoid training an\nML model. While SageMaker is a powerful ML tool, it's not suitable when pre-trained services can suffice.\nOption C is incorrect because Amazon Comprehend is a natural language processing (NLP) service used for\nanalyzing text, not images. It can't detect unwanted content in photos. CloudFront functions are used for\nlightweight processing of HTTP requests at the edge, such as modifying headers or redirecting requests.\nOption D is incorrect because Amazon Rekognition Video is designed for analyzing video streams, not\nindividual images. While it could technically process a single-frame video, it's overkill and more expensive\nthan using Rekognition for image analysis.\nIn summary, option B is the most efficient and cost-effective solution because it uses a pre-trained AWS\nservice (Rekognition) to fulfill the requirement of preventing unwanted content in uploaded photos without\nrequiring any ML model training. It is also a more efficient implementation utilizing a Lambda function URL\nthat the web application can easily invoke, ensuring real-time analysis and filtering of images.\nFurther Reading:\nAmazon Rekognition: https://aws.amazon.com/rekognition/\nAWS Lambda: https://aws.amazon.com/lambda/\nLambda Function URLs: https://docs.aws.amazon.com/lambda/latest/dg/lambda-urls.html",
    "links": [
      "https://aws.amazon.com/rekognition/",
      "https://aws.amazon.com/lambda/",
      "https://docs.aws.amazon.com/lambda/latest/dg/lambda-urls.html"
    ]
  },
  {
    "question": "CertyIQ\nA company uses AWS to run its ecommerce platform. The platform is critical to the company's operations and has\na high volume of traffic and transactions. The company configures a multi-factor authentication (MFA) device to\nsecure its AWS account root user credentials. The company wants to ensure that it will not lose access to the root\nuser account if the MFA device is lost.\nWhich solution will meet these requirements?",
    "options": {
      "B": "Adding multiple MFA devices for the root user account provides redundancy and"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Adding multiple MFA devices for the root user account provides redundancy and\nensures continued access even if one MFA device is lost or unavailable. Here's why:\nThe AWS root user has complete, unrestricted access to all AWS resources and services in an account.\nSecuring it with MFA is a best practice. However, losing the MFA device presents a significant risk of being\nlocked out of the entire AWS account.\nOption B directly addresses this risk by implementing a contingency plan. By configuring multiple MFA\ndevices (such as a hardware token and a virtual MFA app on a different phone), the company creates a backup\naccess method. If the primary MFA device is lost, the company can still authenticate using the secondary\ndevice, maintaining uninterrupted access to the root user account and all its privileges. This provides\nresilience against the failure or loss of a single MFA device.\nOption A, while seemingly a good idea, doesn't completely address the root user problem. While a backup\nadministrator account could be useful, losing access to the root account is still a critical vulnerability. You\nwouldn't be able to perform all root-level tasks. Options C and D are both reactive and inadequate. Creating a\nnew administrator account (C) or attaching admin policies to another user (D) after losing access to the root\naccount is not a valid solution because you would need the existing root user to perform these actions. They\nalso involve a period of downtime, which is unacceptable for a critical ecommerce platform.\nBy using multiple MFA devices, the company adheres to security best practices while ensuring business\ncontinuity. This proactive approach avoids a single point of failure for root user access.\nSupporting Links:\nAWS Documentation on Multi-Factor Authentication (MFA):\nAWS Security Best Practices - IAM: Specifically, review the sections about securing the root user.",
    "links": []
  },
  {
    "question": "CertyIQ\nA social media company is creating a rewards program website for its users. The company gives users points when\nusers create and upload videos to the website. Users redeem their points for gifts or discounts from the company's\naffiliated partners. A unique ID identifies users. The partners refer to this ID to verify user eligibility for rewards.\nThe partners want to receive notification of user IDs through an HTTP endpoint when the company gives users\npoints. Hundreds of vendors are interested in becoming affiliated partners every day. The company wants to\ndesign an architecture that gives the website the ability to add partners rapidly in a scalable way.\nWhich solution will meet these requirements with the LEAST implementation effort?",
    "options": {
      "B": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Choose an"
    },
    "answer": "B",
    "explanation": "The best solution to meet the requirements of rapid partner onboarding, scalability, and minimal\nimplementation effort is B. Create an Amazon Simple Notification Service (Amazon SNS) topic. Choose an\nendpoint protocol. Subscribe the partners to the topic. Publish user IDs to the topic when the company\ngives users points.\nHere's why:\nScalability and Ease of Onboarding: Amazon SNS is designed for high-throughput, scalable message\ndelivery. New partners can simply subscribe to the topic without requiring code changes on the company's\nside. This makes onboarding hundreds of vendors daily manageable.\nLoose Coupling: SNS facilitates loose coupling between the rewards website and the partners. The website\nsimply publishes user IDs to the SNS topic, and SNS handles the delivery to subscribed partners. This\ndecoupling simplifies maintenance and allows partners to consume notifications independently.\nEndpoint Flexibility: SNS supports various endpoint protocols (HTTP, HTTPS, email, SMS, AWS Lambda,\nSQS) which provides partners with flexibility in how they receive notifications. The company does not have to\nimplement individual mechanisms for each partner's preferred method.\nMinimal Implementation Effort: Setting up an SNS topic and publishing messages to it requires minimal\ncode. The company simply needs to integrate the SNS API into their rewards system. The partners handle\ntheir subscription and consumption logic.\nAlternatives are Less Efficient:\nA (Timestream and Lambda): This approach requires the company to maintain a list of partners and iterate\nthrough it for each user ID, which adds complexity and overhead. Lambda has invocation limits and scaling\nconcerns if the list of partners becomes very large.\nC (Step Functions): Step Functions are designed for orchestrating complex workflows. Sending notifications\nto partners does not require a complex workflow. Implementing a task for each partner is not efficient and\nadds unnecessary complexity.\nD (Kinesis Data Streams): Kinesis Data Streams is designed for real-time data processing and analytics, not\nfor notification delivery. It requires implementing producer and consumer applications, adding significant\ncomplexity.\nAuthoritative Links:\nAmazon SNS: https://aws.amazon.com/sns/\nSNS FAQs: https://aws.amazon.com/sns/faqs/",
    "links": [
      "https://aws.amazon.com/sns/",
      "https://aws.amazon.com/sns/faqs/"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to extract the names of ingredients from recipe records that are stored as text files in an\nAmazon S3 bucket. A web application will use the ingredient names to query an Amazon DynamoDB table and\ndetermine a nutrition score.\nThe application can handle non-food records and errors. The company does not have any employees who have\nmachine learning knowledge to develop this solution.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "A": "Here's a detailed justification:"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's a detailed justification:\nOption A: This solution leverages Amazon S3 Event Notifications, AWS Lambda, and Amazon Comprehend. S3\nEvent Notifications trigger the Lambda function whenever a new recipe file is uploaded (PutObject). The\nLambda function then uses Amazon Comprehend, a natural language processing (NLP) service, to extract the\ningredient names from the text file. Amazon Comprehend is designed for text analysis, including entity\nrecognition, which makes it suitable for identifying ingredients. The extracted ingredient names are then\nstored in the DynamoDB table. This approach minimizes manual effort, cost-effectively automates the\nprocess, and requires no machine learning expertise from the company.\nWhy other options are incorrect:\nOption B (Amazon Forecast): Amazon Forecast is for time-series forecasting, not text analysis or ingredient\nextraction. It is completely unsuitable for the task and would incur unnecessary costs.\nOption C (Amazon Polly, Amazon SNS, Manual Calculation): This solution involves converting text to speech\nusing Amazon Polly, notifying employees via Amazon SNS, and manually calculating the nutrition score. This\nis labor-intensive, error-prone, and expensive due to the high human involvement. It completely defeats the\npurpose of automation.\nOption D (Amazon SageMaker): Amazon SageMaker is a powerful machine learning platform, but using it for\na simple ingredient extraction task is overkill. It requires machine learning expertise to train and deploy a\nmodel, which the company lacks. Furthermore, it would be significantly more expensive than using Amazon\nComprehend.\nCost-Effectiveness and Automation:\nOption A provides a balance between automation and cost. Amazon Comprehend offers pay-as-you-go\npricing, only charging for the amount of text processed. Lambda is also cost-effective, as it only charges for\nthe compute time used when the function is executed. This makes the solution scalable and cost-efficient for\nprocessing varying numbers of recipe records. EventBridge introduces overhead, for no increase in solution\naccuracy.\nRelevant Cloud Computing Concepts:\nEvent-driven architecture: S3 Event Notifications trigger the Lambda function, enabling an event-driven\narchitecture where services react to changes in the S3 bucket.\nServerless computing: Lambda allows the company to run code without provisioning or managing servers.\nManaged services: Amazon Comprehend and DynamoDB are managed services, meaning AWS handles the\nunderlying infrastructure and maintenance, reducing the operational overhead for the company.\nNatural Language Processing (NLP): Amazon Comprehend provides NLP capabilities to extract meaning from\ntext, which is crucial for identifying ingredients.\nAuthoritative Links:\nAmazon S3 Event Notifications:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon Comprehend: https://aws.amazon.com/comprehend/\nAmazon DynamoDB: https://aws.amazon.com/dynamodb/",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/comprehend/",
      "https://aws.amazon.com/dynamodb/"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to create an AWS Lambda function that will run in a VPC in the company's primary AWS account.\nThe Lambda function needs to access files that the company stores in an Amazon Elastic File System (Amazon\nEFS) file system. The EFS file system is located in a secondary AWS account. As the company adds files to the file\nsystem, the solution must scale to meet the demand.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "B": "Create a VPC peering connection between the VPCs that are in the primary account",
      "A": "Creating a new EFS file system and using AWS DataSync: This involves significant data transfer costs and",
      "C": "Creating a second Lambda function in the secondary account: This adds complexity to the architecture",
      "D": "Moving the contents of the file system to a Lambda layer: Lambda layers have size limitations. An entire"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Create a VPC peering connection between the VPCs that are in the primary account\nand the secondary account.\nHere's why:\nCost-Effectiveness: VPC peering is generally a cost-effective solution for connecting VPCs in different AWS\naccounts. It avoids data transfer charges that might be incurred with other methods (like copying data to a\nnew EFS).\nDirect Network Connectivity: VPC peering establishes a direct network connection between the two VPCs,\nenabling the Lambda function in the primary account to access the EFS file system in the secondary account\nas if they were in the same network.\nScalability: EFS is designed to scale automatically to meet demand. Connecting via VPC peering allows the\nLambda function to leverage the scalability of the existing EFS file system.\nSecurity: VPC peering allows you to control the traffic flow between the two VPCs using security group rules\nand network ACLs.\nWhy other options are not the most cost-effective or efficient:\nA. Creating a new EFS file system and using AWS DataSync: This involves significant data transfer costs and\nongoing synchronization overhead, making it less cost-effective. It also duplicates the data, increasing\nstorage costs.\nC. Creating a second Lambda function in the secondary account: This adds complexity to the architecture\nand introduces latency due to inter-function calls. It is not the most straightforward or efficient solution for\naccessing the file system.\nD. Moving the contents of the file system to a Lambda layer: Lambda layers have size limitations. An entire\nfile system is likely to exceed this limit. Furthermore, layers are read-only, preventing the Lambda function\nfrom modifying the files.\nSupporting Documentation:\nVPC Peering: https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\nEFS: https://aws.amazon.com/efs/\nLambda VPC: https://docs.aws.amazon.com/lambda/latest/dg/services-vpc.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html",
      "https://aws.amazon.com/efs/",
      "https://docs.aws.amazon.com/lambda/latest/dg/services-vpc.html"
    ]
  },
  {
    "question": "CertyIQ\nA financial company needs to handle highly sensitive data. The company will store the data in an Amazon S3\nbucket. The company needs to ensure that the data is encrypted in transit and at rest. The company must manage\nthe encryption keys outside the AWS Cloud.\nWhich solution will meet these requirements?",
    "options": {
      "D": "Further reading on client-side encryption with S3:"
    },
    "answer": "D",
    "explanation": "The correct solution is to encrypt the data at the company's data center before uploading it to the S3 bucket.\nThis approach meets the requirement of managing encryption keys outside the AWS Cloud.\nOptions A, B, and C all involve using AWS KMS for key management. While using KMS for encryption provides\nrobust security, it contradicts the explicit requirement of managing the encryption keys outside of AWS.\nThese options place the key management responsibility within the AWS ecosystem, violating the stated\nconstraint.\nOption D, encrypting the data at the company's data center before storing it in S3, aligns perfectly with the\nrequirement. By performing encryption on-premises, the company retains full control and management of the\nencryption keys. This is known as client-side encryption. The data is encrypted before it is transmitted to S3,\nsatisfying the encryption in transit requirement. Additionally, the data remains encrypted at rest in the S3\nbucket, fulfilling the at-rest encryption requirement. While S3 offers its own server-side encryption options,\nclient-side encryption provides the desired key management control. The encrypted data is stored in S3,\noffering durability and availability without AWS ever accessing the unencrypted data or the encryption keys.\nThis addresses the high sensitivity of the data.\nTherefore, the company can ensure data encryption both in transit and at rest while maintaining complete\ncontrol over the encryption keys outside the AWS environment using option D.\nFurther reading on client-side encryption with S3:\nAWS Documentation - Protecting Data Using Client-Side Encryption:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to run its payment application on AWS. The application receives payment notifications from\nmobile devices. Payment notifications require a basic validation before they are sent for further processing.\nThe backend processing application is long running and requires compute and memory to be adjusted. The\ncompany does not want to manage the infrastructure.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "D",
    "explanation": "The optimal solution leverages serverless components to minimize operational overhead. API Gateway\nprovides a managed API endpoint for receiving payment notifications. Integrating API Gateway with Lambda\nallows for serverless validation of these notifications, offloading infrastructure management. Lambda\nfunctions are ideal for short-lived tasks like validation. The validated notifications can then be sent to the\nbackend processing application.\nFor the backend, ECS with Fargate provides a fully managed container orchestration service. Fargate\nabstracts away the underlying infrastructure, eliminating the need to manage EC2 instances or Kubernetes\nnodes. ECS allows you to easily deploy, manage, and scale your containerized backend application without\noperational burdens. It's particularly suitable for long-running applications that require compute and memory\nadjustments, as ECS can handle scaling based on resource utilization.\nOption A introduces unnecessary complexity with EKS Anywhere and a standalone cluster, requiring\nsignificant infrastructure management. Option B, while using EKS, uses self-managed nodes, increasing\noperational overhead. Step Functions are better suited for orchestrating complex workflows, and are overkill\nfor basic validation. Option C utilizes EC2 Spot Instances, which can be interrupted and require handling\ninstance terminations, leading to increased operational overhead. Further, SQS and EventBridge are primarily\nfor asynchronous communication, while API Gateway enables synchronous request/response interaction,\nwhich fits the requirement of validating requests before proceeding.\nIn summary, API Gateway, Lambda, ECS with Fargate offer the most serverless and managed approach,\nminimizing operational overhead while meeting the requirements of validating notifications and running a\nscalable backend application.\nRelevant links:\nAPI Gateway: https://aws.amazon.com/api-gateway/\nLambda: https://aws.amazon.com/lambda/\nECS: https://aws.amazon.com/ecs/\nFargate: https://aws.amazon.com/fargate/",
    "links": [
      "https://aws.amazon.com/api-gateway/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/ecs/",
      "https://aws.amazon.com/fargate/"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is designing a user authentication solution for a company. The solution must invoke two-\nfactor authentication for users that log in from inconsistent geographical locations, IP addresses, or devices. The\nsolution must also be able to scale up to accommodate millions of users.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Configure Amazon Cognito user pools for user authentication. Enable the risk-"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Configure Amazon Cognito user pools for user authentication. Enable the risk-\nbased adaptive authentication feature with multifactor authentication (MFA).\nHere's a detailed justification:\nAmazon Cognito User Pools are a fully managed service specifically designed for user authentication. They\ncan easily scale to accommodate millions of users, fulfilling one of the core requirements. Cognito offers\nfeatures like self-service registration, sign-in, password recovery, and user profile management, reducing the\noperational overhead.\nCritically, Cognito user pools support risk-based adaptive authentication, a key component of the question's\nrequirements. This feature leverages machine learning to detect unusual sign-in activity based on factors like\nlocation, IP address, and device. When suspicious activity is detected, the system can automatically trigger\nmulti-factor authentication (MFA) to verify the user's identity. This ensures that MFA is only invoked when\nneeded, improving user experience while enhancing security.\nOption B is incorrect because Cognito Identity Pools (Federated Identities) provide temporary AWS\ncredentials to users who are already authenticated via an external identity provider (like Facebook, Google, or\nan existing corporate directory). They don't handle user authentication directly. While they support MFA, they\ndon't offer the risk-based adaptive authentication required.\nOption C is incorrect because IAM users are designed for AWS service management, not end-user\nauthentication for applications. Managing millions of IAM users for this purpose is impractical and doesn't\nprovide the necessary features for a scalable user authentication solution. While IAM supports MFA, it doesn't\nprovide adaptive authentication.\nOption D is incorrect because AWS IAM Identity Center (successor to AWS Single Sign-On) is designed for\nproviding single sign-on access to multiple AWS accounts and applications for users managed within IAM\nIdentity Center or through connected identity providers. While it supports MFA, it's not primarily designed for\nthe direct user authentication of millions of end-users like Cognito is, and doesn't offer risk-based adaptive\nauthentication natively. It is better suited for authenticating organization employees and granting them AWS\naccess.\nIn summary, only Amazon Cognito user pools with risk-based adaptive authentication provide the necessary\nscalability, user management features, and the ability to selectively invoke MFA based on user risk factors,\nmaking it the best solution for the stated requirements.\nReference links:\nAmazon Cognito: https://aws.amazon.com/cognito/\nAmazon Cognito User Pools: https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-\nidentity-pools.html\nRisk-based adaptive authentication in Cognito:\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-advanced-security.html",
    "links": [
      "https://aws.amazon.com/cognito/",
      "https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-",
      "https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-advanced-security.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has an Amazon S3 data lake. The company needs a solution that transforms the data from the data\nlake and loads the data into a data warehouse every day. The data warehouse must have massively parallel\nprocessing (MPP) capabilities.\nData analysts then need to create and train machine learning (ML) models by using SQL commands on the data.\nThe solution must use serverless AWS services wherever possible.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The correct solution is C because it optimally leverages serverless AWS services to meet the requirements.\nAWS Glue is a fully managed, serverless ETL (extract, transform, load) service, perfectly suited for\ntransforming data from the S3 data lake. It can handle various data formats and complexities, scaling\nautomatically as needed. Amazon Redshift Serverless provides a data warehouse with MPP capabilities\nwithout the need to provision and manage infrastructure. This aligns with the serverless requirement and\nenables efficient querying and analysis of large datasets. Amazon Redshift ML allows data analysts to create,\ntrain, and deploy machine learning models directly from the data warehouse using SQL, simplifying the ML\nworkflow and removing the need to move data to separate ML platforms.\nOption A is less suitable because while Amazon EMR can transform data, it's not inherently serverless.\nManaging an EMR cluster involves more operational overhead than AWS Glue. Option B uses Amazon Aurora\nServerless, which is a relational database, not a data warehouse. Aurora doesn't inherently have MPP\ncapabilities needed for large-scale data analysis. Option D uses Amazon Athena, which is a serverless query\nservice for S3. While serverless, Athena's ML capabilities aren't as integrated and performant as Redshift ML\nfor data warehouse-based ML model training. Also, Athena doesn't serve as a data warehouse in this scenario.\nTherefore, only option C fulfills all the requirements of a serverless architecture, data transformation, MPP\ndata warehouse, and SQL-based ML model creation.\nSupporting links:\nAWS Glue: https://aws.amazon.com/glue/\nAmazon Redshift Serverless: https://aws.amazon.com/redshift/serverless/\nAmazon Redshift ML: https://aws.amazon.com/redshift/features/ml/",
    "links": [
      "https://aws.amazon.com/glue/",
      "https://aws.amazon.com/redshift/serverless/",
      "https://aws.amazon.com/redshift/features/ml/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs containers in a Kubernetes environment in the company's local data center. The company wants to\nuse Amazon Elastic Kubernetes Service (Amazon EKS) and other AWS managed services. Data must remain locally\nin the company's data center and cannot be stored in any remote site or cloud to maintain compliance.\nWhich solution will meet these requirements?",
    "options": {
      "C": "Install an AWS Outposts rack in the company's data center.",
      "A": "Deploy AWS Local Zones in the company's data center: Local Zones are extensions of AWS Regions but",
      "B": "Use an AWS Snowmobile in the company's data center: AWS Snowmobile is a petabyte-scale data",
      "D": "Install an AWS Snowball Edge Storage Optimized node in the data center: Snowball Edge is an edge"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Install an AWS Outposts rack in the company's data center.\nJustification:\nThe company's primary requirement is to run containerized applications using Amazon EKS while keeping all\ndata within their on-premises data center due to compliance restrictions. AWS Outposts directly addresses\nthis need by extending AWS infrastructure and services to the company's physical location.\nAWS Outposts: Outposts provides a fully managed and configurable compute and storage racks built with\nAWS-designed hardware. They are specifically designed to run AWS services locally while connecting to AWS\ncloud for management and control plane operations. With Outposts, the company can run Amazon EKS\nclusters within their data center, ensuring that Kubernetes workloads and their associated data remain on-\npremises.\nLet's analyze why the other options are not the best fit:\nA. Deploy AWS Local Zones in the company's data center: Local Zones are extensions of AWS Regions but\nare geographically closer to users. However, Local Zones are AWS-owned and operated. You cannot deploy\nthem within a company's private data center. Local Zones do not allow customer-controlled data residency\nwithin the company's own infrastructure.\nB. Use an AWS Snowmobile in the company's data center: AWS Snowmobile is a petabyte-scale data\ntransfer service. It is used for migrating large datasets into or out of AWS, which doesn't fulfill the\nrequirement of running Kubernetes workloads permanently on-premises and does not help in using EKS\nlocally.\nD. Install an AWS Snowball Edge Storage Optimized node in the data center: Snowball Edge is an edge\ncomputing, data migration, and data transport device. While Snowball Edge offers compute capabilities, it is\nprimarily intended for temporary or intermittent workloads and data transfer. It's not a permanent solution for\nrunning EKS clusters with strict data residency requirements. Also, while it offers edge compute, it doesn't\nrun a fully fledged EKS service.\nIn summary, AWS Outposts is the only solution that allows the company to use Amazon EKS locally within\ntheir data center, keeping data on-premises and adhering to their compliance requirements, by essentially\nextending the AWS cloud to their physical infrastructure.\nSupporting Links:\nAWS Outposts: https://aws.amazon.com/outposts/\nAmazon EKS: https://aws.amazon.com/eks/",
    "links": [
      "https://aws.amazon.com/outposts/",
      "https://aws.amazon.com/eks/"
    ]
  },
  {
    "question": "CertyIQ\nA social media company has workloads that collect and process data. The workloads store the data in on-premises\nNFS storage. The data store cannot scale fast enough to meet the companys expanding business needs. The\ncompany wants to migrate the current data store to AWS.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the most cost-effective solution, along with supporting\nconcepts and links:\nThe core requirement is to migrate on-premises NFS data to AWS cost-effectively while addressing\nscalability issues. Amazon S3 is significantly more scalable and generally cheaper for large data storage than\nAmazon EFS. AWS Storage Gateway facilitates the transition from on-premises storage to AWS storage\nservices. There are different types of Storage Gateway that each serve different purposes.\nOption A (Volume Gateway): Volume Gateway provides block-based storage, mimicking a SAN in AWS. It isn't\noptimized for file-based data like the NFS system the company currently uses, making it unsuitable.\nOption B (S3 File Gateway): S3 File Gateway presents a file system interface that connects to S3. It is a good\nfit for migrating the data to S3 which is cost-effective, scalable object storage. The NFS data on-premises can\nbe easily copied to the file gateway and then to S3. S3 Lifecycle policies can then transition the data to even\ncheaper storage tiers such as S3 Glacier or S3 Intelligent-Tiering based on access patterns.\nOption C (EFS Standard-IA): While EFS is a fully managed NFS file system in the cloud, using EFS Standard-\nIA is generally more expensive than S3, especially for large amounts of data where infrequent access is\nexpected. EFS is also not ideal for large scale data storage and analysis workloads.\nOption D (EFS One Zone-IA): Similar to option C, EFS is more expensive and not a perfect fit. Additionally,\nEFS One Zone has less availability and redundancy than EFS Standard, which might not be ideal for the\ncompany's data.\nTherefore, option B provides the most cost-effective solution. S3 is a cheaper storage option than EFS, and\nthe S3 File Gateway enables seamless data migration from NFS. Lifecycle policies further optimize costs by\nmoving less frequently accessed data to lower-cost S3 storage tiers.\nAuthoritative Links:\nAWS Storage Gateway: https://aws.amazon.com/storagegateway/\nAmazon S3: https://aws.amazon.com/s3/\nAmazon S3 Lifecycle: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-\nexamples.html\nAmazon EFS: https://aws.amazon.com/efs/",
    "links": [
      "https://aws.amazon.com/storagegateway/",
      "https://aws.amazon.com/s3/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-",
      "https://aws.amazon.com/efs/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses high concurrency AWS Lambda functions to process a constantly increasing number of messages\nin a message queue during marketing events. The Lambda functions use CPU intensive code to process the\nmessages. The company wants to reduce the compute costs and to maintain service latency for its customers.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D: Configure provisioned concurrency for the Lambda functions and increase the\nmemory according to AWS Compute Optimizer recommendations. Here's why:\nProblem Analysis: The company faces high compute costs and latency issues due to CPU-intensive Lambda\nfunctions processing a constantly increasing message queue. Optimizing for cost and latency is the goal.\nProvisioned Concurrency: Provisioned concurrency pre-initializes Lambda function instances, drastically\nreducing cold starts and improving latency, which is critical for handling constantly increasing messages and\nmaintaining service levels during peak marketing events. Reserved concurrency, on the other hand, only limits\nthe number of concurrent executions, not eliminating cold starts, making it a less effective solution for\nlatency issues.\nMemory Allocation and CPU Performance: Increasing memory allocation for Lambda functions often\ntranslates to more CPU power. For CPU-intensive workloads, this can significantly improve performance and\nreduce execution time, thus lowering overall compute costs as the functions complete faster.\nAWS Compute Optimizer: This service analyzes your AWS resource utilization and provides recommendations\nfor optimal instance types and memory settings. Using its recommendations ensures the Lambda functions\nare adequately sized for the workload, maximizing performance and cost efficiency. Without this optimization,\nsimply increasing or decreasing memory could lead to either under-utilization (wasted resources) or\ninsufficient resources, negatively impacting performance.\nWhy other options are incorrect:\nA: Decreasing memory decreases CPU allocation, negatively impacting CPU-intensive tasks and potentially\nincreasing execution time and cost.\nB: While increasing memory is good, reserved concurrency does not mitigate cold starts like provisioned\nconcurrency does.\nC: Decreasing memory decreases CPU allocation, negatively impacting CPU-intensive tasks and potentially\nincreasing execution time and cost.\nIn summary, provisioned concurrency addresses the latency issue stemming from cold starts, while increasing\nmemory based on Compute Optimizer recommendations ensures adequate CPU power, improving\nperformance and reducing cost.\nAuthoritative Links:\nAWS Lambda Provisioned Concurrency: https://docs.aws.amazon.com/lambda/latest/dg/configuration-\nconcurrency.html\nAWS Lambda Function Configuration: https://docs.aws.amazon.com/lambda/latest/dg/configuration-\nfunction-common.html\nAWS Compute Optimizer: https://aws.amazon.com/compute-optimizer/",
    "links": [
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-",
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-",
      "https://aws.amazon.com/compute-optimizer/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs its workloads on Amazon Elastic Container Service (Amazon ECS). The container images that the\nECS task definition uses need to be scanned for Common Vulnerabilities and Exposures (CVEs). New container\nimages that are created also need to be scanned.\nWhich solution will meet these requirements with the FEWEST changes to the workloads?",
    "options": {},
    "answer": "A",
    "explanation": "The most efficient solution to scan container images for CVEs in an ECS environment with minimal workload\nchanges is to leverage Amazon ECR's built-in scanning capabilities.\nOption A proposes using Amazon ECR and configuring scan-on-push. ECR provides native container image\nstorage and integrated security scanning. Setting \"scan on push\" automatically triggers a scan when a new\nimage is pushed to the repository. The \"basic scan\" offers a cost-effective solution by providing vulnerability\ninformation based on publicly available databases. This requires minimal configuration changes to the\nexisting ECS setup, as it primarily involves altering the image repository location to ECR and configuring scan\nsettings. It integrates seamlessly with the existing container deployment pipeline.\nOption B is incorrect because Amazon Macie is primarily designed for discovering and protecting sensitive\ndata within S3 buckets, not for scanning container images for vulnerabilities. While technically feasible to\nstore images in S3 and trigger scans, it is not Macie's intended use case and would require substantial custom\nscripting to interpret the image data and perform vulnerability assessments.\nOption C is incorrect because migrating the workload to Amazon EKS introduces unnecessary complexity.\nWhile EKS is a valid container orchestration platform, switching from ECS solely for vulnerability scanning is\nan over-engineered solution. Furthermore, the \"enhanced scan\" of ECR requires AWS Inspector, which\nintroduces further complexity for configuration, compared to the basic scan that ECR natively supports. The\nquestion requires the FEWEST changes to the workloads.\nOption D is also incorrect because using S3, Lambda, and Amazon Inspector is significantly more complex\nthan using ECR's native scanning features. This approach necessitates writing and maintaining custom\nLambda code to interact with Inspector, managing S3 event triggers, and handling the scan results. This\nintroduces more overhead and potential points of failure compared to ECR's integrated solution.\nIn summary, Amazon ECR with scan on push filters offers the simplest, most integrated, and least disruptive\napproach for scanning container images for CVEs within an ECS environment, fulfilling the requirements of\nthe prompt. The other options involve more complex configurations and integrations.\nRelevant links for further research:\nAmazon ECR Image Scanning: https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-\nscanning.html\nAmazon ECS: https://aws.amazon.com/ecs/\nAmazon Macie: https://aws.amazon.com/macie/\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon Inspector: https://aws.amazon.com/inspector/",
    "links": [
      "https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-",
      "https://aws.amazon.com/ecs/",
      "https://aws.amazon.com/macie/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/inspector/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses an AWS Batch job to run its end-of-day sales process. The company needs a serverless solution\nthat will invoke a third-party reporting application when the AWS Batch job is successful. The reporting application\nhas an HTTP API interface that uses username and password authentication.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the correct solution and why the other options are less\nsuitable for the given scenario:\nJustification for Option B:\nOption B is the most suitable choice because it offers a serverless, event-driven approach to invoke the third-\nparty reporting application upon the successful completion of the AWS Batch job. Amazon EventBridge\nScheduler is designed to trigger actions based on schedules or events. In this case, it detects the\nSUCCEEDED event from the AWS Batch job. A Lambda function serves as the intermediary, triggered by\nEventBridge, handling the authentication and invocation of the third-party reporting application's HTTP API\nusing the provided username and password. This ensures that the credentials are secure and managed within\nthe Lambda function. This is a clean, decoupled architecture. AWS Lambda offers a serverless compute\nenvironment that scales automatically and eliminates the need for managing servers. The combination of\nEventBridge and Lambda allows for a flexible and cost-effective integration with the third-party application.\nThe Lambda function is triggered only when the AWS Batch job is successful, optimizing resource utilization\nand costs.\nWhy other options are incorrect:\nOption A: While EventBridge API destinations offer a way to invoke HTTP endpoints directly, using\nEventBridge Scheduler in this context is not accurate. EventBridge Scheduler is used for scheduling based on\na cron expression and is not triggered by events directly like normal eventBridge rules. This would require\nconfiguring a complex scheduled polling mechanism which is less efficient.\nOption C: While API Gateway can be used as an intermediary, having the Batch job directly publishing to an\nAPI Gateway is less ideal. The Batch job would need additional code to handle HTTP requests and responses,\nadding complexity to the batch process. Additionally, API Gateway's HTTP proxy integration generally doesn't\nhandle authentication as cleanly as the Lambda approach. The credentials would either need to be embedded\nin the API Gateway configuration or passed along with the Batch job. It is also not serverless in the truest\nsense since API gateway require some settings configurations and you have to configure the Batch Job to\npoint to it\nOption D: This option introduces unnecessary complexity. While it's functional, adding API Gateway as an\nintermediary between the Batch job and the Lambda function doesn't provide a significant advantage. It adds\noverhead and complexity without a clear benefit. Directly triggering the Lambda function from EventBridge is\nsimpler and more efficient. Similar to Option C, it introduces extra complexity.\nAuthoritative Links:\nAmazon EventBridge: https://aws.amazon.com/eventbridge/\nAWS Lambda: https://aws.amazon.com/lambda/\nAWS Batch: https://aws.amazon.com/batch/\nAmazon EventBridge Scheduler: https://aws.amazon.com/scheduler/",
    "links": [
      "https://aws.amazon.com/eventbridge/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/batch/",
      "https://aws.amazon.com/scheduler/"
    ]
  },
  {
    "question": "CertyIQ\nA company collects and processes data from a vendor. The vendor stores its data in an Amazon RDS for MySQL\ndatabase in the vendor's own AWS account. The companys VPC does not have an internet gateway, an AWS Direct\nConnect connection, or an AWS Site-to-Site VPN connection. The company needs to access the data that is in the\nvendor database.\nWhich solution will meet this requirement?",
    "options": {
      "C": "Answer: C"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it leverages AWS PrivateLink to securely and privately access the vendor's\nRDS database without exposing it to the public internet.\nHere's a detailed justification:\nRequirement: The company needs to access a vendor's RDS database, but the company's VPC lacks direct\ninternet access and private connectivity options like Direct Connect or Site-to-Site VPN.\nAWS PrivateLink: This service provides private connectivity between VPCs, AWS services, and on-premises\nnetworks, without exposing traffic to the public internet. It creates a private endpoint in the consumer VPC\n(the company's VPC) that connects to a service provider's service (the vendor's RDS database via an NLB).\nNetwork Load Balancer (NLB): NLBs are suitable for TCP traffic and can front databases, providing high\navailability and scalability. Placing an NLB in front of the RDS database allows PrivateLink to access the\ndatabase in a controlled manner. The vendor uses NLB to expose their RDS database to the VPC endpoint\nservice (PrivateLink).\nVPC Peering Inadequacy: VPC peering alone isn't enough because the company's VPC lacks an internet\ngateway, Direct Connect, or VPN connection. Without one of these, inter-VPC routing won't work. Also, even if\nVPC peering was set up with a route, it isn't the most secure way to expose an RDS database as the entire\nVPC network would be connected.\nDirect Connect Overhead: Suggesting the vendor sign up for AWS Direct Connect (option A) is overly\ncomplex and expensive for a single database access requirement. Direct Connect is typically used for high-\nbandwidth, low-latency, dedicated connections, which isn't necessary here.\nClient VPN & VPC Peering: Option B also uses VPC Peering with the added complexity of configuring client\nVPN. Setting up a client VPN is a more complex solution and does not align with the requirements of using a\nprivate and secure connection.\nTransit Gateway & VPC Peering: Option D is less efficient than PrivateLink for this specific use case.\nAlthough Transit Gateway can connect VPCs, using PrivateLink offers a more direct and secure path for\ndatabase access without unnecessary routing complexity.\nSecurity: PrivateLink provides enhanced security by limiting the attack surface and ensuring data remains\nwithin the AWS network. The vendor controls who can access their database by accepting or rejecting\nconnection requests to their VPC endpoint service.\nIn summary, option C provides the most efficient, secure, and cost-effective solution by utilizing AWS\nPrivateLink and NLB to create a private connection for accessing the vendor's RDS database without exposing\nit to the public internet or requiring complex network configurations.\nAuthoritative Links:\nAWS PrivateLink: https://aws.amazon.com/privatelink/\nNetwork Load Balancer: https://aws.amazon.com/elasticloadbalancing/network-load-balancer/",
    "links": [
      "https://aws.amazon.com/privatelink/",
      "https://aws.amazon.com/elasticloadbalancing/network-load-balancer/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to set up Amazon Managed Grafana as its visualization tool. The company wants to visualize data\nfrom its Amazon RDS database as one data source. The company needs a secure solution that will not expose the\ndata over the internet.\nWhich solution will meet these requirements?",
    "options": {
      "C": "In summary, Option C uses AWS PrivateLink to provide a secure, private connection between Amazon"
    },
    "answer": "C",
    "explanation": "The correct solution is C: Create an Amazon Managed Grafana workspace without a VPC, create an AWS\nPrivateLink endpoint to establish a connection between Amazon Managed Grafana and Amazon RDS, and set\nup Amazon RDS as a data source in Amazon Managed Grafana.\nHere's why:\nSecurity and Avoiding Public Exposure: The question explicitly requires a secure solution that does not\nexpose data over the internet. Options A and D involve creating public endpoints for the RDS database, which\ndirectly contradicts this requirement. Public endpoints are accessible from the internet, making the database\nvulnerable to potential security threats.\nAWS PrivateLink for Private Connectivity: AWS PrivateLink provides private connectivity between VPCs and\nsupported AWS services (including Amazon RDS) without exposing traffic to the public internet. It establishes\na secure, private connection using private IP addresses. This aligns perfectly with the requirement for secure\naccess and avoiding public exposure.\nAWS PrivateLink Documentation\nAmazon Managed Grafana and Data Sources: Amazon Managed Grafana needs a way to access the RDS\ndatabase to visualize its data. AWS PrivateLink provides this secure channel. After establishing the\nPrivateLink connection, Amazon RDS can be configured as a data source within the Grafana workspace.\nVPC Consideration for Grafana: While Option B suggests a Grafana workspace within a VPC, it creates a\nprivate endpoint for the RDS database, which is not necessary when using AWS PrivateLink. Option C\nleverages PrivateLink directly, which is more efficient for secure connectivity between Grafana and RDS,\nirrespective of whether the Grafana workspace is within a VPC. The key is to avoid public endpoints, and\nPrivateLink achieves this regardless of Grafana being inside a VPC.\nIn summary, Option C uses AWS PrivateLink to provide a secure, private connection between Amazon\nManaged Grafana and Amazon RDS, avoiding the need for public endpoints and ensuring data is not exposed\nover the internet. This directly addresses the security requirements of the scenario.",
    "links": []
  },
  {
    "question": "CertyIQ\nA company hosts a data lake on Amazon S3. The data lake ingests data in Apache Parquet format from various\ndata sources. The company uses multiple transformation steps to prepare the ingested data. The steps include\nfiltering of anomalies, normalizing of data to standard date and time values, and generation of aggregates for\nanalyses.\nThe company must store the transformed data in S3 buckets that data analysts access. The company needs a\nprebuilt solution for data transformation that does not require code. The solution must provide data lineage and\ndata profiling. The company needs to share the data transformation steps with employees throughout the\ncompany.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C: Configure AWS Glue DataBrew to transform the data. Share the transformation steps\nwith employees by using DataBrew recipes.\nHere's a detailed justification:\nAWS Glue DataBrew is a visual data preparation tool that allows users to clean and normalize data without\nwriting code. This directly addresses the requirement of a \"prebuilt solution for data transformation that does\nnot require code.\" DataBrew provides a visual interface to perform tasks like filtering anomalies, normalizing\ndata, and creating aggregates.\nDataBrew inherently offers data lineage and data profiling features. Data lineage is provided by tracking the\ntransformations applied to the data, showing how it was modified at each step. Data profiling is available\nwithin DataBrew, allowing users to understand the data's characteristics (e.g., data types, distributions,\nmissing values) before and after transformations.\nDataBrew uses \"recipes\" to encapsulate the transformation steps. These recipes can be easily shared with\nother employees, fulfilling the requirement to share transformation steps throughout the company. Sharing is\nfacilitated by DataBrew's inherent sharing and collaboration features.\nOption A, AWS Glue Studio, while visually oriented, primarily helps build ETL pipelines for data integration and\nrequires more coding knowledge than DataBrew for complex transformations. Also, it's not focused on\nproviding data profiling to end users.\nOption B, Amazon EMR Serverless, is an on-demand data analytics runtime that provides a serverless\nenvironment for running big data frameworks such as Spark and Hive. It focuses on heavy lifting analytics and\nnot on the no-code data profiling and transformation requested in the question. It is also not focused on\nallowing end-users to develop their own data cleaning routines.\nOption D, Amazon Athena, allows you to query data in S3 using SQL. While Athena can be used for data\ntransformation, it requires writing SQL queries, which violates the requirement of a code-free solution.\nSharing the queries could work for sharing transformation logic, but this is not ideal for users who lack strong\nSQL skills. Athena doesn't natively provide robust data profiling in the same way that Glue DataBrew does.\nTherefore, Glue DataBrew is the most appropriate solution because it provides a no-code environment for data\ntransformation, offers built-in data lineage and profiling, and allows easy sharing of transformation steps via\nrecipes.\nSupporting Documentation:\nAWS Glue DataBrew: https://aws.amazon.com/glue/databrew/\nDataBrew Features: https://docs.aws.amazon.com/databrew/latest/dg/what-is.html",
    "links": [
      "https://aws.amazon.com/glue/databrew/",
      "https://docs.aws.amazon.com/databrew/latest/dg/what-is.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect runs a web application on multiple Amazon EC2 instances that are in individual target groups\nbehind an Application Load Balancer (ALB). Users can reach the application through a public website.\nThe solutions architect wants to allow engineers to use a development version of the website to access one\nspecific development EC2 instance to test new features for the application. The solutions architect wants to use an\nAmazon Route 53 hosted zone to give the engineers access to the development instance. The solution must\nautomatically route to the development instance even if the development instance is replaced.\nWhich solution will meet these requirements?",
    "options": {
      "B": "The ALB listener rule examines the incoming requests and, based on the hostname or"
    },
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A is the correct solution and why the others are not, along with\nsupporting concepts and links:\nJustification for Option A:\nOption A is the most suitable because it leverages the Application Load Balancer's (ALB) capabilities for\ncontent-based routing and ensures automatic routing to the development instance even if it's replaced. It\nmaintains a consistent DNS endpoint through Route 53 while providing a flexible routing mechanism. The A\nrecord in Route 53 pointing to the ALB ensures that all traffic destined for the development website is\ndirected to the ALB. The ALB listener rule examines the incoming requests and, based on the hostname or\npath (defining the \"development website\"), forwards those specific requests only to the target group that\ncontains the development EC2 instance. This configuration allows engineers to access the development\ninstance using a dedicated subdomain or path without impacting the production application. If the\ndevelopment instance is replaced, only the target group membership needs updating, without changing the\nDNS records. This approach provides a robust, scalable, and easily maintainable solution. The other instances\nremain untouched, and the development instance's availability doesn't directly depend on a specific IP\naddress.\nWhy other options are incorrect:\nOption B: Relying on a public IP address for the development instance is not a best practice for several\nreasons. Public IPs can change upon instance replacement. Directly exposing an instance with a public IP\nincreases the attack surface and violates the principle of least privilege. It bypasses the load balancer,\neliminating its benefits for health checks, scalability, and traffic distribution.\nOption C: Redirecting requests to a public IP address after they hit the ALB is inefficient and unnecessary.\nThe ALB is perfectly capable of routing directly to the appropriate instance. A redirect adds latency and\ncomplexity without providing any additional value. Like option B, using a public IP directly is less secure and\nharder to manage.\nOption D: Placing all instances in the same target group defeats the purpose of having separate development\nand production environments. All instances would receive traffic indiscriminately. This removes the isolation\nbetween environments, potentially disrupting the production application during development testing.\nSupporting Concepts:\nApplication Load Balancer (ALB): An ALB operates at the application layer (layer 7) of the OSI model,\nallowing for content-based routing, host-based routing, and path-based routing. It can inspect the contents of\nthe HTTP requests (headers, URLs, etc.) and route traffic accordingly.\nTarget Groups: A target group is a collection of targets (e.g., EC2 instances) that receive traffic from a load\nbalancer. You can define health checks for the targets in a target group.\nListeners and Rules: ALB listeners listen for incoming connection requests. Listener rules define how the ALB\nroutes requests to different target groups based on conditions such as hostname, path, or HTTP headers.\nAmazon Route 53: A scalable and highly available DNS web service. It translates domain names into IP\naddresses, enabling users to access applications using human-readable names.\nA Record: A DNS record that maps a hostname to an IPv4 address.\nAuthoritative Links:\nApplication Load Balancers:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\nTarget Groups for ALB: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/tutorial-\napplication-load-balancer-target-groups.html\nListener Rules: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-\nlisteners.html\nAmazon Route 53: https://aws.amazon.com/route53/",
    "links": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/tutorial-",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-",
      "https://aws.amazon.com/route53/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a container application on a Kubernetes cluster in the company's data center. The application\nuses Advanced Message Queuing Protocol (AMQP) to communicate with a message queue. The data center cannot\nscale fast enough to meet the companys expanding business needs. The company wants to migrate the workloads\nto AWS.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "B": "Migrate the container application to Amazon Elastic Kubernetes Service (Amazon"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Migrate the container application to Amazon Elastic Kubernetes Service (Amazon\nEKS). Use Amazon MQ to retrieve the messages.\nHere's a detailed justification:\nThe question emphasizes migrating a containerized application running on Kubernetes and the need to\nmaintain AMQP compatibility with minimal operational overhead.\nAmazon EKS is a managed Kubernetes service that allows the company to migrate their existing Kubernetes\nworkloads to AWS without significant changes to their application architecture or operational practices. This\naligns with the requirement of minimal operational overhead because AWS manages the Kubernetes control\nplane, relieving the company of this responsibility.\nAmazon MQ is a managed message broker service that supports various message brokers, including\nRabbitMQ, which uses the AMQP protocol. By using Amazon MQ, the company can continue to use AMQP for\nmessage queuing without needing to manage the underlying infrastructure or make significant code changes.\nThis direct AMQP compatibility is a crucial factor because the application currently relies on it.\nLet's analyze why the other options are less suitable:\nOption A (Amazon ECS and Amazon SQS): ECS is a container orchestration service, but Amazon SQS does\nnot support the AMQP protocol. The application would require significant code changes to switch from AMQP\nto SQS's message format, increasing operational overhead and complexity.\nOption C (Amazon EC2 and Amazon MQ): While EC2 instances can run the application and Amazon MQ\nprovides AMQP support, managing EC2 instances for container applications is more complex and involves\nmore operational overhead than using a managed Kubernetes service like EKS. The company has an existing\nKubernetes setup, and replicating that manually on EC2 would be an inefficient approach.\nOption D (AWS Lambda and Amazon SQS): Lambda functions might not be suitable for directly running a\ncontainer application. Also, as stated before, Amazon SQS doesn't natively support AMQP.\nTherefore, option B offers the best solution by leveraging a managed Kubernetes service (EKS) to minimize\noperational overhead while maintaining AMQP compatibility through Amazon MQ, requiring the fewest\nchanges to the existing application architecture.\nRelevant Links:\nAmazon EKS: https://aws.amazon.com/eks/\nAmazon MQ: https://aws.amazon.com/mq/",
    "links": [
      "https://aws.amazon.com/eks/",
      "https://aws.amazon.com/mq/"
    ]
  },
  {
    "question": "CertyIQ\nAn online gaming company hosts its platform on Amazon EC2 instances behind Network Load Balancers (NLBs)\nacross multiple AWS Regions. The NLBs can route requests to targets over the internet. The company wants to\nimprove the customer playing experience by reducing end-to-end load time for its global customer base.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Global Accelerator Benefits: AGA offers features like:"
    },
    "answer": "D",
    "explanation": "The correct answer is D: Create a standard accelerator in AWS Global Accelerator. Configure the existing\nNLBs as target endpoints.\nHere's why this solution is optimal and why the others are not:\nAWS Global Accelerator (AGA) and Performance: AGA is designed to improve the availability and\nperformance of applications for global users. It achieves this by utilizing the AWS global network\ninfrastructure and strategically located edge locations. It routes user traffic to the closest healthy endpoint\n(NLB in this case) using Anycast static IP addresses. This reduces latency and improves end-to-end load time\nby minimizing the distance data travels over the public internet. A standard accelerator will provide the\nperformance benefits needed.\nWhy other options are incorrect:\nA (ALBs instead of NLBs): While Application Load Balancers (ALBs) offer advanced features like content-\nbased routing, they don't inherently reduce latency for a global user base in the same way as AGA. Replacing\nNLBs with ALBs alone would not directly address the issue of minimizing distance over the public internet.\nB (Route 53 weighted routing): Amazon Route 53 with weighted routing balances traffic across Regions, but\nit doesn't optimize network paths to minimize latency in the same way as AGA. It simply distributes traffic, not\nnecessarily to the closest or best-performing endpoint for each user. Therefore, it won't significantly improve\nend-to-end load time. Also, Route 53 relies on DNS resolution which adds overhead and is not as dynamic as\nGlobal Accelerator.\nC (Additional NLBs and EC2 in more Regions): This would require a significant infrastructure investment and\nongoing operational overhead. While it could potentially improve latency for some users closer to the new\nRegions, it doesn't offer the same level of global network optimization as AGA. It's also not as cost-effective\nas leveraging the existing NLBs with AGA.\nGlobal Accelerator Benefits: AGA offers features like:\nStatic Anycast IP Addresses: Provide a fixed entry point to your application, simplifying DNS management\nand improving reliability.\nTraffic Dial: Allows you to control the percentage of traffic directed to each endpoint.\nFailover: Automatically detects unhealthy endpoints and reroutes traffic to healthy ones.\nGlobal network usage: Utilizes AWS global network infrastructure providing cost-effective, quick, and\nperformant routing.\nIn summary, AWS Global Accelerator provides the most efficient and scalable solution to improve the\ncustomer playing experience by reducing end-to-end load time for a global customer base, leveraging the\nexisting NLB infrastructure. It provides faster performance, lower costs, easier management, and better\nscalability than the other options.\nSupporting Links:\nAWS Global Accelerator\nNetwork Load Balancer\nApplication Load Balancer\nAmazon Route 53",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has an on-premises application that uses SFTP to collect financial data from multiple vendors. The\ncompany is migrating to the AWS Cloud. The company has created an application that uses Amazon S3 APIs to\nupload files from vendors.\nSome vendors run their systems on legacy applications that do not support S3 APIs. The vendors want to continue\nto use SFTP-based applications to upload data. The company wants to use managed services for the needs of the\nvendors that use legacy applications.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "B": "Create an AWS Transfer Family endpoint for vendors that use legacy applications."
    },
    "answer": "B",
    "explanation": "The correct answer is B. Create an AWS Transfer Family endpoint for vendors that use legacy applications.\nHere's why:\nAWS Transfer Family is a fully managed service specifically designed to facilitate secure file transfers into\nand out of Amazon S3, Amazon EFS, and Amazon FSx for Windows File Server using protocols like SFTP,\nFTPS, and FTP. This aligns perfectly with the requirement of supporting legacy SFTP-based applications while\nintegrating with S3 for the company's cloud-native application. The key benefit is that it's a managed service,\nrelieving the company of the operational burden of managing servers, patching, and scaling the SFTP\ninfrastructure. Vendors can continue using their existing SFTP clients to transfer files directly to S3,\nsimplifying their workflow.\nOption A, using AWS DMS, is incorrect because AWS DMS is primarily designed for database migrations, not\nfile transfers. It's not the right tool for this use case, and providing vendors access to a DMS instance would\nbe inappropriate and create security risks.\nOption C, configuring an EC2 instance to run an SFTP server, introduces significant operational overhead. The\ncompany would be responsible for managing the EC2 instance, ensuring its security, scaling it to handle\ntraffic, and maintaining the SFTP server software. This negates the benefit of using managed services.\nOption D, using Amazon S3 File Gateway, involves creating an SMB file share. While File Gateway can\nintegrate on-premises applications with S3, it requires the vendors to adapt to a new protocol (SMB) when\nthey currently use SFTP. This doesn't align with the requirement of supporting their existing SFTP-based\napplications. Furthermore, it's designed more for on-premises access to cloud storage, rather than directly\nfacilitating vendor uploads. The operational overhead is also higher than using AWS Transfer Family.\nTherefore, AWS Transfer Family provides the most straightforward, secure, and managed solution to bridge\nthe gap between legacy SFTP systems and the cloud-native S3 environment with minimal operational effort.\nSupporting Links:\nAWS Transfer Family: https://aws.amazon.com/transfer/",
    "links": [
      "https://aws.amazon.com/transfer/"
    ]
  },
  {
    "question": "CertyIQ\nA marketing team wants to build a campaign for an upcoming multi-sport event. The team has news reports from\nthe past five years in PDF format. The team needs a solution to extract insights about the content and the\nsentiment of the news reports. The solution must use Amazon Textract to process the news reports.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "C": "Provide the extracted insights to Amazon Comprehend for analysis. Save the",
      "A": "Amazon Athena and S3: While Athena can query data in S3, it's primarily a query service for structured",
      "B": "Amazon DynamoDB and SageMaker: Storing extracted insights in DynamoDB is a viable option. However,",
      "D": "Amazon S3 and QuickSight: Storing extracted insights in S3 is reasonable. However, QuickSight is"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Provide the extracted insights to Amazon Comprehend for analysis. Save the\nanalysis to an Amazon S3 bucket.\nHere's why:\nAmazon Comprehend for Sentiment Analysis: Amazon Comprehend is a natural language processing (NLP)\nservice specifically designed for tasks like sentiment analysis and key phrase extraction. It's purpose-built for\nanalyzing text and identifying the overall sentiment (positive, negative, or neutral) as well as extracting\nrelevant entities and topics. This directly fulfills the requirement to extract insights and sentiments.\nLeast Operational Overhead: Comprehend is a managed service, meaning AWS handles the underlying\ninfrastructure, scaling, and maintenance. You simply provide the text, and Comprehend returns the analysis\nresults. This minimizes operational overhead.\nS3 for Storage: Storing the analysis results in an S3 bucket is a cost-effective and scalable way to persist the\ndata. It's also readily accessible for further reporting or integration with other AWS services.\nLet's examine why the other options are less suitable:\nA. Amazon Athena and S3: While Athena can query data in S3, it's primarily a query service for structured\ndata. The extracted insights from Textract might not be readily structured for Athena without additional\nprocessing. It also doesn't directly address the sentiment analysis requirement.\nB. Amazon DynamoDB and SageMaker: Storing extracted insights in DynamoDB is a viable option. However,\nbuilding a sentiment model with SageMaker is an overkill. Comprehend is already providing managed\nsentiment analysis and entails more development and maintenance.\nD. Amazon S3 and QuickSight: Storing extracted insights in S3 is reasonable. However, QuickSight is\nprimarily a visualization tool and doesn't provide built-in sentiment analysis capabilities. It would require\nadditional processing steps to perform sentiment analysis on the extracted text before visualization.\nIn summary, Option C leverages Amazon Comprehend, a managed service designed specifically for sentiment\nanalysis, thereby minimizing operational overhead and directly addressing the requirements of the question.\nThe S3 bucket serves as a simple and scalable storage solution for the analysis results.\nAuthoritative Links:\nAmazon Comprehend: https://aws.amazon.com/comprehend/\nAmazon Textract: https://aws.amazon.com/textract/\nAmazon S3: https://aws.amazon.com/s3/",
    "links": [
      "https://aws.amazon.com/comprehend/",
      "https://aws.amazon.com/textract/",
      "https://aws.amazon.com/s3/"
    ]
  },
  {
    "question": "CertyIQ\nA company's application runs on Amazon EC2 instances that are in multiple Availability Zones. The application\nneeds to ingest real-time data from third-party applications.\nThe company needs a data ingestion solution that places the ingested raw data in an Amazon S3 bucket.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Here's why:"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's why:\nOption A utilizes Amazon Kinesis Data Streams for real-time data ingestion. Kinesis Data Streams are\ndesigned for high-throughput, real-time data streaming from multiple sources. Each EC2 instance can feed its\ndata into the Kinesis Data Stream. Then, Amazon Kinesis Data Firehose is employed to consume the data from\nthe Kinesis Data Stream and reliably deliver it to an S3 bucket. Kinesis Data Firehose offers seamless\nintegration with S3, automatically handling data buffering, compression, and encryption. This setup efficiently\nand reliably captures the real-time data and stores it in S3 for further processing or analysis.\nOption B, AWS DMS, is generally used for migrating databases, not for ingesting real-time streams of raw\ndata from applications. Using replication instances as the source endpoints doesn't align with the requirement\nof handling real-time data from multiple EC2 instances.https://aws.amazon.com/dms/\nOption C, AWS DataSync, focuses on transferring large amounts of data between on-premises storage and\nAWS storage services. It's not designed for real-time data ingestion. It's also not the most efficient solution\nfor numerous, continuous updates from multiple instances.https://aws.amazon.com/datasync/\nOption D, AWS Direct Connect, establishes a dedicated network connection from your on-premises\nenvironment to AWS. While it improves network performance, it is not necessary or cost-effective for\ningesting data from EC2 instances already running within AWS. Direct PUT operations might be possible but\nwould require significant custom implementation compared to a managed streaming service like\nKinesis.https://aws.amazon.com/directconnect/\nTherefore, the Kinesis Data Streams and Kinesis Data Firehose combination provides the most efficient,\nscalable, and reliable solution for real-time data ingestion from multiple sources (EC2 instances) into an S3\nbucket, making Option A the best choice. Kinesis is built precisely for real-time data processing and seamless\nintegration with S3.https://aws.amazon.com/kinesis/data-streams/https://aws.amazon.com/kinesis/data-\nfirehose/",
    "links": [
      "https://aws.amazon.com/dms/",
      "https://aws.amazon.com/datasync/",
      "https://aws.amazon.com/directconnect/",
      "https://aws.amazon.com/kinesis/data-streams/https://aws.amazon.com/kinesis/data-"
    ]
  },
  {
    "question": "CertyIQ\nA companys application is receiving data from multiple data sources. The size of the data varies and is expected to\nincrease over time. The current maximum size is 700 K",
    "options": {
      "B": "Store the large data as objects in an Amazon S3 bucket. In a DynamoDB table,"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Store the large data as objects in an Amazon S3 bucket. In a DynamoDB table,\ncreate an item that has an attribute that points to the S3 URL of the data.\nHere's why this solution is the most operationally efficient:\nDynamoDB Item Size Limit: DynamoDB has a hard limit of 400 KB per item. Directly storing data exceeding\nthis limit will fail.\nOffloading Large Objects to S3: Amazon S3 is designed for storing large, unstructured data. S3 offers\nvirtually unlimited storage capacity and excellent durability and availability. By storing the large data in S3,\nwe bypass DynamoDB's size limitations for individual items.\nReference in DynamoDB: Storing a pointer (S3 URL) in DynamoDB allows you to maintain metadata or\nidentifiers related to the large data. This keeps DynamoDB items within the size limits while providing a link to\nthe actual data.\nOperational Efficiency: This approach minimizes the complexity of data manipulation. No complex splitting,\nfiltering, or compression is required. S3 handles storage and retrieval efficiently.\nCost Optimization: S3 storage is generally cheaper than DynamoDB storage, especially for large data\nvolumes.\nScalability and Performance: S3 is highly scalable and offers excellent performance for data retrieval.\nDynamoDB provides fast access to the metadata.\nWhy other options are less suitable:\nA: Using DocumentDB introduces a separate database technology, increasing operational overhead\n(managing two databases). Filtering data to fit DynamoDB limits might result in data loss.\nC: Splitting data into multiple DynamoDB items requires careful management of the split data, potentially\nimpacting performance. This approach is complex and adds significant overhead. BatchWriteItem also has its\nlimitations.\nD: Compression can help, but it might not always reduce the data size to below the 400 KB limit. It also adds\ncomputational overhead to both write and read operations. It is possible to use compression, but a solution to\nthe issue would require combining it with S3 to store the data.\nAuthoritative Links:\nAmazon DynamoDB Limits:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ServiceQuotas.html\nAmazon S3: https://aws.amazon.com/s3/",
    "links": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ServiceQuotas.html",
      "https://aws.amazon.com/s3/"
    ]
  },
  {
    "question": "CertyIQ\nA company is migrating a legacy application from an on-premises data center to AWS. The application relies on\nhundreds of cron jobs that run between 1 and 20 minutes on different recurring schedules throughout the day.\nThe company wants a solution to schedule and run the cron jobs on AWS with minimal refactoring. The solution\nmust support running the cron jobs in response to an event in the future.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the best solution for migrating cron jobs to AWS, along with\nsupporting concepts and links:\nThe core requirement is to migrate existing cron jobs to AWS with minimal refactoring, supporting future\nevent-driven triggers, and accommodating runtimes from 1 to 20 minutes. Let's analyze each option:\nOption A (EventBridge Scheduler + Lambda): Lambda functions have execution time limits. While Lambda\nfunctions are suitable for short tasks, they are typically limited to a maximum execution duration (currently 15\nminutes). Many cron jobs exceed this limit, making Lambda\nunsuitable.https://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents.html\nOption B (AWS Batch + ECS): AWS Batch is designed for batch processing jobs, particularly those with\ndependencies or complex workflows. While it could handle cron jobs, it introduces unnecessary complexity\nand overhead for simple scheduled tasks. Event-driven scheduling isn't inherent, requiring additional\nconfiguration. The core problem is its inherent design for background jobs rather than scheduled, time-\nsensitive tasks.https://docs.aws.amazon.com/batch/latest/userguide/what-is-batch.html\nOption C (EventBridge Scheduler + Fargate): Amazon EventBridge Scheduler allows you to create schedules\nthat trigger targets on a recurring basis. AWS Fargate is a serverless compute engine for containers that lets\nyou run containers without managing servers or clusters. It's an ideal compute platform for running\ncontainerized cron jobs of various durations. Packaging the cron jobs into containers offers flexibility and\nreduces refactoring. EventBridge Scheduler supports triggering jobs based on schedules or in response to\nother events, fulfilling the \"event in the future\" requirement. This approach provides a simple, scalable, and\ncost-effective solution for running cron jobs in the\ncloud.https://aws.amazon.com/fargate/https://aws.amazon.com/eventbridge/scheduler/\nOption D (Step Functions + Wait State + Fargate): Step Functions could be used, but introducing a Step\nFunction workflow with Wait states for simple scheduling adds significant complexity. It's an overkill solution\nwhen a simpler approach like EventBridge Scheduler can directly trigger tasks. The Wait state isn't ideal for\nreliable scheduling.https://aws.amazon.com/step-functions/\nTherefore, option C provides the most straightforward and appropriate solution by leveraging EventBridge\nScheduler for cron job scheduling and AWS Fargate for running the containerized jobs, which accommodates\nthe specified runtime, minimal refactoring, and future event-driven needs, making it the best choice.",
    "links": [
      "https://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents.html",
      "https://docs.aws.amazon.com/batch/latest/userguide/what-is-batch.html",
      "https://aws.amazon.com/fargate/https://aws.amazon.com/eventbridge/scheduler/",
      "https://aws.amazon.com/step-functions/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses Salesforce. The company needs to load existing data and ongoing data changes from Salesforce\nto Amazon Redshift for analysis. The company does not want the data to travel over the public internet.\nWhich solution will meet these requirements with the LEAST development effort?",
    "options": {
      "A": "D. VPC peering connection: VPC peering connections are designed to connect VPCs within AWS. You cannot",
      "B": "AWS Direct Connect: Direct Connect provides a dedicated network connection between your on-premises"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it provides the most secure and efficient solution for transferring data from\nSalesforce to Amazon Redshift without traversing the public internet, with minimal development effort.\nHere's a detailed justification:\nAWS PrivateLink: This service allows you to access services hosted on the AWS network or by other AWS\ncustomers in a secure and private manner. It establishes a private connection between your VPC and the\nservice without exposing your traffic to the public internet. Salesforce supports AWS PrivateLink.\nAmazon AppFlow: This is a fully managed integration service that enables you to securely transfer data\nbetween SaaS applications (like Salesforce) and AWS services (like Amazon Redshift). It's designed for ease\nof use and requires minimal coding. AppFlow integrates seamlessly with PrivateLink for secure data transfer.\nhttps://aws.amazon.com/privatelink/\nhttps://aws.amazon.com/appflow/\nNow, let's examine why the other options are less suitable:\nA. VPN connection: While a VPN provides a secure connection, it's more complex to set up and manage than\nPrivateLink. It also involves more overhead in terms of performance and configuration. Furthermore, AWS\nGlue DataBrew is primarily a data preparation tool, not the optimal choice for transferring data from\nSalesforce to Redshift. DataBrew does not natively support direct connections to SaaS applications through\nprivate connections.\nB. AWS Direct Connect: Direct Connect provides a dedicated network connection between your on-premises\nenvironment and AWS. While it avoids the public internet, it's more expensive and requires a physical\nconnection to AWS, which might be overkill for this use case involving a SaaS application like Salesforce.\nUsing DataBrew here still has the same limitation as in option A.\nD. VPC peering connection: VPC peering connections are designed to connect VPCs within AWS. You cannot\nestablish a direct VPC peering connection with Salesforce.\nTherefore, option C offers the ideal combination of security (using PrivateLink), ease of use (using AppFlow),\nand cost-effectiveness for the specified requirements. It requires the least amount of development effort by\nleveraging managed services specifically designed for these types of integration scenarios.",
    "links": [
      "https://aws.amazon.com/privatelink/",
      "https://aws.amazon.com/appflow/"
    ]
  },
  {
    "question": "CertyIQ\nA company recently migrated its application to AWS. The application runs on Amazon EC2 Linux instances in an\nAuto Scaling group across multiple Availability Zones. The application stores data in an Amazon Elastic File\nSystem (Amazon EFS) file system that uses EFS Standard-Infrequent Access storage. The application indexes the\ncompany's files. The index is stored in an Amazon RDS database.\nThe company needs to optimize storage costs with some application and services changes.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A is the most cost-effective solution for the given scenario:\nThe primary goal is to reduce storage costs while maintaining application functionality after a migration to\nAWS. The current setup uses EFS Standard-Infrequent Access, which, while cheaper than EFS Standard, can\nstill be relatively expensive compared to Amazon S3, particularly for infrequently accessed data. The key to\ncost optimization lies in identifying a storage solution that automatically tiers data based on access patterns\nand integrates well with the application.\nOption A leverages Amazon S3 with Intelligent-Tiering. Intelligent-Tiering automatically moves data between\nfrequent, infrequent, and archive access tiers based on usage patterns, without any operational overhead or\nimpact on performance. [https://aws.amazon.com/s3/storage-classes/intelligent-tiering/]. By copying the files\nto S3 and updating the application to use the Amazon S3 API, the company benefits from the lower storage\ncosts of S3 and the automated tiering capabilities of Intelligent-Tiering. This ensures that frequently\naccessed files are readily available while infrequently accessed files are stored at a lower cost.\nOption B, deploying Amazon FSx for Windows File Server, is less cost-effective because FSx for Windows File\nServer is generally more expensive than S3. It's more suited for Windows-based applications requiring native\nWindows file system compatibility. The migration effort is also significant.\nOption C, using Amazon FSx for OpenZFS, offers high performance but is also generally more expensive than\nS3. The complexity of setting up and managing an FSx for OpenZFS file system adds operational overhead.\nOption D, using S3 Glacier Flexible Retrieval, is not the most optimal solution despite its very low cost. S3\nGlacier Flexible Retrieval (formerly known as S3 Glacier) is suitable for archival data with infrequent access\nand retrieval times can be several hours. This is likely to impact the application's performance and may not\nmeet the company's requirements for readily accessible files, even if accessed infrequently. Furthermore,\n\"standard retrievals\" from Glacier are designed for infrequent, bulk access, not the type of on-demand access\nthe application may still require for some files. [https://aws.amazon.com/s3/storage-classes/glacier/].\nTherefore, migrating to S3 with Intelligent-Tiering is the most cost-effective solution because it provides a\nbalance between storage cost, performance, and ease of integration with the existing application after a\nsimple API update. It directly addresses the company's need to optimize storage costs without significant\napplication rework or performance degradation.",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/intelligent-tiering/].",
      "https://aws.amazon.com/s3/storage-classes/glacier/]."
    ]
  },
  {
    "question": "CertyIQ\nA robotics company is designing a solution for medical surgery. The robots will use advanced sensors, cameras,\nand AI algorithms to perceive their environment and to complete surgeries.\nThe company needs a public load balancer in the AWS Cloud that will ensure seamless communication with\nbackend services. The load balancer must be capable of routing traffic based on the query strings to different\ntarget groups. The traffic must also be encrypted.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C: Use an Application Load Balancer (ALB) with a certificate attached from AWS\nCertificate Manager (ACM). Use query parameter-based routing.\nHere's why:\nApplication Load Balancer (ALB): ALBs operate at the application layer (Layer 7) of the OSI model. This\nallows them to make routing decisions based on the content of the HTTP requests, including query strings.\nNetwork Load Balancers (NLBs) operate at Layer 4 (transport layer) and are not capable of query string-based\nrouting. Gateway Load Balancers are designed for virtual appliances and are not appropriate for this scenario.\nQuery Parameter-Based Routing: The requirement specifies that the load balancer must route traffic based\non query strings. ALBs are explicitly designed to support this type of routing, allowing you to direct requests\nto different target groups based on the values in the query parameters.\nEncryption (HTTPS): The requirement states that traffic must be encrypted. ALBs support HTTPS listeners,\nallowing you to encrypt traffic between clients and the load balancer. AWS Certificate Manager (ACM) is the\nrecommended service for provisioning, managing, and deploying SSL/TLS certificates for use with AWS\nservices, including ALBs. Importing certificates into IAM is less secure than ACM.\nACM Integration: ACM simplifies the process of obtaining and managing SSL/TLS certificates for use with\nAWS services. The ALB can directly integrate with ACM to automatically handle certificate renewal and\ndeployment, reducing operational overhead.\nOption A and D are incorrect because NLBs do not support query parameter-based routing.\nOption B is incorrect as it is not designed to be used with Application load balancing.\nSupporting Documentation:\nApplication Load Balancers:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\nALB Listener Rules: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-\nlisteners.html\nAWS Certificate Manager: https://aws.amazon.com/certificate-manager/",
    "links": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-",
      "https://aws.amazon.com/certificate-manager/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an application that runs on a single Amazon EC2 instance. The application uses a MySQL database\nthat runs on the same EC2 instance. The company needs a highly available and automatically scalable solution to\nhandle increased traffic.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The optimal solution involves distributing the application and database across multiple, scalable resources for\nhigh availability and automatic scaling. Option C achieves this effectively.\nHere's a detailed justification:\n1. Application Scalability: Deploying the application on EC2 instances within an Auto Scaling group\nensures automatic scaling. As traffic increases, the Auto Scaling group automatically launches more\nEC2 instances to handle the load, and when traffic decreases, it scales down, optimizing costs. An\nApplication Load Balancer (ALB) distributes incoming traffic across these EC2 instances, providing a\nsingle point of entry and improving application\navailability.https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-\nscaling.htmlhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\n2. Database Scalability and High Availability: Moving the MySQL database to Amazon Aurora\nServerless MySQL provides automatic scaling and high availability. Aurora Serverless automatically\nscales the database capacity based on application demand, so you only pay for what you use. It also\noffers built-in fault tolerance and automatic recovery, ensuring high availability.\nhttps://aws.amazon.com/rds/aurora/serverless/\n3. Why other options are incorrect:\nA: While using Auto Scaling group and ALB is correct for the application tier, Amazon Redshift is a\ndata warehouse service and not suitable for transactional workloads of a MySQL database.\nB: Target groups configured behind an ALB are acceptable, but the description is less precise.\nMoreover, while Amazon RDS for MySQL cluster offers HA, it may require more manual intervention\nfor scaling than Aurora Serverless.\nD: ElastiCache is a caching service and not a replacement for a relational database like MySQL. It is\nnot designed to be used with MySQL connector for primary data storage.\nTherefore, Option C is the most appropriate solution because it combines automatic scaling and high\navailability for both the application and the database layers, meeting the company's requirements effectively.",
    "links": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://aws.amazon.com/rds/aurora/serverless/"
    ]
  },
  {
    "question": "CertyIQ\nA company is planning to migrate data to an Amazon S3 bucket. The data must be encrypted at rest within the S3\nbucket. The encryption key must be rotated automatically every year.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A because it offers the least operational overhead while meeting the encryption and key\nrotation requirements. Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3) automatically\nhandles key rotation without any manual intervention. Amazon S3 rotates these keys regularly, ensuring data\nis protected by new encryption keys periodically. This aligns directly with the requirement for annual key\nrotation.\nOption B involves AWS KMS customer-managed keys with automatic key rotation, which although viable,\nintroduces more complexity. Managing KMS keys, even with automatic rotation, requires more overhead than\nsimply relying on S3's built-in encryption.\nOption C requires manual key rotation, which contradicts the requirement for minimal operational overhead.\nManual rotation is prone to human error and necessitates scheduled tasks and monitoring.\nOption D is overly complex. Importing customer key material into KMS keys adds a significant layer of\noperational overhead. While it allows for key rotation, it's much more difficult to manage than S3-managed\nkeys and is not necessary given the scenario's requirements.\nSSE-S3 simplifies the process by abstracting the key management away from the user, adhering to security\nbest practices and reducing the operational burden. It ensures that data at rest in the S3 bucket is encrypted\nand that key rotation occurs automatically without any administrator intervention.\nTherefore, leveraging SSE-S3 is the most straightforward and efficient approach, aligning with the principle\nof least privilege and minimal operational complexity in cloud solutions.\nFurther reading:\nProtecting Data Using Server-Side Encryption\nAWS Key Management Service (KMS)\nSSE-S3",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is migrating applications from an on-premises Microsoft Active Directory that the company manages to\nAWS. The company deploys the applications in multiple AWS accounts. The company uses AWS Organizations to\nmanage the accounts centrally.\nThe company's security team needs a single sign-on solution across all the company's AWS accounts. The\ncompany must continue to manage users and groups that are in the on-premises Active Directory.\nWhich solution will meet these requirements?",
    "options": {
      "D": "Option C is insufficient. While creating a two-way trust relationship with AWS Directory Service is necessary"
    },
    "answer": "B",
    "explanation": "Option B is the correct solution because it leverages AWS IAM Identity Center (successor to AWS SSO) for\ncentralized single sign-on and integrates seamlessly with the existing on-premises Active Directory. IAM\nIdentity Center enables users to access multiple AWS accounts with a single set of credentials. Establishing a\ntwo-way forest trust using AWS Directory Service for Microsoft Active Directory (AD Connector or a managed\nAD) allows IAM Identity Center to authenticate users directly against the on-premises AD. This avoids\nreplicating user data to AWS and maintains the on-premises AD as the source of truth.\nOption A is incorrect because creating an Enterprise Edition Active Directory in AWS Directory Service and\nmaking it the identity source for IAM Identity Center would require migrating users and groups to the AWS-\nmanaged AD, which contradicts the requirement to continue managing users and groups in the on-premises\nAD.\nOption C is insufficient. While creating a two-way trust relationship with AWS Directory Service is necessary\nfor integration, it doesn't provide the single sign-on functionality across multiple accounts. This option lacks\nthe IAM Identity Center component for centralized access management.\nOption D is more complex and less efficient. Deploying an IdP on EC2 introduces unnecessary management\noverhead and complexity. IAM Identity Center is designed for native integration with identity providers, and\nusing AWS Directory Service in conjunction with IAM Identity Center offers a managed and simplified solution.\nFurthermore, the AWS-managed solutions can ensure higher availability and scalability compared to an EC2-\nbased IdP. IAM Identity Center directly supports AD integration through Directory Service, avoiding the need\nto build a custom integration.\nTherefore, enabling IAM Identity Center and configuring a two-way forest trust between the on-premises\nActive Directory and IAM Identity Center using AWS Directory Service is the optimal solution to meet the\ncompany's requirements.\nRelevant Documentation:\nWhat is AWS IAM Identity Center (successor to AWS Single Sign-On)?\nAWS Directory Service\nHow AWS IAM Identity Center (successor to AWS Single Sign-On) works with Active Directory",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is planning to deploy its application on an Amazon Aurora PostgreSQL Serverless v2 cluster. The\napplication will receive large amounts of traffic. The company wants to optimize the storage performance of the\ncluster as the load on the application increases.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "C",
    "explanation": "The optimal solution for cost-effectively optimizing storage performance for an Aurora PostgreSQL\nServerless v2 cluster under increasing load is option C, configuring the cluster storage type as General\nPurpose.\nHere's why:\nAurora Serverless v2's Storage Scalability: Aurora Serverless v2 is designed to automatically scale both\ncompute and storage resources based on application needs. It automatically increases storage capacity as\ndata grows, negating the need for manual intervention in most cases.\nGeneral Purpose (gp2/gp3) SSDs: General Purpose SSDs are a cost-effective storage option that provides a\nbalance of performance and cost for a wide variety of workloads. They are well-suited for databases that\nexperience variable I/O patterns, which is typical during workload spikes.\nCost-Effectiveness Comparison: Provisioned IOPS (option B) is significantly more expensive. It allows you to\nspecify the number of IOPS you need, but this comes at a higher cost than General Purpose storage. Since the\napplication load will only experience periodic spikes and Aurora Serverless v2 already handles storage\nscaling, the excess capacity provisioned will likely be unused, costing unnecessarily.\nAurora Standard vs. Aurora I/O-Optimized: Aurora I/O-Optimized storage configuration (option D) comes at\nadditional cost. While Aurora Standard storage configuration (option A) would still technically work, General\nPurpose is the optimal cost-effective configuration. General Purpose storage offers the best balance of cost\nand performance.\nAurora's Storage Management: Aurora manages storage internally, and General Purpose storage is\nappropriate for most workloads. It abstracts away the complexities of managing storage performance,\nallowing developers to focus on the application logic.\nIn summary, choosing General Purpose storage leverages Aurora Serverless v2's built-in scalability and\nprovides a good balance of performance and cost for handling large amounts of traffic and optimizing storage\nperformance as the load increases.\nRelevant Links:\nAurora Storage: https://aws.amazon.com/rds/aurora/storage-optimization/\nAurora Serverless v2: https://aws.amazon.com/rds/aurora/serverless/",
    "links": [
      "https://aws.amazon.com/rds/aurora/storage-optimization/",
      "https://aws.amazon.com/rds/aurora/serverless/"
    ]
  },
  {
    "question": "CertyIQ\nA financial services company that runs on AWS has designed its security controls to meet industry standards. The\nindustry standards include the National Institute of Standards and Technology (NIST) and the Payment Card\nIndustry Data Security Standard (PCI DSS).\nThe company's third-party auditors need proof that the designed controls have been implemented and are\nfunctioning correctly. The company has hundreds of AWS accounts in a single organization in AWS Organizations.\nThe company needs to monitor the current state of the controls across accounts.\nWhich solution will meet these requirements?",
    "options": {
      "D": "Here's why:"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Here's why:\nThe company requires continuous monitoring of security control implementation and effectiveness across\nhundreds of AWS accounts in an AWS Organization, with a need to demonstrate compliance with NIST and\nPCI DSS standards to auditors.\nAWS Security Hub is designed to provide a comprehensive view of the security state of your AWS resources.\nIt aggregates, organizes, and prioritizes security alerts and findings from various AWS services (like\nGuardDuty, Inspector, Macie) and supported third-party solutions. It centralizes security management across\naccounts. (https://aws.amazon.com/security-hub/)\nBy designating a Security Hub delegated administrator account from the Organizations management account,\nthe company can centrally manage Security Hub settings and view findings across all member accounts.\nSecurity Hub has built-in support for security standards like NIST and PCI DSS. By enabling these standards,\nSecurity Hub automatically checks your environment against the controls defined in those standards,\nproviding a compliance score and detailed findings on non-compliant resources. This is crucial for audit\npurposes.\nAmazon Inspector (Option A) focuses on automated security vulnerability assessments of EC2 instances and\ncontainer images. While valuable for finding vulnerabilities, it doesn't provide a comprehensive view of\ncompliance against industry standards across the entire AWS environment in the same way as Security Hub.\nAmazon GuardDuty (Option B) is a threat detection service that monitors for malicious activity and\nunauthorized behavior. It doesn't specifically address compliance monitoring against standards like NIST and\nPCI DSS, though its findings can contribute to overall security posture.\nAWS CloudTrail (Option C) records API calls made within your AWS environment. While it's essential for audit\ntrails and security analysis, it doesn't directly provide compliance assessments or reporting against specific\nindustry standards. CloudTrail provides data for compliance, not compliance itself.\nSecurity Hub's Standards enable easy compliance tracking and reporting against desired benchmarks,\nallowing the company to demonstrably prove to third-party auditors that controls are implemented and\nfunctional.\nTherefore, Security Hub is the most appropriate solution for centrally monitoring compliance with NIST and\nPCI DSS across multiple AWS accounts, fulfilling the company's requirements for security monitoring and\nauditability.",
    "links": [
      "https://aws.amazon.com/security-hub/)"
    ]
  },
  {
    "question": "CertyIQ\nA company uses an Amazon S3 bucket as its data lake storage platform. The S3 bucket contains a massive amount\nof data that is accessed randomly by multiple teams and hundreds of applications. The company wants to reduce\nthe S3 storage costs and provide immediate availability for frequently accessed objects.\nWhat is the MOST operationally efficient solution that meets these requirements?",
    "options": {
      "A": "Create an S3 Lifecycle rule to transition objects to the S3 Intelligent-Tiering",
      "B": "Store objects in Amazon S3 Glacier. Use S3 Select to provide applications with access to the data. S3",
      "C": "Use data from S3 storage class analysis to create S3 Lifecycle rules to automatically transition objects",
      "D": "Transition objects to the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create an AWS"
    },
    "answer": "A",
    "explanation": "The optimal solution is A. Create an S3 Lifecycle rule to transition objects to the S3 Intelligent-Tiering\nstorage class.\nHere's a detailed justification:\nThe problem requires cost reduction and immediate availability for frequently accessed S3 objects in a data\nlake environment with random access patterns.\nS3 Intelligent-Tiering is designed for scenarios where access patterns are unknown or change over time. It\nautomatically moves data between frequent, infrequent, and archive access tiers based on usage patterns,\noptimizing storage costs without performance impact. This aligns perfectly with the requirement for both cost\nreduction and immediate availability of frequently accessed objects.\nOperationally Efficient: Implementing an S3 Lifecycle rule to transition objects to S3 Intelligent-Tiering is a\nstraightforward and automated process. It requires minimal operational overhead as S3 handles the tiering\nautomatically based on access patterns. No manual intervention or custom code is necessary.\nWhy other options are less suitable:\nB. Store objects in Amazon S3 Glacier. Use S3 Select to provide applications with access to the data. S3\nGlacier is designed for archival storage with infrequent access. While cost-effective, retrieving data from\nGlacier can take hours, violating the requirement for immediate availability. S3 Select can query data in\nGlacier without retrieving the entire object, but it still doesn't provide instant access.\nC. Use data from S3 storage class analysis to create S3 Lifecycle rules to automatically transition objects\nto the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. This option requires analyzing\nstorage access patterns first and then creating lifecycle rules. S3 Standard-IA is suitable for infrequently\naccessed data, but objects accessed frequently won't benefit and will incur retrieval charges if moved there.\nIntelligent-Tiering is more efficient at doing this without requiring analysis and with multiple tiers available.\nD. Transition objects to the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create an AWS\nLambda function to transition objects to the S3 Standard storage class when they are accessed by an\napplication. This approach is complex and introduces unnecessary operational overhead. It requires writing\nand maintaining a Lambda function to monitor object access and transition them back to S3 Standard. S3\nIntelligent-Tiering handles this automatically and more efficiently. Introducing a Lambda function also adds\npotential points of failure and increased cost for Lambda invocations.\nAuthoritative Links for Further Research:\nS3 Intelligent-Tiering: https://aws.amazon.com/s3/storage-classes/intelligent-tiering/\nS3 Lifecycle: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-\nexamples.html\nS3 Storage Classes: https://aws.amazon.com/s3/storage-classes/",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/intelligent-tiering/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-",
      "https://aws.amazon.com/s3/storage-classes/"
    ]
  },
  {
    "question": "CertyIQ\nA company has 5 TB of datasets. The datasets consist of 1 million user profiles and 10 million connections. The user\nprofiles have connections as many-to-many relationships. The company needs a performance efficient way to find\nmutual connections up to five levels.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The most appropriate solution is B: Use Amazon Neptune to store the datasets with edges and vertices, and\nquery the data to find connections. Here's why:\nAmazon Neptune is a fully managed graph database service. Graph databases are specifically designed for\nhandling highly connected data and performing relationship-based queries, such as finding mutual\nconnections. The dataset describeduser profiles and their connectionsperfectly aligns with a graph\ndatabase model where user profiles are vertices (nodes) and connections are edges. Neptune's architecture is\noptimized for traversing relationships, enabling highly efficient searches for mutual connections up to five\nlevels deep. This traversal is significantly faster than using relational databases or data lakes for the same\ntask.\nOptions A and D, which propose using Amazon S3 with Athena or Amazon RDS, are less suitable. While both\ncan store the data, they rely on SQL JOIN operations for finding connections. SQL JOINs become\ncomputationally expensive and slow as the number of JOINs increases, especially when traversing multiple\nlevels of connections. Athena, while useful for querying data in S3, is not optimized for complex relationship\ntraversals like Neptune is. RDS, as a relational database, can also struggle with the performance demands of\nmulti-level relationship queries.\nOption C, using Amazon QuickSight, is primarily a visualization tool. While QuickSight can visualize data from\nvarious sources, it doesn't provide the underlying data storage or efficient querying capabilities needed to\nfind mutual connections. It would rely on a different service to perform the actual connection finding.\nIn summary, Neptune's graph database architecture is inherently designed for the type of relationship-based\nqueries needed in this scenario. The efficiency of graph traversal in Neptune makes it the optimal choice for\nfinding mutual connections up to five levels in a large dataset.\nFurther research:\nAmazon Neptune: https://aws.amazon.com/neptune/\nGraph Databases: https://aws.amazon.com/products/databases/graph/",
    "links": [
      "https://aws.amazon.com/neptune/",
      "https://aws.amazon.com/products/databases/graph/"
    ]
  },
  {
    "question": "CertyIQ\nA company needs a secure connection between its on-premises environment and AWS. This connection does not\nneed high bandwidth and will handle a small amount of traffic. The connection should be set up quickly.\nWhat is the MOST cost-effective method to establish this type of connection?",
    "options": {
      "C": "It's relatively quick to configure and doesn't require dedicated hardware,"
    },
    "answer": "D",
    "explanation": "The most cost-effective solution for a secure, low-bandwidth connection between an on-premises\nenvironment and AWS, which needs to be set up quickly, is an AWS Site-to-Site VPN connection. Here's why:\nAWS Site-to-Site VPN: Establishes an encrypted tunnel over the public internet between your on-premises\nnetwork and your Amazon VPC. It's relatively quick to configure and doesn't require dedicated hardware,\nmaking it suitable for low-bandwidth needs. Pricing is based on VPN connection hours and data transfer out,\nwhich aligns well with infrequent or small traffic volumes.\nAWS Direct Connect: Designed for high-bandwidth, low-latency, and dedicated network connections. While\nsecure, it involves higher costs due to dedicated port fees, cross-connect charges, and longer setup times.\nDirect Connect is overkill for the stated requirements.\nClient VPN: Allows individual users to securely connect to AWS resources. It's designed for remote access\nand not a persistent site-to-site connection, making it unsuitable for connecting an entire on-premises\nnetwork.\nBastion Host on Amazon EC2: Requires launching and managing an EC2 instance, incurring ongoing compute\ncosts. While a bastion host provides secure access, it is primarily designed for accessing resources within the\nVPC and does not establish a secure, site-to-site connection between the on-premises network and AWS. It\nalso adds operational overhead. Site-to-Site VPN inherently establishes an encrypted tunnel without needing\nmanual setup of protocols.\nThe key factor is cost-effectiveness for low bandwidth. Site-to-Site VPN strikes a balance between security\nand cost, utilizing existing internet infrastructure.\nTherefore, the AWS Site-to-Site VPN offers the best combination of security, speed of deployment, and cost-\neffectiveness for the given scenario.\nAuthoritative Links:\nAWS Site-to-Site VPN: https://aws.amazon.com/vpn/site-to-site-vpn/\nAWS Direct Connect: https://aws.amazon.com/directconnect/\nAWS Client VPN: https://aws.amazon.com/vpn/client-vpn/",
    "links": [
      "https://aws.amazon.com/vpn/site-to-site-vpn/",
      "https://aws.amazon.com/directconnect/",
      "https://aws.amazon.com/vpn/client-vpn/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an on-premises SFTP file transfer solution. The company is migrating to the AWS Cloud to scale\nthe file transfer solution and to optimize costs by using Amazon S3. The company's employees will use their\ncredentials for the on-premises Microsoft Active Directory (AD) to access the new solution. The company wants to\nkeep the current authentication and file access mechanisms.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "D": "AD"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it provides a managed, scalable, and secure SFTP solution integrated with\nthe existing on-premises Active Directory (AD) using AWS Transfer Family and AD Connector, minimizing\noperational overhead. AWS Transfer Family is a fully managed service that simplifies secure file transfers into\nand out of Amazon S3. It directly supports SFTP, FTP, and FTPS protocols. Choosing the AWS Directory\nService option as the identity provider within Transfer Family allows for seamless integration with AD. AD\nConnector establishes a secure connection between AWS and the on-premises AD, enabling users to\nauthenticate with their existing AD credentials. This avoids the need to create new user accounts or modify\nexisting ones.\nOption A, using S3 File Gateway, introduces more operational overhead because it requires managing an S3\nFile Gateway appliance and configuring SMB shares. While it supports AD authentication, it's primarily\ndesigned for hybrid cloud file sharing, not as a direct replacement for an SFTP solution.\nOption B involves setting up and managing an Auto Scaling group with EC2 instances running an SFTP server,\nwhich is far more complex and requires significant operational effort for patching, scaling, and security. CPU\nutilization scaling is reactive and might not be the most efficient for handling file transfer workloads.\nOption D is similar to C but lacks the crucial detail of using AD Connector. Simply choosing AWS Directory\nService might not be sufficient to connect to the existing on-premises AD without the necessary connector to\nbridge the gap. AD Connector is specifically designed for this purpose.\nTherefore, option C provides the most straightforward and least operationally intensive method to migrate the\non-premises SFTP solution to AWS while preserving the existing authentication mechanism via AD Connector\nand utilizing the scalable and managed AWS Transfer Family.\nReferences:\nAWS Transfer Family: https://aws.amazon.com/transfer/\nAD Connector: https://aws.amazon.com/directoryservice/ad-connector/\nS3 File Gateway: https://aws.amazon.com/storagegateway/file/",
    "links": [
      "https://aws.amazon.com/transfer/",
      "https://aws.amazon.com/directoryservice/ad-connector/",
      "https://aws.amazon.com/storagegateway/file/"
    ]
  },
  {
    "question": "CertyIQ\nA company is designing an event-driven order processing system. Each order requires multiple validation steps\nafter the order is created. An idempotent AWS Lambda function performs each validation step. Each validation\nstep is independent from the other validation steps. Individual validation steps need only a subset of the order\nevent information.\nThe company wants to ensure that each validation step Lambda function has access to only the information from\nthe order event that the function requires. The components of the order processing system should be loosely\ncoupled to accommodate future business changes.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C because it leverages Amazon EventBridge's event routing and transformation\ncapabilities to meet the specific requirements of the order processing system.\nHere's why:\nLoose Coupling: EventBridge promotes loose coupling by decoupling the order creation process from the\nvalidation steps. The order creation system publishes events to EventBridge without needing to know about\nthe validation functions. Each validation function subscribes to specific events based on rules.\nTargeted Data Delivery: EventBridge's input transformer is the key. It allows you to define how the original\nevent data is transformed before it's passed to the target Lambda function. This ensures that each validation\nfunction receives only the required subset of order information, enhancing security and reducing processing\noverhead.\nEvent-Driven Architecture: The solution aligns with an event-driven architecture, where validation steps are\ntriggered by the occurrence of an order event, making the system responsive and scalable.\nIdempotency: While all options could potentially be implemented to accommodate idempotent Lambda\nfunctions, EventBridge provides the cleanest separation of concerns and transformation capabilities that\nalign with the other requirements.\nWhy other options are less suitable:\nA (SQS with transformation Lambda): This introduces an extra Lambda function to transform data, which\nadds complexity and potential points of failure. While SQS provides reliable message queuing, it doesn't offer\nthe same level of filtering and transformation as EventBridge. The tight coupling between the transformation\nLambda and SQS queues makes it less flexible.\nB (SNS with message filtering): While SNS can fan out messages to multiple subscribers, its message\nfiltering capabilities are more limited compared to EventBridge's input transformer. SNS filtering primarily\nworks on message attributes, not on the message body content, making it less suitable for extracting specific\ndata subsets.\nD (SQS with parallel invocation Lambda): This approach creates significant complexity. Managing parallel\nLambda invocations introduces challenges around error handling, concurrency, and resource management.\nThe synchronous invocation also reduces the benefits of the event-driven nature of the system.\nAuthoritative Links:\nAmazon EventBridge: https://aws.amazon.com/eventbridge/\nEventBridge Input Transformer:\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/event_transformer.html\nEvent-Driven Architecture: https://aws.amazon.com/event-driven-architecture/",
    "links": [
      "https://aws.amazon.com/eventbridge/",
      "https://docs.aws.amazon.com/eventbridge/latest/userguide/event_transformer.html",
      "https://aws.amazon.com/event-driven-architecture/"
    ]
  },
  {
    "question": "CertyIQ\nA company is migrating a three-tier application to AWS. The application requires a MySQL database. In the past,\nthe application users reported poor application performance when creating new entries. These performance issues\nwere caused by users generating different real-time reports from the application during working hours.\nWhich solution will improve the performance of the application when it is moved to AWS?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C because it addresses the root cause of the performance issue: report generation\nimpacting the main database's performance.\nHere's a detailed justification:\nUnderstanding the Problem: The users experienced poor application performance due to real-time report\ngeneration during business hours, overloading the database.\nWhy Option A is Incorrect: DynamoDB, while suitable for many use cases, would require significant\napplication refactoring to support the existing MySQL database structure and reporting requirements. This\nisn't an optimal solution for a simple migration.\nWhy Option B is Incorrect: Simply increasing the compute resources on an EC2 instance hosting the MySQL\ndatabase may alleviate the problem slightly, but it doesn't isolate report queries from affecting transactional\nperformance. The reports will still contend for resources with the main application workload on the same\ndatabase instance.\nWhy Option C is Correct: Amazon Aurora MySQL Multi-AZ provides high availability and performance. More\nimportantly, read replicas allow you to offload read-intensive workloads, such as report generation, from the\nprimary database instance. This isolation prevents report queries from impacting the performance of the\nprimary database used for transactions. Configuring the application to use the reader endpoint for report\ngeneration ensures these queries are directed to the read replicas, not the primary instance. This is a standard\npractice for optimizing read-heavy workloads in relational databases.\nMulti-AZ: Ensures high availability.\nRead Replicas: Offload read traffic (reports), isolating the primary database for transactional workloads.\nReader Endpoint: Provides a single point of access for all read replicas.\nWhy Option D is Incorrect: The backup instance of an Aurora cluster is not intended to be used for serving\nread traffic. It's primarily for failover and point-in-time recovery. Using it for report generation could interfere\nwith its primary purpose and potentially impact recovery time if a failover is needed. Furthermore, backup\ninstances might not be consistently up-to-date and ready to serve read requests in the way a dedicated read\nreplica is.\nIn summary: Option C provides a cost-effective and efficient solution by using Aurora's read replicas to\noffload report generation, thereby isolating the read workload from the primary database and improving\noverall application performance during peak hours.\nAuthoritative Links:\nAmazon Aurora Read Replicas:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.ReadReplicas.html\nAmazon Aurora Multi-AZ Deployments:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.MultiAZ.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.ReadReplicas.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.MultiAZ.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is expanding a secure on-premises network to the AWS Cloud by using an AWS Direct Connect\nconnection. The on-premises network has no direct internet access. An application that runs on the on-premises\nnetwork needs to use an Amazon S3 bucket.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "C",
    "explanation": "The most cost-effective solution for accessing an S3 bucket from an on-premises network with no direct\ninternet access via AWS Direct Connect is creating a VPC and an Amazon S3 interface endpoint.\nHere's why:\nInterface Endpoints (AWS PrivateLink): Interface endpoints, powered by AWS PrivateLink, provide private\nconnectivity to AWS services, including S3, without exposing traffic to the public internet. This ensures\nsecure communication over your Direct Connect connection.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\nCost-Effectiveness: Interface endpoints are generally more cost-effective than NAT gateways for accessing\nS3, especially when considering the data transfer charges.\nSecurity: Interface endpoints keep traffic within the AWS network and the Direct Connect connection,\navoiding exposure to the public internet, which aligns with the requirement for a secure connection.\nLet's analyze why the other options are less suitable:\nPublic VIF: Using a public VIF would route traffic over the public internet, which contradicts the requirement\nto avoid direct internet access.\nNAT Gateway: NAT gateways provide internet access to instances within a private subnet. While you could\nroute traffic through a NAT gateway in a VPC connected via Direct Connect, this is intended for instances\nrequiring broader internet access, not solely for accessing S3. It also incurs data processing charges for the\nNAT gateway itself, adding to the cost.\nVPC Peering: VPC peering allows you to connect two VPCs, but it doesn't directly solve the problem of\naccessing S3 from on-premises without internet access. You would still need a way to reach the S3 service\nfrom the peered VPC, potentially using a NAT gateway, making it less optimal than an interface endpoint.\nIn summary, an S3 interface endpoint provides a secure, private, and cost-effective solution by allowing direct\naccess to S3 from the on-premises network through the Direct Connect connection, without traversing the\npublic internet.",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html"
    ]
  },
  {
    "question": "CertyIQ\nA company serves its website by using an Auto Scaling group of Amazon EC2 instances in a single AWS Region.\nThe website does not require a database.\nThe company is expanding, and the company's engineering team deploys the website to a second Region. The\ncompany wants to distribute traffic across both Regions to accommodate growth and for disaster recovery\npurposes. The solution should not serve traffic from a Region in which the website is unhealthy.\nWhich policy or resource should the company use to meet these requirements?",
    "options": {
      "B": "An Amazon Route 53 multivalue answer routing policy. Here's why:",
      "A": "An Amazon Route 53 simple routing policy: Simple routing policy doesn't inherently support health",
      "C": "An Application Load Balancer in one Region with a target group that specifies the EC2 instance IDs from",
      "D": "An Application Load Balancer in one Region with a target group that specifies the IP addresses of the"
    },
    "answer": "B",
    "explanation": "The correct answer is B. An Amazon Route 53 multivalue answer routing policy. Here's why:\nDisaster Recovery and High Availability: The company requires a disaster recovery solution that\nautomatically avoids unhealthy Regions. Route 53 multivalue answer routing is designed to distribute traffic\nto multiple healthy endpoints (in this case, the different Regions hosting the website).\nHealth Checks: Route 53 can be configured with health checks for each endpoint. If a Region's health check\nfails, Route 53 will automatically stop routing traffic to that Region, ensuring users are only served by healthy\nwebsite instances.\nMulti-Region Distribution: Multivalue answer routing allows Route 53 to return multiple IP addresses in\nresponse to a DNS query. Clients will randomly choose one of these addresses to connect to, effectively\ndistributing traffic across the healthy Regions.\nSimplicity: Compared to more complex solutions, Route 53 multivalue answer routing provides a relatively\nstraightforward setup for multi-Region traffic distribution and failover.\nLet's examine why the other options are less suitable:\nA. An Amazon Route 53 simple routing policy: Simple routing policy doesn't inherently support health\nchecks or automatic failover. It only returns a single IP address, making it unsuitable for distributing traffic\nacross Regions and handling failures.\nC. An Application Load Balancer in one Region with a target group that specifies the EC2 instance IDs from\nboth Regions: ALBs are regional resources. While cross-zone load balancing is possible within a Region, ALBs\ncannot directly manage EC2 instances in another Region using instance IDs within a single target group.\nFurthermore, if the entire Region where the ALB is located fails, the entire solution becomes unavailable.\nD. An Application Load Balancer in one Region with a target group that specifies the IP addresses of the\nEC2 instances from both Regions: While technically possible to add IPs from a different Region, this is not\nthe intended use case for ALB target groups and introduces complexities with managing and updating the IPs,\nespecially with dynamic scaling. More importantly, a single ALB in one region isn't ideal for cross-region DR\nand high availability as failure of that region will impact the entire solution. This also violates network\nboundaries.In conclusion, Route 53 multivalue answer routing, combined with health checks, provides the\nsimplest and most effective way to distribute traffic across multiple Regions, ensure high availability, and\nautomatically fail over from unhealthy Regions, fulfilling the company's requirements.\nSupporting Links:\nAmazon Route 53 Routing Policies: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-\npolicy.html\nRoute 53 Health Checks: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-\ncreating-deleting.html",
    "links": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-"
    ]
  },
  {
    "question": "CertyIQ\nA company runs its applications on Amazon EC2 instances that are backed by Amazon Elastic Block Store (Amazon\nEBS). The EC2 instances run the most recent Amazon Linux release. The applications are experiencing availability\nissues when the company's employees store and retrieve files that are 25 GB or larger. The company needs a\nsolution that does not require the company to transfer files between EC2 instances. The files must be available\nacross many EC2 instances and across multiple Availability Zones.\nWhich solution will meet these requirements?",
    "options": {
      "C": "Mount an Amazon Elastic File System (Amazon EFS) file system across all the EC2",
      "A": "Amazon S3: While S3 is excellent for object storage, it might not be the ideal solution if the employees",
      "B": "EBS Snapshot: EBS snapshots are point-in-time copies of EBS volumes. Mounting a snapshot as a new EBS",
      "D": "AMI with Instance Store: Instance store volumes are ephemeral, meaning they lose their data when the"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Mount an Amazon Elastic File System (Amazon EFS) file system across all the EC2\ninstances. Instruct the employees to access the files from the EC2 instances.\nHere's why:\nRequirement for Availability Across EC2 Instances and AZs: The problem specifies that files need to be\navailable across many EC2 instances and multiple Availability Zones. Amazon EFS is designed specifically for\nthis purpose. It provides a scalable, elastic, and fully managed file system service. EFS file systems can be\nmounted concurrently by multiple EC2 instances in multiple AZs within an AWS Region. This satisfies the\navailability requirement effectively.\nNo File Transfer Between EC2 Instances: Using EFS eliminates the need to transfer files between EC2\ninstances. All instances access the same central file system. When an employee saves a file to the EFS mount\npoint on one instance, the file is immediately accessible from all other instances with the EFS file system\nmounted.\nHandling Large Files: EFS is designed to handle large files efficiently. It scales automatically as data is added\nor removed, so it can easily accommodate 25 GB files without performance degradation.\nWhy Other Options Are Incorrect:\nA. Amazon S3: While S3 is excellent for object storage, it might not be the ideal solution if the employees\nrequire a traditional file system interface. Using S3 would require code or applications to be aware of S3 APIs\nand interact with the storage through HTTP, which is not a natural file system interaction. Though compatible\nwith large files, it introduces more complexity than a file system.\nB. EBS Snapshot: EBS snapshots are point-in-time copies of EBS volumes. Mounting a snapshot as a new EBS\nvolume would create a separate, isolated copy of the data. This does not provide a shared file system that\nupdates across all EC2 instances, meaning the instances aren't working with one singular filesystem.\nD. AMI with Instance Store: Instance store volumes are ephemeral, meaning they lose their data when the\ninstance is stopped or terminated. This contradicts the need for persistent storage and availability.\nFurthermore, the AMI does not create a system where the files are available to all running instances after the\nAMI is launched.\nIn summary, EFS offers the best balance of features, satisfying the requirements of shared access across EC2\ninstances and Availability Zones, eliminating the need for file transfers, and supporting the large file sizes\nmentioned in the problem.\nAuthoritative Links:\nAmazon EFS: https://aws.amazon.com/efs/\nAmazon EC2 Instance Store: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\nAmazon EBS Snapshots: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html\nAmazon S3: https://aws.amazon.com/s3/",
    "links": [
      "https://aws.amazon.com/efs/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html",
      "https://aws.amazon.com/s3/"
    ]
  },
  {
    "question": "CertyIQ\nA company is running a highly sensitive application on Amazon EC2 backed by an Amazon RDS database.\nCompliance regulations mandate that all personally identifiable information (PII) be encrypted at rest.\nWhich solution should a solutions architect recommend to meet this requirement with the LEAST amount of\nchanges to the infrastructure?",
    "options": {
      "C": "It mentions EBS encryption and RDS encryption with KMS keys."
    },
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the best solution, adhering to the prompt's constraints:\nThe requirement is to encrypt PII at rest for both the EC2 instance and RDS database, minimizing\ninfrastructure changes. Option C, configuring SSL encryption using AWS Key Management Service (AWS\nKMS) keys, directly addresses this. RDS already offers encryption at rest using KMS keys. Enabling this\nfeature is straightforward and requires minimal modification to the existing RDS instance. Furthermore, the\nEC2 instance's connection to RDS can be secured using SSL/TLS encryption, protecting data in transit as well.\nOption A is incorrect because AWS Certificate Manager (ACM) primarily manages SSL/TLS certificates for\nsecuring network connections. While ACM certificates are crucial for encrypting data in transit, they don't\ndirectly encrypt the underlying database or EC2 volumes at rest.\nOption B, deploying AWS CloudHSM, is overly complex for this scenario. CloudHSM provides dedicated\nhardware security modules for key management, offering higher levels of security compliance but at a\nsignificantly increased cost and operational overhead. It's generally reserved for organizations with stringent\nregulatory requirements beyond basic encryption at rest. CloudHSM introduces more infrastructure changes\nand administrative burden than necessary.\nOption D is also valid, but less ideal than C. It mentions EBS encryption and RDS encryption with KMS keys.\nWhile EBS encryption is crucial for the EC2 instance itself, the focus of the question is encrypting the RDS\ndatabase containing the PII. Option C encapsulates the core requirement for RDS encryption while also\nproviding a mechanism (SSL/TLS) to protect the EC2 instance's data in transit to RDS. Option D implies\nadditional EBS volume management, making it more complex compared to configuring SSL and RDS\nencryption at rest.\nTherefore, Option C offers the most efficient and least disruptive path to encrypting sensitive data at rest for\nthe RDS database and securing the connection from the EC2 instance, all while leveraging the existing AWS\nKMS service. It balances security with ease of implementation, fulfilling the core requirement with minimal\nalterations to the infrastructure.\nSupporting links:\nRDS Encryption: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\nAWS KMS: https://aws.amazon.com/kms/\nSSL/TLS with RDS: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html",
      "https://aws.amazon.com/kms/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an AWS Lambda function in private subnets in a VP",
    "options": {
      "C": "Provision a gateway endpoint for Amazon S3 in the VPC. Update the route tables of",
      "A": "Replace the EC2 NAT instance with an AWS managed NAT gateway: While a NAT Gateway is a better",
      "B": "Increase the size of the EC2 NAT instance in the VPC to a network optimized instance type: While",
      "D": "Provision a transit gateway. Place transit gateway attachments in the private subnets where the Lambda"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Provision a gateway endpoint for Amazon S3 in the VPC. Update the route tables of\nthe subnets accordingly.\nHere's why:\nProblem: The Lambda function in private subnets is experiencing timeouts due to network congestion on the\nEC2 NAT instance when uploading data to S3. The goal is to access S3 without going over the internet,\nthereby alleviating the strain on the NAT instance.\nSolution C: Gateway Endpoint\nMechanism: A gateway endpoint for S3 provides a direct, private path from your VPC to S3, completely\nbypassing the internet. It is highly available and scales with the traffic demand.\nImplementation: Creating a gateway endpoint involves selecting the S3 service and the route tables you wish\nto associate with it. Once the endpoint is created, you need to update the route tables for the subnets where\nthe Lambda function resides to route traffic destined for S3 through the gateway endpoint.\nBenefits:\nAvoids Internet: Keeps traffic within the AWS network, improving security and reducing latency.\nScalability: AWS handles the scalability and availability of the endpoint.\nCost-Effective: Gateway endpoints are free to use; you are only charged for the data stored in and\ntransferred out of S3.\nWhy other options are incorrect:\nA. Replace the EC2 NAT instance with an AWS managed NAT gateway: While a NAT Gateway is a better\nsolution than a NAT Instance, because it is managed and highly available, it does not meet the requirement of\navoiding the internet. The Lambda function would still need to route all traffic through the NAT Gateway to\naccess S3.\nB. Increase the size of the EC2 NAT instance in the VPC to a network optimized instance type: While\nupgrading the EC2 NAT instance may reduce congestion temporarily, it doesn't eliminate the internet\ndependency or the bottleneck of the NAT instance itself. This is not as effective as providing a direct\nconnection using a gateway endpoint.\nD. Provision a transit gateway. Place transit gateway attachments in the private subnets where the Lambda\nfunction is running: A transit gateway is an option for connecting multiple VPCs and on-premises networks\nbut is overkill and more expensive for the simple use case of providing access to S3 from private subnets. It\ndoes not solve the primary issue of reducing internet traffic because routing traffic to S3 through a transit\ngateway still involves traffic going through the transit gateway's route table to the S3 service, which isn't\nnecessarily a private path without additional configurations like VPC endpoints.\nRelevant concepts and resources:\nVPC Endpoints: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\nGateway Endpoints: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html\nNAT Gateway vs NAT Instance: https://aws.amazon.com/premiumsupport/knowledge-center/vpc-nat-\ngateway-instance/\nIn summary, a gateway endpoint offers the most efficient and cost-effective solution to provide private,\nscalable, and reliable access to S3 for the Lambda function without traversing the internet.",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/vpc-nat-"
    ]
  },
  {
    "question": "CertyIQ\nA news company that has reporters all over the world is hosting its broadcast system on AWS. The reporters send\nlive broadcasts to the broadcast system. The reporters use software on their phones to send live streams through\nthe Real Time Messaging Protocol (RTMP).\nA solutions architect must design a solution that gives the reporters the ability to send the highest quality streams.\nThe solution must provide accelerated TCP connections back to the broadcast system.\nWhat should the solutions architect use to meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "AWS Global Accelerator is the optimal solution because it's designed to optimize TCP and UDP traffic, making\nit ideal for real-time media streaming using RTMP. It uses the AWS global network to route traffic to the\nnearest healthy application endpoint, reducing latency and improving connection reliability, crucial for live\nbroadcasts from geographically dispersed reporters. CloudFront, while a CDN, is primarily for caching static\nand dynamic content, and doesn't optimize live streaming ingest in the same way. AWS Client VPN is for\nsecure access to AWS resources and isn't designed for real-time streaming optimization. EC2 instances with\nElastic IP addresses lack the global network optimization capabilities needed for accelerated TCP\nconnections. Global Accelerator's Anycast static IPs provide a single entry point for reporters, simplifying the\nconnection process and facilitating seamless failover. By leveraging the AWS global network backbone,\nGlobal Accelerator minimizes jitter and packet loss, leading to higher quality streams. Therefore, option B\ndirectly addresses the need for accelerated TCP connections and highest quality streams for live broadcasts\nfrom a global network of reporters.\nHere are authoritative links for further research:\nAWS Global Accelerator: https://aws.amazon.com/global-accelerator/\nAWS Global Accelerator FAQs: https://aws.amazon.com/global-accelerator/faqs/",
    "links": [
      "https://aws.amazon.com/global-accelerator/",
      "https://aws.amazon.com/global-accelerator/faqs/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) to run its self-managed\ndatabase. The company has 350 TB of data spread across all EBS volumes. The company takes daily EBS\nsnapshots and keeps the snapshots for 1 month. The daily change rate is 5% of the EBS volumes.\nBecause of new regulations, the company needs to keep the monthly snapshots for 7 years. The company needs to\nchange its backup strategy to comply with the new regulations and to ensure that data is available with minimal\nadministrative effort.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the most cost-effective solution, along with supporting\ninformation and links:\nThe core requirement is to retain monthly snapshots of EBS volumes for 7 years with minimal administrative\noverhead and cost-effectiveness.\nOption B is the most suitable because it leverages the EBS Snapshots Archive tier, which is specifically\ndesigned for long-term storage of snapshots that are infrequently accessed. This archive tier offers\nsignificantly lower storage costs compared to the standard EBS snapshot tier, making it the most cost-\neffective option for the 7-year retention requirement. The solution continues the existing daily snapshot\npolicy for one month, ensuring fast recovery if needed, then moves the monthly snapshot to the archive tier\nfor long-term compliance. This provides a balance between operational restore needs and regulatory\ndemands.\nOption A involves moving the monthly snapshot to Amazon S3 Glacier Deep Archive. While Glacier Deep\nArchive offers very low storage costs, restoring data from Glacier is time-consuming and could impact\nrecovery time objectives (RTO). EBS Snapshots Archive is designed for EBS data, allowing for potentially\nfaster and easier restores than Glacier when necessary. Furthermore, using S3 would require additional\nconfiguration for managing the backup process, increasing administrative overhead.\nOption C keeps the monthly snapshots in the standard EBS snapshot tier for 7 years. This option is the most\nexpensive because the standard tier is designed for frequently accessed snapshots, not long-term archival.\nStoring snapshots in the standard tier for 7 years would incur significantly higher storage costs compared to\nusing the archive tier.\nOption D uses EBS Direct APIs and stores the snapshots in Amazon S3 Infrequent Access (IA). While S3 IA is\ncheaper than standard S3, it is still more expensive than EBS Snapshots Archive. Moreover, using EBS Direct\nAPIs would require developing and maintaining custom code for managing snapshots, increasing\nadministrative overhead. EBS Direct APIs are useful in niche cases, but here, it is an unnecessary\ncomplication.\nBy utilizing the EBS Snapshots Archive tier, option B achieves the required 7-year retention period at the\nlowest possible cost while minimizing administrative effort. It also integrates seamlessly with the existing\nEBS snapshot policy, minimizing disruption to the current backup process.\nKey Concepts & Links:\nEBS Snapshots Archive: A low-cost storage tier within EBS Snapshots, ideal for long-term retention and\ncompliance. https://aws.amazon.com/ebs/snapshots/archive/\nEBS Snapshots: Point-in-time backups of EBS volumes. https://aws.amazon.com/ebs/snapshots/\nCost Optimization: Selecting the right storage tier based on access frequency to minimize costs.\nGlacier Deep Archive: A low-cost storage service for archiving data that is rarely accessed.\nhttps://aws.amazon.com/glacier/deep-archive/",
    "links": [
      "https://aws.amazon.com/ebs/snapshots/archive/",
      "https://aws.amazon.com/ebs/snapshots/",
      "https://aws.amazon.com/glacier/deep-archive/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an application on several Amazon EC2 instances that store persistent data on an Amazon Elastic\nFile System (Amazon EFS) file system. The company needs to replicate the data to another AWS Region by using\nan AWS managed service solution.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "A",
    "explanation": "The question asks for the most cost-effective AWS-managed solution to replicate data from an EFS file\nsystem to another Region.\nOption A, using the EFS-to-EFS backup solution, is the most cost-effective because it leverages a managed\nservice specifically designed for replicating EFS data across regions. This eliminates the need for custom\nscripting or infrastructure management. EFS Replication (part of EFS Backup and Recovery) automates the\nprocess, handling incremental updates and data consistency.\nOption B involves copying data to S3 and then using S3 Cross-Region Replication. This requires additional S3\nstorage costs and the initial overhead of transferring data to S3. While S3 replication is efficient, the initial\nstep of copying to S3 adds unnecessary cost and complexity compared to the direct EFS-to-EFS replication.\nOption C, creating a VPC peer and using rsync, necessitates managing the rsync process and the cross-region\nVPC peering connection. It requires manual configuration, monitoring, and potential troubleshooting. This\nadds operational overhead and costs associated with maintaining the infrastructure and the script. It is less\nmanaged and more prone to failure than a dedicated EFS replication service.\nOption D, using AWS Backup, is also a viable solution, but it primarily focuses on data backup and recovery,\nnot continuous replication. AWS Backup creates point-in-time snapshots, which is valuable for disaster\nrecovery, but may not offer the same level of continuous replication as EFS Replication. Moreover, frequent\nbackups can lead to increased storage costs. EFS-to-EFS replication is optimized for replicating data, making\nit more cost-effective in this scenario.\nTherefore, EFS-to-EFS replication is the most cost-effective and managed solution for replicating data\nbetween EFS file systems across regions.\nRefer to these links for more information:\nAmazon EFS Replication: Backup and recovery",
    "links": []
  },
  {
    "question": "CertyIQ\nAn ecommerce company is migrating its on-premises workload to the AWS Cloud. The workload currently consists\nof a web application and a backend Microsoft SQL database for storage.\nThe company expects a high volume of customers during a promotional event. The new infrastructure in the AWS\nCloud must be highly available and scalable.\nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": {},
    "answer": "C",
    "explanation": "The best solution for a highly available and scalable e-commerce application with minimal administrative\noverhead involves leveraging managed services like Amazon RDS with Multi-AZ deployment and EC2\ninstances behind an Auto Scaling group and Application Load Balancer.\nOption C is correct because it suggests using EC2 instances in an Auto Scaling group across multiple\nAvailability Zones (AZs) behind an Application Load Balancer (ALB). The Auto Scaling group ensures the web\napplication can handle increased traffic by automatically scaling the number of instances based on demand.\nThe ALB distributes traffic across these instances, providing high availability.\nFor the database, it proposes migrating to Amazon RDS with Multi-AZ deployment. RDS Multi-AZ provides\nhigh availability by synchronously replicating data to a standby instance in a different AZ. In case of a failure\nin the primary AZ, RDS automatically fails over to the standby instance, minimizing downtime. RDS handles\nmuch of the administrative overhead associated with managing a database, such as patching, backups, and\nfailover.\nOption A is less ideal because using read replicas for the database is not the most straightforward way to\nachieve high availability for the write operations required by the e-commerce application. Read replicas\nprimarily improve read performance and are not designed for automatic failover in the same way as Multi-AZ.\nOption B is flawed because using EC2 instances across separate AWS Regions for the database adds\nsignificant complexity to database replication and management, increasing administrative overhead. Region-\nbased failover is more complex than AZ-based failover.\nOption D involves managing SQL Server on EC2 instances across multiple AZs which requires manual setup\nand management of database replication, backups, patching, and failover. This introduces significant\nadministrative overhead, making it unsuitable for the \"least administrative overhead\" requirement.\nIn summary, option C achieves high availability and scalability with minimal administrative overhead by\nutilizing managed services like Auto Scaling, ALB, and RDS Multi-AZ.\nRelevant Links:\nAmazon RDS Multi-AZ\nAmazon EC2 Auto Scaling\nApplication Load Balancer",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has an on-premises business application that generates hundreds of files each day. These files are\nstored on an SMB file share and require a low-latency connection to the application servers. A new company policy\nstates all application-generated files must be copied to AWS. There is already a VPN connection to AWS.\nThe application development team does not have time to make the necessary code modifications to move the\napplication to AWS.\nWhich service should a solutions architect recommend to allow the application to copy files to AWS?",
    "options": {
      "D": "AWS Storage Gateway. Here's why:"
    },
    "answer": "D",
    "explanation": "The correct answer is D. AWS Storage Gateway. Here's why:\nAWS Storage Gateway facilitates connecting on-premises applications to AWS storage services. Specifically,\nthe File Gateway configuration allows you to store files as objects in Amazon S3 while maintaining SMB\naccess from the on-premises application. Because the question states that the application development team\nhas no time to make modifications to the code, a solution that requires them to implement the copy/move files\n(such as EFS) isn't a suitable solution.\nStorage Gateway's File Gateway addresses the key requirement of copying files to AWS without modifying\nthe application code. The application continues to write files to the SMB file share, and the File Gateway\nseamlessly uploads them to S3 in the background.\nAmazon EFS (A) is a fully managed NFS file system for use with AWS cloud services and on-premises\nresources. While EFS can be mounted on-premises with Direct Connect or VPN, it would require application\ncode changes to write directly to the EFS mount point instead of the existing SMB share. This option is ruled\nout by the question's constraints.\nAmazon FSx for Windows File Server (B) provides fully managed, native Microsoft Windows file systems. It\ncould be used, but it necessitates a more complex migration strategy, which is not needed for simply copying\nthe files to AWS for archival. It's typically used for migrating entire Windows file servers to the cloud.\nAWS Snowball (C) is suitable for large-scale data migration, but it's a one-time transfer solution. The question\nstates the application generates hundreds of files daily, indicating an ongoing need for replication, rendering\nSnowball impractical for this use case.\nTherefore, AWS Storage Gateway - File Gateway is the best option because it provides the needed bridge\nbetween the on-premises SMB file share and S3, automatically copies files, and requires no application code\nchanges.\nFor further research:\nAWS Storage Gateway: https://aws.amazon.com/storagegateway/\nFile Gateway documentation:\nhttps://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html",
    "links": [
      "https://aws.amazon.com/storagegateway/",
      "https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has 15 employees. The company stores employee start dates in an Amazon DynamoDB table. The\ncompany wants to send an email message to each employee on the day of the employee's work anniversary.\nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": {},
    "answer": "C",
    "explanation": "The most operationally efficient solution is to use AWS Lambda and Amazon SNS (Option C). Here's why:\nServerless Architecture: Lambda offers a serverless environment, eliminating the need to manage EC2\ninstances (as in Options A and B). This reduces operational overhead related to patching, scaling, and\ninfrastructure maintenance.\nScheduled Execution: Lambda functions can be scheduled to run using Amazon CloudWatch Events (now\nAmazon EventBridge), providing a built-in mechanism for daily execution without relying on cron jobs on EC2\ninstances.\nScalability and Cost-Effectiveness: Lambda automatically scales based on demand. For only 15 employees,\nthe cost of running Lambda will likely be significantly lower than maintaining a dedicated EC2 instance. You\nonly pay for the compute time you consume.\nDirect Email Integration: SNS can directly send email messages, simplifying the workflow. SQS (Options B\nand D) introduces an unnecessary queue when a direct notification mechanism (SNS) is available. SQS is\nbeneficial when decoupling components and managing message delivery, but that's not a clear requirement\nhere.\nDynamoDB Scan Efficiency: Although scanning a DynamoDB table isn't ideal for larger datasets, with only 15\nrecords, the scan operation will be relatively quick and cost-effective. Consider optimizing DynamoDB query if\nthe employee count significantly increases in the future.\nReduced Complexity: Combining Lambda, CloudWatch Events, and SNS results in a simpler and easier-to-\nmanage architecture compared to solutions involving EC2 instances and cron jobs.\nIn Summary: Option C leverages serverless technologies to minimize operational overhead, automate the\nanniversary notification process, and optimize costs for a small number of employees.\nAuthoritative Links:\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon SNS: https://aws.amazon.com/sns/\nAmazon EventBridge: https://aws.amazon.com/eventbridge/\nDynamoDB Scan: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html",
    "links": [
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/sns/",
      "https://aws.amazon.com/eventbridge/",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html"
    ]
  },
  {
    "question": "CertyIQ\nA companys application is running on Amazon EC2 instances within an Auto Scaling group behind an Elastic Load\nBalancing (ELB) load balancer. Based on the application's history, the company anticipates a spike in traffic during\na holiday each year. A solutions architect must design a strategy to ensure that the Auto Scaling group proactively\nincreases capacity to minimize any performance impact on application users.\nWhich solution will meet these requirements?",
    "options": {
      "B": "Create a recurring scheduled action to scale up the Auto Scaling group before the"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Create a recurring scheduled action to scale up the Auto Scaling group before the\nexpected period of peak demand.\nHere's a detailed justification:\nThe requirement is to proactively increase capacity before the expected traffic spike to minimize performance\nimpact. Scheduled scaling is the ideal solution for this because the company knows when the traffic increase\nwill occur (during the holiday). This allows them to schedule an Auto Scaling group size increase ahead of\ntime.\nOption B directly addresses proactive scaling. Scheduled actions allow you to configure the Auto Scaling\ngroup to increase its desired capacity at a specific time and date, or on a recurring schedule. This means EC2\ninstances will be launched and ready to serve traffic before the actual spike begins.\nOption A (CloudWatch alarm based on CPU utilization) is a reactive scaling method. While useful, it only scales\nup after the CPU utilization reaches 90%. This means users may experience performance issues before the\nscaling happens. This is not proactive, and it doesn't address the need to prepare before the traffic spike.\nOption C (Increasing minimum and maximum instances) only sets the boundaries for scaling, it doesn't\nactually trigger the scaling event. It is possible the group will not scale up to the maximum specified and stay\nbetween the old limits. Also, changing the minimum number of instances alone does not guarantee instances\nwill be ready to handle peak load proactively. You would still need a mechanism (like scheduled actions or\nalarms) to trigger scaling.\nOption D (SNS notification of instance launch events) is a monitoring mechanism, not a scaling strategy. It\ndoesn't trigger scaling; it just provides notification about events that have already occurred. This is useful for\nauditing and troubleshooting but doesn't solve the proactive scaling requirement.\nTherefore, scheduled scaling (Option B) is the only solution that proactively increases the capacity of the Auto\nScaling group before the holiday traffic spike, ensuring minimal performance impact on users.\nFor further research, refer to the AWS documentation on scheduled scaling for Auto Scaling groups:\nAWS Documentation - Scheduled Scaling",
    "links": []
  },
  {
    "question": "CertyIQ\nA company uses Amazon RDS for PostgreSQL databases for its data tier. The company must implement password\nrotation for the databases.\nWhich solution meets this requirement with the LEAST operational overhead?",
    "options": {},
    "answer": "A",
    "explanation": "The most efficient solution for password rotation in Amazon RDS for PostgreSQL, minimizing operational\noverhead, is to utilize AWS Secrets Manager with automatic rotation enabled.\nHere's why:\nAWS Secrets Manager is specifically designed for managing secrets like database credentials. It offers built-\nin functionality for rotating these secrets automatically. This eliminates the need for custom scripting or\ncomplex configurations.\nAutomatic Rotation: Secrets Manager's automatic rotation feature directly integrates with RDS for\nPostgreSQL. You configure a rotation schedule, and Secrets Manager handles the process of generating a\nnew password, updating it in the database, and updating the stored secret, all without manual intervention.\nReduced Operational Overhead: This approach drastically reduces the operational burden. You avoid writing\nand maintaining custom Lambda functions (as in option C) or manually rotating passwords.\nSecurity Best Practices: Secrets Manager adheres to security best practices by encrypting secrets at rest\nand in transit.\nParameter Store (Option B and C): While Systems Manager Parameter Store can store secrets, it lacks native\nautomatic rotation capabilities for RDS databases. Option C requires additional coding and maintenance of a\nLambda function.\nAWS KMS (Option D): KMS is for encrypting data, not managing secrets and rotating them automatically. It's\nnot designed for password management.\nCost-Effectiveness: Secrets Manager's pricing is based on the number of secrets stored and API calls. The\nminimal operational effort often translates to lower overall costs compared to building and maintaining\ncustom solutions.\nIn contrast, options B, C, and D involve manual configuration, scripting, or using services not directly intended\nfor secret rotation, leading to higher operational overhead and potential for errors. By leveraging the built-in\nfeatures of AWS Secrets Manager, the company can achieve secure and automated password rotation for its\nRDS for PostgreSQL databases with minimal administrative effort.\nAuthoritative Links:\nAWS Secrets Manager: https://aws.amazon.com/secrets-manager/\nRotating Secrets: https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html\nRotating Secrets for Amazon RDS for PostgreSQL databases\nhttps://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets-rds.html",
    "links": [
      "https://aws.amazon.com/secrets-manager/",
      "https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html",
      "https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets-rds.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs its application on Oracle Database Enterprise Edition. The company needs to migrate the\napplication and the database to AWS. The company can use the Bring Your Own License (BYOL) model while\nmigrating to AWS. The application uses third-party database features that require privileged access.\nA solutions architect must design a solution for the database migration.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B because it directly addresses all the requirements in the most cost-effective manner.\nThe company wants to use the BYOL model for Oracle, and both RDS for Oracle and RDS Custom for Oracle\nsupport this. However, the key differentiator is the need for privileged access to support third-party database\nfeatures.\nAmazon RDS for Oracle (option A) is a managed service, which means AWS handles the underlying\ninfrastructure and operating system. While convenient, this limits the level of control and access a user has.\nSpecifically, privileged access required for the third-party features is not available in standard RDS Oracle\ninstances. Replacing these features with AWS Lambda would involve significant application refactoring,\nincreasing cost and complexity.\nAmazon DynamoDB (option C) is a NoSQL database, and migrating an Oracle database using DMS would\nnecessitate a complete application rewrite, making it a complex and costly solution. It doesn't support BYOL\nfor Oracle licenses.\nAmazon RDS for PostgreSQL (option D) would also require a database migration using DMS and extensive\ncode refactoring to remove the Oracle-specific third-party features. This is a significant effort and cost. It\ndoesn't support BYOL for Oracle licenses.\nAmazon RDS Custom for Oracle (option B) is designed to provide database administrators with operating\nsystem and database access. It allows for customized settings, including the ability to install and manage\nthird-party features that require privileged access. Because it also supports BYOL, this option allows the\ncompany to maintain its licensing model and meets its technical requirements without costly refactoring or a\ncomplete migration to a different database engine. This makes it the most cost-effective solution.\nSupporting documentation:\nAmazon RDS Custom: https://aws.amazon.com/rds/custom/ - Explains the benefits of customization for\nmeeting application requirements.\nAWS Database Migration Service: https://aws.amazon.com/dms/ - Discusses the service used to migrate\ndatabases to AWS.\nAmazon RDS for Oracle: https://aws.amazon.com/rds/oracle/ - Describes the managed Oracle database\nservice.",
    "links": [
      "https://aws.amazon.com/rds/custom/",
      "https://aws.amazon.com/dms/",
      "https://aws.amazon.com/rds/oracle/"
    ]
  },
  {
    "question": "CertyIQ\nA large international university has deployed all of its compute services in the AWS Cloud. These services include\nAmazon EC2, Amazon RDS, and Amazon DynamoD",
    "options": {
      "B": "While it can execute backup scripts, it lacks the centralized backup"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Use AWS Backup to configure and monitor all backups for the services in use.\nHere's why:\nAWS Backup is a fully managed backup service that centralizes and automates data protection across various\nAWS services. It provides a single pane of glass for configuring, managing, and monitoring backups of EC2\ninstances, EBS volumes, RDS databases, DynamoDB tables, EFS file systems, and other AWS resources. This\ndirectly addresses the university's need to centralize management and automate backups using AWS native\noptions. https://aws.amazon.com/backup/\nOption A is incorrect because using third-party software and AWS Storage Gateway introduces unnecessary\ncomplexity and goes against the requirement to use AWS native options. Also, a tape gateway is more\nsuitable for archiving data rather than for regular backup and restore operations.\nOption C is incorrect because AWS Config primarily focuses on configuration management and compliance,\nnot backup. While it can detect configuration changes, it doesn't provide comprehensive backup and recovery\ncapabilities. Moreover, directly triggering snapshots with Config is not its primary purpose.\nOption D is incorrect because AWS Systems Manager State Manager is designed for configuration\nmanagement and automation of tasks across EC2 instances, not specifically for managing backups of diverse\ndata sources like RDS and DynamoDB. While it can execute backup scripts, it lacks the centralized backup\nmanagement and monitoring capabilities of AWS Backup. It requires writing and maintaining custom scripts,\nwhich the university wants to avoid. AWS Backup offers a managed and centralized solution.",
    "links": [
      "https://aws.amazon.com/backup/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to build a map of its IT infrastructure to identify and enforce policies on resources that pose\nsecurity risks. The company's security team must be able to query data in the IT infrastructure map and quickly\nidentify security risks.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B: Use Amazon Neptune to store the data. Use SPARQL to query the data to identify\nsecurity risks. Here's why:\nThe scenario calls for mapping IT infrastructure to identify and enforce security policies. This fundamentally\ndescribes a graph database use case. A graph database excels at representing relationships between entities,\nmaking it ideal for IT infrastructure mapping where resources are interconnected and dependencies are\nimportant for identifying security risks.\nAmazon Neptune is a fully managed graph database service. It's designed for storing and querying highly\nconnected data. SPARQL (SPARQL Protocol and RDF Query Language) is a query language specifically\ndesigned for querying graph databases. It enables efficient traversal and analysis of relationships within the\ngraph. This allows the security team to quickly identify resources with vulnerabilities or policy violations\nbased on their relationships with other elements in the infrastructure.\nOptions A, C, and D are less suitable:\nAmazon RDS (Relational Database Service) and Amazon Redshift (Data Warehouse): While you could model\nthe data in a relational database, querying relationships would involve complex and inefficient JOIN\noperations, leading to performance bottlenecks and increased operational overhead. Neither service is\ninherently designed for graph-like data or fast relationship traversal.\nAmazon DynamoDB (NoSQL Database): DynamoDB is a key-value and document database. While it offers\nflexibility, modelling and querying complex relationships is less intuitive and requires more application logic\ncompared to a graph database. PartiQL, DynamoDB's query language, can perform queries, but it won't be as\nefficient as SPARQL for traversing relationships in a highly interconnected IT infrastructure map.\nNeptune, with its graph-specific design and SPARQL query capabilities, provides the most efficient and\nperformant solution with the least operational overhead for mapping and querying IT infrastructure\nrelationships to identify security risks. The other options require more complex modelling, custom querying,\nand result in poorer performance.\nAuthoritative Links:\nAmazon Neptune: https://aws.amazon.com/neptune/\nSPARQL: https://www.w3.org/TR/rdf-sparql-query/",
    "links": [
      "https://aws.amazon.com/neptune/",
      "https://www.w3.org/TR/rdf-sparql-query/"
    ]
  },
  {
    "question": "CertyIQ\nA large company wants to provide its globally located developers separate, limited size, managed PostgreSQL\ndatabases for development purposes. The databases will be low volume. The developers need the databases only\nwhen they are actively working.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "B": "Let's break down why:"
    },
    "answer": "B",
    "explanation": "The most cost-effective solution is B. Let's break down why:\nAurora Serverless v1 is the most cost-effective option for intermittent, low-volume workloads. It\nautomatically scales capacity based on application demand and shuts down completely when not in use,\neliminating charges during idle periods. https://aws.amazon.com/rds/aurora/serverless/\nAWS Service Catalog allows administrators to create and manage catalogs of IT services that are approved\nfor use on AWS. This ensures developers only launch pre-approved, correctly sized databases.\nhttps://aws.amazon.com/servicecatalog/\nOption A is not ideal because standard Aurora instances are not designed for frequent start/stop cycles.\nThis can be time-consuming and potentially impact availability. The costs associated with manual start/stop\nprocedures may also exceed the advantages of Aurora Serverless v1.\nOption C uses a single Aurora Serverless cluster, but it doesn't adequately address the requirement for\nseparate databases for each developer. Sharing a cluster could lead to contention and data security\nconcerns. Creating databases within a single serverless cluster also doesn't enforce size limitations.\nOption D focuses on monitoring and terminating idle RDS databases, but it lacks the proactive enforcement\nof size restrictions. Also, this is a reactive approach.\nCombining Aurora Serverless with AWS Service Catalog provides a robust solution: Service Catalog enables\nthe provisioning of Aurora Serverless databases with size restrictions, while Aurora Serverless ensures cost\noptimization by scaling and pausing based on usage patterns.",
    "links": [
      "https://aws.amazon.com/rds/aurora/serverless/",
      "https://aws.amazon.com/servicecatalog/"
    ]
  },
  {
    "question": "CertyIQ\nA company is building a web application that serves a content management system. The content management\nsystem runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances run in an\nAuto Scaling group across multiple Availability Zones. Users are constantly adding and updating files, blogs, and\nother website assets in the content management system.\nA solutions architect must implement a solution in which all the EC2 instances share up-to-date website content\nwith the least possible lag time.\nWhich solution meets these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B: Copying the website assets to Amazon EFS and mounting it on each EC2 instance\nprovides a shared, consistently updated, and low-latency file system accessible to all instances. This\napproach directly addresses the requirement for all EC2 instances to share up-to-date website content with\nminimal lag.\nHere's why:\nShared File System: Amazon EFS provides a shared file system that can be simultaneously mounted by\nmultiple EC2 instances. This eliminates the need to copy or synchronize data between instances.\nConsistency: EFS offers strong consistency, ensuring that all instances see the latest changes to the website\nassets.\nLow Latency: EFS is designed for low-latency access, making it suitable for serving website content. Changes\nare immediately available to all instances.\nScalability: EFS automatically scales its storage capacity to accommodate growing website assets.\nIntegration: EFS integrates seamlessly with EC2 and Auto Scaling groups.\nLet's examine why the other options are less suitable:\nA: Relying on copying assets from the newest EC2 instance is complex and error-prone. It introduces\ninconsistency during updates and requires intricate configuration changes. ALBs do not make changes to\nwebsite assets.\nC: Using Amazon S3 for website assets requires regular synchronization to EBS volumes. Hourly\nsynchronization introduces significant lag, which conflicts with the requirement for minimal lag time and is not\nefficient. S3 is generally best for static object storage and delivery, not a constantly changing file system.\nD: EBS snapshots are point-in-time copies, and restoring them as secondary volumes would not provide a\nmechanism for real-time updates. The instances would have a stale version of the website assets.\nIn summary, Amazon EFS provides the most effective, scalable, and consistent solution for sharing website\nassets across multiple EC2 instances with minimal latency.\nSupporting Links:\nAmazon EFS Documentation\nAmazon EC2 Documentation",
    "links": []
  },
  {
    "question": "CertyIQ\nA company's web application consists of multiple Amazon EC2 instances that run behind an Application Load\nBalancer in a VP",
    "options": {
      "C": "An Amazon RDS for MySQL DB instance contains the data. The company needs the ability to"
    },
    "answer": "A",
    "explanation": "The correct answer is A because it provides a comprehensive threat detection and response mechanism\ntailored to the scenario. Here's a detailed justification:\nAmazon GuardDuty is a threat detection service that continuously monitors your AWS accounts and\nworkloads for malicious activity and unauthorized behavior. It uses machine learning, anomaly detection, and\nintegrated threat intelligence to identify threats. https://aws.amazon.com/guardduty/\nAmazon EventBridge (formerly CloudWatch Events) allows you to build event-driven applications at scale. It\ncan filter events from various AWS services, including GuardDuty, and trigger actions based on these events.\nhttps://aws.amazon.com/eventbridge/\nAWS Lambda lets you run code without provisioning or managing servers. In this solution, a Lambda function\nis used to dynamically adjust AWS WAF rules based on GuardDuty findings, providing automated remediation.\nhttps://aws.amazon.com/lambda/\nAWS WAF (Web Application Firewall) protects web applications from common web exploits and bots. By\ndynamically adjusting WAF rules based on GuardDuty findings, the application can mitigate emerging threats.\nhttps://aws.amazon.com/waf/\nThis combination provides a closed-loop threat detection and response system. GuardDuty detects threats,\nEventBridge triggers a Lambda function based on specific GuardDuty findings, and Lambda updates WAF\nrules to block the identified malicious activity.\nOption B is incorrect because AWS Firewall Manager primarily manages WAF rules and other security policies\nacross multiple accounts and applications. While it provides centralized management, it doesn't inherently\nperform threat detection in the same way as GuardDuty.\nOption C is incorrect because Amazon Inspector primarily assesses EC2 instances and container images for\nvulnerabilities. While valuable, it doesn't focus on runtime threat detection like GuardDuty. Updating WAF\nrules based on Inspector findings would be less effective than reacting to actual malicious activity detected\nby GuardDuty. Adding a VPC Network ACL is a good security practice, but it won't dynamically adapt to new\nthreats based on monitoring, thus is secondary to continuous threat detection.\nOption D is incorrect because Amazon Macie discovers and protects sensitive data stored in Amazon S3. While\nimportant for data security, it's not designed for threat detection in the same way as GuardDuty. Adding a VPC\nNetwork ACL is a good security practice, but it won't dynamically adapt to new threats based on monitoring,\nthus is secondary to continuous threat detection.\nThe most effective approach is to use GuardDuty for continuous threat detection and automate the response\nby adjusting WAF rules based on its findings using EventBridge and Lambda. This offers a dynamic and\nautomated security posture against suspicious activities.",
    "links": [
      "https://aws.amazon.com/guardduty/",
      "https://aws.amazon.com/eventbridge/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/waf/"
    ]
  },
  {
    "question": "CertyIQ\nA company is planning to run a group of Amazon EC2 instances that connect to an Amazon Aurora database. The\ncompany has built an AWS CloudFormation template to deploy the EC2 instances and the Aurora DB cluster. The\ncompany wants to allow the instances to authenticate to the database in a secure way. The company does not\nwant to maintain static database credentials.\nWhich solution meets these requirements with the LEAST operational effort?",
    "options": {},
    "answer": "C",
    "explanation": "Here's a detailed justification for why option C is the correct answer, along with supporting information:\nThe problem requires a secure and operationally efficient method for EC2 instances to authenticate to an\nAurora database without using static credentials. The key is to avoid managing and rotating database\nusernames and passwords within the EC2 instances or storing them in Parameter Store.\nOption C, utilizing IAM database authentication for Aurora, provides the most secure and streamlined\napproach. IAM database authentication enables authentication using IAM roles and policies instead of\ndatabase usernames and passwords. This aligns with the principle of least privilege and reduces the risk of\ncredential compromise.\nHere's why option C is superior to the others:\nOption A (Static Credentials in CloudFormation): Embedding credentials directly in the CloudFormation\ntemplate or passing them as parameters is a security risk. These credentials could be exposed if the template\nis compromised or if instance metadata is accessed improperly. It also creates an ongoing maintenance\nburden.\nOption B (Static Credentials in Parameter Store): While better than Option A, storing static credentials in\nParameter Store still requires managing and rotating those credentials. If these credentials are ever\ncompromised, you would need to manually rotate them, which could disrupt your application. It doesn't\ncompletely eliminate the risk or the operational overhead.\nOption D (IAM Authentication with IAM User): While using IAM is correct, associating an IAM user directly\nwith the EC2 instance is less flexible and scalable than using an IAM role. Roles are designed for EC2\ninstances and other AWS services, as they can be assumed dynamically. IAM users are usually associated\nwith human identities.\nOption C directly addresses the problem by:\n1. Enabling IAM database authentication on the Aurora DB cluster, avoiding traditional database\ncredentials.\n2. Creating a database user authorized for IAM authentication.\n3. Associating an IAM role with the EC2 instances. This role grants the necessary permissions to access\nthe database. The application running on the EC2 instances can then assume this role and\nauthenticate to the database using temporary AWS credentials obtained through the instance\nmetadata service.\n4. This approach eliminates the need to manage and rotate database credentials. Permissions are\ncentrally managed through IAM policies, simplifying security management and reducing the attack\nsurface. The short-lived, automatically rotated AWS credentials are far more secure.\nIn short, IAM database authentication provides a seamless, secure, and scalable solution for authenticating\nEC2 instances to Aurora DB clusters without the burden of managing static database credentials, making it\nthe best fit for the problem's requirements and operational efficiency.\nSupporting documentation:\nIAM Database Authentication for MySQL and PostgreSQL:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html\nIAM Roles for Amazon EC2: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-\nec2.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to configure its Amazon CloudFront distribution to use SSL/TLS certificates. The company does\nnot want to use the default domain name for the distribution. Instead, the company wants to use a different domain\nname for the distribution.\nWhich solution will deploy the certificate without incurring any additional costs?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C, requesting an Amazon-issued public certificate from AWS Certificate Manager (ACM)\nin the us-east-1 Region. Here's why:\nCustom Domain Names and SSL/TLS: To use a custom domain name (e.g., www.example.com) with a\nCloudFront distribution, you need an SSL/TLS certificate that validates your ownership of that domain.\nCloudFront uses this certificate to encrypt communication between users and CloudFront edge locations.\nAWS Certificate Manager (ACM): ACM is the preferred way to provision, manage, and deploy SSL/TLS\ncertificates for use with AWS services. ACM handles the complexities of obtaining and renewing certificates.\nACM Pricing: ACM provides free public SSL/TLS certificates for use with AWS services like CloudFront,\nElastic Load Balancing, and API Gateway. This fulfills the requirement of avoiding additional costs. Private\nACM certificates do incur costs.\nACM Region for CloudFront: Crucially, ACM certificates used with CloudFront must be requested or imported\ninto the us-east-1 (N. Virginia) Region, regardless of where your other AWS resources are located. CloudFront\nis a global service, and its certificate management is centralized in this region. The certificate is then\ndistributed globally to CloudFront edge locations.\nPublic vs. Private Certificates: For public-facing websites and applications using CloudFront, you need a\npublic certificate trusted by major browsers and operating systems. These are the certificates ACM issues for\nfree. Private certificates, while they provide encryption, are typically used for internal communication within\nan organization and would not be trusted by external clients without additional configuration.\nTherefore, only requesting a public certificate from ACM in the us-east-1 region provides both SSL/TLS\nsupport for your custom domain in CloudFront and avoids any additional costs.\nRelevant Documentation:\nUsing SSL/TLS Certificates with CloudFront:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront.html\nAWS Certificate Manager Pricing: https://aws.amazon.com/certificate-manager/pricing/\nRequesting a Public Certificate: https://docs.aws.amazon.com/acm/latest/userguide/gs-acm-request-\npublic.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront.html",
      "https://aws.amazon.com/certificate-manager/pricing/",
      "https://docs.aws.amazon.com/acm/latest/userguide/gs-acm-request-"
    ]
  },
  {
    "question": "CertyIQ\nA company creates operations data and stores the data in an Amazon S3 bucket. For the company's annual audit,\nan external consultant needs to access an annual report that is stored in the S3 bucket. The external consultant\nneeds to access the report for 7 days.\nThe company must implement a solution to allow the external consultant access to only the report.\nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D: Generate a presigned URL that has the required access to the location of the report\non the S3 bucket. Share the presigned URL with the external consultant.\nHere's why: Presigned URLs offer the most operationally efficient and secure method for granting temporary,\nrestricted access to specific S3 objects. A presigned URL is a URL generated by someone with valid AWS\ncredentials that grants access to a specific S3 object for a limited time. The URL includes embedded\ncredentials, so the user doesn't need their own AWS account to access the object.\nOption A is incorrect because creating a public static website and moving the data there exposes all\noperations data publicly, which violates the requirement for limited access to only the annual report. It also\ninvolves unnecessary data migration.\nOption B is incorrect because enabling public access to the entire S3 bucket for 7 days is a significant\nsecurity risk. It exposes all data in the bucket, not just the report, and contradicts the principle of least\nprivilege.\nOption C is incorrect because creating a new IAM user and sharing access keys introduces the overhead of\nmanaging IAM users and rotating credentials. It's also less secure, as the consultant could potentially use the\ncredentials for longer than the intended 7 days or access other resources.\nPresigned URLs are operationally efficient because they don't require the management of IAM users or\nbucket policies. They are also secure because they provide temporary, object-level access. After the specified\nexpiry time, the URL becomes invalid, preventing further access. This aligns precisely with the requirement of\nproviding access only to the report for 7 days, making option D the most suitable choice.\nRelevant Documentation:\nAWS S3 Presigned URLs:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html"
    ]
  },
  {
    "question": "CertyIQ\nA company plans to run a high performance computing (HPC) workload on Amazon EC2 Instances. The workload\nrequires low-latency network performance and high network throughput with tightly coupled node-to-node\ncommunication.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Configure the EC2 instances to be part of a cluster placement group."
    },
    "answer": "A",
    "explanation": "The correct answer is A. Configure the EC2 instances to be part of a cluster placement group.\nHere's a detailed justification:\nCluster placement groups are specifically designed for applications that require low-latency, high-throughput\nnetwork performance, and tight node-to-node communication, as is typical for HPC workloads. When EC2\ninstances are launched within a cluster placement group, they are placed in close proximity to each other\nwithin a single Availability Zone. This proximity minimizes network latency between the instances.\nPlacement groups, and especially cluster placement groups, leverage specialized networking infrastructure\nwithin AWS that minimizes latency and maximizes bandwidth between instances. This is crucial for tightly\ncoupled HPC applications where communication overhead can significantly impact overall performance. By\ngrouping the instances, AWS can provide network performance superior to what would be possible if the\ninstances were scattered across different racks or Availability Zones.\nOption B, Dedicated Instance tenancy, provides dedicated hardware for your instances, but it doesn't\nguarantee low-latency network communication. While it might offer performance benefits in terms of\nresource isolation, it doesn't directly address the node-to-node communication requirements of HPC\nworkloads.\nOption C, Spot Instances, are cost-effective but can be interrupted with little notice. This unpredictability\nmakes them unsuitable for HPC workloads that require continuous operation to maintain performance.\nInterruptions can disrupt ongoing calculations and negatively impact the overall time to completion.\nOption D, On-Demand Capacity Reservation, guarantees that capacity will be available for your instances\nwhen you need them. However, it doesn't directly influence network latency or throughput. While important\nfor ensuring resources are available, it doesn't solve the critical network performance issues of HPC\napplications.\nIn summary, cluster placement groups are the most appropriate solution because they directly address the\nneed for low-latency, high-throughput network performance required for tightly coupled node-to-node\ncommunication in HPC workloads.\nFurther Reading:\nAWS Documentation on Placement Groups\nAWS HPC Documentation",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has primary and secondary data centers that are 500 miles (804.7 km) apart and interconnected with\nhigh-speed fiber-optic cable. The company needs a highly available and secure network connection between its\ndata centers and a VPC on AWS for a mission-critical workload. A solutions architect must choose a connection\nsolution that provides maximum resiliency.\nWhich solution meets these requirements?",
    "options": {
      "C": "Two AWS Direct Connect connections from each of the primary and secondary data"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Two AWS Direct Connect connections from each of the primary and secondary data\ncenters terminating at two Direct Connect locations on two separate devices.\nHere's a detailed justification:\nThe requirement is for a highly available and secure network connection between the on-premises data\ncenters (primary and secondary) and an AWS VPC for a mission-critical workload. Resiliency is the key\nconcern. AWS Direct Connect provides a dedicated network connection from on-premises to AWS, bypassing\nthe public internet for enhanced security and consistent performance.\nOption C offers the highest level of redundancy and resilience. Having two Direct Connect connections from\neach data center ensures that if one connection fails, the other will maintain connectivity. Terminating these\nconnections at two separate Direct Connect locations on separate devices further mitigates the risk of a\nsingle point of failure. For instance, a failure at one Direct Connect location (e.g., power outage, hardware\nissue) will not completely disrupt connectivity, as the other location will still be operational. Using multiple\nDirect Connect locations makes it resilient to AWS infrastructure maintenance or regional events. This\napproach is ideal for mission-critical workloads.\nOption A only connects from the primary data center, leaving the secondary data center unconnected,\nthereby missing redundancy from the secondary site.\nOption B has single points of failure. A single device at the Direct Connect location could fail, resulting in\ncomplete disconnection.\nOption D has a single point of failure at each data center (single Direct Connect connection). A single location\ncould fail, resulting in complete disconnection.\nTherefore, option C offers the maximum resiliency and high availability required by the scenario.\nReference:\nAWS Direct Connect Resiliency Recommendations: https://aws.amazon.com/blogs/networking-and-content-\ndelivery/aws-direct-connect-resiliency-recommendations/\nAWS Direct Connect FAQs: https://aws.amazon.com/directconnect/faqs/",
    "links": [
      "https://aws.amazon.com/blogs/networking-and-content-",
      "https://aws.amazon.com/directconnect/faqs/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs several Amazon RDS for Oracle On-Demand DB instances that have high utilization. The RDS DB\ninstances run in member accounts that are in an organization in AWS Organizations.\nThe company's finance team has access to the organization's management account and member accounts. The\nfinance team wants to find ways to optimize costs by using AWS Trusted Advisor.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": {},
    "answer": "A",
    "explanation": "Here's a detailed justification for the correct answer, along with supporting concepts and links:\nThe goal is to optimize costs for highly utilized RDS for Oracle instances. AWS Trusted Advisor provides\nrecommendations for cost optimization.\nA is correct: Using Trusted Advisor in the management account provides an aggregated view of all member\naccounts within the AWS Organization. This centralizes cost optimization recommendations and allows the\nfinance team to have a comprehensive overview of potential savings across all RDS instances.\nC is correct: The \"Amazon RDS Reserved Instance Optimization\" Trusted Advisor check specifically focuses\non identifying opportunities to save money by purchasing Reserved Instances (RIs) for RDS. Since the RDS\ninstances have high utilization, they are good candidates for Reserved Instances, which offer significant cost\nsavings compared to On-Demand pricing for consistent, predictable workloads. This directly addresses the\ncompany's objective of cost optimization. The Trusted Advisor RI optimization check analyzes your RDS usage\npatterns and suggests potential RI purchases.\nWhy other options are incorrect:\nB: While checking Trusted Advisor in member accounts is helpful, it doesn't provide the consolidated view the\nfinance team in the management account likely requires for overall cost management.\nD: \"Amazon RDS Idle DB Instances\" checks are relevant for cost optimization but target instances that aren't\nbeing used. The problem states the instances are highly utilized, making this check less relevant in this\nscenario.\nE: Compute optimization is more relevant to EC2 instances. While related, directly addressing RDS Reserved\nInstance optimization is more focused on the specifics of the question. AWS Compute Optimizer is a separate\nservice for analyzing and recommending optimal AWS compute resources. It does not directly address RDS RI\npurchasing recommendations.\nKey Concepts:\nAWS Organizations: Enables centralized management and governance across multiple AWS accounts.\nAWS Trusted Advisor: Provides recommendations to optimize AWS infrastructure for cost, security,\nperformance, reliability, and service limits.\nAmazon RDS Reserved Instances (RIs): Offer significant cost savings compared to On-Demand pricing in\nexchange for a commitment to use the DB instance for a specific period.\nSupporting Links:\nAWS Organizations: https://aws.amazon.com/organizations/\nAWS Trusted Advisor: https://aws.amazon.com/premiumsupport/technology/trusted-advisor/\nAmazon RDS Reserved Instances: https://aws.amazon.com/rds/reserved-instances/\nAWS Compute Optimizer: https://aws.amazon.com/compute-optimizer/",
    "links": [
      "https://aws.amazon.com/organizations/",
      "https://aws.amazon.com/premiumsupport/technology/trusted-advisor/",
      "https://aws.amazon.com/rds/reserved-instances/",
      "https://aws.amazon.com/compute-optimizer/"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is creating an application. The application will run on Amazon EC2 instances in private\nsubnets across multiple Availability Zones in a VP",
    "options": {
      "C": "AWS PrivateLink Interface Endpoint: While PrivateLink is a secure and private way to connect to S3 (or",
      "A": "Create a gateway endpoint for Amazon S3 in the VPC. In the route tables for the",
      "B": "NAT Gateway: While a NAT gateway allows instances in private subnets to access the internet (and",
      "D": "NAT Gateway per AZ: This approach attempts to improve availability by using a NAT gateway in each"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Create a gateway endpoint for Amazon S3 in the VPC. In the route tables for the\nprivate subnets, add an entry for the gateway endpoint.\nHere's a detailed justification:\nThe requirement is to minimize data transfer costs when EC2 instances in private subnets frequently access\nS3 buckets containing confidential information. A gateway endpoint for S3 is the most cost-effective and\nsecure solution for this scenario.\nGateway Endpoints for S3: Gateway endpoints provide connectivity to S3 without requiring traffic to traverse\nthe internet. This is crucial because data transfer within an AWS Region using gateway endpoints is free. This\ndrastically reduces data transfer costs compared to using NAT gateways or internet gateways, which incur\ndata transfer charges. Additionally, gateway endpoints operate at the VPC route table level, directing traffic\ndestined for S3 to the endpoint directly.\nWhy other options are less suitable:\nB. NAT Gateway: While a NAT gateway allows instances in private subnets to access the internet (and\npotentially S3 if configured), all traffic traverses the NAT gateway, incurring data transfer charges for all data\nmoved between EC2 and S3. Also, using NAT gateways increases latency.\nC. AWS PrivateLink Interface Endpoint: While PrivateLink is a secure and private way to connect to S3 (or\nother AWS services), it primarily benefits third-party services or connecting between VPCs. For resources\nwithin the same VPC, a gateway endpoint for S3 offers the same level of security at no additional cost. Also,\nPrivateLink Interface Endpoints, while offering enhanced security, are more expensive compared to Gateway\nEndpoints.\nD. NAT Gateway per AZ: This approach attempts to improve availability by using a NAT gateway in each\nAvailability Zone. However, it doesn't address the core issue of minimizing data transfer costs, as the NAT\ngateways still incur charges for data transferred to S3. It is also costly.\nSecurity: A gateway endpoint allows you to control access to S3 buckets using VPC endpoint policies,\nensuring only authorized instances can access specific S3 resources. Because all traffic is contained within\nthe AWS network, security is enhanced compared to routing traffic via the public internet.In summary, a\ngateway endpoint for S3 optimizes network architecture by providing free, secure, and direct connectivity\nbetween EC2 instances in private subnets and S3 buckets, thereby minimizing data transfer costs while\nmaintaining a high level of security.\nAuthoritative Links:\nVPC Endpoints: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\nGateway VPC Endpoints: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html\nAWS PrivateLink: https://aws.amazon.com/privatelink/",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html",
      "https://aws.amazon.com/privatelink/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to relocate its on-premises MySQL database to AWS. The database accepts regular imports\nfrom a client-facing application, which causes a high volume of write operations. The company is concerned that\nthe amount of traffic might be causing performance issues within the application.\nHow should a solutions architect design the architecture on AWS?",
    "options": {
      "B": "Migrating a MySQL database to"
    },
    "answer": "A",
    "explanation": "The correct answer is A: Provision an Amazon RDS for MySQL DB instance with Provisioned IOPS SSD storage.\nMonitor write operation metrics by using Amazon CloudWatch. Adjust the provisioned IOPS if necessary.\nHere's why:\nRDS for MySQL suitability: The company is using MySQL on-premises and wants to migrate it to AWS. RDS\nfor MySQL provides a managed MySQL database service, simplifying administration tasks like backups,\npatching, and scaling. This makes it a suitable option for a lift-and-shift migration.\nProvisioned IOPS for write-heavy workloads: The database experiences high write operations due to regular\nimports. Provisioned IOPS (PIOPS) SSD storage is designed for I/O-intensive workloads. By provisioning a\nspecific number of IOPS, the company can guarantee a consistent level of performance for write operations.\nGeneral Purpose SSD (gp2/gp3) storage, while cost-effective, might not provide the required consistent\nperformance for heavy write loads.\nMonitoring with CloudWatch: Amazon CloudWatch provides metrics for monitoring the database's\nperformance, including write operations. By monitoring these metrics, the company can identify any\nperformance bottlenecks related to I/O and adjust the provisioned IOPS accordingly. This proactive approach\nensures optimal performance and prevents the application from experiencing issues.\nWhy other options are less suitable:\nB: ElastiCache is generally used for caching read operations to reduce load on the database. In this scenario,\nthe concern is high write operations, so ElastiCache is not directly helpful.\nC: Amazon DocumentDB is a NoSQL database compatible with MongoDB. Migrating a MySQL database to\nDocumentDB would require significant application changes and might not be feasible or desirable if the\ncompany wants to maintain its existing relational data model and SQL-based queries.\nD: Amazon EFS is a network file system and is not suitable for hosting a relational database like MySQL. EFS\nis designed for shared file storage across multiple EC2 instances, not for database storage.\nIn summary, using RDS for MySQL with Provisioned IOPS and actively monitoring performance metrics using\nCloudWatch is the most appropriate way to design the architecture on AWS to address the company's\nconcerns about high write operations.\nRelevant Links:\nAmazon RDS: https://aws.amazon.com/rds/\nAmazon RDS Storage Types:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html\nAmazon CloudWatch: https://aws.amazon.com/cloudwatch/",
    "links": [
      "https://aws.amazon.com/rds/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html",
      "https://aws.amazon.com/cloudwatch/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an application in the AWS Cloud that generates sensitive archival data files. The company wants\nto rearchitect the application's data storage. The company wants to encrypt the data files and to ensure that third\nparties do not have access to the data before the data is encrypted and sent to AWS. The company has already\ncreated an Amazon S3 bucket.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The requirement states that the company wants to ensure third parties cannot access the data before it is\nencrypted and sent to AWS. This necessitates client-side encryption. Server-side encryption, regardless of\nthe method (SSE-S3, SSE-KMS, or SSE-C), encrypts the data after it has been received by AWS. Therefore,\noptions A, B, and C are incorrect because they all use server-side encryption.\nOption D, client-side encryption with a key stored in AWS KMS, addresses the core requirement. With client-\nside encryption, the application encrypts the data before sending it to S3. Storing the encryption key in AWS\nKMS provides secure key management, enabling the application to retrieve and use the key for encryption\noperations. This ensures that the data is protected even during transit to S3. Furthermore, because the\nencryption happens before the data reaches AWS, third parties cannot intercept and read the data. Using\nKMS ensures proper key rotation and access control.\nClient-side encryption allows the company to maintain full control over the encryption process. The\napplication handles encryption using the AWS SDK and the encryption key retrieved from KMS. After\nencrypting the data locally, the application then uploads the encrypted file to S3. Because the data is already\nencrypted, unauthorized access during transit or at rest on S3 is prevented.\nTherefore, the correct solution is to configure the application to use client-side encryption with a key stored in\nAWS KMS and then store the encrypted archival files in the S3 bucket.\nRelevant Documentation:\nAWS KMS: https://aws.amazon.com/kms/\nProtecting Data Using Server-Side Encryption:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html\nProtecting Data Using Client-Side Encryption:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html",
    "links": [
      "https://aws.amazon.com/kms/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html"
    ]
  },
  {
    "question": "CertyIQ\nA company uses Amazon RDS with default backup settings for its database tier. The company needs to make a\ndaily backup of the database to meet regulatory requirements. The company must retain the backups for 30 days.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "B": "Modify the RDS database to have a retention period of 30 days for automated"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Modify the RDS database to have a retention period of 30 days for automated\nbackups.\nHere's why:\nAutomated Backups are the Simplest: RDS automated backups are designed for exactly this purpose -\ncreating and retaining backups for a specified duration. The default settings might not meet the 30-day\nretention requirement, so adjusting this setting is the most straightforward approach.\nReduced Operational Overhead: Modifying the backup retention period through the RDS console or AWS CLI\nis a one-time configuration change. This drastically reduces the operational overhead compared to solutions\ninvolving custom scripting or manual snapshots.\nCompliance with Requirements: This solution directly addresses the company's need for daily backups\nretained for 30 days to meet regulatory requirements.\nA is Incorrect: While a Lambda function could create snapshots, it adds significant operational overhead for\ndevelopment, deployment, monitoring, and maintenance. It's also unnecessary given the built-in RDS\nautomated backup functionality.\nC is Incorrect: Systems Manager Maintenance Windows are primarily for scheduling maintenance tasks like\npatching, not for managing RDS backup retention. While you could theoretically use it to initiate a backup\nretention change, it's overkill and less direct than simply configuring the retention period within RDS itself.\nD is Incorrect: Manual snapshots are not automated. They require intervention every day, which introduces\nsignificant operational overhead. Also, manually managing retention policies for these snapshots would add\nmore complexity.\nIn summary, utilizing RDS's built-in automated backup feature and adjusting the retention period to 30 days\nis the simplest, most efficient, and least operationally intensive way to meet the company's requirements for\ndaily backups with a 30-day retention policy. This aligns with cloud best practices of leveraging managed\nservices and minimizing custom code.\nAuthoritative Links:\nAmazon RDS Backups:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html\nAmazon RDS Backup Retention:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.Retention",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.Retention"
    ]
  },
  {
    "question": "CertyIQ\nA company that runs its application on AWS uses an Amazon Aurora DB cluster as its database. During peak usage\nhours when multiple users access and read the data, the monitoring system shows degradation of database\nperformance for the write queries. The company wants to increase the scalability of the application to meet peak\nusage demands.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "C",
    "explanation": "The optimal and most cost-effective solution is to create an Aurora read replica. Read replicas provide\nincreased read capacity by offloading read workloads from the primary Aurora instance. This directly\naddresses the performance degradation issue experienced during peak read-heavy periods. Aurora read\nreplicas share the same underlying storage as the primary instance, leading to minimal latency and data\nconsistency. The application can be updated to direct read queries to the read replica's endpoint, leaving the\nprimary instance to handle write operations without being burdened by read traffic. This separation of read\nand write operations allows the primary instance to efficiently manage write queries and maintain database\nperformance. Options A and D are less cost-effective, as they involve maintaining an entirely separate\ndatabase or data warehouse. Option B, using DAX, is more suitable for DynamoDB and not as efficient and\ncost-effective for Aurora as leveraging Aurora's native read replica functionality. Using Aurora's read replica\noffers the advantage of seamless integration within the Aurora ecosystem.\nReference:\nAmazon Aurora Read Replicas",
    "links": []
  },
  {
    "question": "CertyIQ\nA company's near-real-time streaming application is running on AWS. As the data is ingested, a job runs on the\ndata and takes 30 minutes to complete. The workload frequently experiences high latency due to large amounts of\nincoming data. A solutions architect needs to design a scalable and serverless solution to enhance performance.\nWhich combination of steps should the solutions architect take? (Choose two.)",
    "options": {
      "A": "Use Amazon Kinesis Data Firehose to ingest the data: Kinesis Data Firehose is designed for real-time data",
      "B": "Use AWS Lambda with AWS Step Functions to process the data: While Lambda is serverless, its execution",
      "C": "Use AWS Database Migration Service (AWS DMS) to ingest the data: AWS DMS is designed for database",
      "D": "Use Amazon EC2 instances in an Auto Scaling group to process the data: While EC2 Auto Scaling"
    },
    "answer": "A",
    "explanation": "The combination of using Amazon Kinesis Data Firehose for ingestion and AWS Fargate with Amazon ECS for\nprocessing is the most suitable serverless and scalable solution for this near-real-time streaming application.\nA. Use Amazon Kinesis Data Firehose to ingest the data: Kinesis Data Firehose is designed for real-time data\nstreaming into data lakes, data stores, and analytics services. It automatically scales to match the throughput\nof incoming data, handling large volumes with ease. This eliminates the need to manage the underlying\ninfrastructure for ingestion, making it a serverless choice. https://aws.amazon.com/kinesis/data-firehose/\nE. Use AWS Fargate with Amazon Elastic Container Service (Amazon ECS) to process the data: AWS\nFargate provides serverless compute for containers. Instead of managing EC2 instances, Fargate handles the\nunderlying infrastructure. ECS provides the container orchestration layer, enabling you to deploy, manage,\nand scale containerized applications. Using Fargate with ECS allows the processing job to scale dynamically\nbased on the incoming data volume from Kinesis Data Firehose. Containerization helps ensure consistent\nexecution of the 30-minute processing job. https://aws.amazon.com/fargate/, https://aws.amazon.com/ecs/\nWhy other options are less suitable:\nB. Use AWS Lambda with AWS Step Functions to process the data: While Lambda is serverless, its execution\ntime is limited (currently up to 15 minutes). The 30-minute processing time exceeds this limit, making Lambda\nunsuitable.\nC. Use AWS Database Migration Service (AWS DMS) to ingest the data: AWS DMS is designed for database\nmigrations, not real-time data streaming ingestion. It's not appropriate for this use case.\nD. Use Amazon EC2 instances in an Auto Scaling group to process the data: While EC2 Auto Scaling\nprovides scalability, it introduces infrastructure management overhead. The requirement is for a serverless\nsolution; thus Fargate is preferred.",
    "links": [
      "https://aws.amazon.com/kinesis/data-firehose/",
      "https://aws.amazon.com/fargate/,",
      "https://aws.amazon.com/ecs/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a web application on multiple Amazon EC2 instances in a VP",
    "options": {
      "C": "A gateway",
      "B": "Option C is incorrect because S3 buckets cannot be deployed inside a VPC. S3 is a regional service managed"
    },
    "answer": "A",
    "explanation": "The correct solution is A: Create a gateway VPC endpoint for Amazon S3 and a route in the VPC route table to\nthe endpoint. This is because gateway VPC endpoints for S3 provide a direct, private connection to S3 within\nthe AWS network, bypassing the public internet. EC2 instances within the VPC can then access S3 without\nrequiring public IP addresses or traversing the internet. The route table entry directs traffic destined for S3 to\nthe gateway endpoint. This approach ensures that sensitive data transfer remains secure and compliant with\nthe requirement of not being sent over the public internet.\nOption B is incorrect because an internal Network Load Balancer cannot target an S3 bucket directly. Load\nbalancers distribute traffic to instances, IP addresses, or Lambda functions, but S3 is object storage and not a\nservice that can be directly targeted by an NLB.\nOption C is incorrect because S3 buckets cannot be deployed inside a VPC. S3 is a regional service managed\nby AWS and exists outside the VPC.\nOption D is incorrect because AWS Direct Connect is used to establish a private connection between an on-\npremises environment and AWS. While it provides a private connection, it is more complex and costly than\nusing a gateway VPC endpoint for the purpose of privately accessing S3 from within a VPC. A gateway\nendpoint is specifically designed for this scenario and is the simpler, more cost-effective solution.\nFor further reading, refer to the AWS documentation on VPC endpoints:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html and gateway endpoints specifically:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html.",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html."
    ]
  },
  {
    "question": "CertyIQ\nA company runs its production workload on Amazon EC2 instances with Amazon Elastic Block Store (Amazon EBS)\nvolumes. A solutions architect needs to analyze the current EBS volume cost and to recommend optimizations. The\nrecommendations need to include estimated monthly saving opportunities.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D: Use AWS Compute Optimizer to generate EBS volume recommendations for\noptimization.\nAWS Compute Optimizer directly addresses the requirements of analyzing EBS volume costs and providing\noptimization recommendations with estimated savings. Compute Optimizer analyzes the configuration and\nutilization metrics of your EC2 instances and EBS volumes. It uses machine learning to identify under-utilized\nor over-provisioned EBS volumes and suggests right-sizing options. These suggestions often involve\nmigrating to different EBS volume types or reducing the size of existing volumes. Crucially, Compute\nOptimizer estimates the potential cost savings associated with each recommendation, fulfilling the need for\nquantifiable monthly savings estimates.\nOption A is incorrect because Amazon Inspector focuses on security vulnerabilities within your infrastructure,\nnot performance optimization or cost analysis of EBS volumes. Option B, AWS Systems Manager, is a\nmanagement service that helps automate operational tasks across your AWS resources but doesn't provide\nEBS volume optimization recommendations or cost savings estimates. Option C, Amazon CloudWatch,\nprovides monitoring and observability data, but it does not directly offer recommendations for EBS volume\noptimization or calculate associated cost savings. While you can derive insights from CloudWatch metrics, it\nrequires manual analysis and doesn't provide the automated analysis and recommendations offered by\nCompute Optimizer.\nTherefore, AWS Compute Optimizer is the most suitable tool for identifying EBS volume optimization\nopportunities and estimating monthly savings, aligning directly with the problem statement's requirements.\nRelevant Link:\nAWS Compute Optimizer: https://aws.amazon.com/compute-optimizer/",
    "links": [
      "https://aws.amazon.com/compute-optimizer/"
    ]
  },
  {
    "question": "CertyIQ\nA global company runs its workloads on AWS. The company's application uses Amazon S3 buckets across AWS\nRegions for sensitive data storage and analysis. The company stores millions of objects in multiple S3 buckets\ndaily. The company wants to identify all S3 buckets that are not versioning-enabled.\nWhich solution will meet these requirements?",
    "options": {
      "B": "Use Amazon S3 Storage Lens to identify all S3 buckets that are not versioning-"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Use Amazon S3 Storage Lens to identify all S3 buckets that are not versioning-\nenabled across Regions.\nHere's why:\nAmazon S3 Storage Lens is a cloud storage analytics feature that provides organization-wide visibility into\nobject storage usage and activity. It offers a single view across all your buckets to identify trends and outliers,\nflag data protection gaps, and optimize storage costs. A core capability is its ability to identify buckets that do\nnot have versioning enabled, directly addressing the company's requirement to identify all S3 buckets across\nmultiple AWS Regions that lack versioning. The dashboards and metrics provided by S3 Storage Lens are\ndesigned for this type of analysis, providing a comprehensive view that can scale to millions of objects and\nmultiple regions. https://aws.amazon.com/s3/storage-lens/\nOption C is incorrect because IAM Access Analyzer for S3 is primarily focused on identifying buckets with\naccess configurations that might expose data unintentionally to external entities. While it deals with bucket\nsecurity, it doesn't directly report on versioning status. Its focus is on external access, not internal\nconfiguration settings like versioning. https://aws.amazon.com/iam/features/access-analyzer/\nOption D is incorrect because an S3 Multi-Region Access Point is designed to provide a single endpoint for\naccessing data stored in multiple S3 buckets across different AWS Regions. Its primary function is to simplify\naccess and improve performance in multi-region deployments, not to analyze bucket configurations such as\nversioning status. It doesn't inherently provide information on which buckets have versioning enabled.\nhttps://aws.amazon.com/s3/features/multi-region-access-points/",
    "links": [
      "https://aws.amazon.com/s3/storage-lens/",
      "https://aws.amazon.com/iam/features/access-analyzer/",
      "https://aws.amazon.com/s3/features/multi-region-access-points/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to enhance its ecommerce order-processing application that is deployed on AWS. The\napplication must process each order exactly once without affecting the customer experience during unpredictable\ntraffic surges.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "A",
    "explanation": "The requirement is to process e-commerce orders exactly once, even during traffic surges, without affecting\nthe customer experience. Option A, using Amazon SQS FIFO queues and a Lambda function, is the best\nsolution for this scenario.\nHere's why:\nExactly-Once Processing: SQS FIFO (First-In, First-Out) queues are designed to guarantee that messages are\nprocessed exactly once, in the order they are sent, eliminating duplicates and ensuring consistent order\nprocessing. This is crucial for e-commerce where duplicate order processing can lead to customer\ndissatisfaction and financial losses.\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\nHandling Traffic Surges: SQS is a fully managed queue service that automatically scales to handle\nunpredictable traffic surges. It decouples the order placement process from the actual order processing,\nensuring that the application remains responsive to customers even during peak loads.\nAsynchronous Processing: Using SQS allows for asynchronous order processing. The customer doesn't have\nto wait for the order to be fully processed before receiving confirmation. This improves the customer\nexperience by providing immediate feedback and freeing up resources on the application servers.\nLambda Integration: AWS Lambda functions can be directly triggered by SQS messages. This allows for\nserverless and event-driven order processing. When a new order is placed in the queue, Lambda automatically\ninvokes the processing function. Lambda also scales automatically and integrates seamlessly with SQS.\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\nThe other options are less suitable:\nOption B (SNS): SNS standard topics do not guarantee message ordering or exactly-once delivery. They are\ndesigned for high-throughput, but at the expense of potential message duplication and out-of-order delivery.\nOption C (AppFlow): AppFlow is a data integration service designed for transferring data between AWS\nservices and SaaS applications. It's not the right tool for real-time order processing with exactly-once\nguarantees.\nOption D (X-Ray and CloudWatch): X-Ray is a tracing service for debugging distributed applications.\nCloudWatch is a monitoring service. Neither provides the queueing and processing capabilities required for\nreliable order processing.",
    "links": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has two AWS accounts: Production and Development. The company needs to push code changes in the\nDevelopment account to the Production account. In the alpha phase, only two senior developers on the\ndevelopment team need access to the Production account. In the beta phase, more developers will need access to\nperform testing.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The proposed solution (D) suggests creating an IAM group in the Production account, adding it as a principal\nin a trust policy that specifies the Production account, and then adding developers to the group. This is\nincorrect because the goal is to allow developers from the Development account to access resources in the\nProduction account, not developers already in the Production account to access Production account\nresources. A trust policy would normally specify the Development account as the trusted entity to allow\ncross-account access.\nThe correct approach leverages IAM roles for cross-account access (Option C). Here's a detailed explanation:\n1. IAM Role in Production Account: Create an IAM role within the Production account. This role will\ndefine the permissions developers will have when accessing Production resources. The role's policy\ndocument specifies what actions the role can perform on which resources.\n2. Trust Policy: Crucially, the role's trust policy dictates who is allowed to assume this role. The trust\npolicy should specify the Development account's account ID as a trusted principal. This means only\nentities (users or roles) from the Development account can assume this Production role. This is how\ncross-account access is established.\n3. Developer Access: In the Development account, grant the necessary developers (initially the two\nsenior developers, and later more developers during the beta phase) permission to assume the IAM\nrole in the Production account. This can be done by attaching an IAM policy to their individual IAM\nusers or to an IAM group they belong to in the Development account. This policy will use the\nsts:AssumeRole action, specifying the ARN (Amazon Resource Name) of the Production account role.\nThis setup ensures that:\nLeast Privilege: Developers in the Development account only gain access to the Production account\nresources explicitly permitted by the Production account's role.\nAuditing: All actions taken in the Production account by developers are auditable under the assumed role,\nlinking their actions back to the Development account users who assumed the role.\nScalability: As more developers require access during the beta phase, you can simply add them to the\nDevelopment account's IAM group with the sts:AssumeRole permission, avoiding the need to modify the\nProduction account's role configuration frequently.\nCentralized Control: The Production account maintains control over who can access its resources via the trust\npolicy and the permissions attached to the role.\nOption A is not scalable or centrally managed. Option B has the role in the wrong account. Developers need to\nassume a role in the Production account to access production resources.\nAuthoritative Links:\nIAM Roles: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\nIAM Trust Policies: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_policytrust.html\nAssumeRole API: https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html",
    "links": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_policytrust.html",
      "https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to restrict access to the content of its web application. The company needs to protect the\ncontent by using authorization techniques that are available on AWS. The company also wants to implement a\nserverless architecture for authorization and authentication that has low login latency.\nThe solution must integrate with the web application and serve web content globally. The application currently has\na small user base, but the company expects the application's user base to increase.\nWhich solution will meet these requirements?",
    "options": {
      "B": "While Lambda@Edge provides global"
    },
    "answer": "A",
    "explanation": "The correct answer is A because it provides a serverless, scalable, and globally distributed solution for\nauthentication and authorization with low latency.\nLet's break down why the other options are less suitable:\nOption B: AWS Directory Service for Microsoft Active Directory is primarily designed for enterprises already\nusing Active Directory and wanting to extend it to AWS. It's not ideal for a new, cloud-native application, and\nit's certainly not a serverless authentication solution. Moreover, Lambda alone might struggle to provide the\nlow-latency global authorization needed without a globally distributed caching mechanism. An ALB does not\nserve web content directly; it requires underlying instances.\nOption C: While Amazon Cognito provides authentication and Lambda can handle authorization, S3 Transfer\nAcceleration only accelerates uploads to S3, not content delivery. It doesn't address the requirement of\nserving web content globally. The web application needs more than just faster uploads.\nOption D: AWS Directory Service has the same problem as in Option B. While Lambda@Edge provides global\nauthorization capabilities, Elastic Beanstalk, while useful for deployment, adds operational overhead and\ndoesn't inherently provide the global content delivery capabilities that CloudFront does. It also doesn't\ndirectly support serving web content globally without additional configurations for CDN.\nOption A leverages the following key AWS services:\nAmazon Cognito: Provides secure and scalable authentication. It handles user sign-up, sign-in, and access\ncontrol for web and mobile applications. Cognito is serverless and can scale to millions of users.\nhttps://aws.amazon.com/cognito/\nLambda@Edge: Allows you to run Lambda functions at CloudFront edge locations. This enables low-latency\nauthorization checks as requests are routed to the nearest edge location. It is a serverless compute service.\nhttps://aws.amazon.com/lambda/edge/\nAmazon CloudFront: A content delivery network (CDN) that distributes web content globally, improving\nperformance and reducing latency for users worldwide. It integrates seamlessly with Lambda@Edge for edge-\nbased authorization. https://aws.amazon.com/cloudfront/\nBy combining these services, option A delivers a serverless, highly scalable, and globally distributed\nauthorization solution with low latency, perfectly addressing the problem statement's requirements.\nLambda@Edge allows for custom authorization logic to be executed close to the user, decreasing latency.\nCloudFront enables global content delivery, ensuring a good user experience irrespective of user location.\nCognito is the standard AWS service for user authentication.",
    "links": [
      "https://aws.amazon.com/cognito/",
      "https://aws.amazon.com/lambda/edge/",
      "https://aws.amazon.com/cloudfront/"
    ]
  },
  {
    "question": "CertyIQ\nA development team uses multiple AWS accounts for its development, staging, and production environments.\nTeam members have been launching large Amazon EC2 instances that are underutilized. A solutions architect\nmust prevent large instances from being launched in all accounts.\nHow can the solutions architect meet this requirement with the LEAST operational overhead?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D: Create an organization in AWS Organizations in the management account with the\ndefault policy. Create a service control policy (SCP) that denies the launch of large EC2 instances, and\napply it to the AWS accounts.\nHere's a detailed justification:\nAWS Organizations and SCPs for Centralized Control: AWS Organizations allows you to centrally manage\nand govern multiple AWS accounts. Service Control Policies (SCPs) are a powerful feature within\nOrganizations that enable you to define guardrails and restrictions on the AWS services and actions that users\nand roles can perform within member accounts.\nLeast Operational Overhead: SCPs offer the least operational overhead because they are centrally managed\nfrom the Organizations management account. You define the policy once and apply it to multiple accounts,\neliminating the need to configure individual IAM policies in each account. This significantly reduces\nadministrative effort and ensures consistent enforcement across the environment.\nPreventing Large Instance Launches: The SCP can be written to explicitly deny the ec2:RunInstances action\nwhen the instance type specified in the request matches a large instance size (e.g., m5.2xlarge, c5.4xlarge).\nThis effectively prevents developers from launching such instances in any account where the SCP is in effect.\nIAM Policies vs. SCPs: While IAM policies (option A) can also be used to restrict instance launches, they\nwould need to be updated and applied to every user and account, resulting in significant administrative\noverhead. IAM roles (option C) also require per-account configuration.\nResource Access Manager (RAM) Inapplicability: AWS Resource Access Manager (RAM) is used for sharing\nAWS resources between accounts, not for restricting actions within accounts. Option B is therefore not a\nsuitable solution for this problem.\nCentralized Enforcement and Auditability: SCPs provide centralized enforcement, ensuring that the\nrestrictions are always in place, even if users attempt to bypass them. They also improve auditability, allowing\nadministrators to easily track which policies are applied to which accounts.\nTherefore, using AWS Organizations and SCPs to deny the launch of large EC2 instances is the most efficient\nand scalable solution that meets the requirement with the least operational overhead.\nRelevant Links:\nAWS Organizations documentation\nService Control Policies (SCPs)",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has migrated a fleet of hundreds of on-premises virtual machines (VMs) to Amazon EC2 instances. The\ninstances run a diverse fleet of Windows Server versions along with several Linux distributions. The company\nwants a solution that will automate inventory and updates of the operating systems. The company also needs a\nsummary of common vulnerabilities of each instance for regular monthly reviews.\nWhat should a solutions architect recommend to meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "Option B is the most appropriate solution to automate inventory, updates, and vulnerability assessments for\nthe company's EC2 instances. AWS Systems Manager Patch Manager effectively addresses the need for\noperating system inventory and automated patching on both Windows and Linux instances. Patch Manager\ncentralizes the management of patching across the diverse fleet, ensuring consistent and up-to-date systems.\nAmazon Inspector complements Patch Manager by providing vulnerability assessments. Inspector\nautomatically assesses EC2 instances for software vulnerabilities and unintended network exposure. By\nconfiguring monthly reports in Inspector, the company gains a summary of common vulnerabilities of each\ninstance, fulfilling the requirement for regular monthly reviews.\nOption A is incorrect because AWS Security Hub primarily focuses on security posture management by\naggregating findings from other AWS security services like Inspector and GuardDuty. While Security Hub\nprovides a consolidated view of security alerts, it doesn't directly perform vulnerability scanning and\nassessment at the instance level like Inspector does.Option C is incorrect because AWS Shield Advanced\nprotects against DDoS attacks and doesn't manage EC2 instance patching or vulnerability assessments. AWS\nConfig focuses on configuration management and compliance and can't automate patching.Option D is\nincorrect because Amazon GuardDuty provides threat detection by analyzing logs and network activity and\ndoesn't automate patching or vulnerability assessments. AWS Config focuses on configuration management\nand compliance and can't automate patching.\nTherefore, the combination of Systems Manager Patch Manager for patching and Amazon Inspector for\nvulnerability assessment, coupled with monthly reporting, comprehensively addresses the company's\nrequirements.\nRelevant links for further research:\nAWS Systems Manager Patch Manager: https://docs.aws.amazon.com/systems-\nmanager/latest/userguide/patch-manager.html\nAmazon Inspector: https://docs.aws.amazon.com/inspector/latest/userguide/whatis.html",
    "links": [
      "https://docs.aws.amazon.com/systems-",
      "https://docs.aws.amazon.com/inspector/latest/userguide/whatis.html"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts its application in the AWS Cloud. The application runs on Amazon EC2 instances in an Auto\nScaling group behind an Elastic Load Balancing (ELB) load balancer. The application connects to an Amazon\nDynamoDB table.\nFor disaster recovery (DR) purposes, the company wants to ensure that the application is available from another\nAWS Region with minimal downtime.\nWhich solution will meet these requirements with the LEAST downtime?",
    "options": {
      "B": "Option C"
    },
    "answer": "C",
    "explanation": "The goal is to achieve minimal downtime DR for an application using EC2, ELB, and DynamoDB. Option C\noffers the best approach by pre-configuring most resources and leveraging DynamoDB's global table feature.\nHere's why C is superior:\n1. DynamoDB Global Tables: DynamoDB global tables provide near real-time replication of data across\nAWS Regions. This ensures that the DR Region has the latest data, minimizing data loss during a\nfailover. https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables.html\n2. CloudFormation Template: Using a CloudFormation template allows you to define your EC2 and ELB\ninfrastructure as code. When a DR event occurs, you can quickly launch the stack in the DR Region.\nThis automates the deployment process and reduces the time needed to bring the application back\nonline.\n3. DNS Failover: Configuring DNS failover (using services like Amazon Route 53) allows you to redirect\ntraffic from the primary Region to the DR Region in the event of a failure. This is crucial for ensuring\nthat users can still access the application, albeit from the DR Region.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html\nWhy other options are less ideal:\nOption A: While it creates Auto Scaling group and ELB beforehand, without a CloudFormation template, the\nconfiguration might be inconsistent and difficult to manage.\nOption B: Similar to Option A, lacks the pre-configured infrastructure management benefit of Auto Scaling.\nOption D: Using CloudWatch alarms and Lambda for Route 53 updates adds complexity and potential latency\nto the failover process. Direct DNS failover is faster and simpler. Furthermore, a 10-minute evaluation period is\ntoo long for achieving minimal downtime.\nIn summary, option C minimizes downtime by replicating data in near real-time with DynamoDB global tables\nand utilizing infrastructure as code (CloudFormation) to rapidly deploy the application in the DR region and\nemploying DNS failover to seamlessly switch traffic to the DR Region.",
    "links": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables.html",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an application on Amazon EC2 instances in a private subnet. The application needs to store and\nretrieve data in Amazon S3 buckets. According to regulatory requirements, the data must not travel across the\npublic internet.\nWhat should a solutions architect do to meet these requirements MOST cost-effectively?",
    "options": {
      "D": "Deploy an S3 gateway endpoint to access the S3 buckets.",
      "C": "While they also avoid public internet traffic, they incur"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Deploy an S3 gateway endpoint to access the S3 buckets.\nHere's a detailed justification:\nThe primary requirement is that data transfers between the EC2 instances and S3 buckets must not traverse\nthe public internet while also aiming for a cost-effective solution.\nGateway Endpoints: Gateway endpoints are designed specifically for accessing S3 and DynamoDB from\nwithin a VPC without using public IPs or NAT gateways. They operate at Layer 3 (Network Layer) and are\nhighly cost-effective because there are no data processing or hourly charges associated with their use. Traffic\nto S3 via a gateway endpoint remains within the AWS network.\nInterface Endpoints: Interface endpoints use AWS PrivateLink, which provides private connectivity to AWS\nservices using private IP addresses within your VPC. While they also avoid public internet traffic, they incur\nhourly charges and data processing charges, making them less cost-effective than gateway endpoints for S3\naccess when only S3 is involved. Interface endpoints operate at Layer 4 (Transport Layer).\nNAT Gateway: A NAT gateway allows instances in a private subnet to connect to the internet or other AWS\nservices. However, traffic routed through a NAT gateway traverses the public internet to reach S3. This\nviolates the stated requirement of keeping data off the public internet.\nAWS Storage Gateway: AWS Storage Gateway is used to integrate on-premises environments with AWS\nstorage services. It is not the correct solution for EC2 instances already running within AWS that need private\naccess to S3. It also is more complex and costly than the endpoint solutions.\nTherefore, deploying an S3 gateway endpoint is the most cost-effective way to meet the requirement of\naccessing S3 from a private subnet without traversing the public internet. Gateway endpoints are free of\nusage charges beyond the standard S3 storage and data transfer costs, and they are designed specifically for\nS3 and DynamoDB access.It leverages the AWS backbone and is the most direct way for the given scenario.\nFurther Research:\nVPC Endpoints: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\nGateway Endpoints: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html\nInterface Endpoints: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-interface.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-interface.html"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts an application on Amazon EC2 instances that run in a single Availability Zone. The application is\naccessible by using the transport layer of the Open Systems Interconnection (OSI) model. The company needs the\napplication architecture to have high availability.\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": {
      "B": "Configure a Network Load Balancer in front of the EC2 instances.",
      "D": "Create an Auto Scaling group for the EC2 instances. Configure the Auto Scaling group to use multiple",
      "A": "Configure new EC2 instances in a different Availability Zone. Use Amazon Route 53 to route traffic to all",
      "C": "Configure a Network Load Balancer for TCP traffic to the instances. Configure an Application Load"
    },
    "answer": "B",
    "explanation": "Here's a breakdown of why options B and D are the most cost-effective solutions for achieving high\navailability for the described application:\nB. Configure a Network Load Balancer in front of the EC2 instances.\nHigh Availability: A Network Load Balancer (NLB) is designed for handling TCP traffic at high throughput and\nlow latency. Critically, NLBs operate at layer 4 of the OSI model (transport layer), aligning with the\napplication's described usage. They automatically distribute incoming traffic across multiple healthy EC2\ninstances.\nAvailability Zone Awareness: NLBs can distribute traffic across multiple Availability Zones within a region.\nThis means that if one Availability Zone experiences an outage, the NLB can automatically route traffic to\ninstances in other healthy Availability Zones, ensuring continuous application availability.\nHealth Checks: NLBs continuously monitor the health of registered EC2 instances. If an instance fails a health\ncheck, the NLB stops sending traffic to that instance, preventing users from experiencing disruptions.\nCost-Effectiveness: NLBs are highly scalable and cost-efficient. You pay for what you use, based on the\nnumber of connections and the amount of data processed. NLBs are generally more cost-effective than\nsolutions that require complex custom configurations or larger infrastructure footprints.\nD. Create an Auto Scaling group for the EC2 instances. Configure the Auto Scaling group to use multiple\nAvailability Zones. Configure the Auto Scaling group to run application health checks on the instances.\nHigh Availability: Auto Scaling groups (ASGs) allow you to maintain a desired number of EC2 instances\nautomatically. By configuring the ASG to span multiple Availability Zones, you ensure that if one AZ fails,\ninstances in other AZs can continue to serve traffic.\nAutomatic Recovery: ASGs can automatically replace unhealthy instances. When an EC2 instance fails a\nhealth check, the ASG terminates the unhealthy instance and launches a new instance to maintain the desired\ncapacity. This self-healing capability enhances application availability.\nHealth Checks: ASGs integrate with Elastic Load Balancing (ELB) health checks. This allows the ASG to\nmonitor the health of instances based on the application's responsiveness.\nCost-Effectiveness: ASGs enable you to scale your infrastructure up or down based on demand. This helps\nyou optimize your costs by only paying for the resources you need. Also, the automated nature of ASGs\nreduces the need for manual intervention, which can save time and resources.\nWhy other options are less optimal:\nA. Configure new EC2 instances in a different Availability Zone. Use Amazon Route 53 to route traffic to all\ninstances. While this improves availability, Route 53 alone lacks the dynamic health-checking capabilities of\nNLB or ASG health checks. It's slower to react to instance failures and less efficient in distributing load.\nC. Configure a Network Load Balancer for TCP traffic to the instances. Configure an Application Load\nBalancer for HTTP and HTTPS traffic to the instances. This is unnecessarily complex and expensive if the\napplication primarily uses TCP, as stated. An ALB is designed for HTTP/HTTPS traffic, and using both would\nintroduce extra overhead.\nE. Create an Amazon CloudWatch alarm. Configure the alarm to restart EC2 instances that transition to a\nstopped state. Restarting instances is not an effective high-availability strategy because it can cause\ndowntime. This approach does not prevent disruptions as well as an NLB with an ASG.\nAuthoritative Links:\nNetwork Load Balancer: https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\nAuto Scaling Groups: https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html\nHigh Availability: https://aws.amazon.com/reliability/high-availability/",
    "links": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html",
      "https://aws.amazon.com/reliability/high-availability/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses Amazon S3 to host its static website. The company wants to add a contact form to the webpage.\nThe contact form will have dynamic server-side components for users to input their name, email address, phone\nnumber, and user message.\nThe company expects fewer than 100 site visits each month. The contact form must notify the company by email\nwhen a customer fills out the form.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "B": "Here's"
    },
    "answer": "B",
    "explanation": "The most cost-effective solution for a low-traffic contact form on an S3-hosted website is option B. Here's\nwhy:\nScalability and Cost: Lambda and API Gateway offer a serverless architecture. You only pay for the actual\ninvocations, making it incredibly cheap for under 100 monthly visits. ECS (A) would involve running containers\nconstantly, incurring higher costs even when idle. EC2 (D) involves substantial fixed costs for the instance.\nAmplify Hosting (C) can be suitable, but the chosen components make it less economical compared to a fully\nserverless approach.\nDynamic Content Handling: API Gateway and Lambda easily handle dynamic content. The Lambda function\ncan generate and serve the initial contact form. The other Lambda function is triggered by the API Gateway\nupon form submission.\nEmail Notification: SNS provides a straightforward way to send email notifications. When the contact form is\nsubmitted, the Lambda function publishes a message to the SNS topic, which in turn triggers an email to the\ncompany. SES is needed to send the mail using AWS.\nSimplicity: API Gateway, Lambda, and SNS are straightforward services to configure and integrate. SQS (C) is\nnot directly used to send mails, and it is less appropriate for notification of the contact forms.\nOption D is highly inefficient and costly. Migrating to EC2 for a simple contact form overcomplicates the\nsetup, resulting in higher maintenance overhead and costs. Client-side scripting might work for basic form\nvalidation, but it can't handle server-side logic or send emails directly. Option A, while functional, introduces\nmore complexity than needed and may incur higher costs due to ECS cluster maintenance. Option C is\nreasonable, but SQS is not specifically suited to send email notifications.\nAuthoritative Links:\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon API Gateway: https://aws.amazon.com/api-gateway/\nAmazon SNS: https://aws.amazon.com/sns/\nAmazon SES: https://aws.amazon.com/ses/",
    "links": [
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/api-gateway/",
      "https://aws.amazon.com/sns/",
      "https://aws.amazon.com/ses/"
    ]
  },
  {
    "question": "CertyIQ\nA company creates dedicated AWS accounts in AWS Organizations for its business units. Recently, an important\nnotification was sent to the root user email address of a business unit account instead of the assigned account\nowner. The company wants to ensure that all future notifications can be sent to different employees based on the\nnotification categories of billing, operations, or security.\nWhich solution will meet these requirements MOST securely?",
    "options": {},
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A is the most secure and appropriate solution, along with\nsupporting concepts and links:\nOption A is the most secure because it utilizes alternate contacts and a centrally managed email strategy. By\nconfiguring each AWS account to use a single, company-managed email address (that authorized personnel\ncan access) for the root user, the company retains greater control and auditing capabilities. This avoids tying\ncritical notifications to individual employee email addresses which can pose a security risk if an employee\nleaves the organization. Furthermore, configuring alternate contacts with distribution lists for billing, security,\nand operations teams ensures that the right stakeholders receive the right notifications without granting\nunnecessary root user access. This approach follows the principle of least privilege.\nOption B is less secure. Distributing root user email to distribution lists raises security concerns. It increases\nthe attack surface because more individuals potentially have access to highly sensitive information and\nactions associated with the root user.\nOption C is less secure. While using individual company-managed emails seems better, it still ties root user\naccess to specific individuals. If that person leaves, there's a risk the email isn't properly transitioned. Also,\nrelying on individuals rather than distribution lists for functional roles (billing, security) is operationally brittle.\nOption D is less secure. Using aliases and centralized mailboxes for root user emails can be acceptable, but it\nneeds careful management. The primary issue here is it only creates ONE distribution list for each category\n(billing, security, operations) across all accounts. This isn't ideal; each AWS account should ideally have its\nown specific distribution lists for alternate contacts, improving isolation and reducing the risk of cross-\naccount information leakage.\nIn summary, Option A provides the best balance of security, operational efficiency, and adherence to the\nprinciple of least privilege by centralizing root user access management and leveraging alternate contacts\nwith dedicated distribution lists.\nRelevant links for further research:\nAWS Organizations best practices: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_best-\npractices.html\nAWS Account Root User: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html\nAlternate Contacts: https://docs.aws.amazon.com/accounts/latest/reference/using-orgs-managing-\naccounts.html",
    "links": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_best-",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html",
      "https://docs.aws.amazon.com/accounts/latest/reference/using-orgs-managing-"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an ecommerce application on AWS. Amazon EC2 instances process purchases and store the\npurchase details in an Amazon Aurora PostgreSQL DB cluster.\nCustomers are experiencing application timeouts during times of peak usage. A solutions architect needs to\nrearchitect the application so that the application can scale to meet peak usage demands.\nWhich combination of actions will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": {
      "C": "Update the application to send the purchase requests to an Amazon Simple Queue Service (Amazon"
    },
    "answer": "B",
    "explanation": "The best combination of actions to address the timeouts and scale the ecommerce application cost-\neffectively is B and C.\nB. Configure the application to use an Amazon ElastiCache cluster in front of the Aurora PostgreSQL DB\ncluster:\nElastiCache acts as an in-memory cache, storing frequently accessed data. This significantly reduces the load\non the Aurora PostgreSQL database by serving read requests from the cache instead of the database. This\ndirectly addresses the performance bottlenecks caused by heavy read operations during peak usage. Using\nElastiCache improves response times and reduces database load, enabling the application to handle more\nconcurrent requests. This also avoids costly resizing of database instance to address peak read\ndemands.https://aws.amazon.com/elasticache/\nC. Update the application to send the purchase requests to an Amazon Simple Queue Service (Amazon\nSQS) queue. Configure an Auto Scaling group of new EC2 instances that read from the SQS queue:\nIntroducing an SQS queue decouples the purchase request processing from the immediate user interaction.\nWhen a customer initiates a purchase, the request is placed in the SQS queue. An Auto Scaling group of EC2\ninstances then retrieves these requests from the queue and processes them asynchronously. This ensures\nthat the application remains responsive to user requests, even during peak loads. The Auto Scaling group\nautomatically scales up or down based on the queue depth, providing elasticity and cost optimization. By\noffloading processing to a queue, the application can handle a much higher volume of requests without\nexperiencing timeouts. This distributed queueing is a classic pattern for handling spiky\nworkloads.https://aws.amazon.com/sqs/\nWhy other options are not as suitable:\nA: While RDS Proxy can help manage database connections, simply retrying purchases on the same\noverloaded EC2 instances will likely perpetuate the problem and not effectively address the fundamental\nscaling bottleneck.\nD: Lambda functions have execution time limits and might not be suitable for long-running purchase\nprocessing tasks. Furthermore, retrying long running tasks from Lambda may lead to throttling issues.\nE: API Gateway usage plans primarily manage API request rates and quotas. While useful for controlling\naccess, they do not directly address the underlying scaling issues related to database load or processing\ncapacity.\nTherefore, using ElastiCache for caching and SQS with Auto Scaling for asynchronous processing is the most\ncost-effective and efficient approach to scaling the ecommerce application to handle peak usage demands.",
    "links": [
      "https://aws.amazon.com/elasticache/",
      "https://aws.amazon.com/sqs/"
    ]
  },
  {
    "question": "CertyIQ\nA company that uses AWS Organizations runs 150 applications across 30 different AWS accounts. The company\nused AWS Cost and Usage Report to create a new report in the management account. The report is delivered to an\nAmazon S3 bucket that is replicated to a bucket in the data collection account.\nThe companys senior leadership wants to view a custom dashboard that provides NAT gateway costs each day\nstarting at the beginning of the current month.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The requirement is to create a custom dashboard showing daily NAT gateway costs from the beginning of the\nmonth, sourced from the AWS Cost and Usage Report (CUR) across 30 AWS accounts managed by AWS\nOrganizations. QuickSight and Athena are the appropriate services to achieve this goal.\nOption B suggests using QuickSight for dashboarding and Athena to query the CUR data. QuickSight is a\nbusiness intelligence service that allows you to create interactive dashboards and visualizations. Athena is a\nserverless query service that allows you to analyze data stored in Amazon S3 using standard SQL. The CUR is\ndelivered to an S3 bucket, making it easily accessible to Athena. Athena can then query the CUR data to\nextract the daily NAT gateway costs. Finally, QuickSight can connect to Athena to visualize the data in a\ncustom dashboard.\nOption A suggests using QuickSight for dashboarding and AWS DataSync to query the CUR report. While\nDataSync efficiently moves large amounts of data between on-premises storage and AWS, it does not provide\na mechanism for querying and analyzing the report data directly. DataSync is primarily used for data\nmigration and replication, not SQL-based querying. This approach wouldn't readily allow for extraction of the\nspecific daily NAT Gateway costs.\nOption C suggests using CloudWatch for dashboarding and AWS DataSync to query the CUR report.\nCloudWatch is primarily a monitoring service for AWS resources and applications. It is suitable for visualizing\nmetrics, logs, and events, but not for querying complex structured data like the CUR. Like option A, DataSync\nis not designed for querying the CUR report.\nOption D suggests using CloudWatch for dashboarding and Athena to query the CUR report. While Athena can\nquery the CUR data, CloudWatch is not the best tool for creating interactive dashboards. It's mainly intended\nfor monitoring operational metrics, logs, and events. QuickSight is purpose-built for data visualization and\ncreating custom dashboards.\nTherefore, using QuickSight for the dashboard and Athena for querying the CUR data is the most suitable and\ncost-effective solution. Athena offers the flexibility to query and filter the CUR data according to the\nspecified requirements (daily NAT gateway costs from the beginning of the month), and QuickSight enables\nvisualizing that data in a customized dashboard.\nRelevant links for further research:\nAWS QuickSight: https://aws.amazon.com/quicksight/\nAmazon Athena: https://aws.amazon.com/athena/\nAWS Cost and Usage Report: https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html\nAWS CloudWatch: https://aws.amazon.com/cloudwatch/\nAWS DataSync: https://aws.amazon.com/datasync/",
    "links": [
      "https://aws.amazon.com/quicksight/",
      "https://aws.amazon.com/athena/",
      "https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html",
      "https://aws.amazon.com/cloudwatch/",
      "https://aws.amazon.com/datasync/"
    ]
  },
  {
    "question": "CertyIQ\nA company is hosting a high-traffic static website on Amazon S3 with an Amazon CloudFront distribution that has\na default TTL of 0 seconds. The company wants to implement caching to improve performance for the website.\nHowever, the company also wants to ensure that stale content is not served for more than a few minutes after a\ndeployment.\nWhich combination of caching methods should a solutions architect implement to meet these requirements?\n(Choose two.)",
    "options": {},
    "answer": "A",
    "explanation": "Let's break down why options A and C are the best choices for implementing caching in this scenario while\nminimizing stale content.\nOption A: Set the CloudFront default TTL to 2 minutes.\nSetting a default TTL (Time To Live) on the CloudFront distribution is a fundamental step for enabling caching.\nTTL dictates how long CloudFront edge locations will store a copy of the content before checking back with\nthe origin (S3 in this case) for a fresh version. A TTL of 2 minutes directly addresses the requirement to avoid\nserving stale content for more than a few minutes. With a 0-second TTL, CloudFront currently fetches every\nrequest from S3, negating the benefits of a CDN. A short TTL allows for more frequent updates while still\nreducing the load on S3 and improving latency for users.\nOption C: Add a Cache-Control private directive to the objects in Amazon S3.\nThe Cache-Control: private directive is crucial because it specifies that the content should only be cached by\nthe browser (viewer) making the request and not by intermediate caches such as proxies or, importantly,\nCloudFront. However, in this case the object is stored in S3, so this option will not achieve the goal of caching\nthe object on the CloudFront side. A Cache-Control: public on the S3 object, along with a CloudFront TTL,\nallows CloudFront to cache the object. But this setup is not enough to fulfill the requirements of the use case,\nit needs the option A to be enabled. So option C is not suitable to be implemented and it is not part of the right\nanswer.\nThe correct answer is AE.\nOption A: Set the CloudFront default TTL to 2 minutes.\nSetting a default TTL on the CloudFront distribution is fundamental for enabling caching. TTL dictates how\nlong CloudFront edge locations will store content before checking back with the origin (S3) for a fresh\nversion. A TTL of 2 minutes directly addresses the requirement to avoid serving stale content for more than a\nfew minutes. A zero-second TTL causes CloudFront to fetch every request from S3, negating CDN benefits.\nOption E: Add a Cache-Control max-age directive of 24 hours to the objects in Amazon S3. On deployment,\ncreate a CloudFront invalidation to clear any changed files from edge caches.\nAdding Cache-Control: max-age=86400 (24 hours) to S3 objects instructs CloudFront to cache the objects for\nup to 24 hours. This maximizes caching effectiveness and reduces S3 origin requests. The key here is the\nsubsequent CloudFront invalidation. After a deployment with content changes, an invalidation tells\nCloudFront to remove existing cached versions of specific files. This forces CloudFront to fetch the latest\nversions from S3, ensuring users always receive the most up-to-date content. This avoids the problem of the\npotentially stale objects.\nLet's examine why the other options are incorrect:\nOption B: Setting a default TTL on the S3 bucket itself is not applicable. S3 buckets don't have TTL settings in\nthe way that CloudFront distributions do. S3 focuses on storage and object management, not content delivery\nand caching.\nOption D: While Lambda@Edge can be used for adding or modifying headers, it's unnecessary and adds\ncomplexity to this scenario. Setting the Cache-Control header directly on the S3 objects is a simpler and more\nefficient approach.\nAuthoritative Links:\nCloudFront TTL: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html\nCloudFront Invalidation:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html\nS3 Cache-Control: https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingMetadata.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingMetadata.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs its application by using Amazon EC2 instances and AWS Lambda functions. The EC2 instances run\nin private subnets of a VP",
    "options": {
      "C": "The solution would therefore fail to fulfill a critical requirement."
    },
    "answer": "C",
    "explanation": "The correct answer is C: Purchase a Compute Savings Plan. Connect the Lambda functions to the private\nsubnets that contain the EC2 instances.\nHere's a breakdown of why this is the most cost-effective and functional solution:\nLambda and VPC Access: Lambda functions, by default, don't have access to resources inside a VPC. To grant\nthis access, you need to connect them to the VPC's subnets. The question explicitly states that the Lambda\nfunctions require direct network access to the EC2 instances. Therefore, the Lambda function must be inside\nof the VPC.\nPrivate Subnets: Since the EC2 instances reside in private subnets, connecting the Lambda functions to those\nsame private subnets allows them to communicate directly with the EC2 instances without exposing them to\nthe public internet. Option B's proposal of using public subnets introduces unnecessary security risks and is\nnot aligned with best practices when EC2 instances are designed to be private.\nSavings Plans (Compute vs. EC2 Instance): Savings Plans offer discounted pricing in exchange for a\ncommitment to a consistent amount of compute usage over a one- or three-year term. There are two types\nrelevant here: Compute Savings Plans and EC2 Instance Savings Plans. A Compute Savings Plan is a better\noption than an EC2 Instance Savings Plan in this situation. This is because a Compute Savings Plan gives you\nflexibility across EC2, AWS Lambda, and AWS Fargate. Since the question stated that the application will use\nboth EC2 instances and Lambda functions (and that the number of Lambda functions will increase), a\nCompute Savings Plan better addresses the overall compute needs.\nCost Optimization: A Compute Savings Plan provides cost savings on both the EC2 instances and Lambda\nfunctions. This is essential to meet the \"minimize costs\" requirement. Since both EC2 and Lambda resources\nare in the compute footprint, a Compute Savings Plan is preferred over an EC2 Instance Savings Plan.\nLambda in Lambda Service VPC (Option D): The solution specifies that the Lambda functions need direct\nnetwork access to the EC2 instances. Lambda functions kept in the Lambda service VPC do not have direct\nnetwork access to a customer's VPC. The solution would therefore fail to fulfill a critical requirement.\nIn summary, connecting the Lambda functions to the existing private subnets and purchasing a Compute\nSavings Plan provides the required connectivity and cost-effectiveness across both EC2 and Lambda, making\noption C the best solution.\nSupporting Documentation:\nAWS Lambda VPC Networking: https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html\nAWS Savings Plans: https://aws.amazon.com/savingsplans/\nChoosing the right Savings Plan: https://aws.amazon.com/savingsplans/faq/",
    "links": [
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html",
      "https://aws.amazon.com/savingsplans/",
      "https://aws.amazon.com/savingsplans/faq/"
    ]
  },
  {
    "question": "CertyIQ\nA company has deployed a multi-account strategy on AWS by using AWS Control Tower. The company has\nprovided individual AWS accounts to each of its developers. The company wants to implement controls to limit\nAWS resource costs that the developers incur.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "B",
    "explanation": "The most effective and least operationally intensive solution to control AWS resource costs for developers in\na multi-account AWS Control Tower environment is to use AWS Budgets. Here's why:\nAWS Budgets: This service is designed specifically for cost management. You can create budgets at the\naccount level, perfectly aligning with individual developer accounts. (https://aws.amazon.com/aws-cost-\nmanagement/aws-budgets/)\nBudget Alerts: AWS Budgets allow setting alerts based on actual and forecasted costs. Developers receive\ntimely notifications when they are approaching or exceeding their allocated budget, promoting cost\nawareness.\nBudget Actions: AWS Budgets Actions offer automated responses to budget breaches. Applying a DenyAll\npolicy to a developer's IAM role upon exceeding the budget prevents further resource provisioning,\neffectively capping costs.\nLeast Operational Overhead: AWS Budgets integrates seamlessly with AWS accounts and IAM.\nConfiguration requires minimal custom coding or infrastructure, resulting in lower operational overhead\ncompared to other options.\nLet's analyze why the other options are less ideal:\nOption A (Tagging and Lambda): While tagging is a good practice for cost allocation, relying on developers to\nconsistently tag resources and using Lambda to enforce it is prone to errors and requires ongoing\nmaintenance. required-tags checks compliance but doesn't actively limit costs.\nOption C (Cost Explorer and Anomaly Detection): Cost Explorer and Anomaly Detection are valuable for\nmonitoring but don't prevent developers from exceeding budgets. They provide reactive alerts, not proactive\ncost control.\nOption D (Service Catalog and Lambda): Service Catalog can limit resource types, but controlling costs\nwithin that limit is challenging. Using Lambda to start/stop resources introduces complexity and might disrupt\ndevelopment workflows. It also limits developer flexibility.\nIn summary, AWS Budgets, with its alerting and action capabilities, offers the most direct and automated way\nto limit AWS resource costs for developers while minimizing operational burden in an AWS Control Tower\nmulti-account setup.",
    "links": [
      "https://aws.amazon.com/aws-cost-"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is designing a three-tier web application. The architecture consists of an internet-facing\nApplication Load Balancer (ALB) and a web tier that is hosted on Amazon EC2 instances in private subnets. The\napplication tier with the business logic runs on EC2 instances in private subnets. The database tier consists of\nMicrosoft SQL Server that runs on EC2 instances in private subnets. Security is a high priority for the company.\nWhich combination of security group configurations should the solutions architect use? (Choose three.)",
    "options": {
      "B": "This is essential because the ALB acts as the entry point for all external HTTPS requests. By"
    },
    "answer": "A",
    "explanation": "Let's break down why options A, C, and E are the correct security group configurations for this three-tier\narchitecture, focusing on the principle of least privilege and defense in depth.\nA: Configure the security group for the web tier to allow inbound HTTPS traffic from the security group for\nthe ALB. This is essential because the ALB acts as the entry point for all external HTTPS requests. By\nallowing inbound traffic only from the ALB's security group, you restrict access to the web tier to only\nlegitimate requests routed through the load balancer, preventing direct access from the internet. This\nadheres to the principle of least privilege.\nC: Configure the security group for the database tier to allow inbound Microsoft SQL Server traffic from the\nsecurity group for the application tier. The application tier needs to communicate with the database tier to\nread and write data. Restricting inbound SQL Server (typically port 1433) traffic only from the application\ntier's security group is crucial. This prevents unauthorized access to the database from any other source,\nwhich is a fundamental security best practice.\nE: Configure the security group for the application tier to allow inbound HTTPS traffic from the security\ngroup for the web tier. The application tier processes the business logic based on requests it receives from\nthe web tier. Allowing inbound HTTPS (or, ideally, internal HTTP on port 80 if using TLS termination at the\nweb tier) traffic only from the web tier's security group ensures that only the web tier can initiate requests to\nthe application tier.\nNow, let's discuss why the other options are incorrect:\nB: Configure the security group for the web tier to allow outbound HTTPS traffic to 0.0.0.0/0. Allowing\nunrestricted outbound traffic (0.0.0.0/0) from the web tier is a security risk. While outbound internet access\nmay be necessary for some operations (e.g., updates, logging), it should be limited to specific, known\ndestinations and protocols. In many setups, the EC2 instance in the private subnet can leverage a NAT\ngateway in the public subnet. If the instance only has to connect to AWS services, VPC endpoints are a more\nsecure and preferred solution.\nD: Configure the security group for the database tier to allow outbound HTTPS traffic and Microsoft SQL\nServer traffic to the security group for the web tier. The database tier should not be initiating connections\nback to the web tier. The communication flow is typically from the web tier to the application tier to the\ndatabase tier. Allowing the database tier to initiate connections to other tiers is a security violation.\nF: Configure the security group for the application tier to allow outbound HTTPS traffic and Microsoft SQL\nServer traffic to the security group for the web tier. Similar to option D, the application tier needs to\ncommunicate only with the database tier, not the web tier. The standard pattern has the web tier fronting the\napplication tier.\nIn summary, the correct security group configuration focuses on restricting traffic based on the principle of\nleast privilege, allowing only necessary communication between tiers and blocking all other access.\nFurther Reading:\nAWS Security Groups\nNetwork Access Control Lists (NACLs)\nAWS Security Best Practices",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has released a new version of its production application. The company's workload uses Amazon EC2,\nAWS Lambda, AWS Fargate, and Amazon SageMaker.\nThe company wants to cost optimize the workload now that usage is at a steady state. The company wants to cover\nthe most services with the fewest savings plans.\nWhich combination of savings plans will meet these requirements? (Choose two.)",
    "options": {},
    "answer": "C",
    "explanation": "Here's a breakdown of why the correct answer is C and D, and why the other options are less suitable:\nWhy C (Purchase a SageMaker Savings Plan) is correct: SageMaker Savings Plans are specifically designed\nto cover the costs associated with using Amazon SageMaker. Since the company's workload includes\nSageMaker, purchasing this savings plan directly addresses a key service contributing to their overall\nexpenses. This is the most direct and cost-effective way to optimize SageMaker costs.\nWhy D (Purchase a Compute Savings Plan for Lambda, Fargate, and Amazon EC2) is correct: Compute\nSavings Plans offer flexibility and apply to EC2, Lambda, and Fargate usage. This single savings plan covers a\nsubstantial portion of the company's workload (Lambda, Fargate, and EC2), providing significant cost\noptimization across these services with a single commitment. Compute Savings Plan provides the flexibility to\nchange instance types, operating systems, tenancies, and AWS Regions while still benefiting from the savings\nplan price.\nWhy A (Purchase an EC2 Instance Savings Plan for Amazon EC2 and SageMaker) is incorrect: EC2 Instance\nSavings Plans are designed for EC2 instance usage within a specific instance family and AWS Region. While it\ncovers EC2, it does not apply to SageMaker directly.\nWhy B (Purchase a Compute Savings Plan for Amazon EC2, Lambda, and SageMaker) is incorrect: While\nCompute Savings Plans do apply to EC2 and Lambda, they do not cover SageMaker costs directly. Using a\nSageMaker Savings Plan to cost optimize SageMaker will be more effective.\nWhy E (Purchase an EC2 Instance Savings Plan for Amazon EC2 and Fargate) is incorrect: EC2 Instance\nSavings Plans are specific to EC2 instance families within a region and cannot be directly used for Fargate.\nFargate is covered by compute Savings Plans.\nBy selecting a SageMaker Savings Plan (C) and a Compute Savings Plan (D), the company efficiently covers\nall their core services (EC2, Lambda, Fargate, and SageMaker) with minimal complexity, achieving cost\noptimization across their entire workload.\nHere are authoritative links for further research:\nAWS Savings Plans: https://aws.amazon.com/savingsplans/\nSageMaker Savings Plans: https://aws.amazon.com/sagemaker/pricing/\nEC2 Instance Savings Plans: https://aws.amazon.com/savingsplans/compute-savings-plans/\nCompute Savings Plans: https://aws.amazon.com/savingsplans/compute-savings-plans/",
    "links": [
      "https://aws.amazon.com/savingsplans/",
      "https://aws.amazon.com/sagemaker/pricing/",
      "https://aws.amazon.com/savingsplans/compute-savings-plans/",
      "https://aws.amazon.com/savingsplans/compute-savings-plans/"
    ]
  },
  {
    "question": "CertyIQ\nA company uses a Microsoft SQL Server database. The company's applications are connected to the database. The\ncompany wants to migrate to an Amazon Aurora PostgreSQL database with minimal changes to the application\ncode.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": {
      "C": "Migrate the database schema and data by using the AWS Schema Conversion Tool (AWS SCT) and AWS",
      "B": "Enable Babelfish on Aurora PostgreSQL to run the SQL queries from the applications.",
      "A": "Use the AWS Schema Conversion Tool (AWS SCT) to rewrite the SQL queries in the applications: AWS",
      "D": "Use Amazon RDS Proxy to connect the applications to Aurora PostgreSQL: RDS Proxy is used for"
    },
    "answer": "B",
    "explanation": "The correct answer is BC. Here's why:\nB. Enable Babelfish on Aurora PostgreSQL to run the SQL queries from the applications.\nBabelfish for Aurora PostgreSQL is specifically designed to allow Aurora PostgreSQL to understand and\nprocess SQL Server Transact-SQL (T-SQL) commands directly. This drastically minimizes the need for\napplication code changes during migration from Microsoft SQL Server. By enabling Babelfish, the applications\ncan continue to send T-SQL queries, and Babelfish translates them into a format Aurora PostgreSQL can\nunderstand, thus fulfilling the requirement of minimal application changes. This approach avoids rewriting\nqueries in the application code, making it the quickest method to get compatible behavior.\nhttps://aws.amazon.com/rds/aurora/babelfish/\nC. Migrate the database schema and data by using the AWS Schema Conversion Tool (AWS SCT) and AWS\nDatabase Migration Service (AWS DMS).\nAWS SCT helps convert the database schema from Microsoft SQL Server to Aurora PostgreSQL. It identifies\npotential compatibility issues and provides recommendations for resolving them. AWS DMS then migrates the\nactual data from the source SQL Server database to the Aurora PostgreSQL database. This is the standard\napproach for migrating database schemas and data to AWS, especially when dealing with different database\nengines. It ensures that the schema is properly converted and the data is transferred accurately and\nefficiently.\nhttps://aws.amazon.com/dms/https://aws.amazon.com/sct/\nWhy other options are incorrect:\nA. Use the AWS Schema Conversion Tool (AWS SCT) to rewrite the SQL queries in the applications: AWS\nSCT primarily focuses on schema conversion and reporting, not directly rewriting application queries. While\nSCT can provide guidance for adapting queries, automating this in application code is beyond its main\nfunction and would involve significant manual effort, conflicting with the \"minimal changes\" requirement.\nD. Use Amazon RDS Proxy to connect the applications to Aurora PostgreSQL: RDS Proxy is used for\nmanaging database connections and improving application scalability, resilience, and security. It does not\nhandle query translation or schema conversion, so it wouldn't address the core requirement of application\ncompatibility with Aurora PostgreSQL.\nE. Use AWS Database Migration Service (AWS DMS) to rewrite the SQL queries in the applications: AWS\nDMS is designed for data migration, not for rewriting SQL queries in the application layer. Its primary function\nis to replicate data changes between database systems.",
    "links": [
      "https://aws.amazon.com/rds/aurora/babelfish/",
      "https://aws.amazon.com/dms/https://aws.amazon.com/sct/"
    ]
  },
  {
    "question": "CertyIQ\nA company plans to rehost an application to Amazon EC2 instances that use Amazon Elastic Block Store (Amazon\nEBS) as the attached storage.\nA solutions architect must design a solution to ensure that all newly created Amazon EBS volumes are encrypted\nby default. The solution must also prevent the creation of unencrypted EBS volumes.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B: Use AWS Config. Configure the encrypted-volumes identifier. Apply the default\nAWS Key Management Service (AWS KMS) key.\nHere's why:\nAWS Config allows you to assess, audit, and evaluate the configurations of your AWS resources. By using\nAWS Config, you can create rules that check whether your EBS volumes are encrypted. The encrypted-volumes\nmanaged rule specifically checks for EBS volume encryption. You can configure this rule to automatically\nremediate non-compliant resources, specifically unencrypted EBS volumes. By applying a default AWS KMS\nkey to this rule, any newly created EBS volumes that are not explicitly encrypted using a KMS key during\ncreation will be automatically encrypted using the specified key.\nOption A (Configure the EC2 account attributes) while useful for default encryption, does not inherently\nprevent the creation of unencrypted volumes. Users could still explicitly disable encryption during volume\ncreation. AWS Config provides the crucial enforcement aspect.\nOption C (AWS Systems Manager) involves creating copies of EBS volumes. This is unnecessary overhead and\ndoesn't directly prevent unencrypted volumes from being created initially. It's a reactive, rather than\nproactive, approach.\nOption D (AWS Migration Hub) is primarily for tracking application migrations and does not directly control\nEBS volume encryption policies. It's not relevant to the problem of enforcing encryption on newly created EBS\nvolumes.\nTherefore, AWS Config with the encrypted-volumes rule and a default KMS key is the most suitable solution to\nensure default encryption and prevent unencrypted volume creation. It proactively enforces the required\nconfiguration.\nRelevant Documentation:\nAWS Config managed rules for EBS: https://docs.aws.amazon.com/config/latest/developerguide/encrypted-\nvolumes.html\nAWS Config overview: https://aws.amazon.com/config/",
    "links": [
      "https://docs.aws.amazon.com/config/latest/developerguide/encrypted-",
      "https://aws.amazon.com/config/"
    ]
  },
  {
    "question": "CertyIQ\nAn ecommerce company wants to collect user clickstream data from the company's website for real-time analysis.\nThe website experiences fluctuating traffic patterns throughout the day. The company needs a scalable solution\nthat can adapt to varying levels of traffic.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A: \"Use a data stream in Amazon Kinesis Data Streams in on-demand mode to capture\nthe clickstream data. Use AWS Lambda to process the data in real time.\"\nHere's a detailed justification:\nThe scenario requires capturing and processing clickstream data in real-time with fluctuating traffic. Amazon\nKinesis Data Streams is designed for high-throughput, real-time data streaming. On-demand mode within\nKinesis Data Streams provides a scalable and cost-effective solution for varying traffic patterns. With on-\ndemand capacity mode, Kinesis Data Streams automatically scales in response to your application's\nthroughput. This eliminates the need for capacity planning and manual scaling.\nAWS Lambda, being a serverless compute service, can process data in real-time, triggered by events from\nKinesis Data Streams. Lambda functions scale automatically with the incoming traffic, ensuring that data is\nprocessed efficiently even during peak periods. This combination offers a fully managed, scalable, and cost-\neffective solution for real-time clickstream analysis.\nOption B is incorrect because Amazon Kinesis Data Firehose is best suited for loading data into data lakes or\ndata warehouses like Amazon S3, Amazon Redshift, or Splunk. While it handles scaling well, it's optimized for\nbatching and delivering data rather than real-time, per-record processing. AWS Glue is an ETL service, ideal\nfor data transformation and cataloging, but it's not directly integrated for real-time processing in the same\nway as Lambda with Kinesis Data Streams.\nOption C is incorrect because Amazon Kinesis Video Streams is designed for streaming video and audio data,\nnot general clickstream data. Although it can handle real-time data, it is not the appropriate tool for handling\nclickstream events. Using AWS Glue in combination is also unsuitable for real-time processing needs.\nOption D is incorrect because Amazon Managed Service for Apache Flink (formerly Amazon Kinesis Data\nAnalytics) is used for processing and analyzing streaming data using SQL or Java/Scala code. While it is\ncapable of real-time processing, it introduces more complexity than necessary for simply transforming and\nforwarding clickstream data. It might be an over-engineered solution if basic transformations are sufficient.\nKinesis Data Analytics captures data, and that is not what it primarily does.\nTherefore, the combination of Kinesis Data Streams (on-demand) for scalable data ingestion and AWS\nLambda for real-time processing best addresses the scenario's requirements.\nRelevant Links:\nAmazon Kinesis Data Streams: https://aws.amazon.com/kinesis/data-streams/\nAWS Lambda: https://aws.amazon.com/lambda/\nKinesis Data Streams On-Demand: https://aws.amazon.com/blogs/aws/amazon-kinesis-data-streams-on-\ndemand-auto-scale-throughput-at-no-extra-cost/",
    "links": [
      "https://aws.amazon.com/kinesis/data-streams/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/blogs/aws/amazon-kinesis-data-streams-on-"
    ]
  },
  {
    "question": "CertyIQ\nA global company runs its workloads on AWS. The company's application uses Amazon S3 buckets across AWS\nRegions for sensitive data storage and analysis. The company stores millions of objects in multiple S3 buckets\ndaily. The company wants to identify all S3 buckets that are not versioning-enabled.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B: Use Amazon S3 Storage Lens to identify all S3 buckets that are not versioning-\nenabled across Regions.\nHere's why:\nAmazon S3 Storage Lens is designed to provide organization-wide visibility into object storage, identify cost\noptimization opportunities, and apply data protection best practices. A key feature of S3 Storage Lens is its\nability to aggregate metrics across multiple accounts and regions. It provides a central dashboard to view S3\nstorage usage, activity trends, and identify anomalies. Among the many metrics it tracks, S3 Storage Lens\nprovides the capability to report on buckets that do not have versioning enabled. This makes it the ideal tool\nfor the company's requirement to identify such buckets across all regions.\nOption A is incorrect because AWS CloudTrail primarily captures API calls made to AWS services. While\nCloudTrail can record calls related to S3 versioning, it doesn't natively provide a consolidated view of all S3\nbuckets and their versioning status across regions. It would require significant post-processing and analysis\nof CloudTrail logs to achieve the desired outcome, making it inefficient.\nOption C is incorrect because IAM Access Analyzer for S3 focuses on identifying buckets with access control\nlist (ACL) settings that allow public access. It's designed to help you discover unintentionally shared buckets\nand resources. While Access Analyzer provides insights into access permissions, it doesn't directly assess or\nreport on the versioning status of S3 buckets.\nOption D is incorrect because S3 Multi-Region Access Points are used to simplify access to data stored in S3\nbuckets across multiple AWS regions. They provide a single endpoint for applications to access data,\nregardless of the region where the data is stored. They do not have the built-in functionality to assess or\nreport on the versioning status of S3 buckets.\nIn summary, S3 Storage Lens is specifically built to provide the required visibility and reporting functionality\nfor the company's use case, making it the most suitable solution.\nReference links:\nAmazon S3 Storage Lens\nAWS CloudTrail\nIAM Access Analyzer for S3\nAmazon S3 Multi-Region Access Points",
    "links": []
  },
  {
    "question": "CertyIQ\nA company needs to optimize its Amazon S3 storage costs for an application that generates many files that cannot\nbe recreated. Each file is approximately 5 MB and is stored in Amazon S3 Standard storage.\nThe company must store the files for 4 years before the files can be deleted. The files must be immediately\naccessible. The files are frequently accessed in the first 30 days of object creation, but they are rarely accessed\nafter the first 30 days.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "A": "This is cheaper than S3 Standard but more expensive than Glacier"
    },
    "answer": "A",
    "explanation": "The question asks for the most cost-effective solution for storing infrequently accessed files in S3 while\nensuring immediate accessibility and long-term retention.\nOption A proposes using S3 Glacier Instant Retrieval after 30 days. This class is designed for infrequently\naccessed data that requires immediate retrieval, making it suitable given the requirement for immediate\naccessibility. The files are deleted after 4 years, fulfilling the retention requirement.\nOption B suggests S3 One Zone-IA. While cheaper than S3 Standard, it only stores data in a single availability\nzone, posing a risk if that zone becomes unavailable. The question states the data cannot be recreated,\ntherefore availability must be prioritised, making this a risky option.\nOption C recommends S3 Standard-IA. This is cheaper than S3 Standard but more expensive than Glacier\nInstant Retrieval for infrequently accessed data requiring immediate access.\nOption D suggests using S3 Standard-IA for a portion of the time and then moving to S3 Glacier Flexible\nRetrieval (formerly Glacier) after 4 years. Glacier Flexible Retrieval is unsuitable as the files will need to be\ndeleted, not accessed.\nConsidering the infrequent access, requirement for immediate retrieval, and long-term retention, S3 Glacier\nInstant Retrieval provides the best balance of cost and accessibility. Using S3 Glacier Instant Retrieval will\nreduce storage costs compared to S3 Standard-IA or S3 Standard whilst retaining immediate accessibility.\nTherefore, option A is the most cost-effective and appropriate choice.\nFurther research can be done here:https://aws.amazon.com/s3/storage-\nclasses/https://aws.amazon.com/glacier/",
    "links": [
      "https://aws.amazon.com/s3/storage-",
      "https://aws.amazon.com/glacier/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs its critical storage application in the AWS Cloud. The application uses Amazon S3 in two AWS\nRegions. The company wants the application to send remote user data to the nearest S3 bucket with no public\nnetwork congestion. The company also wants the application to fail over with the least amount of management of\nAmazon S3.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The correct solution is D: \"Set up Amazon S3 to use Multi-Region Access Points in an active-active\nconfiguration with a single global endpoint. Configure S3 Cross-Region Replication.\" This solution directly\naddresses all the requirements of the question.\nHere's a detailed justification:\n1. Nearest S3 Bucket Selection and Reduced Network Congestion: S3 Multi-Region Access Points\n(MRAP) provide a single global endpoint that intelligently routes requests to the geographically\nclosest S3 bucket within the MRAP configuration. This ensures user data is sent to the nearest\nbucket, minimizing latency and avoiding public network congestion.\nhttps://aws.amazon.com/s3/features/multi-region-access-points/\n2. Active-Active Configuration: Setting up MRAP in an active-active configuration allows both S3\nbuckets in different regions to handle requests simultaneously. This maximizes availability and\nperformance by distributing the load across multiple regions. Option D specifies an active-active\nconfiguration, fulfilling this requirement.\n3. Failover with Minimal Management: S3 MRAP handles failover automatically. If one region becomes\nunavailable, MRAP automatically routes requests to the healthy region. This minimizes the need for\nmanual intervention during failover events, reducing operational overhead.\n4. Data Synchronization: S3 Cross-Region Replication (CRR) keeps the S3 buckets in different regions\nsynchronized. This ensures data consistency in the event of a failover, guaranteeing that the\napplication can continue operating without data loss.\n5. Single Global Endpoint: MRAP provides a single global endpoint for accessing data across all\nconfigured regions. This simplifies the application's configuration and eliminates the need to manage\nmultiple regional endpoints.Option A is incorrect as simply configuring the application to use regional\nS3 endpoints doesn't handle failover. Option B employs active-passive, which doesn't take advantage\nof distributing user load over the regions. Option C would require more operational management to\nmaintain consistent data.",
    "links": [
      "https://aws.amazon.com/s3/features/multi-region-access-points/"
    ]
  },
  {
    "question": "CertyIQ\nA company is migrating a data center from its on-premises location to AWS. The company has several legacy\napplications that are hosted on individual virtual servers. Changes to the application designs cannot be made.\nEach individual virtual server currently runs as its own EC2 instance. A solutions architect needs to ensure that the\napplications are reliable and fault tolerant after migration to AWS. The applications will run on Amazon EC2\ninstances.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The correct solution is C, which creates an AMI of each application instance and launches two new EC2\ninstances from the AMI in separate Availability Zones, fronted by a Network Load Balancer (NLB). This\napproach addresses the requirements of reliability and fault tolerance without requiring application design\nchanges.\nOption C achieves fault tolerance by distributing the application instances across multiple Availability Zones.\nIf one Availability Zone experiences an outage, the other instance will continue to serve traffic. The NLB\nensures that traffic is routed only to healthy instances. AMIs ensure a consistent and reproducible instance\nconfiguration for easy deployment. The NLB is suitable because it can handle TCP traffic and maintains\nsource IP addresses, critical for some legacy applications.\nOption A is not ideal because an Auto Scaling group with a minimum and maximum of one instance does not\nprovide fault tolerance. While it can recreate the instance if it fails, there will still be downtime during the\nrecovery. Furthermore, an Application Load Balancer (ALB) might not be suitable for all legacy applications,\nas ALBs work best with HTTP/HTTPS traffic.\nOption B relies on backups, which introduce recovery time objective (RTO) and recovery point objective (RPO)\nissues. Restoring from backup is not a seamless failover. The backup process does not provide immediate\nfault tolerance.\nOption D involves refactoring applications for containerization. The question specifically states that\napplication design changes cannot be made, so this is invalid. While using ECS and Fargate offers benefits, it's\noutside the constraint of no application code changes.\nIn summary, Option C provides the best balance of fault tolerance, minimal changes (only infrastructure), and\nutilizes standard AWS services.\nRelevant links:\nAmazon Machine Images (AMI)\nNetwork Load Balancer (NLB)\nAvailability Zones",
    "links": []
  },
  {
    "question": "CertyIQ\nA company wants to isolate its workloads by creating an AWS account for each workload. The company needs a\nsolution that centrally manages networking components for the workloads. The solution also must create accounts\nwith automatic security controls (guardrails).\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "B": "Here's why:"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Here's why:\nRationale:\nThe question focuses on centralizing network management and automating security guardrails while\nminimizing operational overhead for isolating workloads across AWS accounts.\nAWS Organizations: AWS Organizations is designed for centralized management of multiple AWS accounts.\nIt allows you to create and manage accounts programmatically and apply policies across them.\nhttps://aws.amazon.com/organizations/\nNetworking Account: Creating a dedicated networking account centralizes network resources like VPCs,\nsubnets, route tables, and other network services.\nAWS Resource Access Manager (RAM): AWS RAM enables you to share AWS resources, such as subnets,\nwith other accounts within your AWS organization. This avoids replicating network infrastructure in each\nworkload account, reducing operational overhead. https://aws.amazon.com/ram/\nOption B leverages these AWS services effectively: AWS Organizations for account creation and centralized\ncontrol, a dedicated networking account for network resource management, and AWS RAM for sharing these\nnetwork resources with workload accounts. It's a less operationally intensive approach compared to deploying\nVPCs in each workload account as this centralizes the effort.\nWhy other options are less suitable:\nOption A (AWS Control Tower + AWS RAM): While Control Tower excels at setting up and governing multi-\naccount environments with pre-defined guardrails, using it solely with RAM for subnet sharing doesn't fully\nexploit its capabilities. Control Tower often comes with more opinionated and prescriptive network\nconfigurations.\nOptions C and D (Transit Gateway): Deploying a VPC in each workload account and routing traffic through a\ntransit gateway introduces unnecessary complexity and operational overhead. Managing individual VPCs,\nroute tables, and transit gateway attachments for each workload account becomes cumbersome. Although\nTransit Gateway supports central inspection, it necessitates more configuration per account.",
    "links": [
      "https://aws.amazon.com/organizations/",
      "https://aws.amazon.com/ram/"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts a website on Amazon EC2 instances behind an Application Load Balancer (ALB). The website\nserves static content. Website traffic is increasing. The company wants to minimize the website hosting costs.\nWhich solution will meet these requirements?",
    "options": {
      "B": "Option D: AWS Amplify with EC2 Instances: This option defeats the purpose of moving away from EC2"
    },
    "answer": "A",
    "explanation": "The best solution for minimizing website hosting costs for static content served from EC2 instances behind an\nALB, given increasing traffic, is to move the website to Amazon S3 and configure an Amazon CloudFront\ndistribution.\nHere's why:\nAmazon S3 for Static Content: S3 is a cost-effective, highly scalable, and durable object storage service\nideal for storing static website content like HTML, CSS, JavaScript, images, and videos. S3's pay-as-you-go\npricing model makes it much more affordable than running EC2 instances for static content delivery.\nAmazon CloudFront for Content Delivery: CloudFront is a content delivery network (CDN) that caches\nwebsite content at edge locations globally. By caching content closer to users, CloudFront reduces latency,\nimproves website performance, and, most importantly, significantly offloads traffic from the origin (in this\ncase, S3).\nCost Optimization: Offloading static content to S3 and serving it through CloudFront dramatically reduces\nthe load on EC2 instances, potentially allowing you to downsize or even eliminate some instances, leading to\nsignificant cost savings. You avoid paying for EC2 compute time, bandwidth, and storage for serving static\nassets. CloudFront also provides features like compression and caching configuration to further optimize\ndelivery and reduce data transfer costs.\nScalability and Reliability: S3 and CloudFront are designed for high scalability and availability. They\nautomatically scale to handle traffic spikes without requiring manual intervention.\nLet's look at why the other options are not ideal:\nOption B: Amazon ElastiCache with S3: ElastiCache is an in-memory caching service, but it's designed for\ndynamic content or frequently accessed data that benefits from low-latency access. It is not typically used in\nconjunction with S3 for serving static content as the primary purpose. CloudFront already provides caching\ncapabilities suited for that purpose. ElastiCache also incurs its own costs.\nOption C: AWS Amplify with ALB: AWS Amplify is a good choice for deploying and hosting full-stack web\napplications, including static content, but introducing an ALB in front of it is unnecessary and adds complexity\nand cost. Amplify itself is a hosting platform and can serve the content directly without an ALB.\nOption D: AWS Amplify with EC2 Instances: This option defeats the purpose of moving away from EC2\ninstances to minimize costs. EC2 instances still handle the caching, incurring the same cost concerns. AWS\nAmplify is a hosting platform and does not need to configure EC2 instances.\nAuthoritative Links:\nAmazon S3: https://aws.amazon.com/s3/\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nAWS Amplify: https://aws.amazon.com/amplify/",
    "links": [
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/amplify/"
    ]
  },
  {
    "question": "CertyIQ\nA company is implementing a shared storage solution for a media application that the company hosts on AWS. The\ncompany needs the ability to use SMB clients to access stored data.\nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": {
      "D": "Create an Amazon FSx for Windows File Server file system. Connect the"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Create an Amazon FSx for Windows File Server file system. Connect the\napplication server to the file system.\nHere's a detailed justification:\nAmazon FSx for Windows File Server is a fully managed Windows file system built on Windows Server. It\nprovides native Windows file system capabilities and features like SMB (Server Message Block) protocol\nsupport. This directly addresses the requirement of using SMB clients to access stored data. Furthermore,\nbeing a fully managed service, FSx for Windows File Server offloads administrative overhead related to\nhardware provisioning, patching, backups, and other maintenance tasks.\nOption A, using AWS Storage Gateway Volume Gateway, requires managing volumes on-premises or in EC2,\nadding complexity to the solution. While it can present storage to applications via iSCSI, it doesn't inherently\nfulfill the SMB requirement without additional configuration.\nOption B, using AWS Storage Gateway Tape Gateway, is designed for archival purposes and not suitable for\nactive media application storage as it deals with virtual tapes.\nOption C, creating an EC2 Windows instance and configuring a file share, involves significant administrative\noverhead for managing the EC2 instance, operating system, file server role, backups, and security. This\ncontradicts the requirement of minimizing administrative overhead.\nTherefore, Amazon FSx for Windows File Server offers the least administrative overhead because it is a fully\nmanaged, purpose-built service for Windows file shares that natively supports SMB, simplifying deployment\nand ongoing management for the media application's shared storage needs.\nRelevant Links:\nAmazon FSx for Windows File Server: https://aws.amazon.com/fsx/windows/\nSMB Protocol: https://en.wikipedia.org/wiki/Server_Message_Block",
    "links": [
      "https://aws.amazon.com/fsx/windows/",
      "https://en.wikipedia.org/wiki/Server_Message_Block"
    ]
  },
  {
    "question": "CertyIQ\nA company is designing its production application's disaster recovery (DR) strategy. The application is backed by a\nMySQL database on an Amazon Aurora cluster in the us-east-1 Region. The company has chosen the us-west-1\nRegion as its DR Region.\nThe company's target recovery point objective (RPO) is 5 minutes and the target recovery time objective (RTO) is\n20 minutes. The company wants to minimize configuration changes.\nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": {
      "B": "Convert the Aurora cluster to an Aurora global database. Configure managed"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Convert the Aurora cluster to an Aurora global database. Configure managed\nfailover.\nHere's why this solution is the most operationally efficient and meets the RPO/RTO requirements:\nAurora Global Database is specifically designed for disaster recovery scenarios with low RPO and RTO across\ndifferent AWS Regions. It uses storage-based replication to replicate data with minimal lag, typically under a\nsecond, which easily satisfies the 5-minute RPO. This is accomplished by replicating data at the storage layer\nusing dedicated infrastructure.\nThe managed failover feature of Aurora Global Database automates the failover process in case of a disaster.\nThis reduces the recovery time significantly, aligning with the 20-minute RTO goal. It handles promoting a\nread replica to a writer instance in the DR region.\nOption A, creating an Aurora read replica, is a viable DR strategy. However, it involves manual intervention for\nfailover, which increases the RTO. It will also likely take some time to promote the read replica to a writer\ninstance.\nOption C, using Cross-Region Replication, requires creating a new Aurora cluster, which might involve more\nconfiguration and setup than simply converting to Aurora Global Database. Also, this is deprecated by\nAmazon. The preferred mechanism for DR is Aurora Global Database.\nOption D, using AWS DMS, introduces complexity and potential latency, making it less suitable for the\nstringent RPO of 5 minutes. DMS is designed more for database migrations or scenarios where schema\ntransformation is required, not for continuous, low-latency replication for DR. Also DMS does not handle the\nfailover, the user is responsible for it.\nTherefore, Aurora Global Database with managed failover provides the most operationally efficient,\nautomated, and optimized solution for meeting the RPO/RTO requirements in a DR scenario. It minimizes\nconfiguration changes because it leverages the existing Aurora cluster with just the upgrade to global\ndatabase, and it handles the complex failover process automatically.\nSupporting Documentation:\nAurora Global Database: https://aws.amazon.com/rds/aurora/features/global-database/\nAchieving Fast Failover with Amazon Aurora Global Database:\nhttps://aws.amazon.com/blogs/database/achieving-fast-failover-with-amazon-aurora-global-database/",
    "links": [
      "https://aws.amazon.com/rds/aurora/features/global-database/",
      "https://aws.amazon.com/blogs/database/achieving-fast-failover-with-amazon-aurora-global-database/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a critical data analysis job each week before the first day of the work week. The job requires at\nleast 1 hour to complete the analysis. The job is stateful and cannot tolerate interruptions. The company needs a\nsolution to run the job on AWS.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Here's why:"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's why:\nRequirement of at least 1-hour execution time: Lambda (Option B) has a maximum execution time limit of 15\nminutes. This makes it unsuitable for a job that requires at least an\nhour.https://docs.aws.amazon.com/lambda/latest/dg/configuration-function-common.html\nStateful and Interruptions Intolerant: The job's stateful nature and intolerance to interruptions require a\nstable and persistent environment during execution.\nFargate and ECS: Option A, using Fargate and ECS, provides a suitable environment. Fargate allows you to\nrun containers without managing the underlying EC2 instances, simplifying operations. ECS manages the\ncontainer orchestration.\nEventBridge Scheduler: EventBridge Scheduler enables precise scheduling, ensuring the job runs weekly\nbefore the first day of the work week. This is more robust than relying on crontab (Option C) on EC2 instances,\nwhich can be susceptible to time drift or instance issues.https://aws.amazon.com/eventbridge/scheduler/\nSpot Instances: Option C uses Spot Instances, which are cost-effective but can be interrupted if the Spot\nprice exceeds your bid. This violates the \"interruptions intolerant\" requirement. Also, relying on crontab isn't\nideal for managed scheduling.\nDataSync: Option D, DataSync, is designed for data transfer and synchronization, not for general-purpose\ndata analysis jobs. It doesn't provide a compute environment to execute the analysis\nlogic.https://aws.amazon.com/datasync/\nContainerization: Containerizing the job (Option A) makes it portable and reproducible, ensuring consistent\nexecution across environments. ECS and Fargate are designed for running containerized applications.\nIn summary, ECS with Fargate gives a stable environment, and EventBridge Scheduler offers reliable\nscheduled execution to comply with the requirements that the job is stateful and cannot tolerate\ninterruptions. Lambda cannot run for an hour. Spot Instances might be interrupted. DataSync serves a\ndifferent purpose.",
    "links": [
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-function-common.html",
      "https://aws.amazon.com/eventbridge/scheduler/",
      "https://aws.amazon.com/datasync/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs workloads in the AWS Cloud. The company wants to centrally collect security data to assess\nsecurity across the entire company and to improve workload protection.\nWhich solution will meet these requirements with the LEAST development effort?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C, configuring a data lake in Amazon Security Lake to collect the security data and\nuploading it to an Amazon S3 bucket. Here's why:\nAmazon Security Lake is specifically designed to aggregate, transform, and manage security data from\nvarious AWS services and third-party sources into a centralized data lake. This minimizes development effort\nbecause the service is built for this exact purpose. It automatically collects security-relevant logs and events,\nnormalizes them into the Open Cybersecurity Schema Framework (OCSF) format, and stores them in a data\nlake you own.\nOption A, using AWS Lake Formation and Glue, would require significant configuration and development\neffort to define crawlers, schemas, and data transformation logic to ingest and normalize diverse security\ndata sources. While Lake Formation provides governance, Security Lake provides out-of-the-box security data\ningestion and normalization.\nOption B, a Lambda function collecting CSV data and uploading to S3, lacks the built-in capabilities for large-\nscale data management, normalization (into OCSF), and centralized security data analysis. It would require\nsubstantial coding and maintenance.\nOption D, using AWS DMS to load data into an RDS cluster, is not suited for collecting and analyzing the\ndiverse, semi-structured security data from various AWS sources. RDS is designed for structured relational\ndata, not the varied log and event data. Moreover, DMS primarily focuses on database migrations, not security\ndata aggregation.\nSecurity Lake addresses the prompt's requirement for central collection and assessment of security data with\nminimal development effort by providing a purpose-built service that automates the collection, normalization,\nand storage processes. The OCSF format ensures interoperability and standardized analysis across different\nsecurity tools. Uploading to S3, configured as the data lake, is the standard method of inputting the data to\nSecurity Lake.\nReferences:\nAmazon Security Lake\nOpen Cybersecurity Schema Framework (OCSF)",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is migrating five on-premises applications to VPCs in the AWS Cloud. Each application is currently\ndeployed in isolated virtual networks on premises and should be deployed similarly in the AWS Cloud. The\napplications need to reach a shared services VP",
    "options": {
      "C": "Add routes between the application VPCs in their subnets and the application VPCs to the"
    },
    "answer": "D",
    "explanation": "The best solution for connecting multiple VPCs with minimal administrative overhead, especially when\nconsidering future scalability for over 100 applications, is deploying an AWS Transit Gateway.\nOption D is the most efficient because a Transit Gateway acts as a central hub, simplifying network\nmanagement. Each application VPC and the shared services VPC connect to the Transit Gateway. Routes\nwithin each VPC subnet point to the Transit Gateway. The Transit Gateway then handles routing traffic\nbetween the connected VPCs based on route tables associated with each VPC attachment.\nOption A is less scalable as it involves manually configuring and maintaining multiple VPN tunnels. VPNs add\ncomplexity and can become a management burden.\nOption B, VPC peering, has a limit on the number of peerings a VPC can have and can quickly become\nunmanageable with a large number of VPCs. It also requires establishing a peering connection between every\npair of VPCs that need to communicate, which is O(n^2) complexity. Additionally, overlapping CIDR blocks are\nnot supported with VPC peering, which could become a constraint.\nOption C, Direct Connect, is meant for creating a dedicated network connection from on-premises to AWS and\nis not relevant for connecting VPCs within AWS. It is also far more expensive and complex than a software-\ndefined networking solution like Transit Gateway.\nTransit Gateway's centralized routing policy and simplified management make it ideal for this scenario. As the\ncompany migrates more applications, new VPCs can simply be attached to the Transit Gateway, and routes\ncan be updated centrally.\nFor further research, refer to the AWS documentation on Transit Gateway: https://aws.amazon.com/transit-\ngateway/ and https://docs.aws.amazon.com/transit-gateway/latest/tgw/what-is-transit-gateway.html",
    "links": [
      "https://aws.amazon.com/transit-",
      "https://docs.aws.amazon.com/transit-gateway/latest/tgw/what-is-transit-gateway.html"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to use Amazon Elastic Container Service (Amazon ECS) to run its on-premises application in a\nhybrid environment. The application currently runs on containers on premises.\nThe company needs a single container solution that can scale in an on-premises, hybrid, or cloud environment. The\ncompany must run new application containers in the AWS Cloud and must use a load balancer for HTTP traffic.\nWhich combination of actions will meet these requirements? (Choose two.)",
    "options": {
      "B": "Set up an Application Load Balancer for cloud ECS services.",
      "A": "Set up an ECS cluster that uses the AWS Fargate launch type for the cloud application containers. Use",
      "C": "Set up a Network Load Balancer for cloud ECS services: NLBs are suited for TCP, UDP, and TLS traffic and",
      "D": "Set up an ECS cluster that uses the AWS Fargate launch type. Use Fargate for the cloud application"
    },
    "answer": "A",
    "explanation": "The correct answer is A and B. Here's why:\nA. Set up an ECS cluster that uses the AWS Fargate launch type for the cloud application containers. Use\nan Amazon ECS Anywhere external launch type for the on-premises application containers.\nAWS Fargate for Cloud Containers: Fargate is a serverless compute engine for containers, ideal for running\nECS tasks without managing the underlying EC2 instances. This aligns perfectly with the requirement to run\nnew application containers in the AWS Cloud in a managed and scalable fashion. It simplifies operational\noverhead.\nECS Anywhere for On-premises Containers: ECS Anywhere allows you to register your on-premises servers\nor virtual machines as external instances within your ECS cluster. This fulfills the requirement to run the\nexisting on-premises application containers within a hybrid environment.\nB. Set up an Application Load Balancer for cloud ECS services.\nALB for HTTP Traffic: The problem specifically mentions the need for a load balancer for HTTP traffic.\nApplication Load Balancers (ALBs) are specifically designed for HTTP and HTTPS traffic. They provide\nadvanced routing capabilities based on the content of the request. This is necessary for the company to\nmanage and distribute HTTP traffic to the containers running on ECS in the cloud.\nWhy other options are incorrect:\nC. Set up a Network Load Balancer for cloud ECS services: NLBs are suited for TCP, UDP, and TLS traffic and\nnot for HTTP-aware routing.\nD. Set up an ECS cluster that uses the AWS Fargate launch type. Use Fargate for the cloud application\ncontainers and the on-premises application containers: Fargate cannot be used directly for on-premises\ncontainers without ECS Anywhere's external instance capability.\nE. Set up an ECS cluster that uses the Amazon EC2 launch type for the cloud application containers. Use\nAmazon ECS Anywhere with an AWS Fargate launch type for the on-premises application containers: While\nEC2 launch type for cloud is a valid alternative, it does not represent the best option for the problem\nstatement that prefers easier managed scalability of cloud. And AWS Fargate launch type with ECS\nAnywhere is incorrect. ECS Anywhere uses external instances, not Fargate.\nSupporting Links:\nAWS Fargate: https://aws.amazon.com/fargate/\nAmazon ECS Anywhere: https://aws.amazon.com/ecs/anywhere/\nApplication Load Balancer (ALB): https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
    "links": [
      "https://aws.amazon.com/fargate/",
      "https://aws.amazon.com/ecs/anywhere/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/"
    ]
  },
  {
    "question": "CertyIQ\nA company is migrating its workloads to AWS. The company has sensitive and critical data in on-premises\nrelational databases that run on SQL Server instances.\nThe company wants to use the AWS Cloud to increase security and reduce operational overhead for the databases.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The best solution for migrating on-premises SQL Server databases to AWS with increased security and\nreduced operational overhead is to use Amazon RDS for SQL Server with Multi-AZ and AWS KMS encryption.\nHere's why:\nReduced Operational Overhead: Amazon RDS is a managed database service, which handles tasks like\npatching, backups, and infrastructure management, significantly reducing the operational burden compared\nto managing databases on EC2 instances.\nSecurity: Using AWS KMS with RDS allows for encryption at rest, protecting sensitive data from unauthorized\naccess. RDS manages the encryption process, simplifying key management.\nHigh Availability and Disaster Recovery: The Multi-AZ deployment option in RDS provides high availability by\nsynchronously replicating data to a standby instance in a different Availability Zone. In case of a failure, RDS\nautomatically fails over to the standby instance, minimizing downtime.\nSuitability for Relational Databases: Amazon RDS for SQL Server is designed to support relational database\nworkloads, ensuring compatibility and performance.\nOption A, migrating to EC2, increases operational overhead because the company would have to manage the\noperating system, database software, backups, patching, and scaling. EC2 doesn't offer the same level of\nmanaged services and automation as RDS. Option C is not suitable because S3 is an object storage service,\nnot a relational database, and Macie is a data discovery and classification service, not a database solution.\nOption D, migrating to DynamoDB, would require significant application rework, as it is a NoSQL database and\nnot a relational database. Relational data structures don't translate to NoSQL structures automatically.\nCloudWatch Logs, while useful for logging, doesn't provide data security in the context of database storage.\nSupporting Links:\nAmazon RDS: https://aws.amazon.com/rds/\nAmazon RDS for SQL Server: https://aws.amazon.com/rds/sqlserver/\nAWS KMS: https://aws.amazon.com/kms/\nAmazon RDS Multi-AZ: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html",
    "links": [
      "https://aws.amazon.com/rds/",
      "https://aws.amazon.com/rds/sqlserver/",
      "https://aws.amazon.com/kms/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to migrate an application to AWS. The company wants to increase the application's current\navailability. The company wants to use AWS WAF in the application's architecture.\nWhich solution will meet these requirements?",
    "options": {
      "B": "Here are some authoritative links for further research:",
      "A": "Here's why:"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's why:\nAvailability: Auto Scaling groups across multiple Availability Zones (AZs) ensure high availability. If one AZ\nfails, the application continues to run in the other AZ. This is a fundamental principle of fault tolerance in\ncloud architecture.\nLoad Balancing: An Application Load Balancer (ALB) distributes incoming traffic across multiple EC2\ninstances, improving performance and availability. The ALB acts as a single point of contact for the\napplication, routing requests to healthy instances.\nWAF Protection: AWS WAF (Web Application Firewall) protects web applications from common web exploits\nand bots. It can be connected to an ALB to filter malicious traffic before it reaches the EC2 instances.\nAuto Scaling Group as Target: Setting the Auto Scaling group as the target for the ALB dynamically registers\nand deregisters instances as they are launched or terminated by the Auto Scaling group, automating the load\nbalancing configuration.\nLet's analyze why the other options are not ideal:\nB: Placement groups are primarily for low-latency network performance between instances, not specifically\nfor high availability or integrating with WAF. WAF cannot be directly connected to a placement group.\nC: Using only two EC2 instances provides less resilience compared to an Auto Scaling group. If one instance\nfails, the application's capacity is halved. Auto Scaling provides automatic scaling and self-healing\ncapabilities.\nD: You cannot directly attach a WAF to an Auto Scaling group. The WAF must be attached to a supported\nresource like an Application Load Balancer or API Gateway.\nIn summary, option A provides the most robust solution for achieving high availability, load balancing, and\nWAF protection by utilizing Auto Scaling groups across multiple Availability Zones, an Application Load\nBalancer, and attaching the WAF to the ALB.\nHere are some authoritative links for further research:\nAWS Auto Scaling: https://aws.amazon.com/autoscaling/\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\nAWS WAF: https://aws.amazon.com/waf/\nHigh Availability Architecture: https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/design-\nfor-high-availability.html",
    "links": [
      "https://aws.amazon.com/autoscaling/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
      "https://aws.amazon.com/waf/",
      "https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/design-"
    ]
  },
  {
    "question": "CertyIQ\nA company manages a data lake in an Amazon S3 bucket that numerous applications access. The S3 bucket\ncontains a unique prefix for each application. The company wants to restrict each application to its specific prefix\nand to have granular control of the objects under each prefix.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Create dedicated S3 access points and access point policies for each application.",
      "B": "S3 Batch Operations and ACLs: While S3 Batch Operations can modify ACLs, it's a much more",
      "D": "Replicating to new buckets: Replicating data to multiple buckets for each application introduces"
    },
    "answer": "B",
    "explanation": "The provided answer \"B\" (Create an S3 Batch Operations job to set the ACL permissions for each object in the\nS3 bucket) is incorrect and involves significant operational overhead.\nHere's why the other option (A) is the better solution, and a detailed explanation:\nCorrect Answer: A. Create dedicated S3 access points and access point policies for each application.\nJustification:\n1. S3 Access Points: S3 Access Points are named network endpoints that are attached to a bucket and\ncan be used to manage data access for shared datasets. They simplify the management of data\naccess at scale for applications using shared S3 buckets. Each access point has a unique hostname\nand associated access policy. https://aws.amazon.com/s3/features/access-points/\n2. Granular Control: Access Point policies are written using IAM policy language. You can use them to\ngrant specific permissions to applications accessing the bucket through the access point. In this\nscenario, each application gets its own access point, and the access point policy restricts access to\nits designated prefix.\n3. Least Operational Overhead: Creating access points and configuring policies is a straightforward\nprocess. It centralizes access management and avoids the need to modify permissions on individual\nobjects, which is a much more complex and time-consuming task, especially as the data lake grows.\n4. Scalability: Access points scale well as the number of applications increases. Adding a new\napplication involves creating a new access point and defining its policy, which is a relatively simple\noperation.\n5. Security Best Practices: Access Points promote the principle of least privilege by granting\napplications only the necessary permissions to access the specific data they need.\nWhy the other options are less suitable:\nB. S3 Batch Operations and ACLs: While S3 Batch Operations can modify ACLs, it's a much more\ncumbersome and less scalable solution than using Access Points. It requires creating and running a Batch\nOperations job every time you need to update permissions, which introduces unnecessary operational\noverhead. ACLs are also a legacy access control mechanism and are less flexible and harder to manage than\nIAM policies.\nC & D. Replicating to new buckets: Replicating data to multiple buckets for each application introduces\nsignificant operational overhead and storage costs. Data replication can also lead to data consistency issues\nand adds complexity to the overall architecture. Creating access points on top of replicated buckets (Option\nD) doesn't reduce the replication overhead.\nIn summary, S3 Access Points provide the most efficient and scalable solution for restricting applications to\nspecific prefixes in an S3 bucket with granular access control. They minimize operational overhead by\ncentralizing access management and leveraging IAM policies for fine-grained permissions.",
    "links": [
      "https://aws.amazon.com/s3/features/access-points/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an application that customers use to upload images to an Amazon S3 bucket. Each night, the\ncompany launches an Amazon EC2 Spot Fleet that processes all the images that the company received that day.\nThe processing for each image takes 2 minutes and requires 512 MB of memory.\nA solutions architect needs to change the application to process the images when the images are uploaded.\nWhich change will meet these requirements MOST cost-effectively?",
    "options": {
      "A": "Use S3 Event Notifications to write a message with image details to an Amazon Simple Queue Service",
      "B": "EC2 Reserved Instance: While Reserved Instances can be cost-effective for steady workloads, they are",
      "D": "Amazon SNS, ECS, and Elastic Beanstalk: While SNS can trigger events, it is better suited for fan-out"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's why:\nA. Use S3 Event Notifications to write a message with image details to an Amazon Simple Queue Service\n(Amazon SQS) queue. Configure an AWS Lambda function to read the messages from the queue and to\nprocess the images.\nEvent-Driven Architecture: S3 Event Notifications trigger the image processing pipeline immediately upon\nupload, enabling real-time processing as required. This avoids the nightly batch processing approach.\nAWS Lambda for Scalable Processing: Lambda functions offer a serverless compute environment perfectly\nsuited for handling individual image processing tasks. They scale automatically based on the number of\nmessages in the SQS queue, ensuring efficient resource utilization. You only pay for the compute time\nconsumed while processing the image, which is cost-effective for event-driven workloads.\nAmazon SQS for Decoupling: SQS acts as a buffer between S3 and Lambda, decoupling the image upload\nprocess from the processing logic. This prevents the application from being affected if the image processing\nfails. SQS ensures that each message is delivered at least once, offering reliability.\nCost Optimization: Lambda's pay-per-use model is highly cost-effective, especially when the processing time\nper image is short (2 minutes) and the memory requirement is relatively low (512 MB). You avoid the overhead\nof maintaining and paying for EC2 instances that might sit idle for periods of time.\nSuitability: With 2 minutes of processing time, lambda is suitable to handle the task.\nWhy the other options are less suitable:\nB. EC2 Reserved Instance: While Reserved Instances can be cost-effective for steady workloads, they are\nless efficient than Lambda for event-driven processing, especially with fluctuating workloads. EC2 instances\nneed to be running even if no images are available, costing more.\nC & D. Amazon SNS, ECS, and Elastic Beanstalk: While SNS can trigger events, it is better suited for fan-out\nscenarios (broadcasting messages to multiple subscribers). In this case, we require each image to be\nprocessed exactly once, which SQS guarantees. ECS and Elastic Beanstalk are more complex to configure\nand manage than Lambda for this specific use case, increasing operational overhead. ECS and Elastic\nBeanstalk can be cost effective where the processing lasts more than 15 minutes, in such a case lambda won't\nbe the best option.\nSupporting Links:\nS3 Event Notifications\nAWS Lambda\nAmazon SQS",
    "links": []
  },
  {
    "question": "CertyIQ\nA company wants to improve the availability and performance of its hybrid application. The application consists of\na stateful TCP-based workload hosted on Amazon EC2 instances in different AWS Regions and a stateless UDP-\nbased workload hosted on premises.\nWhich combination of actions should a solutions architect take to improve availability and performance? (Choose\ntwo.)",
    "options": {
      "A": "Create an accelerator using AWS Global Accelerator. Add the load balancers as endpoints. AWS Global",
      "D": "Configure a Network Load Balancer in each Region to address the EC2 endpoints. Configure a Network",
      "B": "This is optimal for the stateless UDP traffic. The combined regional NLBs"
    },
    "answer": "A",
    "explanation": "The correct answer is AD because it addresses both the stateful TCP and stateless UDP workload\nrequirements while enhancing availability and performance across the hybrid environment.\nA. Create an accelerator using AWS Global Accelerator. Add the load balancers as endpoints. AWS Global\nAccelerator improves application availability and performance by directing traffic to healthy endpoints based\non endpoint health, geographic proximity, and configured weights. It leverages the AWS global network to\nroute traffic to the nearest healthy endpoint, reducing latency and improving user experience, especially\nbeneficial for a hybrid application spread across different regions. Since the EC2 instances are in different\nAWS Regions, Global Accelerator is perfect for TCP traffic. https://aws.amazon.com/global-accelerator/\nD. Configure a Network Load Balancer in each Region to address the EC2 endpoints. Configure a Network\nLoad Balancer in each Region that routes to the on-premises endpoints. Network Load Balancers (NLBs) are\nsuitable for TCP-based workloads that require high performance and low latency. NLBs can handle millions of\nrequests per second while maintaining ultra-low latencies. Using NLBs in each region provides redundancy\nand handles local traffic efficiently. Also, using NLBs for on-premises UDP traffic ensures high throughput\nand the ability to forward traffic directly to the on-premises resources without application-layer processing,\nwhich would be required by an ALB. This is optimal for the stateless UDP traffic. The combined regional NLBs\nfor on-prem are used as endpoints with the Global Accelerator, providing performance and increased\navailability for both TCP and UDP workloads. https://aws.amazon.com/elasticloadbalancing/network-load-\nbalancer/\nWhy other options are incorrect:\nB: CloudFront is primarily for caching static content and improving website performance, not for routing\ntraffic to dynamic application endpoints in a hybrid setup with varying latency requirements. It does not\nefficiently handle stateful TCP connections or stateless UDP connections.\nC: Configuring Application Load Balancers (ALBs) for on-premises endpoints isn't the most efficient design\nfor UDP traffic. ALBs work best with HTTP/HTTPS traffic and perform application-layer processing which\nintroduces overhead for UDP. Using two ALBs per region also adds unnecessary complexity.\nE: ALBs aren't ideal for on-prem UDP traffic because they are designed for HTTP(S) traffic and add\nunnecessary overhead with application layer processing. NLB is much more suitable for UDP traffic due to its\nlow latency and direct forwarding capabilities. Using two different types of load balancers is less consistent\nthan just using NLBs.",
    "links": [
      "https://aws.amazon.com/global-accelerator/",
      "https://aws.amazon.com/elasticloadbalancing/network-load-"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a self-managed Microsoft SQL Server on Amazon EC2 instances and Amazon Elastic Block Store\n(Amazon EBS). Daily snapshots are taken of the EBS volumes.\nRecently, all the companys EBS snapshots were accidentally deleted while running a snapshot cleaning script\nthat deletes all expired EBS snapshots. A solutions architect needs to update the architecture to prevent data loss\nwithout retaining EBS snapshots indefinitely.\nWhich solution will meet these requirements with the LEAST development effort?",
    "options": {
      "C": "Create a 7-day EBS snapshot retention rule in Recycle Bin and apply the rule for all"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Create a 7-day EBS snapshot retention rule in Recycle Bin and apply the rule for all\nsnapshots.\nHere's a detailed justification:\nThe scenario describes a company accidentally deleting all EBS snapshots due to a faulty script. The goal is\nto prevent data loss without indefinitely retaining snapshots and minimizing development effort.\nOption C, using the Recycle Bin, directly addresses the problem. The Recycle Bin for EBS Snapshots allows\nyou to recover accidentally deleted EBS snapshots within a retention period you configure (in this case, 7\ndays). This feature is designed precisely for scenarios where accidental deletion occurs. It acts as a safety\nnet, giving a grace period to restore accidentally deleted snapshots. It avoids the need for complex coding,\nscripting, or significant architectural changes. It also avoids indefinite retention by allowing snapshots to be\npermanently deleted after the retention period (7 days). This option has minimal development effort, as it\ninvolves configuring a retention rule in the Recycle Bin and applying it to the relevant snapshots.\nOption A, changing the IAM policy to deny EBS snapshot deletion, is too restrictive. While it would prevent\naccidental deletion, it also prevents legitimate deletions when they are truly required. This approach lacks\nflexibility and could hinder legitimate operations. IAM policy should be about least privileges, allowing only\nwhat is neccessary for functionality.\nOption B, copying EBS snapshots to another AWS Region, provides data redundancy but does not directly\naddress accidental deletion within the primary region. It adds unnecessary complexity and cost compared to\nthe Recycle Bin. Moreover, it only saves snapshots in the secondary region, which could violate regulatory\nrequirements.\nOption D, copying EBS snapshots to Amazon S3 Standard-IA, is also a form of data backup but is not directly\nsuited for recovering from accidental deletion. It requires more development effort to implement the copy\nprocess and manage the snapshots in S3. Furthermore, it can be costly to retrieve snapshots from S3\nStandard-IA, especially when you need to quickly restore a volume from a snapshot, increasing RTO (recovery\ntime objective).\nTherefore, the Recycle Bin provides the simplest, most cost-effective, and most direct solution for preventing\ndata loss due to accidental EBS snapshot deletion.The recycle bin feature in AWS is a built in, native feature\nfor addressing accidental deletions, minimizing the need for complex custom solutions.\nRelevant AWS Documentation:\nRecycle Bin: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-recycle-bin.html",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-recycle-bin.html"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to use an AWS CloudFormation stack for its application in a test environment. The company\nstores the CloudFormation template in an Amazon S3 bucket that blocks public access. The company wants to\ngrant CloudFormation access to the template in the S3 bucket based on specific user requests to create the test\nenvironment. The solution must follow security best practices.\nWhich solution will meet these requirements?",
    "options": {
      "C": "While this adds a layer of network security, it doesn't address the requirement of granting access based on"
    },
    "answer": "C",
    "explanation": "The requirement is to securely grant CloudFormation access to an S3 template without exposing the bucket\npublicly and only upon explicit user requests. Option C, creating a presigned URL, is the most secure and\nappropriate solution.\nA presigned URL grants temporary access to a specific object in S3. This URL is generated with specific\npermissions and an expiration time. When a user requests to create the test environment, the presigned URL\nis generated and provided to the CloudFormation stack. This allows CloudFormation to access the template\nfor the duration specified in the presigned URL, after which the URL becomes invalid. This adheres to the\nprinciple of least privilege and provides a time-bound, auditable access mechanism.\nOption A, creating a gateway VPC endpoint for S3, would only provide access to S3 from within the VPC.\nWhile this adds a layer of network security, it doesn't address the requirement of granting access based on\nspecific user requests and doesn't prevent potential misuse from within the VPC if the service role isn't\ncorrectly scoped.\nOption B, creating an API Gateway REST API, would introduce unnecessary complexity and cost. It would add\nan additional layer between CloudFormation and S3 without providing significant security benefits over a\npresigned URL. The API would also need to be secured and managed.\nOption D, allowing public access temporarily, is a significant security risk and violates the requirement to\nfollow security best practices. It exposes the template object to anyone on the internet during the time it's\npublicly accessible, potentially leaking sensitive information and making the system vulnerable to attacks.\nTherefore, Option C is the best solution because it directly addresses the requirements for secure, user-\ntriggered access using a temporary and limited scope permission.\nReference:\nUsing Presigned URLs - Amazon Simple Storage Service",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has applications that run in an organization in AWS Organizations. The company outsources operational\nsupport of the applications. The company needs to provide access for the external support engineers without\ncompromising security.\nThe external support engineers need access to the AWS Management Console. The external support engineers\nalso need operating system access to the companys fleet ofAmazon EC2 instances that run Amazon Linux in\nprivate subnets.\nWhich solution will meet these requirements MOST securely?",
    "options": {},
    "answer": "A",
    "explanation": "Here's why option A is the most secure solution:\nIAM Identity Center for Console Access: IAM Identity Center (successor to AWS SSO) is the recommended\nway to provide federated access to the AWS Management Console. It allows the external support engineers\nto use their existing corporate credentials or a directory managed by AWS, rather than creating individual IAM\nusers. This centralizes access control and improves security posture. https://aws.amazon.com/iam/identity-\ncenter/\nSSM Session Manager for Instance Access: Session Manager, a feature of AWS Systems Manager, enables\nsecure and auditable instance access without opening inbound SSH ports or managing SSH keys. This\neliminates the attack surface associated with SSH, making it much more secure.\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\nPrinciple of Least Privilege: By using IAM Identity Center and Session Manager, you can grant the external\nsupport engineers only the necessary permissions to perform their tasks. This follows the principle of least\nprivilege, minimizing the potential impact of a compromised account.\nAuditing and Logging: Session Manager provides detailed auditing and logging of all session activity,\nincluding commands executed. This is crucial for security monitoring and compliance.\nCentralized Access Control: IAM Identity Center provides centralized access control for multiple AWS\naccounts. Since the applications run across an organization using AWS Organizations, it centralizes\nauthentication for external support engineers.\nOptions B, C, and D are less secure because:\nOption B: Sharing local IAM user credentials is a security risk. If one account is compromised, the attacker\ngains access to all resources associated with that account.\nOption C: Opening SSH ports to specific IP addresses is an improvement over opening them to the world, but\nit still introduces a security risk. IP addresses can be spoofed, and SSH is a common attack vector. Also,\nmanaging SSH keys is an overhead.\nOption D: Bastion hosts provide a single point of entry to your infrastructure, which can be a security risk if\nthe bastion host is compromised. It also requires managing another EC2 instance. Sharing IAM credentials will\nreduce security.\nTherefore, Option A is the most secure solution because it uses IAM Identity Center for console access and\nSession Manager for instance access, avoiding the risks associated with SSH and shared IAM credentials.",
    "links": [
      "https://aws.amazon.com/iam/identity-",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html"
    ]
  },
  {
    "question": "CertyIQ\nA company uses Amazon RDS for PostgreSQL to run its applications in the us-east-1 Region. The company also\nuses machine learning (ML) models to forecast annual revenue based on near real-time reports. The reports are\ngenerated by using the same RDS for PostgreSQL database. The database performance slows during business\nhours. The company needs to improve database performance.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "D": "Create a read replica in us-east-1. Configure the reports to be generated from the"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Create a read replica in us-east-1. Configure the reports to be generated from the\nread replica.\nJustification:\nThe primary issue is database performance degradation due to report generation competing with application\nworkloads on the same RDS instance. A read replica allows read-only traffic to be offloaded from the primary\ndatabase, thus freeing up resources for the primary application workload and improving performance during\nbusiness hours.\nOption D is the most cost-effective solution because it utilizes a read replica within the same region (us-east-\n1). Creating a read replica in the same region avoids cross-region data transfer costs, which can be significant,\nespecially for near real-time reporting. Also, network latency between the primary and replica is minimized\nwhen they are located within the same region.\nOption A is incorrect as cross-region read replicas introduce higher latency due to geographical distance,\nwhich can impact the timeliness of the near real-time reports. Additionally, cross-region data transfer incurs\nhigher costs compared to intra-region replication.\nOption B, activating Multi-AZ, primarily focuses on high availability and failover capabilities. While Multi-AZ\nimproves resilience, it does not offload read traffic in the same way a read replica does. The standby database\nin a Multi-AZ setup is primarily for failover purposes and is not designed for consistent read operations.\nTherefore, using the standby database for report generation isn't generally supported, and the secondary is\nnot designed for read operations.\nOption C, using AWS DMS, is an overkill solution for this scenario. AWS DMS is typically used for migrating\ndatabases to different platforms or for creating data warehouses. Using DMS to replicate data for reporting\nadds unnecessary complexity and cost compared to the simplicity and efficiency of a read replica, where\nchanges are automatically replicated. Read replicas are specifically designed for this purpose and are much\neasier to maintain in this case.Because the application is for near real-time reports, logical replication is not a\ngood approach. The read replica works at the storage layer, but DMS works at the logical layer and can\nintroduce latency.\nAuthoritative Links:\nAmazon RDS Read Replicas:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.ReadReplicas.html\nAmazon RDS Multi-AZ Deployments:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\nAWS Data Migration Service: https://aws.amazon.com/dms/",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.ReadReplicas.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html",
      "https://aws.amazon.com/dms/"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts its multi-tier, public web application in the AWS Cloud. The web application runs on Amazon EC2\ninstances, and its database runs on Amazon RDS. The company is anticipating a large increase in sales during an\nupcoming holiday weekend. A solutions architect needs to build a solution to analyze the performance of the web\napplication with a granularity of no more than 2 minutes.\nWhat should the solutions architect do to meet this requirement?",
    "options": {},
    "answer": "B",
    "explanation": "The requirement is to analyze the web application's performance with a granularity of no more than 2 minutes.\nOption A: Sending CloudWatch logs to Amazon Redshift and using QuickSight is more suitable for long-term\ntrend analysis and complex querying across large datasets of log data. It's not the most efficient approach for\nreal-time or near-real-time monitoring at a 2-minute granularity.\nOption B: Enabling detailed monitoring on EC2 instances provides metrics at a 1-minute interval. These metrics\nare readily available in CloudWatch. This allows for a granular view of EC2 performance, making it suitable for\nidentifying performance bottlenecks quickly. Using CloudWatch metrics directly fulfills the 2-minute\ngranularity requirement without complex data transformations.\nOption C: Using Lambda to fetch EC2 logs from CloudWatch Logs introduces unnecessary complexity and\nlatency. While CloudWatch Logs contains detailed information, pulling and processing them via Lambda adds\noverhead and doesn't directly leverage readily available metrics. Furthermore, CloudWatch metrics directly\nprovide aggregated data.\nOption D: Sending EC2 logs to S3 and then using Redshift and QuickSight for analysis is an overly complex\nsolution for near real-time monitoring. Similar to option A, this is better suited for large-scale, historical log\nanalysis and reporting, not the immediate performance monitoring required.\nTherefore, the most appropriate solution is to enable detailed monitoring on the EC2 instances and use the\navailable CloudWatch metrics. This approach directly satisfies the need for a 2-minute granularity and avoids\nunnecessary complexity.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-\nmetrics.htmlhttps://aws.amazon.com/cloudwatch/features/",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-",
      "https://aws.amazon.com/cloudwatch/features/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an application that stores and shares photos. Users upload the photos to an Amazon S3 bucket.\nEvery day, users upload approximately 150 photos. The company wants to design a solution that creates a\nthumbnail of each new photo and stores the thumbnail in a second S3 bucket.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "C",
    "explanation": "The most cost-effective solution is to use S3 event notifications and Lambda functions. This approach utilizes\na serverless architecture, paying only for the compute time used when thumbnails are actually generated.\nOption C is superior because S3 event notifications directly trigger a Lambda function upon each photo\nupload. This event-driven approach ensures that thumbnails are generated almost immediately after an\nupload. The Lambda function performs the image processing and stores the thumbnail in the destination S3\nbucket. Since it's triggered by events, it scales automatically with the number of uploads, and there are no\nidle costs.\nOption A, using a long-running EMR cluster, is highly inefficient and expensive. EMR is designed for large-\nscale data processing and analysis, not for real-time image manipulation. Keeping an EMR cluster running\ncontinuously incurs significant costs, even when not actively processing images. The scheduled script adds\nfurther overhead and delay.\nOption B, using an always-on EC2 instance, is also more costly than Lambda. While potentially faster than\nEMR, an always-on EC2 instance incurs costs even when idle. The scheduled script and instance management\nadd operational overhead.\nOption D, using S3 Storage Lens, is incorrect because S3 Storage Lens is designed for storage analytics and\nvisibility, not for triggering actions upon object creation. It does not invoke Lambda functions based on\nindividual file uploads.\nTherefore, option C is the most cost-effective and efficient solution because it leverages the serverless\ncapabilities of Lambda, ensuring minimal costs and immediate thumbnail generation through S3 event\ntriggers.\nRelevant links:\nAWS Lambda Pricing\nAmazon S3 Event Notifications",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has stored millions of objects across multiple prefixes in an Amazon S3 bucket by using the Amazon\nS3 Glacier Deep Archive storage class. The company needs to delete all data older than 3 years except for a\nsubset of data that must be retained. The company has identified the data that must be retained and wants to\nimplement a serverless solution.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D because it offers the most efficient and serverless approach to meet the complex\nrequirements of filtering and deleting objects within Amazon S3 Glacier Deep Archive. Let's break down why:\nWhy Option D is Correct:\nS3 Inventory: This is crucial for generating a comprehensive list of all objects in the S3 bucket, along with\ntheir metadata, including storage class and last modified date. This inventory serves as the foundation for\nfiltering the data. https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory.html\nAWS Lambda: Lambda functions allow for serverless, event-driven code execution. In this scenario, the\nLambda function can be programmed to filter the S3 Inventory report based on age (older than 3 years) and\nany other criteria that identify the data to be retained. It provides the custom logic needed to decide which\nobjects to delete. https://docs.aws.amazon.com/lambda/latest/dg/welcome.html\nS3 Batch Operations: This service enables you to perform large-scale batch operations on S3 objects. It can\nbe used to invoke the Lambda function on each object identified in the S3 Inventory report. This avoids the\nneed to manage infrastructure or write complex parallel processing code. S3 Batch Operations seamlessly\nintegrates with Lambda for object manipulation.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-basics.html\nGlacier Deep Archive Consideration: The Glacier Deep Archive storage class necessitates restoration of\nobjects before they can be deleted. S3 Batch Operations handles that by automatically initiating the\nnecessary restore operation.\nWhy Other Options Are Incorrect:\nOption A (EC2 Script): This is not serverless, requires managing an EC2 instance, and is less scalable than S3\nBatch Operations. The CLI-based approach is also slower and less efficient for such a large dataset.\nOption B (AWS Batch): While AWS Batch can handle large-scale processing, it's generally not designed for\ndirectly managing S3 objects and object deletion. It would require more complex setup and integration\ncompared to S3 Batch Operations.\nOption C (AWS Glue Crawler): AWS Glue crawlers are primarily for discovering and cataloging data schemas\nin data lakes. It's not the appropriate tool for directly managing and deleting S3 objects based on age. Using it\nto generate a manifest file would also add unnecessary complexity. Glue is better suited for cataloging the\ndata for later analysis or querying, not for deletion purposes.\nIn summary, Option D leverages the power of S3 Inventory, Lambda, and S3 Batch Operations to provide a\nfully serverless, scalable, and efficient solution for filtering and deleting objects from S3 Glacier Deep\nArchive based on specific criteria. It minimizes operational overhead and maximizes performance, making it\nthe most suitable answer.",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/welcome.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-basics.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is building an application on AWS. The application uses multiple AWS Lambda functions to retrieve\nsensitive data from a single Amazon S3 bucket for processing. The company must ensure that only authorized\nLambda functions can access the data. The solution must comply with the principle of least privilege.\nWhich solution will meet these requirements?",
    "options": {
      "C": "Configure a bucket policy to grant access based on"
    },
    "answer": "C",
    "explanation": "The correct answer is C: Create individual IAM roles for each Lambda function. Grant the IAM roles access\nto the S3 bucket. Assign each IAM role as the Lambda execution role for its corresponding Lambda\nfunction.\nHere's why:\nPrinciple of Least Privilege: This principle dictates granting only the minimum necessary permissions to\nperform a task. Option C directly adheres to this by providing each Lambda function with only the permissions\nit needs to access the S3 bucket through its unique IAM role.\nIAM Roles for Lambda: Lambda functions require an IAM role (execution role) that defines the permissions\nthe function has when accessing AWS resources. This role acts as a security identity for the function.\nGranular Access Control: Using individual IAM roles allows fine-grained control. If one Lambda function is\ncompromised, the attacker only gains access to the resources that function's role is permitted to access,\nlimiting the blast radius.\nS3 Bucket Policies: While S3 bucket policies can grant access, directly associating roles to Lambda functions\nis considered best practice because it scales well with many Lambdas.\nWhy other options are incorrect:\nA: Grant full S3 bucket access to all Lambda functions through a shared IAM role. This violates the principle\nof least privilege. All Lambda functions have excessive permissions, increasing the risk if one function is\ncompromised.\nB: Configure the Lambda functions to run within a VPC. Configure a bucket policy to grant access based on\nthe Lambda functions' VPC endpoint IP addresses. Lambda functions run in an AWS managed network. VPC\nEndpoints can be used, but this adds unnecessary complexity and cost for the described scenario. Moreover,\nLambda function IP addresses can change, making IP-based policies unreliable.\nThis approach is more complex than using individual IAM roles.\nMaintaining VPC endpoint IP addresses in the bucket policy would be an ongoing operational burden.\nAuthoritative Links:\nIAM Roles for Lambda: https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html\nIAM Best Practices: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html\nS3 Bucket Policies: https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-iam-policies.html",
    "links": [
      "https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-iam-policies.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has developed a non-production application that is composed of multiple microservices for each of the\ncompany's business units. A single development team maintains all the microservices.\nThe current architecture uses a static web frontend and a Java-based backend that contains the application logic.\nThe architecture also uses a MySQL database that the company hosts on an Amazon EC2 instance.\nThe company needs to ensure that the application is secure and available globally.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "B": "Here's a detailed justification:"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Here's a detailed justification:\nStatic Web Frontend: CloudFront combined with S3 offers a highly scalable, globally distributed, and cost-\neffective solution for hosting static content. S3 provides durable object storage, and CloudFront caches the\ncontent at edge locations for low latency access worldwide. This approach minimizes operational overhead\ncompared to managing servers.\nMicroservices Refactoring: Refactoring the Java-based backend microservices into Lambda functions offers\nseveral benefits:\nServerless: Lambda eliminates the need to manage servers, patching, and scaling. AWS handles the\nunderlying infrastructure.\nScalability: Lambda automatically scales based on demand, ensuring high availability.\nCost Optimization: You only pay for the compute time consumed by the functions.\nAPI Gateway: API Gateway acts as a front door for the Lambda functions, enabling secure and managed\naccess to the microservices. It handles authentication, authorization, rate limiting, and API versioning.\nDatabase Migration: Migrating the MySQL database from an EC2 instance to Amazon RDS for MySQL\nprovides a managed database service. This shift offloads database administration tasks such as patching,\nbackups, and scaling to AWS. RDS offers high availability options (Multi-AZ) and simplifies the database\nmanagement process, significantly reducing operational overhead.\nAlternatives A and D are incorrect because they suggest using EC2 Reserved Instances for the database.\nWhile Reserved Instances can provide cost savings, managing the database on EC2 still requires significant\noperational overhead. Option C is less ideal for routing Lambda functions using a Network Load Balancer. API\nGateway provides more fine-grained control over API access and management for Lambda functions.\nAuthoritative Links:\nAmazon S3: https://aws.amazon.com/s3/\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon API Gateway: https://aws.amazon.com/api-gateway/\nAmazon RDS: https://aws.amazon.com/rds/",
    "links": [
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/api-gateway/",
      "https://aws.amazon.com/rds/"
    ]
  },
  {
    "question": "CertyIQ\nA video game company is deploying a new gaming application to its global users. The company requires a solution\nthat will provide near real-time reviews and rankings of the players.\nA solutions architect must design a solution to provide fast access to the data. The solution must also ensure the\ndata persists on disks in the event that the company restarts the application.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "C": "Deploy an Amazon ElastiCache for Redis cluster. Store the player data in the"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Deploy an Amazon ElastiCache for Redis cluster. Store the player data in the\nElastiCache cluster.\nHere's a detailed justification:\nThe core requirement is fast access to near real-time data (reviews and rankings) for a global user base, with\ndata persistence. ElastiCache for Redis perfectly fits this scenario because it's an in-memory data store. This\nprovides extremely low latency reads and writes, which is crucial for real-time applications. Redis also\nsupports data persistence by periodically writing data to disk (RDB snapshots) or by appending every write\noperation to a log file (AOF). This ensures data survival even if the ElastiCache cluster restarts. The cluster\nsetup distributes the load and improves availability.\nOption A is not suitable because S3 is object storage, not designed for near real-time data access. The latency\nassociated with S3 reads/writes is significantly higher than in-memory solutions. CloudFront caching\nimproves read performance, but the initial request still goes to S3.\nOption B involves managing EC2 instances, which entails significant operational overhead for patching,\nscaling, and ensuring high availability. Furthermore, manually setting up data replication across regions adds\ncomplexity. While Route 53 geolocation improves latency, the underlying EC2 instance setup is complex and\nmore operationally intensive than a managed service.\nOption D, ElastiCache for Memcached, is also an in-memory data store, providing low latency. However,\nMemcached does not offer built-in data persistence. Since data persistence is a strict requirement,\nMemcached is unsuitable. Redis provides both speed and the necessary persistence using RDB and AOF\nfeatures.\nTherefore, ElastiCache for Redis offers the best combination of low latency, scalability, persistence, and\nreduced operational overhead compared to the other options. It's a managed service, simplifying management\ntasks.\nFurther Reading:\nAmazon ElastiCache for Redis: https://aws.amazon.com/elasticache/redis/\nRedis Persistence: https://redis.io/docs/management/persistence/",
    "links": [
      "https://aws.amazon.com/elasticache/redis/",
      "https://redis.io/docs/management/persistence/"
    ]
  },
  {
    "question": "CertyIQ\nA company is designing an application on AWS that processes sensitive data. The application stores and processes\nfinancial data for multiple customers.\nTo meet compliance requirements, the data for each customer must be encrypted separately at rest by using a\nsecure, centralized key management solution. The company wants to use AWS Key Management Service (AWS\nKMS) to implement encryption.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "D",
    "explanation": "The correct solution is D: \"Create separate AWS KMS keys for each customer's data that have granular\naccess control and logging enabled.\"\nHere's a detailed justification:\nAWS KMS is designed for managing encryption keys securely and centrally. The requirement is to encrypt\ndata for each customer separately at rest using KMS with minimal operational overhead.\nOption A is incorrect because storing keys in S3, even with server-side encryption, introduces key\nmanagement complexity outside of KMS, increasing operational overhead and potentially compromising\nsecurity. It goes against the centralized key management requirement. S3 server-side encryption won't\nprovide the level of granular access control needed.\nOption B introduces a hardware security appliance (HSM). While HSMs can be used with KMS (AWS\nCloudHSM), they significantly increase operational overhead because they require managing and maintaining\nphysical hardware within the AWS environment. This contradicts the requirement for minimal operational\noverhead.\nOption C is inadequate for compliance. Using a single KMS key for all customer data violates the requirement\nthat data for each customer must be encrypted separately. If the single key were compromised, all customer\ndata would be at risk. It also hinders granular access control as all customers would technically be using the\nsame key.\nOption D provides the best solution. Creating a separate KMS key for each customer satisfies the requirement\nfor individual encryption. KMS automatically handles key generation, storage, and rotation securely. Granular\naccess control can be implemented using IAM policies to restrict access to each key to only authorized\npersonnel or services for that specific customer. Enabling logging provides audit trails for key usage, which is\nessential for compliance. This approach centralizes key management within AWS KMS, minimizing\noperational overhead while meeting security and compliance needs. KMS is designed to scale and manage\nmany keys effectively. This solution fulfills all given requirements.\nRelevant links for further research:\nAWS KMS Best Practices: https://docs.aws.amazon.com/kms/latest/developerguide/best-practices.html\nAWS KMS Key Management: https://aws.amazon.com/kms/key-management/\nAWS IAM Policies: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html",
    "links": [
      "https://docs.aws.amazon.com/kms/latest/developerguide/best-practices.html",
      "https://aws.amazon.com/kms/key-management/",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to design a resilient web application to process customer orders. The web application must\nautomatically handle increases in web traffic and application usage without affecting the customer experience or\nlosing customer orders.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The correct solution is D, leveraging an Application Load Balancer, EC2 Auto Scaling groups, Amazon SQS,\nand Amazon RDS with Multi-AZ deployment for resilience and scalability in a web application processing\ncustomer orders.\nHere's why:\n1. Application Load Balancer (ALB): ALBs are designed for HTTP/HTTPS traffic and excel at routing\nrequests based on content, making them ideal for managing web traffic and distributing incoming\ncustomer orders across the application instances.\nhttps://aws.amazon.com/elasticloadbalancing/application-load-balancer/\n2. EC2 Auto Scaling Groups: Auto Scaling automatically adjusts the number of EC2 instances based on\ndemand. By placing the application servers within Auto Scaling groups, the solution can\nautomatically scale up during traffic surges and scale down during periods of low activity, ensuring\nconsistent performance and availability. https://aws.amazon.com/autoscaling/\n3. Amazon SQS (Simple Queue Service): SQS provides a reliable message queue for asynchronous\ncommunication. Storing unprocessed orders in SQS acts as a buffer during peak loads. This\ndecoupling allows the web application to quickly acknowledge receipt of the order without needing\nto process it immediately. If the processing servers are temporarily unavailable or overloaded, the\nmessages remain in the queue until they can be successfully processed, preventing data loss and\nensuring eventual consistency. https://aws.amazon.com/sqs/\n4. Amazon RDS with Multi-AZ deployment: RDS with Multi-AZ ensures high availability and durability\nfor the processed customer order data. Multi-AZ automatically provisions and maintains a\nsynchronous standby database in a different Availability Zone. In the event of a primary database\nfailure, RDS automatically fails over to the standby, minimizing downtime and data loss. This is crucial\nfor maintaining the integrity of processed order information.\nhttps://aws.amazon.com/rds/features/multi-az/\nWhy other options are less suitable:\nA: NAT Gateway is for outbound internet access, not for managing incoming web traffic. Lambda is useful, but\nSQS is better at queuing unprocessed orders for resilience. Storing orders in Lambda directly is not scalable\nor durable.\nB: While NLB is for TCP/UDP, ALB is more suited for web applications, providing advanced routing features.\nRedshift is an analytical data warehouse, not an operational database suited for real-time order processing.\nC: GWLB is for virtual appliances like firewalls, not general web traffic. ECS is fine, but using GWLB to capture\nunprocessed orders is an atypical and inefficient usage. DynamoDB is a good choice but the overall\narchitecture is flawed.\nTherefore, solution D offers the most appropriate and robust approach to building a resilient and scalable web\napplication for customer order processing.",
    "links": [
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
      "https://aws.amazon.com/autoscaling/",
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/rds/features/multi-az/"
    ]
  },
  {
    "question": "CertyIQ\nA company is using AWS DataSync to migrate millions of files from an on-premises system to AWS. The files are 10\nKB in size on average.\nThe company wants to use Amazon S3 for file storage. For the first year after the migration, the files will be\naccessed once or twice and must be immediately available. After 1 year, the files must be archived for at least 7\nyears.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "A",
    "explanation": "The best approach for cost-effectively migrating and archiving millions of small files to S3, with the specified\naccess patterns and retention, is to archive them into larger objects first and leverage S3 Glacier tiers.\nHere's why option A is the most cost-effective:\nGrouping Files: Archiving tools (like tar or zip) bundle small files into larger objects. This drastically reduces\nthe overhead associated with storing millions of individual objects in S3. S3 charges are partly based on the\nnumber of objects stored, so reducing the object count saves money.\nInitial Storage Tier (S3 Glacier Instant Retrieval): For the first year, files need to be immediately accessible\n(accessed once or twice). S3 Glacier Instant Retrieval is the lowest-cost archive storage class that offers the\nlowest latency and millisecond retrieval, making it suitable for occasional, immediate access.\nLifecycle Transition to S3 Glacier Deep Archive: After one year, the files can be moved to S3 Glacier Deep\nArchive for long-term storage. Glacier Deep Archive offers the lowest storage cost for data that is rarely\naccessed.\nLifecycle Configuration: S3 Lifecycle rules automate the transition between storage classes based on time or\nother criteria. This automates the move from Glacier Instant Retrieval to Glacier Deep Archive after the initial\nyear.\nWhy other options are less optimal:\nOption B (S3 Standard-IA initially): S3 Standard-IA is designed for infrequently accessed data but is more\nexpensive than Glacier Instant Retrieval for archiving. Also, it does not provide cost benefits in the given\naccess patterns.\nOption C (S3 Glacier Instant Retrieval to Glacier Flexible Retrieval): Glacier Flexible Retrieval is typically\nused for backup and disaster recovery and has retrieval costs. Deep Archive is more cost-effective for the\narchival timeframe of 7 years. Storing millions of small files directly without archiving would also be more\nexpensive.\nOption D (S3 Standard-IA to Deep Archive): S3 Standard-IA is more expensive than Glacier Instant Retrieval\nfor archiving. Storing millions of small files directly without archiving would also be more expensive.\nAuthoritative Links:\nS3 Storage Classes: https://aws.amazon.com/s3/storage-classes/\nS3 Lifecycle Management: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-\nconfiguration-examples.html\nAWS DataSync: https://aws.amazon.com/datasync/",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-",
      "https://aws.amazon.com/datasync/"
    ]
  },
  {
    "question": "CertyIQ\nA company recently performed a lift and shift migration of its on-premises Oracle database workload to run on an\nAmazon EC2 memory optimized Linux instance. The EC2 Linux instance uses a 1 TB Provisioned IOPS SSD (io1) EBS\nvolume with 64,000 IOPS.\nThe database storage performance after the migration is slower than the performance of the on-premises\ndatabase.\nWhich solution will improve storage performance?",
    "options": {
      "B": "D.Change the EC2 Linux instance to a storage optimized instance type. Do not change the Provisioned IOPS"
    },
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A is the correct solution to improve the Oracle database storage\nperformance after the lift-and-shift migration:\nThe core problem is that the database's I/O requirements exceed the current EBS volume's capabilities,\nresulting in performance degradation compared to the on-premises setup. While the volume has 64,000 IOPS,\nthis might not be sufficient for the workload, or the underlying EC2 instance is not fully utilizing it.\nOption A, adding more io1 EBS volumes and using LVM striping, is the best approach because it provides\nincreased aggregate IOPS and throughput. LVM striping distributes I/O requests across multiple physical\nvolumes (EBS volumes in this case), effectively multiplying the available IOPS and throughput. This\nparallelization can significantly reduce latency and improve overall database performance. Each io1 volume\nhas its own independent IOPS capability, and striping combines them.\nOption B is incorrect because a single io1 volume is limited to a maximum of 64,000 IOPS (as of the SAA-C03\nexam time). You can't exceed this limit on a single volume, regardless of the request. AWS documentation\nconfirms this IOPS limit per volume.\nOption C, increasing the size of the io1 volume to 2TB, would increase throughput (MB/s). However, since the\nproblem is the need for more IOPS, simply increasing the volume size won't directly address the bottleneck.\nWhile larger volumes can have higher throughput, they do not circumvent the 64,000 IOPS limit on a single io1\nvolume.\nOption D, changing the EC2 instance type to a storage optimized instance, might help to some extent by\nproviding better networking and EBS optimization. However, it doesn't directly address the primary issue of\ninsufficient IOPS for the database workload. Storage optimized instances generally provide better\nperformance for workloads that heavily rely on EBS, but if the EBS volume itself is the bottleneck, changing\nthe instance type alone won't solve the problem. Moreover, a memory optimized instance is probably already\nwell suited for an Oracle database. The storage-optimized instance might introduce other constraints relative\nto the memory needs of the database.\nIn summary, Option A effectively addresses the performance bottleneck by increasing the overall IOPS\navailable to the database through LVM striping across multiple io1 volumes. This allows the database to\nhandle its I/O requirements more efficiently, improving performance to match or exceed the on-premises\nenvironment.\nAuthoritative Links:\nEBS Volume Types: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\nLVM Overview: Linux documentation and tutorials on Logical Volume Management can provide in-depth\ninformation on striping.",
    "links": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is migrating from a monolithic architecture for a web application that is hosted on Amazon EC2 to a\nserverless microservices architecture. The company wants to use AWS services that support an event-driven,\nloosely coupled architecture. The company wants to use the publish/subscribe (pub/sub) pattern.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "D": "Here's why:"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Here's why:\nThe problem statement emphasizes a serverless, event-driven, loosely coupled microservices architecture\nusing the publish/subscribe pattern and prioritizes cost-effectiveness.\nOption D best fits these criteria:\nAmazon API Gateway HTTP API: HTTP APIs are designed for low-cost, low-latency API calls, which is ideal\nfor a microservices architecture that requires efficient communication. Compared to REST APIs, they offer a\nsimpler and more cost-effective solution when the full features of REST API are not needed.\nAWS Lambda: Lambda provides a serverless compute environment, which aligns with the architectural\nrequirements. The Lambda function serves as the intermediary between API Gateway and the SNS topic,\nenabling event publication.\nAmazon SNS: SNS directly implements the publish/subscribe pattern. A Lambda function can publish events\nto an SNS topic. Subscribers (e.g., Lambda functions, SQS queues, HTTP endpoints) receive these events\nasynchronously, fostering loose coupling.\nCost-Effectiveness: The combination of HTTP API, Lambda, and SNS provides a cost-effective solution due to\npay-per-use pricing models. You only pay for what you use.\nWhy other options are less ideal:\nA: Amazon SQS is a queuing service (point-to-point), not a publish/subscribe service. While SQS supports fan-\nout patterns, it's not its primary function and SNS is more optimized for that use case. Using SQS for pub/sub\nintroduces complexity, management overhead, and potential cost inefficiencies.\nB: While using a REST API and SNS would work, REST APIs offer more features than are needed here and the\nHTTP API offers a lower cost for the same functionality in this case.\nC: Kinesis Data Streams with enhanced fan-out is designed for high-throughput, real-time data streaming, like\nvideo or IoT data. It's overkill and significantly more expensive for this application. WebSocket APIs are also\nspecialized for real-time bi-directional communication which is not needed here.\nIn summary, Option D provides the most cost-effective solution using serverless services that naturally\nimplement the publish/subscribe pattern, promoting loose coupling and efficient communication in a\nmicroservices environment.\nSupporting Documentation:\nAmazon SNS: https://aws.amazon.com/sns/\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon API Gateway: https://aws.amazon.com/api-gateway/\nChoosing between HTTP APIs and REST APIs: https://aws.amazon.com/blogs/compute/choosing-between-\napigateway-rest-apis-and-http-apis/",
    "links": [
      "https://aws.amazon.com/sns/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/api-gateway/",
      "https://aws.amazon.com/blogs/compute/choosing-between-"
    ]
  },
  {
    "question": "CertyIQ\nA company recently migrated a monolithic application to an Amazon EC2 instance and Amazon RDS. The\napplication has tightly coupled modules. The existing design of the application gives the application the ability to\nrun on only a single EC2 instance.\nThe company has noticed high CPU utilization on the EC2 instance during peak usage times. The high CPU\nutilization corresponds to degraded performance on Amazon RDS for read requests. The company wants to reduce\nthe high CPU utilization and improve read request performance.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Here's why:"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's why:\nAddressing CPU Utilization on EC2: The primary issue is the high CPU utilization on the EC2 instance.\nIncreasing the instance size to one with more CPU capacity directly addresses this problem. This provides the\napplication with more processing power to handle the workload.\nAuto Scaling Group Configuration: Configuring an Auto Scaling group with a minimum and maximum size of 1\nensures that exactly one EC2 instance is running at all times. This is crucial because the application is\ndesigned to run on a single EC2 instance, so scaling beyond one instance would break it. The Auto Scaling\ngroup provides the benefit of automatic instance replacement in case of failure, improving availability.\nImproving Read Request Performance on RDS: The high CPU utilization on the EC2 instance impacts the\nperformance of read requests on the RDS database. Creating an RDS read replica and redirecting read\nrequests to it offloads the read workload from the primary RDS instance. This reduces the load on the primary\ndatabase, improving its performance and overall application performance.\nWhy other options are incorrect:\nB: Redirecting all read/write traffic to the replica is not correct. RDS Read Replicas are READ ONLY. Write\ntraffic must go to the primary database instance.\nC: Resizing the RDS DB instance to an instance type that has more CPU capacity is NOT the best solution. The\nRDS performance problems are caused by the read load and can be fixed with an RDS Read Replica. It is not\nrecommended to modify the minimum and maximum size for the autoscaling group as the application is\ntightly coupled and cannot be scaled to more than one instance.\nD: Resizing the RDS DB instance to an instance type that has more CPU capacity is NOT the best solution as\nread replicas are best for read-heavy applications.\nSupporting Resources:\nAmazon EC2 Instance Types: https://aws.amazon.com/ec2/instance-types/\nAmazon EC2 Auto Scaling: https://aws.amazon.com/autoscaling/\nAmazon RDS Read Replicas:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html",
    "links": [
      "https://aws.amazon.com/ec2/instance-types/",
      "https://aws.amazon.com/autoscaling/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to grant a team of developers access to the company's AWS resources. The company must\nmaintain a high level of security for the resources.\nThe company requires an access control solution that will prevent unauthorized access to the sensitive data.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B: Define IAM roles that have fine-grained permissions based on the principle of least\nprivilege. Assign an IAM role to each developer. This solution adheres to security best practices by employing\nthe principle of least privilege and using IAM roles, which are integral for secure access control in AWS.\nHere's why other options are incorrect:\nA: Sharing IAM user credentials is a severe security risk. It violates the principle of least privilege and makes\nauditing impossible since actions cannot be attributed to a specific user.\nC: While IAM access keys grant programmatic access, they are long-term credentials that, if compromised,\ncan lead to unauthorized access. It's preferable to use IAM roles which are short-term credentials assumed by\nentities. Furthermore, managing access at a user-by-user access key level is less scalable and maintainable\nthan using IAM roles.\nD: AWS Cognito user pools are primarily designed for managing user authentication and authorization for web\nand mobile applications. While Cognito can integrate with IAM to grant access to AWS resources, it\nintroduces unnecessary complexity for internal team access, which is better handled directly by IAM.\nIAM roles provide temporary credentials and can be assigned fine-grained permissions, ensuring that\ndevelopers only have access to the specific resources they need to perform their tasks. By following the\nprinciple of least privilege, you minimize the potential impact of any security breaches or accidental\nmisconfigurations. This approach improves security posture and simplifies access management by\ncentralizing permissions in IAM.\nFor further reading, refer to the AWS IAM documentation:\nIAM Roles\nSecurity best practices in IAM",
    "links": []
  },
  {
    "question": "CertyIQ\nA company hosts a monolithic web application on an Amazon EC2 instance. Application users have recently\nreported poor performance at specific times. Analysis of Amazon CloudWatch metrics shows that CPU utilization is\n100% during the periods of poor performance.\nThe company wants to resolve this performance issue and improve application availability.\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": {},
    "answer": "B",
    "explanation": "The question asks for the most cost-effective solution to address performance issues (100% CPU utilization)\nand improve availability for a monolithic web application on a single EC2 instance. The key is to scale\neffectively and cost-efficiently.\nOption B is crucial because creating an AMI from the current server captures the application's state and\nconfiguration. This AMI serves as the base for launching new instances in an Auto Scaling group. Without an\nAMI, recreating the application environment on each new instance would be complex and time-consuming.\nOption E directly addresses both performance and availability. An Auto Scaling group automatically scales\nthe number of EC2 instances based on demand, reducing CPU utilization by distributing the workload across\nmultiple servers (horizontal scaling). The Application Load Balancer (ALB) distributes incoming traffic evenly\nacross these instances, ensuring no single instance is overwhelmed and improving fault tolerance. If one\ninstance fails, the ALB directs traffic to healthy instances, maintaining application availability. This is more\ncost-effective than vertical scaling as it only uses resources when needed.\nOptions A and D are not ideal because AWS Compute Optimizer's recommendations would suggest instance\ntypes to scale the EC2 instance vertically. While vertical scaling can address CPU utilization, it represents a\nsingle point of failure and does not improve availability. Also, vertical scaling has limitations, reaching a point\nwhere further upgrades become extremely expensive and may not fully resolve the problem.\nOption C suggests creating an Auto Scaling group and an ALB to scale vertically. This is incorrect, as Auto\nScaling groups and ALBs are inherently used for horizontal scaling. Vertical scaling refers to increasing the\nresources (CPU, memory) of a single instance.\nTherefore, the combination of creating an AMI (B) and using an Auto Scaling group with an ALB for horizontal\nscaling (E) provides the most cost-effective and reliable solution for improving application performance and\navailability in this scenario.\nRelevant links:\nAmazon EC2 Auto Scaling: https://aws.amazon.com/autoscaling/\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\nAmazon Machine Images (AMIs): https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html",
    "links": [
      "https://aws.amazon.com/autoscaling/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs all its business applications in the AWS Cloud. The company uses AWS Organizations to manage\nmultiple AWS accounts.\nA solutions architect needs to review all permissions that are granted to IAM users to determine which IAM users\nhave more permissions than required.\nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": {
      "C": "Use AWS Identity and Access Management (IAM) Access Analyzer to review all the",
      "A": "Use Network Access Analyzer to review all access permissions in the company's AWS accounts: Network",
      "B": "Create an AWS CloudWatch alarm that activates when an IAM user creates or modifies resources in an",
      "D": "Use Amazon Inspector to find vulnerabilities in existing IAM policies: Amazon Inspector is primarily a"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Use AWS Identity and Access Management (IAM) Access Analyzer to review all the\ncompanys resources and accounts.\nHere's a detailed justification:\nIAM Access Analyzer is a feature of AWS IAM designed to identify resource access granted to principals\noutside of your AWS account or organization. It helps you refine your permissions and implement the principle\nof least privilege by analyzing resource-based policies and IAM policies. Access Analyzer continuously\nmonitors your AWS environment and generates findings when it detects resource access that you may not\nhave intended.\nHere's why the other options are less suitable:\nA. Use Network Access Analyzer to review all access permissions in the company's AWS accounts: Network\nAccess Analyzer focuses on network reachability between resources, not on IAM permissions granted to\nusers. It identifies unintended network access to your AWS resources.\nB. Create an AWS CloudWatch alarm that activates when an IAM user creates or modifies resources in an\nAWS account: While CloudWatch can monitor actions taken by IAM users, it wouldn't provide a\ncomprehensive review of granted permissions. It would only alert you to specific actions, requiring significant\nmanual effort to identify over-permissioned users. Moreover, setting up and maintaining such alarms for every\npotential resource creation/modification across multiple accounts adds significant administrative overhead.\nD. Use Amazon Inspector to find vulnerabilities in existing IAM policies: Amazon Inspector is primarily a\nvulnerability management service that assesses EC2 instances and container images for security\nvulnerabilities. While it might provide some insights related to security best practices, it's not designed to\nanalyze IAM permissions or identify users with excessive privileges.\nIAM Access Analyzer directly addresses the requirement of reviewing permissions and identifying users with\nmore permissions than required with the least administrative overhead because it automates the analysis of\npolicies and identifies potential risks based on external access. This makes it a far more efficient and\neffective solution compared to the other options.\nAuthoritative Links:\nAWS IAM Access Analyzer: https://docs.aws.amazon.com/IAM/latest/UserGuide/access-analyzer.html\nPrinciple of Least Privilege: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-\nleast-privilege",
    "links": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/access-analyzer.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to implement a new data retention policy for regulatory compliance. As part of this policy,\nsensitive documents that are stored in an Amazon S3 bucket must be protected from deletion or modification for a\nfixed period of time.\nWhich solution will meet these requirements?",
    "options": {
      "B": "Activate S3 Object Lock on the required objects and enable compliance mode."
    },
    "answer": "B",
    "explanation": "The correct answer is B. Activate S3 Object Lock on the required objects and enable compliance mode.\nHere's a detailed justification:\nThe requirement is to protect sensitive documents in an S3 bucket from deletion or modification for a fixed\nperiod to meet regulatory compliance. S3 Object Lock directly addresses this need by making objects\nimmutable. This means that once an object is locked, it cannot be deleted or overwritten until the lock expires.\nS3 Object Lock has two modes: governance mode and compliance mode. Governance mode allows users with\nspecific IAM permissions to override the retention settings, offering flexibility but potentially compromising\nstrict compliance. Compliance mode, on the other hand, provides the strongest level of protection. Once an\nobject is locked in compliance mode, its retention settings cannot be altered by any user, including the root\nuser. This ensures that the objects remain protected for the entire retention period, satisfying the regulatory\nrequirement of preventing deletion or modification.\nOption A, using governance mode, is less secure as authorized users could potentially bypass the retention\npolicy. Option C, enabling versioning and a lifecycle policy, is not suitable because versioning doesn't prevent\ndeletion of all versions. A user with the necessary permissions could delete all versions, effectively removing\nthe object. Also, lifecycle policies automate the deletion of objects. Option D, transitioning objects to S3\nGlacier Flexible Retrieval, focuses on cost optimization and long-term archiving rather than preventing\nmodification or deletion during a specific retention period. Glacier storage doesn't inherently offer the\nimmutability provided by Object Lock.\nTherefore, activating S3 Object Lock in compliance mode is the only option that fully guarantees the\nimmutability of objects to meet regulatory compliance, preventing unauthorized deletion or modification for\nthe required duration.\nHere are authoritative links for further research:\nAWS S3 Object Lock: https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html\nAWS S3 Versioning: https://docs.aws.amazon.com/AmazonS3/latest/userguide/versioning-workflows.html\nAWS S3 Lifecycle: https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html\nAWS S3 Glacier: https://aws.amazon.com/glacier/",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/versioning-workflows.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html",
      "https://aws.amazon.com/glacier/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs its customer-facing web application on containers. The workload uses Amazon Elastic Container\nService (Amazon ECS) on AWS Fargate. The web application is resource intensive.\nThe web application needs to be available 24 hours a day, 7 days a week for customers. The company expects the\napplication to experience short bursts of high traffic. The workload must be highly available.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "B": "Configure an ECS capacity provider with Fargate"
    },
    "answer": "B",
    "explanation": "The most cost-effective and highly available solution is B. Configure an ECS capacity provider with Fargate\nfor steady state and Fargate Spot for burst traffic.\nHere's why:\nFargate for Steady State: Fargate provides serverless compute for containers, eliminating the need to\nmanage EC2 instances. Using Fargate for the consistent, base load ensures the application is always available\n(24/7) without the overhead of instance management.\nFargate Spot for Burst Traffic: Fargate Spot provides significant cost savings (up to 70%) compared to\nFargate, by utilizing spare compute capacity. It's ideal for handling short bursts of high traffic, allowing you to\nscale affordably during peak demand. While Fargate Spot instances can be interrupted, the short duration of\nthe expected traffic bursts minimizes the risk of significant disruption and the application's container\norchestration can handle interruptions by quickly rescheduling containers on available capacity.\nCapacity Provider: ECS Capacity Providers allow you to define a strategy for using both Fargate and Fargate\nSpot. You can configure a weighted strategy where Fargate handles the base load and Fargate Spot is used\nfor scaling up during traffic spikes.\nCost Optimization: This approach optimizes cost by using the cheaper Fargate Spot for the fluctuating\ndemand while relying on the reliability of Fargate for consistent performance. Options A and D lack cost\noptimization because they rely solely on Fargate, which is more expensive than Fargate Spot. Option C is less\nreliable, as it relies on spot instances for the steady state.\nLoad Testing (Included implicitly): Option A mentions load testing, which is a best practice for any\ncontainerized application, and this should be done regardless of the chosen deployment strategy. But, doing\nload testing, and manually right sizing the Fargate tasks isn't a complete solution for cost optimisation or\nreliable burst management.\nKey Concepts:\nAmazon ECS: A fully managed container orchestration service.\nAWS Fargate: A serverless compute engine for containers.\nFargate Spot: A pricing option for Fargate that provides significant cost savings for interruptible workloads.\nECS Capacity Providers: Allows you to define a strategy for using different compute resources (e.g., Fargate,\nFargate Spot).\nAuthoritative Links:\nAWS Fargate: https://aws.amazon.com/fargate/\nECS Capacity Providers:\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/capacity_providers.html\nFargate Spot: https://aws.amazon.com/blogs/containers/fargate-spot-ecs-ec2/",
    "links": [
      "https://aws.amazon.com/fargate/",
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/capacity_providers.html",
      "https://aws.amazon.com/blogs/containers/fargate-spot-ecs-ec2/"
    ]
  },
  {
    "question": "CertyIQ\nA company is building an application in the AWS Cloud. The application is hosted on Amazon EC2 instances behind\nan Application Load Balancer (ALB). The company uses Amazon Route 53 for the DNS.\nThe company needs a managed solution with proactive engagement to detect against DDoS attacks.\nWhich solution will meet these requirements?",
    "options": {
      "B": "C.Store the ALB access logs in an Amazon S3 bucket. Configure Amazon GuardDuty to detect and take"
    },
    "answer": "D",
    "explanation": "The correct answer is D, subscribing to AWS Shield Advanced and configuring hosted zones in Route 53 and\nadding ALB resources as protected resources. Here's why:\nAWS Shield Advanced is a managed Distributed Denial of Service (DDoS) protection service that provides\nenhanced detection and mitigation capabilities, including 24/7 access to the AWS DDoS Response Team\n(DRT). The DRT offers proactive engagement, custom mitigation strategies, and expert guidance during DDoS\nevents, aligning perfectly with the requirement for proactive engagement.\nOption A (AWS Config) focuses on configuration management and compliance. While AWS Config can detect\nconfiguration changes that might increase vulnerability to DDoS, it doesn't directly detect DDoS attacks or\nprovide proactive engagement like AWS Shield Advanced does. AWS Config's rules are reactive, not proactive\nin the sense required for active DDoS mitigation support.\nOption B (AWS WAF) offers protection against web application attacks, including some types of DDoS attacks\ntargeting the application layer (Layer 7). However, it doesn't provide the comprehensive protection against all\ntypes of DDoS attacks (including volumetric attacks that target network infrastructure) or the proactive\nengagement offered by AWS Shield Advanced. Also, while WAF helps, it requires manual configuration of\nrules, and the proactive element is missing.\nOption C (Amazon GuardDuty) is a threat detection service that analyzes logs for malicious activity. While it\ncan potentially detect some indicators of a DDoS attack based on log analysis, it is not designed primarily for\nDDoS protection and lacks the specialized mitigation and DRT support that AWS Shield Advanced provides.\nGuardDuty focuses on detecting intrusions and suspicious behavior after they've occurred, not proactively\npreventing and mitigating DDoS attacks.\nAWS Shield Advanced is specifically designed for DDoS protection, handles a broader range of attack types,\nand offers the crucial proactive engagement through the DRT. By associating the ALB (Application Load\nBalancer) as a protected resource and using Route 53 hosted zones, Shield Advanced can effectively protect\nthe application's infrastructure and DNS from DDoS attacks, fulfilling all the requirements.\nFurther Resources:\nAWS Shield: https://aws.amazon.com/shield/\nAWS Shield Advanced Features: https://aws.amazon.com/shield/features/\nAWS DDoS Response Team (DRT): https://aws.amazon.com/premiumsupport/knowledge-center/ddos-\nresponse-team-shield/",
    "links": [
      "https://aws.amazon.com/shield/",
      "https://aws.amazon.com/shield/features/",
      "https://aws.amazon.com/premiumsupport/knowledge-center/ddos-"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts a video streaming web application in a VP",
    "options": {
      "C": "The company uses a Network Load Balancer (NLB) to",
      "B": "This effectively whitelists the allowed traffic sources, preventing"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Recreate the NLB with a security group to allow only trusted IP addresses.\nHere's a detailed justification:\nThe primary goal is to improve application security against unauthorized access attempts with minimal\narchitectural changes. Security groups act as virtual firewalls that control inbound and outbound traffic at the\ninstance level and, importantly, can be associated with Network Load Balancers (NLBs). By recreating the\nNLB and associating a security group with it, the company can explicitly define rules that allow only trusted IP\naddresses to connect to the NLB. This effectively whitelists the allowed traffic sources, preventing\nunauthorized access from any other IP addresses.\nOption A is incorrect because AWS WAF (Web Application Firewall) is designed for HTTP/HTTPS traffic and\ncannot be directly associated with an NLB, which handles TCP/UDP traffic at Layer 4. NLBs operate at a lower\nlayer and are not aware of the application-layer content that WAF needs to inspect.\nOption C, deploying a second NLB in parallel, adds unnecessary complexity and cost. It also doesn't directly\naddress the root cause of the problem, which is unauthorized IP addresses attempting to access the\napplication. Managing two NLBs would also introduce operational overhead.\nOption D, using AWS Shield Advanced, provides enhanced DDoS protection, but DDoS protection focuses on\nmitigating large-scale distributed attacks that aim to overwhelm the application. While Shield Advanced can\noffer some protection against malicious traffic, it's not primarily designed for restricting access based on\nspecific IP addresses or creating a whitelist, like a security group does. Shield Advanced also incurs a higher\ncost compared to using security groups.\nTherefore, using a security group on the NLB provides a simple, cost-effective, and targeted solution that\ndirectly addresses the requirement of allowing only trusted IP addresses to access the application with\nminimal architectural change. Security groups are natively integrated with AWS and are a fundamental\nsecurity component.\nSupporting links:\nAWS Security Groups: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\nNetwork Load Balancer: https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\nAWS WAF: https://aws.amazon.com/waf/\nAWS Shield: https://aws.amazon.com/shield/",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html",
      "https://aws.amazon.com/waf/",
      "https://aws.amazon.com/shield/"
    ]
  },
  {
    "question": "CertyIQ\nA healthcare company is developing an AWS Lambda function that publishes notifications to an encrypted Amazon\nSimple Notification Service (Amazon SNS) topic. The notifications contain protected health information (PHI).\nThe SNS topic uses AWS Key Management Service (AWS KMS) customer managed keys for encryption. The\ncompany must ensure that the application has the necessary permissions to publish messages securely to the SNS\ntopic.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "options": {
      "A": "Create a resource policy for the SNS topic that allows the Lambda function to publish messages to the",
      "B": "Use server-side encryption with AWS KMS keys (SSE-KMS) for the SNS topic instead of customer",
      "C": "Create a resource policy for the encryption key that the SNS topic uses that has the necessary AWS",
      "D": "Specify the Lambda function's Amazon Resource Name (ARN) in the SNS topic's resource policy. While"
    },
    "answer": "A",
    "explanation": "The correct answer is ACF. Let's break down why each choice is either right or wrong:\nA. Create a resource policy for the SNS topic that allows the Lambda function to publish messages to the\ntopic. This is essential. SNS topics require explicit permissions for entities to publish to them. The resource\npolicy on the SNS topic must include a statement granting the Lambda function's IAM role or account the\nsns:Publish permission. This allows the Lambda function to send messages to the SNS topic.\nB. Use server-side encryption with AWS KMS keys (SSE-KMS) for the SNS topic instead of customer\nmanaged keys. This is incorrect. While SSE-KMS provides encryption, the question specifically states\ncustomer-managed keys are being used. Switching to default KMS keys doesn't address the core requirement\nof granting permissions to access the existing customer-managed key for encryption/decryption.\nC. Create a resource policy for the encryption key that the SNS topic uses that has the necessary AWS\nKMS permissions. This is crucial because the SNS topic is encrypted with a customer-managed KMS key. The\nLambda function needs permission to use this key for encryption. The KMS key's resource policy must grant\nthe Lambda function's IAM role permissions to perform KMS actions like kms:GenerateDataKey, kms:Decrypt,\nand kms:Encrypt (depending on how SNS uses the key). Without these permissions, the Lambda function won't\nbe able to publish messages to the encrypted SNS topic.\nD. Specify the Lambda function's Amazon Resource Name (ARN) in the SNS topic's resource policy. While\nincluding the ARN in the SNS topic policy is good practice, it's not sufficient on its own. You need to specify\nwhat the ARN is allowed to do by specifying the appropriate action (sns:Publish). A is the better choice for the\naction as it is more descriptive.\nE. Associate an Amazon API Gateway HTTP API with the SNS topic to control access to the topic by using\nAPI Gateway resource policies. This is incorrect. API Gateway is not required to enable a Lambda to publish\ndirectly to an SNS topic. API Gateway is typically used to expose SNS functionality as a REST API endpoint\nwhich is not described in this scenario. This also adds unnecessary complexity.\nF. Configure a Lambda execution role that has the necessary IAM permissions to use a customer managed\nkey in AWS KMS. This is a vital part of the solution. The Lambda function's IAM role (the execution role)\ndefines what AWS resources the Lambda function is allowed to access. The role must have IAM policy\nstatements granting it permissions to perform KMS actions. This allows the Lambda to call the KMS service to\nencrypt data for SNS.\nIn summary, the Lambda function needs permission to publish to the SNS topic (A) and permission to use the\nKMS key that the SNS topic uses for encryption (C & F). Therefore, ACF is the correct answer.\nSupporting Links:\nSNS Encryption: https://docs.aws.amazon.com/sns/latest/dg/sns-server-side-encryption.html\nKMS Permissions: https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html\nSNS Resource Based Policies: https://docs.aws.amazon.com/sns/latest/dg/sns-using-identity-based-\npolicies.html",
    "links": [
      "https://docs.aws.amazon.com/sns/latest/dg/sns-server-side-encryption.html",
      "https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html",
      "https://docs.aws.amazon.com/sns/latest/dg/sns-using-identity-based-"
    ]
  },
  {
    "question": "CertyIQ\nA company has an employee web portal. Employees log in to the portal to view payroll details. The company is\ndeveloping a new system to give employees the ability to upload scanned documents for reimbursement. The\ncompany runs a program to extract text-based data from the documents and attach the extracted information to\neach employees reimbursement IDs for processing.\nThe employee web portal requires 100% uptime. The document extract program runs infrequently throughout the\nday on an on-demand basis. The company wants to build a scalable and cost-effective new system that will require\nminimal changes to the existing web portal. The company does not want to make any code changes.\nWhich solution will meet these requirements with the LEAST implementation effort?",
    "options": {},
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the best solution, along with supporting concepts and links:\nOption D offers the most cost-effective, scalable, and minimally invasive approach, aligning with the\ncompany's requirements of 100% uptime for the portal, infrequent document extraction, minimal code\nchanges, and minimal implementation effort.\nBy hosting the web portal on Amazon S3, the company benefits from S3's inherent high availability and\ndurability. S3 is designed for 99.999999999% (11 9's) of data durability and 99.99% availability.\nhttps://aws.amazon.com/s3/\nUsing API Gateway in conjunction with Lambda functions allows existing functionalities to be exposed and\nscaled without directly modifying the portal's codebase. API Gateway acts as a front door, routing requests to\nthe appropriate Lambda function. https://aws.amazon.com/api-gateway/\nInvoking a Lambda function when a new document is uploaded via the API (associated with upload) ensures\non-demand execution of the document extraction process only when needed. Lambda offers serverless\ncompute, meaning the company doesn't need to manage servers for this infrequent task, contributing to cost\nsavings. https://aws.amazon.com/lambda/\nThis serverless approach is significantly more cost-effective than running EC2 instances (options A, B, and C)\nconstantly for the document extraction program. The company only pays when the Lambda function\nexecutes.\nOptions A, B, and C require managing EC2 instances, increasing operational overhead and cost, particularly\nfor the infrequent document extraction process.\nOption A using on-demand EC2 for the web portal would ensure high availability but is more expensive\ncompared to serving static content from S3. Using Lambda for document extraction is good but the web\nportal component is not optimal.\nOption B relies on EC2 Spot Instances, which can be interrupted, potentially disrupting the document\nextraction process. Spot Instances are also unsuitable for the web portal which requires 100% uptime.\nOption C suggests using a Savings Plan for both the web portal and the document extraction program running\nin an Auto Scaling group. This means keeping instances running even when the document extraction program\nisn't actively processing documents, resulting in wasted resources.\nTherefore, the serverless architecture using S3, API Gateway, and Lambda is the most efficient solution in\nterms of cost, scalability, and minimal operational overhead, fulfilling all the given requirements.",
    "links": [
      "https://aws.amazon.com/s3/",
      "https://aws.amazon.com/api-gateway/",
      "https://aws.amazon.com/lambda/"
    ]
  },
  {
    "question": "CertyIQ\nA media company has a multi-account AWS environment in the us-east-1 Region. The company has an Amazon\nSimple Notification Service (Amazon SNS) topic in a production account that publishes performance metrics. The\ncompany has an AWS Lambda function in an administrator account to process and analyze log data.\nThe Lambda function that is in the administrator account must be invoked by messages from the SNS topic that is\nin the production account when significant metrics are reported.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": {
      "B": "Implement an Amazon Simple Queue Service (Amazon SQS) queue in the administrator account to buffer",
      "A": "Create an IAM resource policy for the Lambda function that allows Amazon SNS to invoke the function.",
      "C": "Create an IAM policy for the SNS topic that allows the Lambda function to subscribe to the topic. This is",
      "D": "Use an Amazon EventBridge rule in the production account to capture the SNS topic notifications."
    },
    "answer": "A",
    "explanation": "The correct answer is AB. Here's a detailed justification:\nA. Create an IAM resource policy for the Lambda function that allows Amazon SNS to invoke the function.\nThis step is crucial for enabling cross-account invocation. By default, a Lambda function can only be invoked\nby resources within the same account. To allow the SNS topic in the production account to trigger the Lambda\nfunction in the administrator account, we need to grant SNS permission to do so. This is achieved by adding a\nresource-based IAM policy to the Lambda function. This policy explicitly states that the SNS topic (identified\nby its ARN) is allowed to invoke the lambda:InvokeFunction action on the function. Without this permission,\nSNS would be denied access.\nAuthoritative Link: https://docs.aws.amazon.com/lambda/latest/dg/services-sns.html (See section on\n\"Granting SNS Permissions\")\nB. Implement an Amazon Simple Queue Service (Amazon SQS) queue in the administrator account to buffer\nmessages from the SNS topic that is in the production account. Configure the SQS queue to invoke the\nLambda function.\nThis step addresses a common best practice and architectural pattern in event-driven systems. Using an SQS\nqueue as an intermediary buffer between the SNS topic and the Lambda function provides several benefits:\n1. Decoupling: It decouples the publisher (SNS topic) from the subscriber (Lambda function). This\nmeans the SNS topic doesn't need to know about the Lambda function's availability or processing\nspeed.\n2. Buffering/Asynchronous Processing: The SQS queue acts as a buffer, storing messages if the\nLambda function is temporarily unavailable or experiencing processing delays. This prevents\nmessage loss and ensures eventual delivery. This is especially useful for performance metrics, as\nsome degree of latency is tolerable, but the data should not be lost.\n3. Scalability: The SQS queue can handle spikes in traffic by accumulating messages until the Lambda\nfunction can process them.\n4. Error Handling/Retries: SQS Dead Letter Queues (DLQs) can be configured to store messages that\nthe Lambda function fails to process after a certain number of attempts. This provides a mechanism\nfor debugging and handling errors.\nTo implement this, the SNS topic in the production account is configured to send messages to the SQS queue\nin the administrator account. Then, the SQS queue is configured as an event source for the Lambda function,\nmeaning the Lambda function is automatically invoked whenever a new message arrives in the queue. An SQS\nqueue policy will also be necessary to allow the SNS topic to send messages to the queue.\nAuthoritative Link: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-\nconfiguring-lambda-triggers.html (See section on \"Using AWS Lambda with Amazon SQS\")\nWhy other options are incorrect:\nC. Create an IAM policy for the SNS topic that allows the Lambda function to subscribe to the topic. This is\nincorrect because the Lambda function resides in a different account and needs permission to be invoked by\nSNS, not to subscribe to the topic. IAM policies attached to the SNS topic control who can publish to the topic,\nnot who can receive messages.\nD. Use an Amazon EventBridge rule in the production account to capture the SNS topic notifications.\nConfigure the EventBridge rule to forward notifications to the Lambda function that is in the administrator\naccount. While EventBridge could be used to route events cross-account, it adds unnecessary complexity in\nthis case, as SNS to SQS to Lambda is a simpler, more common, and often more cost-effective pattern.\nEventBridge is more appropriate when you need complex routing rules and transformation capabilities.\nE. Store performance metrics in an Amazon S3 bucket in the production account. Use Amazon Athena to\nanalyze the metrics from the administrator account. This option describes a completely different\narchitecture for analyzing data. It doesn't address the requirement of triggering a Lambda function when\nsignificant metrics are reported via SNS. Athena is for batch processing and analysis of data at rest, not for\nreal-time triggering based on SNS messages.",
    "links": [
      "https://docs.aws.amazon.com/lambda/latest/dg/services-sns.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-"
    ]
  },
  {
    "question": "CertyIQ\nA company is migrating an application from an on-premises location to Amazon Elastic Kubernetes Service\n(Amazon EKS). The company must use a custom subnet for pods that are in the company's VPC to comply with\nrequirements. The company also needs to ensure that the pods can communicate securely within the pods' VP",
    "options": {
      "C": "The Amazon"
    },
    "answer": "C",
    "explanation": "The correct answer is C: Use the Amazon VPC CNI plugin for Kubernetes. Define custom subnets in the VPC\ncluster for the pods to use.\nHere's a detailed justification:\nThe primary requirement is to utilize custom subnets for EKS pods within the company's VPC. The Amazon\nVPC CNI (Container Network Interface) plugin directly addresses this. It's designed to integrate Kubernetes\npods seamlessly with the AWS VPC networking environment. This integration allows pods to receive IP\naddresses from the VPC's address space, enabling direct communication within the VPC and with other AWS\nresources. By defining custom subnets specifically for EKS pods, the company gains precise control over the\npod network and adheres to its compliance requirements.\nThe VPC CNI plugin assigns IP addresses to pods from the subnet(s) you specify during cluster creation or\nupdate. You can dedicate particular subnets specifically for pod IP address assignment, ensuring compliance\nand isolation as required. This approach avoids the need for complex workarounds or external networking\nsolutions. It utilizes native AWS networking capabilities for a streamlined and performant solution.\nOption A is incorrect because AWS Transit Gateway manages connectivity between VPCs and on-premises\nnetworks, but it doesn't directly configure subnets for individual pods within an EKS cluster. Transit Gateway\nis not a pod networking solution.\nOption B is incorrect because AWS Direct Connect is used to establish a dedicated network connection from\non-premises to AWS, but it does not handle pod networking within EKS. Direct Connect manages connectivity\nat a higher level. Furthermore, you wouldn't connect to individual EKS pods via Direct Connect.\nOption D is incorrect because Kubernetes network policies control traffic between pods, but they do not\ndictate which subnets pods reside in. Pod anti-affinity rules can influence pod placement but they don't force\nthe use of custom subnets; they only influence which nodes pods land on, and only indirectly impact subnet\nusage if those nodes are configured to use specific subnets. The underlying need to configure the EKS cluster\nto use the desired subnets remains.\nTherefore, the Amazon VPC CNI plugin, configured with the desired custom subnets, directly and efficiently\nfulfills the stated requirements.\nFurther Reading:\nAmazon VPC CNI plugin for Kubernetes:\nhttps://docs.aws.amazon.com/eks/latest/userguide/network_cni_plugin.html\nAmazon EKS Networking: https://aws.amazon.com/eks/networking/",
    "links": [
      "https://docs.aws.amazon.com/eks/latest/userguide/network_cni_plugin.html",
      "https://aws.amazon.com/eks/networking/"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts an ecommerce application that stores all data in a single Amazon RDS for MySQL DB instance\nthat is fully managed by AWS. The company needs to mitigate the risk of a single point of failure.\nWhich solution will meet these requirements with the LEAST implementation effort?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A: Modify the RDS DB instance to use a Multi-AZ deployment. Apply the changes during\nthe next maintenance window.\nHere's why:\nThe primary requirement is to mitigate the risk of a single point of failure for the RDS database with the least\nimplementation effort. Multi-AZ deployments in RDS are designed precisely for this purpose. Enabling Multi-\nAZ creates a synchronous, standby replica of your database in a different Availability Zone. AWS\nautomatically handles failover to the standby replica in case of an infrastructure failure, minimizing downtime\nand human intervention.\nOption A is the most straightforward approach. It involves modifying the existing RDS instance settings to\nenable Multi-AZ. The changes can be applied during a maintenance window to minimize disruption. RDS\nhandles the replication and failover automatically.\nOption B, migrating to DynamoDB, is an entirely different database technology (NoSQL vs. relational). This\nrequires significant application code changes and a complex data migration strategy using AWS DMS. It's not\nthe least effort. A heterogeneous migration introduces considerable complexity compared to simply enabling\nMulti-AZ in RDS.\nOption C, creating a new Multi-AZ RDS instance and restoring from a snapshot, is more involved than simply\nmodifying the existing instance. Restoring from a snapshot means downtime while the data is being copied,\nand it does not provide automatic failover.\nOption D, using EC2 Auto Scaling groups for databases, is not a typical or recommended architecture for RDS.\nRDS is a managed service that handles replication, patching, and backups. Deploying a database on EC2\nwithin an Auto Scaling group requires manual configuration and management of these aspects, increasing\noperational overhead. Using Route 53 simple routing doesn't guarantee data consistency or automatic\nfailover in the event of a database instance failure, which is the primary goal.\nEnabling Multi-AZ directly addresses the high availability requirement with the least amount of administrative\nand development overhead.\nAuthoritative Links:\nAmazon RDS Multi-AZ Deployments:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\nAWS Database Migration Service (DMS): https://aws.amazon.com/dms/",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html",
      "https://aws.amazon.com/dms/"
    ]
  },
  {
    "question": "CertyIQ\nA company has multiple Microsoft Windows SMB file servers and Linux NFS file servers for file sharing in an on-\npremises environment. As part of the company's AWS migration plan, the company wants to consolidate the file\nservers in the AWS Cloud.\nThe company needs a managed AWS storage service that supports both NFS and SMB access. The solution must\nbe able to share between protocols. The solution must have redundancy at the Availability Zone level.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "A",
    "explanation": "The best solution is Amazon FSx for NetApp ONTAP with multi-protocol access because it directly addresses\nthe company's need for a managed file storage service supporting both SMB and NFS protocols with inter-\nprotocol sharing. FSx for NetApp ONTAP is built on NetApp's ONTAP file system, which inherently supports\nboth SMB and NFS access. Configuring multi-protocol access within FSx for NetApp ONTAP allows Windows\nand Linux clients to simultaneously access the same data through their respective preferred protocols. This\neliminates the need for separate file servers or data replication between different services.\nThe requirement for Availability Zone-level redundancy is also met by FSx for NetApp ONTAP. It can be\ndeployed in a Multi-AZ configuration, providing high availability and fault tolerance. In the event of an\nAvailability Zone failure, FSx for NetApp ONTAP automatically fails over to the other Availability Zone,\nensuring minimal downtime.\nOption B is less desirable as it involves managing EC2 instances, handling patching, backups, and scaling,\nincreasing operational overhead compared to a managed service. Option C involves two different FSx\nservices, lacking a unified storage platform, hence no direct sharing is possible and management complexity\nis added. Option D utilizes S3, an object storage service, requiring a file gateway. The File Gateway could add\ncomplexity and potentially impact performance.\nIn summary, Amazon FSx for NetApp ONTAP with multi-protocol access offers a fully managed, highly\navailable solution that directly meets the company's requirements for consolidating file servers, supporting\nboth SMB and NFS protocols, and enabling inter-protocol sharing while providing the desired level of\nredundancy.\nFurther research can be conducted at:\nAmazon FSx for NetApp ONTAP\nMulti-protocol Access with Amazon FSx for NetApp ONTAP",
    "links": []
  },
  {
    "question": "CertyIQ\nA software company needs to upgrade a critical web application. The application currently runs on a single\nAmazon EC2 instance that the company hosts in a public subnet. The EC2 instance runs a MySQL database. The\napplication's DNS records are published in an Amazon Route 53 zone.\nA solutions architect must reconfigure the application to be scalable and highly available. The solutions architect\nmust also reduce MySQL read latency.\nWhich combination of solutions will meet these requirements? (Choose two.)",
    "options": {
      "C": "Migrate the database to an Amazon Aurora MySQL cluster. Create the primary DB instance and reader",
      "B": "Create and configure an Auto Scaling group to launch private EC2 instances in multiple Availability",
      "A": "Launch a second EC2 instance in a second AWS Region. Use a Route 53 failover routing policy to redirect",
      "D": "Create and configure an Auto Scaling group to launch private EC2 instances in multiple AWS Regions."
    },
    "answer": "B",
    "explanation": "The correct answer is BC. Here's why:\nB. Create and configure an Auto Scaling group to launch private EC2 instances in multiple Availability\nZones. Add the instances to a target group behind a new Application Load Balancer: This addresses\nscalability and high availability for the web application. Auto Scaling Groups (ASGs) automatically adjust the\nnumber of EC2 instances based on demand, ensuring the application can handle traffic spikes. Spreading\nthese instances across multiple Availability Zones (AZs) protects against failures in a single AZ. An\nApplication Load Balancer (ALB) distributes incoming traffic evenly across the healthy EC2 instances,\nimproving performance and availability. ALBs offer advanced features like content-based routing, further\nenhancing the application's capabilities. Instances within a private subnet ensures security.\nAuto Scaling Groups Documentation\nApplication Load Balancer Documentation\nC. Migrate the database to an Amazon Aurora MySQL cluster. Create the primary DB instance and reader\nDB instance in separate Availability Zones: This addresses scalability, high availability, and read latency for\nthe database component. Aurora MySQL is a fully managed, MySQL-compatible relational database engine\nthat offers improved performance and availability compared to traditional MySQL. Creating a primary instance\nand a read replica in different AZs provides automatic failover in case the primary instance fails. Read replicas\ncan handle read requests, offloading the primary instance and reducing read latency, significantly improving\napplication performance.\nAmazon Aurora Documentation\nAurora Read Replicas Documentation\nWhy other options are incorrect:\nA. Launch a second EC2 instance in a second AWS Region. Use a Route 53 failover routing policy to redirect\nthe traffic to the second EC2 instance: While this provides disaster recovery across Regions, it doesn't\naddress scalability within the primary Region or database read latency. Also, failover mechanisms are slower\nthan multi-AZ setups, leading to potential downtime during failover.\nD. Create and configure an Auto Scaling group to launch private EC2 instances in multiple AWS Regions.\nAdd the instances to a target group behind a new Application Load Balancer: Deploying the web application\nacross multiple Regions adds significant complexity and cost. Cross-region deployments are more suitable for\ndisaster recovery scenarios.\nE. Migrate the database to an Amazon Aurora MySQL cluster with cross-Region read replicas: Cross-Region\nread replicas are best suited for disaster recovery or serving globally distributed users, not for addressing\nread latency within a single Region. Intra-region read replicas as described in Option C provide the necessary\nlow latency read scale.",
    "links": []
  },
  {
    "question": "CertyIQ\nA company runs thousands of AWS Lambda functions. The company needs a solution to securely store sensitive\ninformation that all the Lambda functions use. The solution must also manage the automatic rotation of the\nsensitive information.\nWhich combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)",
    "options": {
      "D": "Here's why:",
      "A": "Create HTTP security headers by using Lambda@Edge to retrieve and create sensitive information:",
      "B": "Create a Lambda layer that retrieves sensitive information: A Lambda layer can help with code reuse, but"
    },
    "answer": "C",
    "explanation": "The correct answer is C and D. Here's why:\nAWS Secrets Manager (C): This is the ideal service for securely storing and automatically rotating secrets\n(like database credentials, API keys) used by Lambda functions. It's designed specifically for this purpose.\nhttps://aws.amazon.com/secrets-manager/\nAWS Systems Manager Parameter Store (D): While Secrets Manager is preferable for actual secrets\nrequiring rotation, Parameter Store (especially Secure String parameters) can also store sensitive\ninformation, albeit with a bit less automation for rotation compared to Secrets Manager. It's suitable for\nconfiguration data and smaller secrets where automated rotation isn't as critical.\nhttps://aws.amazon.com/systems-manager/features/parameter-store/\nLet's analyze the incorrect options:\nA. Create HTTP security headers by using Lambda@Edge to retrieve and create sensitive information:\nLambda@Edge is intended for customizing content delivered by CloudFront, not for managing secrets used by\ninternal Lambda functions. It's a misapplication of the service.\nB. Create a Lambda layer that retrieves sensitive information: A Lambda layer can help with code reuse, but\nit doesn't inherently provide security or automatic rotation of secrets. The Lambda layer would still need to\nfetch the secrets from a secure location.\nE. Create a Lambda consumer with dedicated throughput to retrieve sensitive information and create\nenvironmental variables: This is an unnecessarily complex and inefficient architecture. It introduces a\ndedicated Lambda function solely for managing secrets, adding operational overhead and potential\nperformance bottlenecks. Environmental variables are also not the most secure way to store highly sensitive\ninformation.\nTherefore, storing sensitive information in either Secrets Manager or Parameter Store provides a more\nmanageable and secure solution for thousands of Lambda functions, with Secrets Manager being the more\nsuitable for scenarios needing automatic rotation.",
    "links": [
      "https://aws.amazon.com/secrets-manager/",
      "https://aws.amazon.com/systems-manager/features/parameter-store/"
    ]
  },
  {
    "question": "CertyIQ\nA company has an internal application that runs on Amazon EC2 instances in an Auto Scaling group. The EC2\ninstances are compute optimized and use Amazon Elastic Block Store (Amazon EBS) volumes.\nThe company wants to identify cost optimizations across the EC2 instances, the Auto Scaling group, and the EBS\nvolumes.\nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C because AWS Compute Optimizer is specifically designed to analyze the utilization\nmetrics of AWS resources like EC2 instances and EBS volumes, and then provide recommendations for cost\noptimization and performance improvement. For Auto Scaling groups, Compute Optimizer analyzes the\nunderlying EC2 instances to provide optimization advice. This makes it the most efficient single tool to meet\nthe company's requirements.\nOption A is less efficient because parsing through the AWS Cost and Usage Report requires more manual\neffort to identify cost recommendations for each resource. While the report provides detailed cost\ninformation, it does not automatically provide optimization suggestions.Option B is not suitable as\nCloudWatch billing alerts are used to notify about cost thresholds, not to provide specific resource\noptimization recommendations.Option D is also less efficient because it combines Compute Optimizer for EC2\ninstances with manual analysis of the Cost and Usage Report for Auto Scaling groups and EBS volumes.\nCompute Optimizer can cover all three resource types directly, making the combination redundant and\nrequiring more effort.\nIn summary, AWS Compute Optimizer offers a consolidated and automated approach to cost optimization\nrecommendations for EC2 instances, EBS volumes, and indirectly, Auto Scaling groups, ensuring the greatest\noperational efficiency.\nReference:\nAWS Compute Optimizer",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is running a media store across multiple Amazon EC2 instances distributed across multiple Availability\nZones in a single VP",
    "options": {
      "C": "Amazon EBS Volume Mounted Across Instances: EBS volumes are block storage designed to be attached",
      "A": "Amazon S3 Bucket with API Calls: While S3 is excellent for object storage, it's accessed via API calls,",
      "B": "Amazon S3 Bucket as a Mounted Volume: S3 is not designed to be mounted as a traditional file system"
    },
    "answer": "D",
    "explanation": "The correct answer is D, configuring an Amazon Elastic File System (Amazon EFS) file system and mounting it\nacross all instances. Here's why:\nRequirement for High-Performance Data Sharing: The company needs a high-performing solution for sharing\ndata.\nData Within the VPC: They want to keep the data within the VPC.\nAmazon EFS for Shared Storage: Amazon EFS is designed to provide scalable, elastic, cloud-native NFS file\nsystems for use with AWS Cloud services and on-premises resources. It allows multiple EC2 instances to\nconcurrently access a shared file system.\nEFS Performance: EFS offers various performance modes, including General Purpose and Max I/O, allowing\nthe company to optimize for their specific media store workload. It also scales automatically as data is added\nor removed.\nEFS Integration with VPC: EFS file systems are mounted to EC2 instances via mount targets created within\nthe VPC, ensuring data remains within the VPC.\nWhy other options are incorrect:\nA. Amazon S3 Bucket with API Calls: While S3 is excellent for object storage, it's accessed via API calls,\nwhich are not ideal for high-performance, file-system-like data sharing. S3 is better suited for storing and\nretrieving individual files, not for continuous file system access.\nB. Amazon S3 Bucket as a Mounted Volume: S3 is not designed to be mounted as a traditional file system\nvolume. While solutions like S3FS exist, they often introduce performance overhead and aren't meant for\nhigh-performance scenarios within a VPC where a native shared file system solution is desired.\nC. Amazon EBS Volume Mounted Across Instances: EBS volumes are block storage designed to be attached\nto a single EC2 instance at a time. They are not designed for shared access by multiple instances\nsimultaneously. Attempting to share an EBS volume would lead to data corruption. There's a multi-attach\noption for EBS but it has many restrictions and is not designed for generalized shared file storage like EFS.\nIn summary, EFS provides the necessary shared file system capability with high performance, VPC integration,\nand scalability needed for the media store application.\nAuthoritative Links:\nAmazon EFS: https://aws.amazon.com/efs/\nAmazon EFS Documentation: https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html",
    "links": [
      "https://aws.amazon.com/efs/",
      "https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html"
    ]
  },
  {
    "question": "CertyIQ\nA company uses an Amazon RDS for MySQL instance. To prepare for end-of-year processing, the company added a\nread replica to accommodate extra read-only queries from the company's reporting tool. The read replica CPU\nusage was 60% and the primary instance CPU usage was 60%.\nAfter end-of-year activities are complete, the read replica has a constant 25% CPU usage. The primary instance\nstill has a constant 60% CPU usage. The company wants to rightsize the database and still provide enough\nperformance for future growth.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the most appropriate solution, along with supporting\nconcepts and links:\nThe scenario highlights a common situation: a temporary performance need (end-of-year processing) fulfilled\nby adding a read replica. Now that the temporary need has passed, the read replica is underutilized,\nconsuming resources without providing commensurate value. The primary instance, however, continues to\noperate at a moderate 60% CPU utilization.\nOption A, deleting the read replica entirely, would eliminate the ability to offload any read traffic from the\nprimary instance. While it reduces cost, it also reduces the overall read scalability and could potentially\nincrease the load on the primary instance if reporting queries increase in the future.\nOption B, resizing the read replica to a smaller instance size, strikes a balance. Since the read replica's CPU\nusage is consistently low (25%), reducing its instance size will lower costs without significantly impacting\nperformance. The primary instance remains unchanged because it's already at a manageable 60% CPU\nutilization. This approach allows the company to retain a read replica for potential future read-heavy\nworkloads or reporting needs, providing flexibility and headroom without unnecessary resource consumption.\nResizing is a non-destructive operation and can be easily reversed if performance degrades after the change.\nOption C is incorrect. The primary instance is already at a manageable 60% CPU utilization, so there's no need\nto resize it to a smaller instance size. Resizing the read replica to a larger instance size is also unnecessary\nsince it's underutilized.\nOption D, deleting the read replica and resizing the primary instance to a larger size, is also incorrect. Deleting\nthe read replica reduces read scalability, and increasing the primary instance size when it's at 60% utilization\nis not the optimal cost-effective solution.\nTherefore, the most efficient and cost-effective solution is to resize the read replica to a smaller instance size\nwhile leaving the primary instance unchanged. This aligns with the principle of rightsizing cloud resources to\nmatch actual workload requirements.\nKey Concepts:\nRead Replicas: Replicas of a database instance that serve read-only traffic, offloading the primary instance\nand improving read scalability.\n(https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html)\nRightsizing: Optimizing cloud resource allocation to match actual workload needs, minimizing costs and\nmaximizing efficiency.\nCost Optimization: Selecting the most cost-effective solution while meeting performance requirements.\nCPU Utilization: A metric that reflects the amount of processing power being used by an instance.\nAuthoritative Links:\nAmazon RDS Read Replicas: https://aws.amazon.com/rds/features/read-replicas/\nAmazon RDS Pricing: https://aws.amazon.com/rds/pricing/",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html)",
      "https://aws.amazon.com/rds/features/read-replicas/",
      "https://aws.amazon.com/rds/pricing/"
    ]
  },
  {
    "question": "CertyIQ\nA company is migrating its databases to Amazon RDS for PostgreSQL. The company is migrating its applications to\nAmazon EC2 instances. The company wants to optimize costs for long-running workloads.\nWhich solution will meet this requirement MOST cost-effectively?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D because it provides the most cost-effective solution for long-running workloads on\nboth Amazon RDS for PostgreSQL and EC2 instances. Here's a detailed breakdown:\nReserved Instances (RIs) and Savings Plans are Cost Optimization Tools: Both RIs and Savings Plans offer\nsignificant discounts compared to On-Demand pricing for consistent usage. They are designed for workloads\nwith predictable, long-term resource requirements.\nLonger Term = Greater Savings: A longer commitment (3 years vs. 1 year) typically yields a larger discount.\nAWS rewards customers who make longer-term reservations with greater cost savings.\nAll Upfront Option for Maximum Discount: Paying upfront (All Upfront) provides the largest discount\ncompared to No Upfront or Partial Upfront options. While it requires a larger initial investment, it results in the\nlowest overall cost over the commitment period.\nEC2 Instance Savings Plan: EC2 Instance Savings Plans provide savings on compute usage, regardless of\ninstance family, size, or AZ, as long as the usage matches the plan's commitment. This offers flexibility.\nReserved Instances for RDS: RDS Reserved Instances work in a similar manner, offering discounted pricing\nfor reserved database capacity.\nWhy other options are less optimal:\nOption A: On-Demand Instances are the most expensive option for long-running workloads. Savings Plans are\nbetter, but only cover EC2, not RDS.\nOption B: 1-year term offers less savings compared to 3-year.\nOption C: Partial Upfront offers less savings compared to All Upfront.\nTherefore, Option D, which combines a 3-year term with the All Upfront payment option for both Reserved\nInstances (for RDS) and Savings Plans (for EC2), maximizes cost savings for the company's long-running\ndatabase and application workloads.\nAuthoritative Links:\nAWS Reserved Instances\nAWS Savings Plans\nAWS RDS Pricing\nAWS EC2 Pricing",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is using an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The company must ensure that\nKubernetes service accounts in the EKS cluster have secure and granular access to specific AWS resources by\nusing IAM roles for service accounts (IRSA).\nWhich combination of solutions will meet these requirements? (Choose two.)",
    "options": {
      "D": "Define an IAM role that includes the necessary permissions. Annotate the Kubernetes service accounts",
      "A": "Create an IAM policy that defines the required permissions. Attach the policy directly to the IAM role of",
      "B": "Implement network policies within the EKS cluster to prevent Kubernetes service accounts from",
      "C": "Modify the EKS cluster's IAM role to include permissions for each Kubernetes service account. Ensure a"
    },
    "answer": "D",
    "explanation": "Here's a detailed justification for why options D and E are the correct solutions for implementing IAM roles for\nservice accounts (IRSA) in an Amazon EKS cluster to grant secure and granular access to AWS resources,\nalong with explanations of why the other options are incorrect:\nWhy D and E are correct:\nD. Define an IAM role that includes the necessary permissions. Annotate the Kubernetes service accounts\nwith the Amazon Resource Name (ARN) of the IAM role. This is a core component of IRSA. You create an IAM\nrole that specifies precisely which AWS resources a Kubernetes service account is allowed to access. The key\nhere is to grant least privilege  only the necessary permissions. Annotating the service account with the IAM\nrole's ARN informs Kubernetes that pods using this service account should assume that role.\nE. Set up a trust relationship between the IAM roles for the service accounts and an OpenID Connect (OIDC)\nidentity provider. This is another crucial aspect of IRSA. EKS integrates with AWS IAM via an OIDC provider.\nThe IAM role created in option D needs a trust policy. This trust policy specifies that the IAM role can be\nassumed by entities authenticated by the EKS cluster's OIDC provider. This trust policy verifies that only\nworkloads from the correct EKS cluster and, specifically, the correct Kubernetes service account, can assume\nthe role. This is how AWS verifies the identity of the Kubernetes service account making the AWS API calls.\nIn summary: Options D and E together establish the IRSA mechanism. An IAM role with specific permissions is\ncreated and trusted by the EKS cluster's OIDC provider. By annotating the service account with the IAM role's\nARN, pods running under the service account can securely assume that IAM role and access allowed AWS\nresources.\nWhy the other options are incorrect:\nA. Create an IAM policy that defines the required permissions. Attach the policy directly to the IAM role of\nthe EKS nodes. Attaching permissions directly to the EKS node's IAM role is overly permissive and does not\nprovide granular access control. It means any pod running on the node (even those not requiring access to\nthose AWS resources) would inherit those permissions, violating the principle of least privilege. Node roles\nare intended for EKS internal operations, not for general application access to AWS services.\nB. Implement network policies within the EKS cluster to prevent Kubernetes service accounts from\naccessing specific AWS services. Network policies control network traffic within the Kubernetes cluster.\nThey do not directly control access to AWS services. While you can restrict egress traffic from pods, this is\nnot the primary mechanism for controlling AWS API access via IAM roles.\nC. Modify the EKS cluster's IAM role to include permissions for each Kubernetes service account. Ensure a\none-to-one mapping between IAM roles and Kubernetes roles. Directly modifying the EKS cluster's IAM role\nfor individual service accounts is not the correct approach. The cluster role is for EKS management plane\noperations, not for workloads running within the cluster. Moreover, attempting a one-to-one mapping between\nKubernetes roles (RBAC) and AWS IAM roles isn't how IRSA works. IRSA maps Kubernetes service accounts\nto IAM roles, not RBAC roles.\nAuthoritative Links:\nIAM roles for service accounts: https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-\naccounts.html\nConfiguring a cluster to use IAM roles for service accounts:\nhttps://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html",
    "links": [
      "https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-",
      "https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html"
    ]
  },
  {
    "question": "CertyIQ\nA company regularly uploads confidential data to Amazon S3 buckets for analysis.\nThe company's security policies mandate that the objects must be encrypted at rest. The company must\nautomatically rotate the encryption key every year. The company must be able to track key rotation by using AWS\nCloudTrail. The company also must minimize costs for the encryption key.\nWhich solution will meet these requirements?",
    "options": {
      "D": "Use server-side encryption with customer managed AWS KMS keys (SSE-KMS)."
    },
    "answer": "D",
    "explanation": "The correct answer is D. Use server-side encryption with customer managed AWS KMS keys (SSE-KMS).\nHere's why:\nEncryption at Rest: All options provide encryption at rest, but the key is how the keys are managed.\nAutomatic Key Rotation: Customer managed KMS keys support automatic key rotation on a schedule that\nyou configure, such as annually, meeting the yearly rotation requirement.\nhttps://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html\nCloudTrail Tracking: AWS KMS integrates with AWS CloudTrail, allowing you to track the usage and rotation\nof your keys. This satisfies the auditing requirement.\nhttps://docs.aws.amazon.com/kms/latest/developerguide/logging-using-cloudtrail.html\nCost Optimization: While KMS keys have a cost associated with them, customer managed KMS keys offer the\nflexibility to optimize costs by managing usage and access control effectively.\nSSE-C Incompatibility: SSE-C requires you to manage the encryption keys, which contradicts the\nrequirement for automatic key rotation.\nSSE-S3 Limitations: SSE-S3 uses S3-managed keys, offering simplicity but no control over key rotation or\nCloudTrail logging of key usage.\nSSE-KMS (AWS Managed Keys) Limitations: While SSE-KMS provides encryption and integration with\nCloudTrail, AWS manages the key rotation schedule, limiting the company's control.\nTherefore, customer managed KMS keys are the ideal choice because they fulfill all the requirements:\nencryption at rest, automatic yearly key rotation, CloudTrail tracking, and cost optimization through usage\ncontrol. They give the company the necessary control and visibility over its encryption keys to meet its\nsecurity policies.",
    "links": [
      "https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html",
      "https://docs.aws.amazon.com/kms/latest/developerguide/logging-using-cloudtrail.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has migrated several applications to AWS in the past 3 months. The company wants to know the\nbreakdown of costs for each of these applications. The company wants to receive a regular report that includes\nthis information.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "C",
    "explanation": "The most cost-effective solution is C, tagging resources and using Cost Explorer. Here's why:\nResource Tagging: Applying tags (key-value pairs) to AWS resources is a fundamental practice for\norganization and cost management. Tags like cost: application-name allow you to categorize and track\nexpenses associated with each application. https://aws.amazon.com/aws-cost-management/aws-resource-\ntagging/\nCost Allocation Tags: Activating these tags ensures that AWS considers them when generating cost reports\nand performing cost analysis. This means your tagged resources will be grouped and summarized accordingly.\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html\nAWS Cost Explorer: Cost Explorer is a free tool within the AWS Billing and Cost Management console. It\nprovides a user-friendly interface to visualize, understand, and manage AWS costs over time. You can filter\nand group costs by the tags you've defined, making it easy to see the breakdown of costs for each application.\nCost Explorer can also generate reports and forecasts. https://aws.amazon.com/aws-cost-management/aws-\ncost-explorer/\nOption A (AWS Budgets and CSV) is less efficient. AWS Budgets are primarily for setting spending limits and\nreceiving alerts, not for detailed cost breakdown analysis of past expenses. Downloading CSV data and\nmanually analyzing it is time-consuming and prone to errors.\nOption B (Cost and Usage Reports to RDS) is more complex and expensive than necessary. While CUR is\npowerful for detailed analysis, loading it into an RDS instance and running SQL queries is overkill for simply\nunderstanding the cost breakdown of different applications. This option incurs RDS costs and requires\ndatabase management expertise. Also, it is difficult to get insights into cost allocation without resource tags\nin place.\nOption D (Tagging and Downloading Bills) requires manual searching within potentially very large bill files. It\nis not as easy and interactive as using Cost Explorer and reporting.\nIn summary, tagging resources, enabling cost allocation tags, and using Cost Explorer provides the most cost-\neffective and user-friendly way to get a regular cost breakdown report for each application.",
    "links": [
      "https://aws.amazon.com/aws-cost-management/aws-resource-",
      "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html",
      "https://aws.amazon.com/aws-cost-management/aws-"
    ]
  },
  {
    "question": "CertyIQ\nAn ecommerce company is preparing to deploy a web application on AWS to ensure continuous service for\ncustomers. The architecture includes a web application that the company hosts on Amazon EC2 instances, a\nrelational database in Amazon RDS, and static assets that the company stores in Amazon S3.\nThe company wants to design a robust and resilient architecture for the application.\nWhich solution will meet these requirements?",
    "options": {
      "B": "This solution incorporates several key AWS services to ensure continuous service."
    },
    "answer": "B",
    "explanation": "The optimal solution for a highly available and resilient e-commerce web application on AWS, as described in\nthe question, is option B. This solution incorporates several key AWS services to ensure continuous service.\nOption B suggests deploying Amazon EC2 instances within an Auto Scaling group spanning multiple\nAvailability Zones (AZs). This is crucial for fault tolerance; if one AZ experiences an issue, the Auto Scaling\ngroup automatically launches instances in other healthy AZs, maintaining application availability. Distributing\ninstances across multiple AZs eliminates a single point of failure for the web application tier.\nSimilarly, deploying a Multi-AZ RDS DB instance is vital for database resilience. In a Multi-AZ configuration,\nAWS automatically provisions and maintains a synchronous standby replica of the database in a different AZ.\nIf the primary DB instance fails, RDS automatically fails over to the standby, minimizing downtime.\nFinally, using Amazon CloudFront to distribute static assets enhances performance and availability.\nCloudFront caches static content at edge locations worldwide, reducing latency for users and offloading\ntraffic from the EC2 instances. The edge locations also provide inherent redundancy for serving static\ncontent.\nOption A fails because it relies on a single Availability Zone for both EC2 and RDS, which introduces a single\npoint of failure. Option C has the same issue with the single AZ EC2 instance. Storing assets directly on EC2\ninstances also creates a scalability bottleneck and doesn't leverage content delivery networks. Option D,\nwhile utilizing serverless technologies, might not be the most cost-effective or performant solution for a\ngeneral web application, and One Zone EFS offers reduced availability compared to multi-AZ options.\nFurthermore, while Lambda and Aurora Serverless v2 could work, the scenario specified explicitly mentions\nhosting the application on EC2 instances.\nTherefore, by leveraging Auto Scaling, Multi-AZ RDS, and CloudFront, Option B establishes a robust and\nresilient architecture that addresses the e-commerce company's requirements for continuous service and\nhigh availability.\nAuthoritative links:\nAuto Scaling: https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\nMulti-AZ RDS: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\nAmazon CloudFront:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html",
    "links": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html"
    ]
  },
  {
    "question": "CertyIQ\nAn ecommerce company runs several internal applications in multiple AWS accounts. The company uses AWS\nOrganizations to manage its AWS accounts.\nA security appliance in the company's networking account must inspect interactions between applications across\nAWS accounts.\nWhich solution will meet these requirements?",
    "options": {
      "B": "Application accounts send traffic to the GWLB endpoint, ensuring all inter-application communication is"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it leverages AWS Gateway Load Balancer (GWLB), specifically designed for\ninspecting, filtering, and steering traffic to virtual appliances. The scenario requires inspecting inter-\napplication traffic across multiple AWS accounts, and GWLB is the optimal solution for this purpose.\nHere's a breakdown:\nGWLB's Purpose: GWLB simplifies the deployment, scaling, and management of virtual appliances like\nfirewalls, intrusion detection systems (IDS), and deep packet inspection (DPI) tools. It integrates seamlessly\nwith VPCs and other AWS services.\nCross-Account Traffic Inspection: The networking account houses the security appliance and GWLB.\nApplication accounts send traffic to the GWLB endpoint, ensuring all inter-application communication is\nrouted through the security appliance for inspection.\nGWLB Endpoint: The GWLB endpoint in each application account provides a private, reliable connection to the\nGWLB in the networking account. Traffic destined for other applications flows through this endpoint and gets\ninspected.\nWhy other options are incorrect:\nA (NLB with interface VPC endpoint): Network Load Balancers are designed for TCP, UDP, and TLS traffic.\nWhile NLBs can direct traffic, they are not purpose-built for appliance insertion and inspection.\nB (ALB): Application Load Balancers are HTTP/HTTPS load balancers. They are not suitable for inspecting\ngeneral network traffic across multiple accounts.\nD (Interface VPC endpoint directly to the appliance): This approach requires managing multiple connections\nand scaling the security appliance independently in each account, making it a less centralized and scalable\nsolution.\nGWLB provides a centralized and managed solution for inspecting network traffic, offering scalability, high\navailability, and simplified management of security appliances. Its design allows for easy insertion and\ninspection of traffic between VPCs and AWS accounts.\nReference Link:\nAWS Gateway Load Balancer",
    "links": []
  },
  {
    "question": "CertyIQ\nA company runs its production workload on an Amazon Aurora MySQL DB cluster that includes six Aurora\nReplicas. The company wants near-real-time reporting queries from one of its departments to be automatically\ndistributed across three of the Aurora Replicas. Those three replicas have a different compute and memory\nspecification from the rest of the DB cluster.\nWhich solution meets these requirements?",
    "options": {
      "A": "Create and use a custom endpoint for the workload. Here's why:"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Create and use a custom endpoint for the workload. Here's why:\nCustom endpoints in Aurora allow you to direct specific read workloads to a defined subset of Aurora Replicas\nwithin your DB cluster. This directly addresses the requirement to send the near-real-time reporting queries\nonly to the three specifically configured replicas. By creating a custom endpoint, the company can specify the\nthree desired Aurora Replicas as members of that endpoint. When the reporting department connects to the\ncustom endpoint, Aurora ensures the queries are routed to those designated replicas.\nOption B, creating a three-node cluster clone, is overkill. Cloning the entire cluster just for reporting adds\nunnecessary cost and management overhead when a custom endpoint can achieve the same result with much\nless complexity. Furthermore, data replication delays between the primary cluster and the clone would also\nbe a concern.\nOption C, using instance endpoints, doesn't provide automatic load balancing or distribution of the workload.\nThe reporting department would need to manually manage which instance endpoint to connect to, which is\nnot ideal for an automatically distributed workload. There's also no guarantee queries won't be directed to the\nprimary instance, potentially impacting production performance.\nOption D, using the reader endpoint, distributes read traffic across all available Aurora Replicas, not just the\nspecific three replicas with different compute and memory specifications. This fails to meet the requirement\nof targeting a particular subset of replicas.\nTherefore, custom endpoints provide the most efficient and targeted way to direct the reporting workload to\nthe desired Aurora Replicas, enabling near-real-time reporting without impacting the primary production\nworkload or other replicas.\nFurther Research:\nAurora Custom Endpoints: - AWS Documentation on Custom Endpoints.",
    "links": []
  },
  {
    "question": "CertyIQ\nA company runs a Node js function on a server in its on-premises data center. The data center stores data in a\nPostgreSQL database. The company stores the credentials in a connection string in an environment variable on the\nserver. The company wants to migrate its application to AWS and to replace the Node.js application server with\nAWS Lambda. The company also wants to migrate to Amazon RDS for PostgreSQL and to ensure that the database\ncredentials are securely managed.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "B": "Store the database credentials as a secret in AWS Secrets Manager. Configure",
      "A": "Systems Manager Parameter Store: While Parameter Store can store sensitive data, it is generally better",
      "C": "Lambda Environment Variables: Storing credentials in Lambda environment variables, even if encrypted, is",
      "D": "AWS KMS: AWS KMS is designed for managing encryption keys, not for storing secrets directly. While you"
    },
    "answer": "B",
    "explanation": "The best solution is B. Store the database credentials as a secret in AWS Secrets Manager. Configure\nSecrets Manager to automatically rotate the credentials every 30 days. Update the Lambda function to\nretrieve the credentials from the secret.\nHere's why:\nSecrets Manager is designed for secret management: AWS Secrets Manager is specifically built to store and\nmanage sensitive information like database credentials, API keys, and other secrets. It provides a secure and\ncentralized location for these secrets. https://aws.amazon.com/secrets-manager/\nAutomatic Rotation: Secrets Manager supports automatic rotation of credentials, which greatly reduces the\noperational overhead of manually rotating them. This is crucial for security as regularly rotating credentials\nlimits the potential damage from compromised credentials.\nIntegration with Lambda: Lambda functions can easily retrieve secrets from Secrets Manager using the AWS\nSDK or the Secrets Manager Lambda extension. This provides a secure and straightforward way to access\ncredentials within the Lambda function.\nLeast Operational Overhead: Using Secrets Manager requires minimal configuration and integration effort\ncompared to other options, reducing operational overhead. You don't have to write custom rotation logic or\nmanage encryption/decryption manually.\nHere's why the other options are less suitable:\nA. Systems Manager Parameter Store: While Parameter Store can store sensitive data, it is generally better\nsuited for configuration data rather than secrets. Secrets Manager provides more robust secret management\ncapabilities, including automatic rotation. Moreover, rotating secrets using Parameter Store is less\nstraightforward.\nC. Lambda Environment Variables: Storing credentials in Lambda environment variables, even if encrypted, is\nnot a best practice. It's difficult to rotate the credentials, and the environment variables are less secure than a\ndedicated secret management service. Creating and scheduling a Lambda function for rotation would require\nsignificant overhead.\nD. AWS KMS: AWS KMS is designed for managing encryption keys, not for storing secrets directly. While you\ncan encrypt secrets with KMS, managing the secret data and its rotation would require significantly more\ncustom code and operational effort compared to Secrets Manager. KMS would encrypt the credentials but\nnot handle the key rotation for the credentials themselves; it would rotate the key used to encrypt them.\nIn summary, Secrets Manager is the optimal solution due to its dedicated secret management capabilities,\nautomatic rotation feature, seamless integration with Lambda, and minimal operational overhead, aligning\nperfectly with the company's requirements for secure credential management and a serverless architecture.",
    "links": [
      "https://aws.amazon.com/secrets-manager/"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to replicate existing and ongoing data changes from an on-premises Oracle database to Amazon\nRDS for Oracle. The amount of data to replicate varies throughout each day. The company wants to use AWS\nDatabase Migration Service (AWS DMS) for data replication. The solution must allocate only the capacity that the\nreplication instance requires.\nWhich solution will meet these requirements?",
    "options": {
      "B": "Create an AWS DMS Serverless replication task to analyze and replicate the data"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Create an AWS DMS Serverless replication task to analyze and replicate the data\nwhile provisioning the required capacity.\nHere's why:\nAWS DMS Serverless: AWS DMS Serverless is designed to automatically scale the replication capacity based\non the workload. It eliminates the need to manually provision and manage the replication instance, making it\nideal for scenarios with varying data replication needs. This aligns perfectly with the company's requirement\nto allocate only the capacity needed at any given time.\nCapacity Provisioning: DMS Serverless automatically provisions the necessary compute and memory\nresources as the data replication workload changes, ensuring efficient resource utilization and cost\noptimization. It handles the underlying infrastructure, allowing the company to focus on the data migration\nitself.\nAlternatives Analysis:\nA (Multi-AZ): Multi-AZ deployments provide high availability and fault tolerance but do not address the\ndynamic capacity requirements. It simply replicates the same instance across AZs, which may be over-\nprovisioned during periods of low data replication volume.\nC (EC2 Auto Scaling): While EC2 Auto Scaling can scale EC2 instances, it requires significant configuration\nand management overhead to integrate with DMS. It also involves a delay in scaling up/down, potentially\nleading to performance bottlenecks or underutilization.\nD (ECS with Fargate): Using ECS with Fargate for DMS replication is not a supported or recommended\napproach. DMS is designed to run on its own replication instance infrastructure, and using ECS would add\nunnecessary complexity and overhead.\nDMS Serverless directly addresses the need for dynamic capacity allocation, simplifying the process and\noptimizing resource utilization.\nSupporting Documentation:\nAWS Database Migration Service (DMS) Serverless: https://aws.amazon.com/dms/serverless/\nAWS DMS Documentation: https://docs.aws.amazon.com/dms/index.html",
    "links": [
      "https://aws.amazon.com/dms/serverless/",
      "https://docs.aws.amazon.com/dms/index.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has a multi-tier web application. The application's internal service components are deployed on\nAmazon EC2 instances. The internal service components need to access third-party software as a service (SaaS)\nAPIs that are hosted on AWS.\nThe company needs to provide secure and private connectivity from the application's internal services to the third-\nparty SaaS application. The company needs to ensure that there is minimal public internet exposure.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D: Use AWS PrivateLink to create a private connection between the application's VPC\nand the third-party SaaS provider.\nHere's a detailed justification:\nAWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises networks,\nwithout exposing your traffic to the public internet. This perfectly aligns with the requirement of minimal\npublic internet exposure for the internal service components accessing the third-party SaaS APIs. With\nPrivateLink, traffic between the application's VPC and the SaaS provider remains within the AWS network.\nOption A, AWS Site-to-Site VPN, is more suitable for connecting an on-premises network to AWS. While it\nprovides secure connectivity, it's not the best solution for connecting to a SaaS provider already hosted on\nAWS.\nOption B, AWS Transit Gateway, is excellent for managing connectivity between multiple VPCs and on-\npremises networks, but it doesn't inherently provide the private connection characteristic of PrivateLink. You\ncould use Transit Gateway, but it would likely be combined with other solutions like VPC peering which\nintroduces more complexity than PrivateLink directly. Also, it is not intended to be used as a direct\nreplacement of AWS PrivateLink.\nOption C suggests configuring PrivateLink to allow only outbound traffic. AWS PrivateLink, by design,\nestablishes a private connection in both directions. While you control security groups to limit traffic, it\ninherently provides bidirectional connectivity for requests and responses. Hence, this is not the primary use\ncase of PrivateLink\nTherefore, using AWS PrivateLink is the most secure and efficient method, establishing a private connection\ndirectly between the company's VPC and the third-party SaaS provider's service, keeping traffic within the\nAWS network and minimizing public internet exposure.\nRelevant Links:\nAWS PrivateLink: https://aws.amazon.com/privatelink/\nAWS Site-to-Site VPN: https://aws.amazon.com/vpn/\nAWS Transit Gateway: https://aws.amazon.com/transit-gateway/",
    "links": [
      "https://aws.amazon.com/privatelink/",
      "https://aws.amazon.com/vpn/",
      "https://aws.amazon.com/transit-gateway/"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect needs to connect a company's corporate network to its VPC to allow on-premises access to\nits AWS resources. The solution must provide encryption of all traffic between the corporate network and the VPC\nat the network layer and the session layer. The solution also must provide security controls to prevent unrestricted\naccess between AWS and the on-premises systems.\nWhich solution meets these requirements?",
    "options": {
      "C": "D (Transit Gateway): AWS Transit Gateway acts as a network transit hub. It provides a central point of"
    },
    "answer": "C",
    "explanation": "Let's break down why option C is the best solution.\nRequirements Breakdown:\nEncryption: Network layer (IPsec) and session layer (SSL/TLS).\nConnectivity: Securely connect the corporate network to the VPC.\nSecurity Controls: Restrict traffic between on-premises and AWS.\nWhy Option C (AWS Site-to-Site VPN) is Correct:\nAWS Site-to-Site VPN provides a secure, encrypted tunnel between your on-premises network and your AWS\nVPC. It uses IPsec encryption at the network layer, fulfilling the first part of the encryption requirement. By\ndefault, VPN connections ensure all data in transit is protected using encryption.\nRoute table entries, alongside network ACLs and Security groups, are the core of managing ingress and\negress traffic in both VPC and subnets. Route tables control traffic flow based on destination IPs while NACLs\nact as a firewall for associated subnets. Security groups add another layer of defense by acting as a virtual\nfirewall for individual instances in a subnet.\nConfiguring security groups and Network ACLs allows for granular control over the traffic permitted between\nthe on-premises network and the VPC. This addresses the requirement for restricting unrestricted access.\nThey can be set to allow only specific ports and protocols necessary for communication, minimizing the attack\nsurface.\nWhy Other Options Are Less Suitable:\nA (Direct Connect): Direct Connect provides a dedicated network connection, but it doesn't inherently provide\nencryption. Encryption must be added via other methods. Furthermore, while route tables handle basic\nrouting, they don't provide the granular access control of security groups and network ACLs.\nB (IAM Policies): IAM policies control access to AWS resources based on identity. This approach secures the\nAWS Management Console but does not address the connectivity or traffic encryption requirements for on-\npremises resources communicating directly with AWS resources in the VPC.\nD (Transit Gateway): AWS Transit Gateway acts as a network transit hub. It provides a central point of\nconnectivity for multiple VPCs and on-premises networks. While it can be used for connectivity, it does not\nautomatically provide network layer encryption for all communication with an on-premise network. The other\noptions such as route tables, NACLs and Security groups are suitable. However, the primary issue is that TGW\nis more complex and usually introduced when multiple VPCs and networks need to be connected. It may be\nmore costly and complex than a simple site-to-site VPN for a single connection.\nAuthoritative Links:\nAWS Site-to-Site VPN: https://aws.amazon.com/vpn/\nSecurity Groups: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\nNetwork ACLs: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html",
    "links": [
      "https://aws.amazon.com/vpn/",
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has a custom application with embedded credentials that retrieves information from a database in an\nAmazon RDS for MySQL DB cluster. The company needs to make the application more secure with minimal\nprogramming effort. The company has created credentials on the RDS for MySQL database for the application\nuser.\nWhich solution will meet these requirements?",
    "options": {
      "A": "AWS KMS: AWS KMS is primarily for encryption key management, not secrets management. While you",
      "B": "Encrypted local storage: Storing credentials in encrypted local storage is better than plain text, but it is",
      "D": "Systems Manager Parameter Store: While Parameter Store can store secrets, it lacks the purpose-built"
    },
    "answer": "C",
    "explanation": "The correct answer is C because it leverages AWS Secrets Manager, which is specifically designed for\nmanaging database credentials and other secrets securely. Secrets Manager provides automatic rotation\ncapabilities, reducing the operational overhead and minimizing the risk of using stale or compromised\ncredentials.\nHere's a detailed justification:\nAWS Secrets Manager is purpose-built for secrets management: It centralizes the storage and management\nof secrets like database credentials, API keys, and OAuth tokens. This provides a secure and auditable\nsolution compared to storing secrets in code or local storage.\nAutomatic Rotation: Secrets Manager enables automatic rotation of database credentials. This is a crucial\nsecurity best practice because it significantly reduces the window of opportunity for attackers to exploit\ncompromised credentials. With automatic rotation, new credentials are automatically generated and updated\nin both the application and the database.\nLambda Integration: Secrets Manager seamlessly integrates with AWS Lambda to handle the actual\ncredential rotation process. Lambda functions can connect to the database, create new credentials, and\nupdate the secret in Secrets Manager.\nMinimal Programming Effort: Using Secrets Manager reduces the programming effort required. The\napplication only needs to retrieve the credentials from Secrets Manager. The complexities of key\nmanagement and rotation are handled by the service and the Lambda function.\nSecurity Best Practices: By storing credentials in a dedicated secrets management service, you adhere to\nsecurity best practices by ensuring the application does not store them in plain text in config files or code.\nSecrets are encrypted in transit and at rest.\nWhy other options are incorrect:\nA. AWS KMS: AWS KMS is primarily for encryption key management, not secrets management. While you\ncould encrypt the credentials, KMS doesn't offer built-in rotation capabilities for database credentials,\nrequiring significant custom development.\nB. Encrypted local storage: Storing credentials in encrypted local storage is better than plain text, but it is\nnot a secure solution for enterprise applications. It is not centralized, difficult to manage, and lacks built-in\nrotation capabilities. Cron job based rotation is also prone to errors and difficult to manage at scale.\nD. Systems Manager Parameter Store: While Parameter Store can store secrets, it lacks the purpose-built\nfeatures of Secrets Manager, particularly automatic rotation, and database integration making it a more\ncomplex solution.\nIn conclusion, AWS Secrets Manager offers the best solution for secure, automated, and easily managed\ndatabase credentials, aligning perfectly with the requirements of minimal programming effort and improved\nsecurity.\nSupporting links:\nAWS Secrets Manager\nRotating AWS Secrets Manager secrets",
    "links": []
  },
  {
    "question": "CertyIQ\nA company wants to move its application to a serverless solution. The serverless solution needs to analyze existing\ndata and new data by using SQL. The company stores the data in an Amazon S3 bucket. The data must be\nencrypted at rest and replicated to a different AWS Region.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A is the best solution, focusing on meeting requirements with the\nleast operational overhead:\nThe problem requires a serverless solution for analyzing data stored in S3 using SQL, with encryption at rest\nand cross-region replication. Let's analyze each aspect:\nServerless SQL Querying: Amazon Athena is a serverless query service that allows you to analyze data in S3\nusing standard SQL. This perfectly aligns with the serverless requirement and the need for SQL querying.\nAmazon RDS, while supporting SQL, necessitates managing database instances, contradicting the serverless\nprinciple and adding operational overhead.\nEncryption at Rest: SSE-KMS (Server-Side Encryption with KMS managed keys) and SSE-S3 (Server-Side\nEncryption with S3 managed keys) both provide encryption at rest. However, SSE-KMS with multi-Region\nkeys provides added flexibility in key management, enabling simpler key rotation across regions and stronger\ncompliance posture.\nCross-Region Replication (CRR): CRR automatically replicates objects from one S3 bucket to another in a\ndifferent AWS Region. This fulfills the cross-region replication requirement for data durability and disaster\nrecovery.\nLeast Operational Overhead: Option A offers the least operational overhead because it leverages Athena's\nserverless nature, eliminating the need to manage a database server like RDS. Utilizing SSE-KMS with multi-\nregion keys provides robust encryption with potentially simpler key management compared to custom\nsolutions. Option B, while providing replication and encryption with SSE-S3, opts for RDS instead of Athena.\nOption C, while attempting to use Athena, uses SSE-S3, which may be less flexible in key management\ncompared to KMS and might require more overhead if keys need to be centrally managed across multiple\nregions. Finally, Option D combines RDS's overhead with the potential complexity of SSE-KMS multi-region\nkeys, making it least efficient among the choices.\nTherefore, creating a new S3 bucket with SSE-KMS multi-Region keys, configuring CRR, loading the data, and\nquerying with Athena provides the most serverless, secure, and manageable solution.\nAuthoritative Links:\nAmazon Athena: https://aws.amazon.com/athena/\nAmazon S3 Encryption: https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html\nAmazon S3 Cross-Region Replication:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html\nAWS KMS multi-region keys: https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-\noverview.html",
    "links": [
      "https://aws.amazon.com/athena/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html",
      "https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-"
    ]
  },
  {
    "question": "CertyIQ\nA company has a web application that has thousands of users. The application uses 8-10 user-uploaded images to\ngenerate AI images. Users can download the generated AI images once every 6 hours. The company also has a\npremium user option that gives users the ability to download the generated AI images anytime.\nThe company uses the user-uploaded images to run AI model training twice a year. The company needs a storage\nsolution to store the images.\nWhich storage solution meets these requirements MOST cost-effectively?",
    "options": {},
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A is the most cost-effective solution for storing images,\nconsidering the usage patterns described, and why other options are less suitable:\nUploaded User Images (Long-Term Archive): The user-uploaded images are only used twice a year for AI\nmodel training. This indicates extremely infrequent access. Amazon S3 Glacier Deep Archive is designed for\nlong-term data archiving with the lowest storage cost. While retrieval is slower (hours), this aligns perfectly\nwith the twice-a-year access requirement, making it highly cost-effective for this data.\nhttps://aws.amazon.com/s3/storage-classes/glacier/\nGenerated AI Images (Premium Users): Premium users require immediate and frequent access to their\ngenerated AI images. S3 Standard provides high availability and performance, ideal for frequently accessed\ndata.\nhttps://aws.amazon.com/s3/storage-classes/\nGenerated AI Images (Non-Premium Users): Non-premium users can only download their images once every\n6 hours. S3 Standard-Infrequent Access (S3 Standard-IA) is suitable for data accessed less frequently but\nrequires rapid access when needed. It offers lower storage costs than S3 Standard, making it more cost-\neffective for this usage pattern.\nhttps://aws.amazon.com/s3/storage-classes/ia/\nWhy other options are less ideal:\nOption B: Moving all generated AI images (including premium) to Glacier Flexible Retrieval is not optimal.\nPremium users need immediate access, and Glacier Flexible Retrieval has retrieval times from minutes to\nhours, degrading their experience. Glacier Flexible Retrieval is a good alternative, but not if immediate access\nis needed.\nOption C & D: Storing uploaded images in S3 One Zone-IA introduces risk. It stores data in a single Availability\nZone. If that AZ becomes unavailable, the data is lost. While cheaper than Standard, it is not suitable for data\nthat needs to be archived as training data since it is an infrequent process. S3 Glacier Deep Archive is cheaper\nand suitable for such infrequent use cases.\nhttps://aws.amazon.com/s3/storage-classes/onezone-ia/\nIn summary, option A aligns storage class with access frequency, providing the best cost optimization without\nsacrificing the required performance for each category of images.",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/glacier/",
      "https://aws.amazon.com/s3/storage-classes/",
      "https://aws.amazon.com/s3/storage-classes/ia/",
      "https://aws.amazon.com/s3/storage-classes/onezone-ia/"
    ]
  },
  {
    "question": "CertyIQ\nA company is developing machine learning (ML) models on AWS. The company is developing the ML models as\nindependent microservices. The microservices fetch approximately 1 GB of model data from Amazon S3 at startup\nand load the data into memory. Users access the ML models through an asynchronous API. Users can send a\nrequest or a batch of requests.\nThe company provides the ML models to hundreds of users. The usage patterns for the models are irregular. Some\nmodels are not used for days or weeks. Other models receive batches of thousands of requests at a time.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D because it leverages asynchronous processing with SQS and containerization with\nECS to handle irregular traffic patterns and large model sizes efficiently. Here's a detailed breakdown:\nAsynchronous API and SQS: Using SQS decouples the API from the ML model processing. The API pushes\nrequests into the SQS queue, providing immediate acknowledgment to the user. This is crucial for handling\nbatches of requests and irregular usage patterns without overwhelming the ML model services. SQS acts as a\nbuffer, smoothing out traffic spikes. https://aws.amazon.com/sqs/\nECS and Containerization: Deploying the ML models as ECS services allows for containerization, which\nensures consistent environments and facilitates easy scaling. Each ECS service instance can load the 1 GB\nmodel data into memory upon startup, as required. https://aws.amazon.com/ecs/\nAuto Scaling based on Queue Size: Auto scaling the ECS cluster and the number of ECS services based on\nthe size of the SQS queue is key to cost optimization. As the queue grows, ECS automatically provisions more\nresources to process the messages, and as the queue shrinks, it scales down to reduce costs when models are\nidle. This addresses the irregular usage patterns effectively. ECS supports scaling based on custom metrics,\nincluding SQS queue length.\nWhy other options are not ideal:\nA (Lambda with NLB): Lambda functions have execution time limits and cold start issues which is not ideal for\nML models which load 1 GB of model data at startup. Also, NLB is designed for TCP traffic not HTTP which is a\nlimitation.\nB (ECS with ALB): While ECS is a viable option, using an ALB directly would not be ideal because the API\nwould need to wait for the ML processing to complete before responding, causing potential timeouts. ECS can\nhandle the model sizes.\nC (Lambda with SQS): Similar to option A, Lambda functions have execution time limits and cold start issues\nwhich is not ideal for ML models which load 1 GB of model data at startup. Lambda is better suited for smaller\nstateless tasks. Auto-scaling on vCPUs is not how Lambda functions are scaled; instead, they scale by the\nnumber of concurrent executions.\nIn conclusion, the combination of SQS for asynchronous message queuing and ECS for containerized ML\nmodel deployment with auto-scaling based on queue size provides the most scalable, cost-effective, and\nreliable solution for handling irregular usage patterns and large model data in this scenario.",
    "links": [
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/ecs/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs a web application on Amazon EC2 instances in an Auto Scaling group behind an Application Load\nBalancer (ALB). The application stores data in an Amazon Aurora MySQL DB cluster.\nThe company needs to create a disaster recovery (DR) solution. The acceptable recovery time for the DR solution is\nup to 30 minutes. The DR solution does not need to support customer usage when the primary infrastructure is\nhealthy.\nWhich solution will meet these requirements?",
    "options": {
      "B": "This provides a controlled failover mechanism.",
      "A": "Here's why:"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's why:\nRecovery Time Objective (RTO): The requirement is an RTO of up to 30 minutes.\nAurora Global Database: Aurora Global Database is designed for disaster recovery. It replicates data with\nminimal latency to a secondary region, enabling a very fast failover. This is crucial for meeting the 30-minute\nRTO. (https://aws.amazon.com/rds/aurora/global-database/)\nActive-Passive Failover: Configuring Route 53 for active-passive failover directs traffic to the primary\nregion's ALB under normal conditions. In a disaster, it automatically reroutes traffic to the DR region's ALB.\nThis provides a controlled failover mechanism.\n(https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html)\nDR Infrastructure on Standby: Deploying a DR infrastructure (ALB, Auto Scaling group, EC2 instances) in a\nsecond region ensures that the application can be quickly brought online in case of a primary region failure.\nSetting the desired capacity of the Auto Scaling group to a minimum value (e.g., zero or a small number of\ninstances) minimizes costs during normal operation while allowing for rapid scaling upon failover.\nOther options analysis:\nB: Active-active Route 53 failover is not suitable in this scenario since the DR solution is only needed in a\nfailover scenario, so no traffic should be sent to the DR region in normal situations.\nC: Restoring the data from backups could violate the 30 minute recovery time objective.\nD: Restoring the infrastructure from backups could violate the 30 minute recovery time objective.",
    "links": [
      "https://aws.amazon.com/rds/aurora/global-database/)",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html)"
    ]
  },
  {
    "question": "CertyIQ\nA company is migrating its data processing application to the AWS Cloud. The application processes several short-\nlived batch jobs that cannot be disrupted. Data is generated after each batch job is completed. The data is\naccessed for 30 days and retained for 2 years.\nThe company wants to keep the cost of running the application in the AWS Cloud as low as possible.\nWhich solution will meet these requirements?",
    "options": {
      "D": "Here's why:"
    },
    "answer": "D",
    "explanation": "The correct solution is D. Here's why:\nEC2 Instance Type: The problem states that batch jobs \"cannot be disrupted.\" Spot Instances can be\nterminated with little warning, which is not suitable for jobs that must complete without interruption. On-\nDemand Instances are a better fit because they provide a guaranteed compute capacity.\nhttps://aws.amazon.com/ec2/pricing/on-demand/\nInitial Storage: Storing the data in Amazon S3 Standard for the first 30 days is optimal because it offers low\nlatency access, which is needed given that data is accessed within this period.\nhttps://aws.amazon.com/s3/storage-classes/\nLong-Term Archival: After 30 days, the data is no longer actively accessed. Amazon S3 Glacier Deep Archive\nis the most cost-effective storage class for data that is infrequently accessed but must be retained for long\nperiods. https://aws.amazon.com/s3/storage-classes/\nData Retention: S3 Lifecycle policies, using expiration, automatically delete data after a specified period (in\nthis case, 2 years). This reduces storage costs and ensures compliance with retention policies.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-examples.html\nWhy other options are not suitable:\nOption A and C (Spot Instances): Not suitable because the workload cannot be interrupted.\nOption A and B (S3 Glacier Instant Retrieval): Glacier Instant Retrieval is more expensive than Glacier Deep\nArchive. Given the data is accessed less frequently after 30 days, using Deep Archive will be more cost-\neffective.\nOption C (S3 Glacier Flexible Retrieval): Glacier Flexible Retrieval is less cost-effective than Glacier Deep\nArchive when infrequent access is acceptable.\nIn summary, Option D balances cost efficiency with the application's requirements for uninterrupted batch job\nprocessing, data accessibility, and long-term data retention.",
    "links": [
      "https://aws.amazon.com/ec2/pricing/on-demand/",
      "https://aws.amazon.com/s3/storage-classes/",
      "https://aws.amazon.com/s3/storage-classes/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-examples.html"
    ]
  },
  {
    "question": "CertyIQ\nA company needs to design a hybrid network architecture. The company's workloads are currently stored in the\nAWS Cloud and in on-premises data centers. The workloads require single-digit latencies to communicate. The\ncompany uses an AWS Transit Gateway transit gateway to connect multiple VPCs.\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": {
      "C": "Establish an AWS Site-to-Site VPN connection to an AWS Direct Connect gateway: This option doesn't",
      "D": "Here's a detailed justification:",
      "B": "Associate an AWS Direct Connect gateway with the transit gateway that is attached to the VPCs.",
      "A": "Establish an AWS Site-to-Site VPN connection to each VPC: While Site-to-Site VPN can provide a"
    },
    "answer": "B",
    "explanation": "The correct answer is BD. Here's a detailed justification:\nB. Associate an AWS Direct Connect gateway with the transit gateway that is attached to the VPCs.\nDirect Connect for Low Latency: AWS Direct Connect establishes a dedicated network connection from your\non-premises environment to AWS. This bypasses the public internet, providing more consistent network\nperformance and lower latency, which is critical for the single-digit millisecond latency requirement.\n(https://aws.amazon.com/directconnect/)\nTransit Gateway Integration: A Direct Connect gateway can be associated with a Transit Gateway. Transit\nGateway acts as a central hub, enabling connectivity between multiple VPCs. By associating the Direct\nConnect gateway with the Transit Gateway, on-premises workloads can communicate with all VPCs\nconnected to the Transit Gateway over the dedicated Direct Connect link, ensuring low-latency\ncommunication between on-premises and cloud resources. This is more efficient than managing individual\nconnections to each VPC.\nD. Establish an AWS Direct Connect connection. Create a transit virtual interface (VIF) to a Direct Connect\ngateway.\nTransit VIF for Transit Gateway Connectivity: To connect your Direct Connect connection to the Transit\nGateway, you need to create a transit virtual interface (VIF). A transit VIF allows you to reach the Direct\nConnect gateway and, through it, the associated Transit Gateway.\n(https://docs.aws.amazon.com/directconnect/latest/UserGuide/multi-account-tgw.html)\nEnd-to-End Dedicated Path: Establishing the Direct Connect connection and creating the transit VIF creates a\ndedicated, low-latency path from your on-premises environment, through the Direct Connect connection, to\nthe Direct Connect gateway, then through the Transit Gateway, and finally to the VPCs.\nWhy other options are less suitable or incorrect:\nA. Establish an AWS Site-to-Site VPN connection to each VPC: While Site-to-Site VPN can provide a\nconnection to AWS, it uses the public internet, which is not suitable for single-digit millisecond latency\nrequirements. The internet introduces variability and is not a dedicated connection. Furthermore, establishing\nmultiple VPN connections becomes cumbersome and costly to manage for multiple VPCs.\nC. Establish an AWS Site-to-Site VPN connection to an AWS Direct Connect gateway: This option doesn't\nmake sense. Direct Connect already provides a dedicated connection. Adding a VPN on top of it is redundant\nand defeats the purpose of using Direct Connect for low latency. It would also add unnecessary complexity\nand cost.\nE. Associate AWS Site-to-Site VPN connections with the transit gateway that is attached to the VPCs:\nSimilar to option A, using VPN connections will not achieve single-digit millisecond latency due to its reliance\non the public internet. While Transit Gateway can route VPN traffic, the inherent limitations of VPNs for\nlatency remain.\nTherefore, establishing a Direct Connect connection with a transit VIF to a Direct Connect gateway, and\nassociating that gateway with the Transit Gateway provides the dedicated, low-latency connectivity required,\nand the transit gateway facilitates the connectivity with multiple VPCs, which meets the requirements of the\nscenario most cost-effectively.",
    "links": [
      "https://aws.amazon.com/directconnect/)",
      "https://docs.aws.amazon.com/directconnect/latest/UserGuide/multi-account-tgw.html)"
    ]
  },
  {
    "question": "CertyIQ\nA global ecommerce company runs its critical workloads on AWS. The workloads use an Amazon RDS for\nPostgreSQL DB instance that is configured for a Multi-AZ deployment.\nCustomers have reported application timeouts when the company undergoes database failovers. The company\nneeds a resilient solution to reduce failover time.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A: Create an Amazon RDS Proxy and assign the proxy to the DB instance.\nJustification:\nThe primary problem is application timeouts during RDS failovers. RDS Multi-AZ deployments provide high\navailability by automatically failing over to a standby instance in case of an issue. However, the failover\nprocess, even though automatic, takes time. During this failover, the application loses its database\nconnection, leading to timeouts.\nRDS Proxy is a fully managed, highly available database proxy that sits between your application and your\nRDS database. Its key feature in this scenario is connection management and failover handling. RDS Proxy\nmaintains a pool of database connections and automatically reconnects to the new primary instance after a\nfailover. It masks the failover event from the application, thus drastically reducing or eliminating application\ntimeouts. The application simply continues using the same connection string via the proxy, and the proxy\nhandles the underlying switchover.\nHere's why other options are not as suitable:\nB: Create a read replica for the DB instance. Move the read traffic to the read replica. Read replicas are\nhelpful for offloading read traffic from the primary instance, improving performance. However, they do not\naddress the connection timeout issue during the failover of the primary instance. Read replicas also need time\nto be promoted to primary after the primary instance fails.\nC: Enable Performance Insights. Monitor the CPU load to identify the timeouts. Performance Insights helps\ndiagnose performance bottlenecks but doesn't prevent or mitigate failover-related timeouts. It can help\nunderstand why performance is degraded, but it doesn't solve the core availability problem.\nD: Take regular automatic snapshots. Copy the automatic snapshots to multiple AWS Regions. Snapshots\nare useful for disaster recovery and point-in-time recovery but do not improve failover time. Restoring from a\nsnapshot is a time-consuming process and unsuitable for minimizing application timeouts during failover.\nAuthoritative Links:\nAmazon RDS Proxy: Provides a complete overview of RDS Proxy features and benefits.\nUsing Amazon RDS Multi-AZ deployments for high availability: Explains how Multi-AZ deployments work and\ntheir limitations regarding failover time.",
    "links": []
  },
  {
    "question": "CertyIQ\nA company has multiple Amazon RDS DB instances that run in a development AWS account. All the instances have\ntags to identify them as development resources. The company needs the development DB instances to run on a\nschedule only during business hours.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "C": "Create AWS Systems Manager State Manager associations to start and stop the"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Create AWS Systems Manager State Manager associations to start and stop the\nRDS instances.\nHere's why:\nAWS Systems Manager (SSM) State Manager allows you to automate tasks on a schedule using SSM\ndocuments. In this case, you can use SSM documents to start and stop RDS instances based on tags. This\nprovides a centralized and auditable way to manage the scheduling. State Manager operates directly within\nAWS and integrates seamlessly with RDS, removing the need for custom code.SSM offers built-in scheduling\ncapabilities and provides visibility into the execution status of the automation. This includes monitoring of\nRDS resources.State Manager is designed for infrastructure management and automation, making it a natural\nfit for this type of task, and requires minimal operational overhead. Using tags for RDS will also create an easy\nfilter to only include resources that need to be turned off.\nWhy other options are less suitable:\nA (CloudWatch alarm & Lambda): This is feasible, but more complex. You'd need to create alarms to identify\ninstances and then use Lambda to start/stop them. SSM provides a cleaner, simpler way without requiring\ncoding and creating alarms.\nB (Trusted Advisor & Lambda): Trusted Advisor focuses on cost optimization, security, and performance best\npractices, not scheduling resources. It wouldn't directly support starting and stopping RDS instances based\non a schedule.\nD (EventBridge & Lambda): While EventBridge can schedule events, it's more suitable for triggering actions\nbased on events. Using it to start/stop RDS instances is overkill compared to SSM's built-in scheduling\ncapabilities.\nIn summary:\nSSM State Manager directly addresses the requirement with a straightforward, centralized, and less\noperationally intensive solution. It allows you to define schedules for starting and stopping RDS instances\nbased on tags, minimizing manual intervention and custom coding.\nAuthoritative Links:\nAWS Systems Manager State Manager: https://docs.aws.amazon.com/systems-\nmanager/latest/userguide/systems-manager-state.html\nAWS Systems Manager Automation: https://docs.aws.amazon.com/systems-\nmanager/latest/userguide/automation-workflows.html",
    "links": [
      "https://docs.aws.amazon.com/systems-",
      "https://docs.aws.amazon.com/systems-"
    ]
  },
  {
    "question": "CertyIQ\nA consumer survey company has gathered data for several years from a specific geographic region. The company\nstores this data in an Amazon S3 bucket in an AWS Region.\nThe company has started to share this data with a marketing firm in a new geographic region. The company has\ngranted the firm's AWS account access to the S3 bucket. The company wants to minimize the data transfer costs\nwhen the marketing firm requests data from the S3 bucket.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Configure the Requester Pays feature on the companys S3 bucket."
    },
    "answer": "A",
    "explanation": "The correct answer is A. Configure the Requester Pays feature on the companys S3 bucket.\nHere's a detailed justification:\nThe primary goal is to minimize data transfer costs when the marketing firm (the requester) accesses data\nfrom the company's S3 bucket. The key is that the requester is incurring data transfer costs to their region.\nOption A (Requester Pays): This configuration makes the requester responsible for the data transfer costs.\nThis is directly aligned with minimizing the company's costs. When the marketing firm accesses the data, they\nwill pay for the data egress from the company's bucket to their region. This fulfills the requirement of\nminimizing the company's data transfer costs.\nOption B (CRR): Cross-Region Replication (CRR) replicates data from one S3 bucket to another in a different\nAWS Region. While it places the data closer to the marketing firm, the company initially pays for the\nreplication costs. Furthermore, the question prioritizes minimizing data transfer costs specifically incurred\nwhen the marketing firm requests data. CRR would incur costs upfront regardless of actual data access.\nOption C (AWS RAM): AWS Resource Access Manager (RAM) allows you to share AWS resources across AWS\naccounts within your organization or organizational units. Sharing the bucket through RAM doesn't inherently\nminimize data transfer costs. The company would still be responsible for the egress costs when the marketing\nfirm retrieves data.\nOption D (S3 Intelligent-Tiering and Sync): S3 Intelligent-Tiering automatically moves data between access\ntiers (frequent, infrequent, archive) based on access patterns. This doesn't address the data transfer costs\nassociated with the marketing firm downloading the data. Syncing the bucket to one of the marketing firms\nbuckets isn't a native S3 feature and would require a custom solution, and further, it wouldn't address the\ncost issue directly compared to Requester Pays.\nTherefore, configuring Requester Pays ensures the marketing firm, as the requester, bears the cost of\ntransferring data out of the company's S3 bucket, effectively minimizing the company's costs.\nAuthoritative Links:\nAmazon S3 Requester Pays: https://docs.aws.amazon.com/AmazonS3/latest/userguide/requester-pays.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/requester-pays.html"
    ]
  },
  {
    "question": "CertyIQ\nA company uses AWS to host its public ecommerce website. The website uses an AWS Global Accelerator\naccelerator for traffic from the internet. The Global Accelerator accelerator forwards the traffic to an Application\nLoad Balancer (ALB) that is the entry point for an Auto Scaling group.\nThe company recently identified a DDoS attack on the website. The company needs a solution to mitigate future\nattacks.\nWhich solution will meet these requirements with the LEAST implementation effort?",
    "options": {
      "A": "Configure an AWS WAF web ACL for the Global Accelerator accelerator to block"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Configure an AWS WAF web ACL for the Global Accelerator accelerator to block\ntraffic by using rate-based rules.\nHere's a detailed justification:\n1. DDoS Mitigation at the Edge: The goal is to mitigate DDoS attacks. AWS recommends handling\nthese attacks as close to the source as possible. Global Accelerator sits at the edge of the AWS\nnetwork.\n2. WAF Integration with Global Accelerator: AWS WAF integrates directly with Global Accelerator.\nThis enables you to create rules to filter malicious traffic before it even reaches your ALB and Auto\nScaling group.\n3. Rate-Based Rules: Rate-based rules within AWS WAF are specifically designed to mitigate DDoS\nattacks. They monitor the rate of requests from each IP address and block those exceeding a defined\nthreshold. This provides protection against volumetric attacks.\n4. Least Implementation Effort: Implementing WAF at the Global Accelerator level requires attaching a\nWAF web ACL to the accelerator. Configuring rate-based rules within the WAF web ACL is relatively\nstraightforward and requires less configuration than other options.\n5. ALB WAF (Option C): While you can use WAF with an ALB, it's less effective for DDoS mitigation. The\ntraffic has already traversed the internet and reached the ALB, consuming resources. Also, Global\nAccelerator provides static IP addresses which makes it easier to track source IPs for rate limiting.\n6. Lambda and VPC Network ACLs (Option B): This option involves a more complex and potentially less\nresponsive implementation. Using Lambda to analyze ALB metrics and update VPC network ACLs\nwould require custom coding, monitoring, and more operational overhead. It's also slower to react to\nattacks compared to WAF rate-based rules. VPC Network ACLs operate at the subnet level and are\nnot as granular or flexible as WAF rules.\n7. CloudFront in Front of Global Accelerator (Option D): This option introduces unnecessary\ncomplexity. Global Accelerator already provides benefits similar to a CDN, such as improved\nperformance through static IP addresses and traffic steering. Adding CloudFront would increase\ncosts and management overhead without providing a significant improvement in DDoS mitigation\ncapabilities that WAF on Global Accelerator wouldn't offer.\nIn summary, configuring an AWS WAF web ACL on the Global Accelerator accelerator with rate-based rules\nprovides an effective and efficient solution for mitigating DDoS attacks by filtering malicious traffic at the\nedge of the AWS network with the least amount of operational burden.\nAuthoritative Links:\nAWS WAF: https://aws.amazon.com/waf/\nAWS Global Accelerator: https://aws.amazon.com/global-accelerator/\nAWS WAF Rate-Based Rules: https://docs.aws.amazon.com/waf/latest/developerguide/waf-rate-based-\nrule.html\nAWS Best Practices for DDoS Mitigation: https://d1.awsstatic.com/whitepapers/DDoS_White_Paper.pdf",
    "links": [
      "https://aws.amazon.com/waf/",
      "https://aws.amazon.com/global-accelerator/",
      "https://docs.aws.amazon.com/waf/latest/developerguide/waf-rate-based-",
      "https://d1.awsstatic.com/whitepapers/DDoS_White_Paper.pdf"
    ]
  },
  {
    "question": "CertyIQ\nA company uses an Amazon DynamoDB table to store data that the company receives from devices. The\nDynamoDB table supports a customer-facing website to display recent activity on customer devices. The company\nconfigured the table with provisioned throughput for writes and reads.\nThe company wants to calculate performance metrics for customer device data on a daily basis. The solution must\nhave minimal effect on the table's provisioned read and write capacity.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The best solution is B: Use an AWS Glue job with the AWS Glue DynamoDB export connector to calculate\nperformance metrics on a recurring schedule. Here's why:\nMinimizing Impact on DynamoDB: The core requirement is to minimize impact on the provisioned read and\nwrite capacity of the DynamoDB table, which serves a customer-facing website. Running queries directly\nagainst the DynamoDB table, as Athena (A) and EMR (D) would do, could consume significant read capacity,\npotentially impacting the website's performance. Similarly, Redshift (C) using the COPY command would\nrequire reading data directly from the DynamoDB table, again consuming read capacity.\nAWS Glue and DynamoDB Export Connector: The AWS Glue DynamoDB export connector is designed\nspecifically for efficiently extracting data from DynamoDB tables without significantly impacting their\nprovisioned throughput. It utilizes DynamoDB's consistent read capabilities in a throttled manner or, ideally,\nleverages DynamoDB backups (snapshots) for data extraction. This ensures the production table remains\nresponsive for customer requests.\nAWS Glue allows you to schedule recurring jobs, making it suitable for calculating daily performance metrics.\nAWS Glue provides powerful data transformation capabilities using Spark or Python, enabling you to easily\ncalculate the required performance metrics.\nAthena Limitations: While Athena can query DynamoDB, it directly consumes read capacity, making it less\ndesirable when minimizing impact is a priority.\nRedshift Limitations: Redshift COPY command reads directly from the DynamoDB table. This directly impacts\nthe read capacity. Moreover, Redshift is not designed for real-time or near-real-time analytics directly on\nDynamoDB data.\nEMR Limitations: EMR using Apache Hive could query DynamoDB, but again, this puts a load on the\nDynamoDB table. Also, setting up and managing an EMR cluster for this specific task is more complex than\nusing AWS Glue, especially for recurring scheduled jobs.\nCost Considerations: AWS Glue is generally more cost-effective for this purpose than Redshift or EMR\nbecause it is a serverless, pay-as-you-go service. You only pay for the time the Glue job runs.\nAuthoritative Links:\nAWS Glue: https://aws.amazon.com/glue/\nAWS Glue DynamoDB Connector: https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-\nconnect.html\nDynamoDB on-demand backup and restore:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html",
    "links": [
      "https://aws.amazon.com/glue/",
      "https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is designing the cloud architecture for a new stateless application that will be deployed on\nAWS. The solutions architect created an Amazon Machine Image (AMI) and launch template for the application.\nBased on the number of jobs that need to be processed, the processing must run in parallel while adding and\nremoving application Amazon EC2 instances as needed. The application must be loosely coupled. The job items\nmust be durably stored.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "C",
    "explanation": "The requirement of durable storage of job items and loose coupling between application components points\ntowards a queuing system. Amazon SQS (Simple Queue Service) is designed for this purpose, providing\nreliable message queuing. SNS (Simple Notification Service), on the other hand, is for pub/sub messaging, not\ndurable storage of individual job items. Therefore, options A and D, which propose using SNS, are incorrect.\nAuto Scaling groups are suitable for dynamically scaling EC2 instances based on demand. Scaling based on\nthe number of items in the SQS queue directly reflects the job backlog and the need for processing power.\nThis allows the Auto Scaling group to add instances when the queue is growing and remove instances when\nthe queue is shrinking, ensuring efficient resource utilization and responsiveness to varying job loads. Scaling\nbased on CPU or network usage, as suggested in option B, may not accurately reflect the volume of jobs\nneeding processing, potentially leading to under- or over-provisioning of EC2 instances.\nTherefore, option C, using SQS for queuing jobs and scaling the Auto Scaling group based on the number of\nitems in the queue, correctly addresses the requirements for durable storage, loose coupling, and dynamic\nscaling based on job volume. This approach ensures that all jobs are reliably stored until processed and that\nthe application scales appropriately to handle the workload.\nHere are some links for further research:\nAmazon SQS: https://aws.amazon.com/sqs/\nAmazon SNS: https://aws.amazon.com/sns/\nAuto Scaling groups: https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html\nLaunch Templates: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-templates.html",
    "links": [
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/sns/",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-templates.html"
    ]
  },
  {
    "question": "CertyIQ\nA global ecommerce company uses a monolithic architecture. The company needs a solution to manage the\nincreasing volume of product data. The solution must be scalable and have a modular service architecture. The\ncompany needs to maintain its structured database schemas. The company also needs a storage solution to store\nproduct data and product images.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D because it offers the most scalable, modular, and operationally efficient solution for\nthe given requirements.\nWhy D is the Best Choice:\nScalable and Modular Architecture: Amazon ECS with Fargate allows the company to break down the\nmonolithic application into microservices using containers, achieving a modular service architecture. Fargate\neliminates the operational overhead of managing EC2 instances for the containers, as AWS handles the\nunderlying infrastructure. https://aws.amazon.com/ecs/fargate/\nStructured Database Schemas: Amazon RDS is suitable for maintaining structured database schemas as the\ncompany requested. Using a Multi-AZ deployment in RDS ensures high availability and disaster recovery for\nthe product data, increasing the system's resilience. https://aws.amazon.com/rds/features/multi-az/\nProduct Images Storage: Amazon S3 is the ideal choice for storing product images because it provides\nscalable, durable, and cost-effective object storage. S3 allows for easy retrieval and integration with the\napplication. https://aws.amazon.com/s3/\nLeast Operational Overhead: Fargate for container management and S3 for object storage minimize\noperational overhead compared to managing EC2 instances or complex orchestration tools like EKS. RDS\nmanaged services reduce operational overhead.\nWhy other options are less suitable:\nA: While EC2 with Auto Scaling and RDS are good, it requires more operational overhead in managing EC2\ninstances.\nB: DynamoDB is a NoSQL database and does not align with the requirement to maintain structured database\nschemas. Also, migrating an existing monolithic application to Lambda functions and event-driven\narchitecture might be an extensive effort.\nC: Amazon EKS is more complex to manage than ECS with Fargate and introduces higher operational\noverhead. Glacier Deep Archive is more suitable for long-term archival data and less suitable for product\nimages that may need frequent access. Step Functions introduces complexity that isn't necessary for this\nscenario.\nTherefore, option D offers the best balance of scalability, modularity, and minimal operational overhead while\nfulfilling the requirements of maintaining structured schemas and storing product images.",
    "links": [
      "https://aws.amazon.com/ecs/fargate/",
      "https://aws.amazon.com/rds/features/multi-az/",
      "https://aws.amazon.com/s3/"
    ]
  },
  {
    "question": "CertyIQ\nA company is migrating an application from an on-premises environment to AWS. The application will store\nsensitive data in Amazon S3. The company must encrypt the data before storing the data in Amazon S3.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A: Encrypt the data by using client-side encryption with customer-managed keys.\nHere's a detailed justification:\nThe requirement is to encrypt sensitive data before it's stored in Amazon S3. This indicates the need for\nclient-side encryption. Client-side encryption involves encrypting the data on the client-side (i.e., before\nuploading it to S3). This provides end-to-end encryption, ensuring data confidentiality even during transit and\nat rest in S3.\nOption A utilizes client-side encryption, which fulfills the core requirement of pre-storage encryption.\nImportantly, it specifies using customer-managed keys. This means the customer has full control over the\nencryption keys, managing their lifecycle and access policies through AWS Key Management Service (KMS).\nThis addresses security best practices and compliance needs often associated with sensitive data. Customer-\nmanaged keys offer greater control and auditability compared to S3-managed keys.\nOption B, Server-Side Encryption with AWS KMS keys (SSE-KMS), encrypts the data after it's received by S3.\nWhile secure, it doesn't meet the specific requirement of encrypting the data before storing it. S3 handles the\nencryption process upon receiving the unencrypted data.\nOption C, Server-Side Encryption with Customer-Provided Keys (SSE-C), also encrypts data on the server side\n(after receipt by S3). While the customer provides the key, they are responsible for managing the key's\nlifecycle and securely providing it to S3 for each request. This adds operational complexity without\nnecessarily enhancing security compared to KMS.\nOption D, Client-Side Encryption with Amazon S3 Managed Keys, is incorrect because S3-managed keys\ngrant Amazon more control over the key lifecycle. The company needs more control given the sensitivity of\nthe data as stated in the problem.\nTherefore, client-side encryption with customer-managed keys is the most suitable option to meet the\nspecific requirement of pre-storage encryption with maximum customer control over the encryption keys. This\napproach aligns with security best practices, compliance requirements, and ensures end-to-end data\nprotection.\nSupporting documentation:\nAWS KMS: https://aws.amazon.com/kms/\nProtecting Data Using Server-Side Encryption:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html\nProtecting Data Using Client-Side Encryption:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html",
    "links": [
      "https://aws.amazon.com/kms/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html"
    ]
  },
  {
    "question": "CertyIQ\nA company wants to create an Amazon EMR cluster that multiple teams will use. The company wants to ensure\nthat each teams big data workloads can access only the AWS services that each team needs to interact with. The\ncompany does not want the workloads to have access to Instance Metadata Service Version 2 (IMDSv2) on the\nclusters underlying EC2 instances.\nWhich solution will meet these requirements?",
    "options": {
      "B": "Create EMR runtime roles. Configure the cluster to use the runtime roles. Use the"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Create EMR runtime roles. Configure the cluster to use the runtime roles. Use the\nruntime roles to submit the big data workloads.\nHere's why:\nEMR Runtime Roles: EMR runtime roles, introduced with EMR 6.1.0 and later, provide a fine-grained access\ncontrol mechanism for big data applications running on EMR clusters. They allow you to assign different IAM\nroles to different applications or jobs within the same cluster. This addresses the requirement of allowing\neach team's workloads to access only the AWS services they need.\nGranular Permissions: Runtime roles enable granular control over permissions at the application level,\nmoving away from the broad permissions granted by the EC2 instance profile. This is crucial for implementing\nthe principle of least privilege.\nIMDSv2 Control: Runtime roles inherently bypass the need to directly interact with the EC2 instance\nmetadata. This is because applications assume the runtime role credentials which are separate from the\ninstance profile, thus effectively addressing the requirement of not using IMDSv2 for workloads.\nVPC Endpoints (Option A - Incorrect): While interface VPC endpoints enhance security by keeping traffic\nwithin the AWS network, they don't address the requirement of assigning different permissions to different\nteams' workloads. All workloads using the VPC endpoints would still operate under the permissions\nassociated with the instance profile or some other global mechanism.\nEC2 Instance Profile (Option C - Incorrect): Using EC2 instance profiles would grant the same set of\npermissions to all workloads running on the cluster's EC2 instances. This contradicts the requirement that\neach team's workloads should access only the AWS services they need.\nEMR Security Configuration (Option D - Incorrect): The EnableApplicationScopedIAMRole option, typically\nused in older EMR versions, is not the ideal way to manage permissions. It would provide less granular control\nthan runtime roles. Furthermore, setting it to 'false' would revert to using the EC2 instance profile, which is\nagainst the requirement.\nIn summary, EMR runtime roles offer the most suitable solution because they allow for granular, application-\nlevel permission management, fulfilling the requirement of controlling AWS service access for each team's\nworkloads and avoiding direct use of IMDSv2.\nReferences:\nUse IAM roles for EMR steps\nEMR Runtime Roles",
    "links": []
  },
  {
    "question": "CertyIQ\nA solutions architect is designing an application that helps users fill out and submit registration forms. The\nsolutions architect plans to use a two-tier architecture that includes a web application server tier and a worker tier.\nThe application needs to process submitted forms quickly. The application needs to process each form exactly\nonce. The solution must ensure that no data is lost.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "A",
    "explanation": "The correct answer is A, using an Amazon SQS FIFO queue. Let's dissect why:\nThe core requirement is to process each form exactly once and ensure no data loss, guaranteeing data\nintegrity in the application's workflow. This necessitates a queuing system with strict ordering and delivery\nguarantees.\nAmazon SQS FIFO (First-In, First-Out) queues directly address these requirements. FIFO queues maintain the\norder in which messages are sent and received, ensuring that forms are processed in the order they were\nsubmitted. More importantly, they provide exactly-once processing semantics when used with message\ndeduplication, meaning each form will be processed only one time even in the event of failures or retries.\nStandard SQS queues (option C) offer best-effort ordering, which doesn't guarantee the precise order of form\nprocessing and allows for duplicate processing, making them unsuitable.\nAmazon API Gateway (option B) is primarily designed for handling API requests and routing traffic. While it\ncan interact with backend services, it doesn't inherently offer the queuing, ordering, and exactly-once\nprocessing capabilities required for this scenario. API Gateway is a request/response service, not a durable\nqueue.\nAWS Step Functions (option D) manages workflows, but a synchronous workflow doesn't inherently provide\nthe queuing and exactly-once processing guarantees required. A synchronous Step Functions workflow\ndirectly invokes the worker, and if the worker fails midway through processing, the workflow will need to be\nmanually retried or include complex error handling logic. While Step Functions can integrate with SQS, using\nSQS FIFO directly simplifies the solution for this specific requirement of ordered and guaranteed delivery. The\nproblem specifically asks to process forms \"quickly.\" Introducing Step Functions adds orchestration overhead\nto the overall process.\nTherefore, Amazon SQS FIFO is the most appropriate solution because it guarantees ordered delivery and\nexactly-once processing of form data between the web application server tier and the worker tier, fulfilling\nthe application's requirements for data integrity and processing accuracy. It eliminates the possibility of\nduplicate form processing and ensures no data loss.\nSupporting Documentation:\nAmazon SQS FIFO queues:\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\nAmazon SQS Message Deduplication:\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queue-\nrecommendations.html\nAWS Step Functions: https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html\nAmazon API Gateway: https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html",
    "links": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queue-",
      "https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html"
    ]
  },
  {
    "question": "CertyIQ\nA finance company uses an on-premises search application to collect streaming data from various producers. The\napplication provides real-time updates to search and visualization features.\nThe company is planning to migrate to AWS and wants to use an AWS native solution.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D because it leverages AWS services specifically designed for real-time streaming data\ningestion, processing, search, and visualization, aligning perfectly with the finance company's requirements.\nHere's a detailed justification:\nAmazon Kinesis Data Streams: This service is designed for real-time ingestion of streaming data. It provides a\nscalable and durable way to collect data from multiple producers, addressing the company's need to gather\ndata from various sources. (https://aws.amazon.com/kinesis/data-streams/)\nAmazon OpenSearch Service (successor to Elasticsearch): OpenSearch is a powerful search and analytics\nengine that excels at indexing and querying large volumes of data in near real-time. It is well-suited for the\ncompany's need for real-time search functionality. Kinesis Data Streams can directly integrate with\nOpenSearch, making the pipeline seamless. (https://aws.amazon.com/opensearch-service/)\nAmazon QuickSight: QuickSight is a business intelligence (BI) service that enables the creation of interactive\ndashboards and visualizations. It can connect to OpenSearch Service to provide insights and real-time\nupdates to the company's data, fulfilling the visualization requirement. (https://aws.amazon.com/quicksight/)\nOptions A, B, and C are less ideal for the following reasons:\nOption A (EC2, S3, Athena, Grafana): Using EC2 for ingestion requires custom development and\nmanagement. S3 is primarily for storage, not real-time search. Athena is suitable for querying data at rest but\nisn't optimized for the real-time updates required.\nOption B (EMR, Redshift, Redshift Spectrum, QuickSight): Amazon EMR is better suited for batch processing\nand is generally not ideal for constant streaming ingestion. Amazon Redshift is a data warehouse designed for\nanalytical workloads, while Kinesis Data Streams with OpenSearch is better for real-time search applications.\nRedshift Spectrum could query data in S3 but is not the best choice for Real-time.\nOption C (EKS, DynamoDB, CloudWatch Dashboards): While EKS can host streaming applications, it adds\noperational complexity. DynamoDB is a NoSQL database, but not primarily designed for real-time search.\nCloudWatch dashboards, while useful for monitoring, are not as robust or feature-rich as QuickSight for\ncomplex data visualization and exploration. They are not optimized for this kind of search visualization.\nTherefore, Kinesis Data Streams, OpenSearch Service, and QuickSight provide a fully managed, scalable, and\nAWS-native solution that best satisfies the finance company's need for real-time data ingestion, processing,\nsearch, and visualization capabilities.",
    "links": [
      "https://aws.amazon.com/kinesis/data-streams/)",
      "https://aws.amazon.com/opensearch-service/)",
      "https://aws.amazon.com/quicksight/)"
    ]
  },
  {
    "question": "CertyIQ\nA company currently runs an on-premises application that usesASP.NET on Linux machines. The application is\nresource-intensive and serves customers directly.\nThe company wants to modernize the application to .NET. The company wants to run the application on containers\nand to scale based on Amazon CloudWatch metrics. The company also wants to reduce the time spent on\noperational maintenance activities.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {
      "A": "Use AWS App2Container to containerize the application. Use an AWS",
      "B": "Amazon ECS on EC2 instances: While ECS on EC2 is a valid option, it increases operational overhead. The",
      "C": "AWS App Runner to containerize the application. Use App Runner to deploy the application to Amazon",
      "D": "AWS App Runner to containerize the application. Use App Runner to deploy the application to Amazon"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Use AWS App2Container to containerize the application. Use an AWS\nCloudFormation template to deploy the application to Amazon Elastic Container Service (Amazon ECS) on\nAWS Fargate.\nHere's why:\nApp2Container: This service simplifies the process of containerizing existing applications, especially .NET\napplications running on Linux, fitting the company's modernization goal. It automates the creation of\ncontainer images, Dockerfiles, and ECS task definitions. https://aws.amazon.com/app2container/\nAmazon ECS on Fargate: Fargate is a serverless compute engine for ECS. This means the company doesn't\nneed to manage the underlying EC2 instances, reducing operational overhead significantly. Fargate handles\npatching, scaling, and availability automatically. https://aws.amazon.com/fargate/\nCloudFormation: Using a CloudFormation template allows for infrastructure as code (IaC), ensuring\nconsistent and repeatable deployments. This is crucial for managing infrastructure at scale and reducing\nmanual configuration errors. It allows for scaling based on CloudWatch metrics through auto-scaling groups\nconfigured within the template. https://aws.amazon.com/cloudformation/\nWhy other options are less suitable:\nB. Amazon ECS on EC2 instances: While ECS on EC2 is a valid option, it increases operational overhead. The\ncompany would be responsible for managing the EC2 instances, including patching, scaling, and ensuring\navailability. This contradicts the requirement to reduce operational maintenance.\nC. AWS App Runner to containerize the application. Use App Runner to deploy the application to Amazon\nElastic Container Service (Amazon ECS) on AWS Fargate: AWS App Runner is a service that directly deploys\ncontainerized web applications and APIs from source code or a container image. While App Runner simplifies\ndeployment, it is less flexible than ECS and CloudFormation for scaling and infrastructure management. Also,\nApp Runner directly deploys the application, there isn't deployment to ECS on Fargate.\nD. AWS App Runner to containerize the application. Use App Runner to deploy the application to Amazon\nElastic Kubernetes Service (Amazon EKS) on Amazon EC2 instances: This option has both increased\noperational overhead, using EC2 instances for EKS, and is also trying to use AWS App Runner to deploy to\nEKS. App Runner doesn't deploy to EKS, and using EKS on EC2 increases the operational overhead. EKS\nmanages Kubernetes, requiring more expertise.\nIn summary, option A offers the best balance between modernization, scalability based on CloudWatch\nmetrics, and minimal operational overhead by leveraging App2Container for containerization, ECS on Fargate\nfor serverless compute, and CloudFormation for infrastructure as code.",
    "links": [
      "https://aws.amazon.com/app2container/",
      "https://aws.amazon.com/fargate/",
      "https://aws.amazon.com/cloudformation/"
    ]
  },
  {
    "question": "CertyIQ\nA company is designing a new internal web application in the AWS Cloud. The new application must securely\nretrieve and store multiple employee usernames and passwords from an AWS managed service.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "D",
    "explanation": "The most suitable solution for securely storing and retrieving employee credentials with the least operational\noverhead is to utilize AWS Secrets Manager in conjunction with AWS CloudFormation and the\nBatchGetSecretValue API.\nSecrets Manager is designed explicitly for managing secrets like passwords, API keys, and database\ncredentials. It offers built-in features for encryption, automatic rotation, and access control, minimizing the\noperational burden associated with managing secrets manually. Storing the credentials here inherently\nprovides a higher level of security compared to Parameter Store (especially for sensitive data like passwords)\nbecause Secrets Manager is built specifically for that purpose.\nCloudFormation enables infrastructure-as-code, which helps you define and provision resources in a\npredictable and repeatable manner. By integrating Secrets Manager with CloudFormation, you can\nautomatically retrieve and configure your application with the stored employee credentials during\ndeployment.\nThe BatchGetSecretValue API (or GetSecretValue if batch retrieval isn't necessary) allows CloudFormation to\nretrieve the usernames and passwords from Secrets Manager directly during the infrastructure deployment\nprocess. This streamlines the deployment and configuration, reduces manual intervention, and ensures\nconsistency across environments.\nOptions A and C use Parameter Store, which while capable of storing secrets, is primarily designed for\nconfiguration data. Secrets Manager offers better security features specifically designed for secrets\nmanagement, making it the preferred choice for this scenario. Furthermore, incorporating AWS Batch (as\nsuggested in options B and C) introduces unnecessary complexity and overhead. AWS Batch is typically\nutilized for running batch computing workloads and does not directly contribute to secure retrieval of secrets\nduring deployment. Using CloudFormation with the API call is sufficient for the needs of the application.\nTherefore, storing the employee credentials in AWS Secrets Manager and retrieving them during deployment\nusing CloudFormation and the appropriate Secrets Manager API call provides the most secure and\noperationally efficient approach.\nSupporting Links:\nAWS Secrets Manager: https://aws.amazon.com/secrets-manager/\nAWS CloudFormation: https://aws.amazon.com/cloudformation/\nSecrets Manager GetSecretValue API:\nhttps://docs.aws.amazon.com/secretsmanager/latest/dev/reference_awscli.html#cli_reference_getsecretvalue\nAWS Systems Manager Parameter Store: https://docs.aws.amazon.com/systems-\nmanager/latest/userguide/systems-manager-parameter-store.html",
    "links": [
      "https://aws.amazon.com/secrets-manager/",
      "https://aws.amazon.com/cloudformation/",
      "https://docs.aws.amazon.com/secretsmanager/latest/dev/reference_awscli.html#cli_reference_getsecretvalue",
      "https://docs.aws.amazon.com/systems-"
    ]
  },
  {
    "question": "CertyIQ\nA company that is in the ap-northeast-1 Region has a fleet of thousands of AWS Outposts servers. The company\nhas deployed the servers at remote locations around the world. All the servers regularly download new software\nversions that consist of 100 files. There is significant latency before all servers run the new software versions.\nThe company must reduce the deployment latency for new software versions.\nWhich solution will meet this requirement with the LEAST operational overhead?",
    "options": {
      "C": "Here's why:"
    },
    "answer": "C",
    "explanation": "The correct answer is C. Here's why:\nThe primary goal is to reduce deployment latency for software updates across globally distributed AWS\nOutposts servers with minimal operational overhead.\nOption C: S3 Transfer Acceleration is the most effective and least complex solution. S3 Transfer\nAcceleration utilizes globally distributed AWS edge locations to accelerate data transfer to an S3 bucket.\nWhen an Outpost server initiates a download using the Transfer Acceleration endpoint, the data is routed\nthrough the nearest edge location, which optimizes the network path and protocol for faster uploads to the S3\nbucket in ap-northeast-1. This inherently accelerates the subsequent downloads by the Outposts. The\noperational overhead is relatively low; it simply involves enabling S3 Transfer Acceleration on the bucket and\nusing the specific endpoint for data transfer.\nWhy other options are suboptimal:\nOption A: Disabling caching in CloudFront defeats the purpose of using a CDN for distributing content closer\nto the Outposts, thus negating the benefit of reduced latency for repeated downloads.\nOption B: Adding S3 replication to a second region introduces unnecessary complexity. While it provides\nredundancy, it does not inherently accelerate the initial download process for the globally distributed\nOutposts, which is the key bottleneck to address. Also, failing over to us-east-1 when the primary origin is ap-\nnortheast-1 will negatively impact the performance.\nOption D: While CloudFront can help with content distribution, it still relies on the initial upload to the origin\n(S3 bucket) in ap-northeast-1. Without accelerating this initial upload, latency will remain a problem, especially\nconsidering the geographical distribution of Outposts. S3 Transfer Acceleration directly addresses the initial\nupload bottleneck. Also the CloudFront needs time to distribute the files to its edge locations globally.\nIn summary, S3 Transfer Acceleration optimizes the data transfer to the S3 bucket, reducing latency with\nminimal setup and management. This is the most efficient approach compared to alternatives that add\ncomplexity without directly addressing the core issue of initial data transfer speed.\nAuthoritative Links:\nAmazon S3 Transfer Acceleration: https://aws.amazon.com/s3/transfer-acceleration/",
    "links": [
      "https://aws.amazon.com/s3/transfer-acceleration/"
    ]
  },
  {
    "question": "CertyIQ\nA company currently runs an on-premises stock trading application by using Microsoft Windows Server. The\ncompany wants to migrate the application to the AWS Cloud.\nThe company needs to design a highly available solution that provides low-latency access to block storage across\nmultiple Availability Zones.\nWhich solution will meet these requirements with the LEAST implementation effort?",
    "options": {
      "A": "Here's why:"
    },
    "answer": "A",
    "explanation": "The correct answer is A. Here's why:\nHigh Availability: The requirement for high availability is best met by a Windows Server cluster spanning\nmultiple Availability Zones (AZs). This ensures that if one AZ fails, the application can failover to the other\nnode in the other AZ.\nLow Latency: Amazon FSx for Windows File Server, especially when deployed in a Multi-AZ configuration,\nprovides low-latency, shared file storage accessible from multiple EC2 instances. This meets the low-latency\naccess requirement to block storage. FSx for Windows File Server also offers native SMB support, which is\ncompatible with Windows Server.\nLeast Implementation Effort: FSx for Windows File Server handles replication and failover automatically,\nreducing the implementation effort compared to manual data synchronization or application-level replication.\nSetting up a Windows Server cluster is a common practice, and using FSx simplifies the shared storage\naspect.\nNow let's examine why the other options are less suitable:\nB: While using EBS is an option, setting up application-level replication introduces complexity and might not\nbe as reliable or performant as a managed file system like FSx. EBS volumes themselves are not inherently\nshared across Availability Zones without replication solutions.\nC: Using FSx for NetApp ONTAP with iSCSI adds unnecessary complexity. The native SMB protocol of FSx for\nWindows File Server is more aligned with a Windows Server environment, making it simpler to implement.\nAlso, the active/standby configuration is not as highly available as a true cluster.\nD: Using EBS io2 volumes and EBS-level replication is more complex than using a shared file system like FSx\nfor Windows File Server. Manually managing EBS replication adds operational overhead. The active/standby\nsetup also limits the application's ability to automatically recover from failures.\nIn summary, option A provides a combination of high availability, low latency, and least implementation effort\nby leveraging a Windows Server cluster with shared storage provided by Amazon FSx for Windows File\nServer.\nSupporting Links:\nAmazon FSx for Windows File Server: https://aws.amazon.com/fsx/windows/\nWindows Server Failover Clustering: https://learn.microsoft.com/en-us/windows-server/failover-\nclustering/failover-clustering-overview",
    "links": [
      "https://aws.amazon.com/fsx/windows/",
      "https://learn.microsoft.com/en-us/windows-server/failover-"
    ]
  },
  {
    "question": "CertyIQ\nA company is designing a web application with an internet-facing Application Load Balancer (ALB).\nThe company needs the ALB to receive HTTPS web traffic from the public internet. The ALB must send only\nHTTPS traffic to the web application servers hosted on the Amazon EC2 instances on port 443. The ALB must\nperform a health check of the web application servers over HTTPS on port 8443.\nWhich combination of configurations of the security group that is associated with the ALB will meet these\nrequirements? (Choose three.)",
    "options": {
      "A": "Allow HTTPS inbound traffic from 0.0.0.0/0 for port 443: The ALB needs to accept HTTPS traffic from the",
      "C": "Allow HTTPS outbound traffic to the web application instances for port 443: The ALB needs to forward",
      "B": "The ALB initiates communication with the web application instances, not the other way around. The web",
      "D": "Allow HTTPS inbound traffic from the web application instances for port 443: This is unnecessary for the"
    },
    "answer": "A",
    "explanation": "The correct answer is ACE. Here's why:\nA. Allow HTTPS inbound traffic from 0.0.0.0/0 for port 443: The ALB needs to accept HTTPS traffic from the\npublic internet (0.0.0.0/0 represents all IP addresses). This is the entry point for users accessing the web\napplication. Without this, the ALB would be inaccessible from the internet.\nC. Allow HTTPS outbound traffic to the web application instances for port 443: The ALB needs to forward\nHTTPS traffic to the web application servers on port 443. This outbound rule allows the ALB to communicate\nwith the backend instances and pass on the decrypted traffic.\nE. Allow HTTPS outbound traffic to the web application instances for the health check on port 8443: The\nALB performs health checks on the EC2 instances to ensure they are healthy and can receive traffic. These\nhealth checks occur over HTTPS on port 8443, as stated in the requirement. This outbound rule is essential\nfor the ALB to monitor the health of the application servers.\nWhy other options are incorrect:\nB. Allow all outbound traffic to 0.0.0.0/0 for port 443: This is overly permissive. The ALB only needs to\ncommunicate with the specific web application instances. Allowing outbound traffic to the entire internet on\nport 443 is a security risk.\nD. Allow HTTPS inbound traffic from the web application instances for port 443: This is unnecessary for the\nALB. The ALB initiates communication with the web application instances, not the other way around. The web\napplication instances don't need to send HTTPS traffic back to the ALB on port 443.\nF. Allow HTTPS inbound traffic from the web application instances for the health check on port 8443: As\nwith option D, The ALB initiates health checks, not the instances. The ALB doesn't need to receive inbound\ntraffic from the web application instances on port 8443.\nSupporting Documentation:\nApplication Load Balancers:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\nSecurity Groups for Your VPC: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html",
    "links": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts an application on AWS. The application gives users the ability to upload photos and store the\nphotos in an Amazon S3 bucket. The company wants to use Amazon CloudFront and a custom domain name to\nupload the photo files to the S3 bucket in the eu-west-1 Region.\nWhich solution will meet these requirements? (Choose two.)",
    "options": {
      "A": "Use AWS Certificate Manager (ACM) to create a public certificate in the us-east-1 Region. Use the",
      "C": "Configure Amazon S3 to allow uploads from CloudFront. Configure S3 Transfer Acceleration. S3"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why options B and D are the correct choices and why the others are\nincorrect.\nOption B: Use AWS Certificate Manager (ACM) to create a public certificate in eu-west-1. Use the\ncertificate in CloudFront.\nThis option is correct because to use a custom domain name with CloudFront, you need an SSL/TLS\ncertificate. ACM certificates are used to provide this encryption for secure HTTPS connections. While ACM\ncertificates for CloudFront distributions can be in us-east-1, which is the global region for certain AWS\nservices, this is only for certificates directly attached to CloudFront for viewer connections. In this scenario,\nwhere the application is hosted in eu-west-1 and you want to use a custom domain for accessing resources in\nthat region via CloudFront, the certificate associated with the origin (S3 bucket in this case) should ideally be\nin the same region. The user uploads photos to the S3 bucket within the eu-west-1 region, and a certificate in\nthe same region is the recommended approach.\nOption D: Configure Amazon S3 to allow uploads from CloudFront origin access control (OAC).\nThis option is correct because CloudFront needs permission to upload objects to the S3 bucket. OAC is the\nrecommended method for controlling access to S3 buckets from CloudFront. It enhances security compared\nto origin access identity (OAI). OAC allows you to restrict S3 bucket access solely to your CloudFront\ndistribution. This ensures that users can only upload photos to S3 via CloudFront and not directly, which\nprotects against bypassing security measures you may have put in place (like content moderation on the\nCloudFront distribution).\nWhy other options are incorrect:\nA. Use AWS Certificate Manager (ACM) to create a public certificate in the us-east-1 Region. Use the\ncertificate in CloudFront. While us-east-1 is used for certificates associated with the CloudFront distribution\nitself for viewer connections, the requirement here is to securely upload to S3 in eu-west-1 through\nCloudFront. Having the S3 origin's certificate in the same region, eu-west-1 (as in option B), is a more standard\nand arguably clearer architectural approach.\nC. Configure Amazon S3 to allow uploads from CloudFront. Configure S3 Transfer Acceleration. S3\nTransfer Acceleration speeds up data transfers between your users and your S3 bucket. While it can improve\nupload speeds, it doesn't directly address the requirement of using a custom domain name with CloudFront\nand doesn't secure the upload process itself. The key requirement is to allow only CloudFront to upload, and\njust enabling Transfer Acceleration doesn't achieve that.\nE. Configure Amazon S3 to allow uploads from CloudFront. Configure an Amazon S3 website endpoint.\nConfiguring an S3 website endpoint is usually for serving static content directly from S3. While you can\ntechnically upload to S3 through the S3 website endpoint using specific forms and methods, this isn't the\nstandard recommended approach when you have CloudFront in the architecture, especially for direct user\nuploads. Moreover, the default S3 website endpoint doesn't support HTTPS with a custom domain without\nsignificant extra configuration (which the other options handle more directly).\nIn summary:\nOptions B and D together provide a solution that uses a certificate to securely upload data (HTTPS with a\ncustom domain via CloudFront) and uses OAC to ensure that only the CloudFront distribution can upload the\nphotos to the S3 bucket.\nAuthoritative Links:\nAWS Certificate Manager: https://aws.amazon.com/certificate-manager/\nCloudFront Origin Access Control (OAC):\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-\nto-s3.html\nS3 Transfer Acceleration: https://aws.amazon.com/s3/transfer-acceleration/\nUsing HTTPS with CloudFront:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https.html",
    "links": [
      "https://aws.amazon.com/certificate-manager/",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-",
      "https://aws.amazon.com/s3/transfer-acceleration/",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https.html"
    ]
  },
  {
    "question": "CertyIQ\nA weather forecasting company collects temperature readings from various sensors on a continuous basis. An\nexisting data ingestion process collects the readings and aggregates the readings into larger Apache Parquet\nfiles. Then the process encrypts the files by using client-side encryption with KMS managed keys (CSE-KMS).\nFinally, the process writes the files to an Amazon S3 bucket with separate prefixes for each calendar day.\nThe company wants to run occasional SQL queries on the data to take sample moving averages for a specific\ncalendar day.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "A",
    "explanation": "The most cost-effective solution for running occasional SQL queries on encrypted Parquet data in S3 is to use\nAmazon Athena.\nHere's why:\nAthena's Cost Efficiency: Athena is a serverless query service. You only pay for the queries you run, making it\nideal for infrequent queries. Options C and D (Redshift and EMR Serverless) involve more significant overhead,\nas you're paying for a cluster or compute environment to be running, even when not actively querying. S3\nSelect (option B) has query cost and throughput limitations.\nDirect Data Access: Athena directly queries data stored in S3, eliminating the need to load data into a\ndatabase like Redshift.\nEncryption Handling: Athena integrates with KMS to handle CSE-KMS encrypted data. You can configure\nAthena to use the same KMS key used to encrypt the files.\nSQL Compatibility: Athena uses Presto, which is a distributed SQL query engine. It has great SQL\ncompatibility.\nParquet Support: Athena natively supports Parquet files, allowing for efficient data processing due to\nParquet's columnar storage format.\nSimplicity: Compared to EMR Serverless, setting up and managing Athena is simpler. EMR Serverless is\noverkill for simple querying.\nTherefore, by using Athena, the company can directly query its encrypted Parquet files in S3 for specific\ncalendar days, calculate moving averages, and only pay for the queries executed, ensuring cost-\neffectiveness.\nAuthoritative Links:\nAmazon Athena: https://aws.amazon.com/athena/\nAmazon S3 Select: https://aws.amazon.com/s3/select/\nAmazon Redshift Spectrum: https://aws.amazon.com/redshift/spectrum/\nAmazon EMR Serverless: https://aws.amazon.com/emr/serverless/",
    "links": [
      "https://aws.amazon.com/athena/",
      "https://aws.amazon.com/s3/select/",
      "https://aws.amazon.com/redshift/spectrum/",
      "https://aws.amazon.com/emr/serverless/"
    ]
  },
  {
    "question": "CertyIQ\nA company is implementing a new application on AWS. The company will run the application on multiple Amazon\nEC2 instances across multiple Availability Zones within multiple AWS Regions. The application will be available\nthrough the internet. Users will access the application from around the world.\nThe company wants to ensure that each user who accesses the application is sent to the EC2 instances that are\nclosest to the users location.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B because it leverages Route 53 geoproximity routing and a Network Load Balancer\n(NLB) to achieve proximity-based routing to application instances across multiple AWS Regions.\nHere's why:\nRoute 53 Geoproximity Routing: This policy routes traffic to your resources based on the geographic location\nof your users and your resources. It considers both latitude and longitude, allowing you to direct users to the\nnearest EC2 instances, thereby minimizing latency and improving user experience. This perfectly aligns with\nthe requirement of sending users to the closest instances.\nNetwork Load Balancer (NLB): NLBs are designed for high performance and low latency, making them ideal\nfor applications where minimizing response time is crucial. NLBs operate at Layer 4 (TCP/UDP) and can handle\nmillions of requests per second while maintaining ultra-low latencies. Furthermore, NLBs support static IP\naddresses per Availability Zone, essential for geoproximity routing where you need to advertise specific IPs\nfor different regions.\nWhy other options are incorrect:\nA (Geolocation Routing): Geolocation routing directs traffic based on pre-defined geographic regions\n(countries or continents). While it directs traffic based on location, it doesn't consider the precise distance like\ngeoproximity. In the question's context, more precise routing based on distance is needed.\nC (Multivalue Answer Routing): Multivalue answer routing returns multiple IP addresses for each request,\nallowing the client to choose one. While it provides high availability, it doesn't consider proximity.\nD (Weighted Routing): Weighted routing distributes traffic based on weights you assign to each resource.\nThis is useful for A/B testing or migrating traffic but doesn't consider the user's location.\nIn conclusion, Route 53 geoproximity routing in conjunction with an NLB provides the most effective solution\nfor directing users to the nearest EC2 instances across multiple AWS Regions, ensuring low latency and\noptimal performance, especially when considering worldwide users.\nAuthoritative Links:\nRoute 53 Routing Policies\nNetwork Load Balancer",
    "links": []
  },
  {
    "question": "CertyIQ\nA financial services company plans to launch a new application on AWS to handle sensitive financial transactions.\nThe company will deploy the application on Amazon EC2 instances. The company will use Amazon RDS for MySQL\nas the database. The companys security policies mandate that data must be encrypted at rest and in transit.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A is the best solution, focusing on minimizing operational\noverhead:\nOption A: Using AWS KMS managed keys for RDS encryption at rest and ACM SSL/TLS certificates for\nencryption in transit is the correct answer.\nEncryption at Rest: Amazon RDS natively supports encryption at rest using AWS Key Management Service\n(KMS). By using KMS managed keys, the company can avoid the operational burden of managing encryption\nkeys themselves. AWS handles key generation, rotation, and storage, reducing administrative overhead.\nEncryption in Transit: AWS Certificate Manager (ACM) simplifies the process of obtaining, managing, and\ndeploying SSL/TLS certificates for use with AWS services and internal connected servers. Using ACM to\ngenerate and manage SSL/TLS certificates is the easiest way to encrypt connections to RDS, and requires no\nextra configuration in your EC2 instances.\nLeast Operational Overhead: Option A requires minimal configuration and management, reducing operational\noverhead. RDS encryption and ACM certificates are AWS managed services, simplifying the implementation\nand maintenance.\nWhy other options are less optimal:\nOption B (IPsec tunnels): IPsec tunnels are complex to set up and maintain, adding significant operational\noverhead. While they provide encryption, they are typically used for site-to-site VPNs, which are not\nnecessary for encrypting connections between EC2 instances and RDS within the same AWS region.\nOption C (Third-party application-level encryption): Implementing application-level encryption adds\nsignificant development and operational overhead. The application needs to handle encryption and\ndecryption, key management, and potential performance impacts. RDS already provides built-in encryption\ncapabilities, making this approach redundant and more complex.\nOption D (VPN Connection): VPN connection and tunnels are used to connect to on-prem or other external\nnetwork and not needed for encryption between EC2 and RDS services within the same AWS region.\nIn conclusion, option A balances security with simplicity and minimizes operational overhead by leveraging\nAWS managed services for encryption at rest and in transit, making it the most efficient solution for the\nfinancial services company's requirements.\nAuthoritative Links:\nRDS Encryption at Rest:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\nAWS Certificate Manager (ACM): https://aws.amazon.com/certificate-manager/\nEncrypting Connections to DB Instances:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL.html",
    "links": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html",
      "https://aws.amazon.com/certificate-manager/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is migrating its on-premises Oracle database to an Amazon RDS for Oracle database. The company\nneeds to retain data for 90 days to meet regulatory requirements. The company must also be able to restore the\ndatabase to a specific point in time for up to 14 days.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D because AWS Backup provides a centralized and automated way to manage backups\nand retention policies across AWS services, including RDS.\nHere's a breakdown:\nRequirement 1: 90-day data retention: AWS Backup allows defining a backup plan with a retention period of\n90 days. This ensures that backups are stored for the required duration to meet regulatory requirements.\nRequirement 2: Point-in-time restore for 14 days: RDS automated backups, when integrated with AWS\nBackup, inherently provide point-in-time recovery (PITR) capabilities within the defined retention period. Even\nthough the total retention is 90 days, you can restore to any point within the last 14 days.\nLeast operational overhead: AWS Backup automates the backup process according to the defined schedule\nand retention policy. This eliminates the need for manual snapshot creation and deletion, reducing operational\noverhead.\nLet's analyze why the other options are less suitable:\nA: While RDS automated backups do offer PITR, extending the retention period to 90 days for automated\nbackups might be more expensive than using AWS Backup. AWS Backup provides more granular control over\nbackup schedules and storage tiers.\nB: Creating manual snapshots every day and manually deleting them introduces significant operational\noverhead. This requires scripting and monitoring to ensure snapshots are created and deleted correctly, which\nis error-prone.\nC: Amazon Aurora Clone is not applicable to RDS for Oracle, and it is for Aurora only, meaning it will not work\nin this case.\nIn summary, AWS Backup simplifies the backup management process and ensures compliance with retention\nrequirements with the least operational overhead.\nRelevant documentation:\nAWS Backup: https://aws.amazon.com/backup/\nBacking Up and Restoring Amazon RDS Databases:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html",
    "links": [
      "https://aws.amazon.com/backup/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is developing a new application that uses a relational database to store user data and application\nconfigurations. The company expects the application to have steady user growth. The company expects the\ndatabase usage to be variable and read-heavy, with occasional writes.\nThe company wants to cost-optimize the database solution. The company wants to use an AWS managed database\nsolution that will provide the necessary performance.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "B": "Deploy the database on Amazon Aurora Serverless."
    },
    "answer": "B",
    "explanation": "The most cost-effective solution for a read-heavy, variable workload relational database, with occasional\nwrites, and steady user growth is B. Deploy the database on Amazon Aurora Serverless.\nHere's why:\nAurora Serverless Cost Optimization: Aurora Serverless is designed to automatically scale compute\nresources up or down based on application needs. You pay only for the resources consumed. For a variable\nworkload with occasional writes, this eliminates the cost of over-provisioning resources when the database\nisn't being heavily utilized. This is critical for cost optimization. https://aws.amazon.com/rds/aurora/serverless/\nRead-Heavy Workload Suitability: Aurora, in general, is optimized for read performance. Aurora Serverless\ninherits these benefits.\nRelational Database Requirement: The application requires a relational database. Aurora is a MySQL and\nPostgreSQL-compatible relational database.\nNot A: Provisioned IOPS (PIOPS) storage is expensive and best suited for consistent, high-performance\nworkloads, not variable ones. It wouldn't be cost-effective for a workload that often idles.\nNot C: DynamoDB is a NoSQL database and therefore not suitable for an application that requires a relational\ndatabase. While DynamoDB on-demand can scale, the database type is a mismatch.\nNot D: Magnetic storage is the slowest and least performant storage option on RDS. While read replicas can\nhelp with read performance, magnetic storage would significantly bottleneck performance and is not ideal\ngiven the need for \"necessary performance\". While read replicas do help with read scaling, the base instance\nwith Magnetic storage would not handle the workload effectively, and the magnetic storage itself is not\nperformance-optimized. Aurora is more efficient at scaling and cost savings for variable workloads.\nIn summary, Aurora Serverless provides the necessary relational database compatibility, automatically scales\nto handle the variable workload, and optimizes costs by charging only for consumed resources, making it the\nmost cost-effective choice.",
    "links": [
      "https://aws.amazon.com/rds/aurora/serverless/"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts its application on several Amazon EC2 instances inside a VP",
    "options": {
      "C": "Update the IAM instance profile policy"
    },
    "answer": "C",
    "explanation": "Create a gateway endpoint for Amazon S3 that is attached to the VPUpdate the IAM instance profile policy\nwith a Deny action and the following condition key.",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is building a cloud-based application on AWS that will handle sensitive customer data. The application\nuses Amazon RDS for the database, Amazon S3 for object storage, and S3 Event Notifications that invoke AWS\nLambda for serverless processing.\nThe company uses AWS IAM Identity Center to manage user credentials. The development, testing, and operations\nteams need secure access to Amazon RDS and Amazon S3 while ensuring the confidentiality of sensitive customer\ndata. The solution must comply with the principle of least privilege.\nWhich solution meets these requirements with the LEAST operational overhead?",
    "options": {
      "B": "Here's why:"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Here's why:\nOption B leverages AWS IAM Identity Center (successor to AWS SSO), which is the most efficient and secure\nmethod for managing user access across multiple AWS accounts and applications, especially when combined\nwith a central directory. Creating permission sets allows administrators to define granular, role-based access\nto AWS resources like RDS and S3, adhering to the principle of least privilege. Assigning teams to groups and\nthen associating those groups with specific permission sets streamlines access management and reduces\noperational overhead significantly. IAM Identity Center also integrates well with existing identity providers\n(like Active Directory), further simplifying user management.\nOption A, while using IAM roles with least privilege, involves managing roles and policies directly for each\nteam. This approach becomes cumbersome and difficult to scale as the number of teams and their access\nrequirements grow. IAM Identity Center provides centralized management, simplifying policy updates and\naccess reviews.\nOption C, creating individual IAM users, is against best practices. It introduces significant operational\noverhead for managing individual credentials and permissions. Moreover, it's harder to track and audit access\nwhen dealing with individual users instead of groups or roles. IAM Access Analyzer helps, but it's a reactive\nmeasure rather than a proactive, centralized solution.\nOption D, using AWS Organizations with separate accounts, is overkill for simply managing access within a\nsingle account. While Organizations is useful for multi-account environments, it adds unnecessary complexity\nand operational overhead when the primary goal is to control access to RDS and S3 within a single AWS\naccount. Cross-account IAM roles also introduce added complexity that IAM Identity Center avoids.\nIAM Identity Center's centralized management capabilities, coupled with the ability to define granular\npermission sets, makes it the most scalable, secure, and least operationally intensive solution for managing\nteam access to RDS and S3 while adhering to the principle of least privilege. It's specifically designed for\nscenarios where you have multiple users and groups requiring different levels of access to AWS resources.\nRelevant links for further research:\nAWS IAM Identity Center: https://aws.amazon.com/iam/identity-center/\nIAM Best Practices: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html\nPrinciple of Least Privilege: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-\nleast-privilege",
    "links": [
      "https://aws.amazon.com/iam/identity-center/",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-"
    ]
  },
  {
    "question": "CertyIQ\nA company has an Amazon S3 bucket that contains sensitive data files. The company has an application that runs\non virtual machines in an on-premises data center. The company currently uses AWS IAM Identity Center.\nThe application requires temporary access to files in the S3 bucket. The company wants to grant the application\nsecure access to the files in the S3 bucket.\nWhich solution will meet these requirements?",
    "options": {
      "B": "Use IAM Roles Anywhere to obtain security credentials in IAM Identity Center that"
    },
    "answer": "B",
    "explanation": "The best solution is B. Use IAM Roles Anywhere to obtain security credentials in IAM Identity Center that\ngrant access to the S3 bucket. Configure the virtual machines to assume the role by using the AWS CLI.\nHere's why:\nIAM Roles Anywhere: This feature allows on-premises applications to use IAM roles for temporary\ncredentials. This eliminates the need to embed long-term credentials directly into the application or virtual\nmachines. https://docs.aws.amazon.com/rolesanywhere/latest/userguide/\nSecurity Best Practices: Storing IAM user credentials (access keys and secret keys) directly on virtual\nmachines or within application code (as in options C and D) is a major security risk. If compromised, these\ncredentials grant potentially broad, persistent access to AWS resources. IAM Roles Anywhere mitigates this\nrisk by providing temporary, short-lived credentials.\nIAM Identity Center Integration: The solution leverages the existing IAM Identity Center (formerly AWS SSO)\nwhich is a centralized identity management service. This ensures consistent and manageable access control\nacross the AWS environment and on-premises resources.\nDynamic Access: The application uses the AWS CLI to assume the IAM role. The AWS CLI is designed to\nhandle temporary credentials acquired through IAM roles or other authentication mechanisms.\nLeast Privilege: By using IAM roles, you can grant the application only the necessary permissions to access\nthe specific S3 bucket and files it requires (principle of least privilege). A bucket policy can further restrict\naccess based on the IAM role assumed.\nWhy other options are not ideal:\nA: An S3 bucket policy based on the on-premises IP address range is less secure as the IP address can be\nspoofed or changed. It is not the recommended approach for secure access, especially for sensitive data.\nC & D: Storing or embedding IAM user credentials on the instances directly or storing them in Secrets\nManager and providing them to the VMs is not recommended for security reasons. If the instance is\ncompromised, the long-term IAM user credentials are also compromised. This is not the best way to use\nSecrets Manager.\nIn conclusion, using IAM Roles Anywhere provides the most secure, scalable, and manageable solution to\ngrant the on-premises application temporary access to the S3 bucket, while integrating seamlessly with the\nexisting IAM Identity Center setup. It avoids the risks associated with managing long-term credentials.",
    "links": [
      "https://docs.aws.amazon.com/rolesanywhere/latest/userguide/"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts its core network services, including directory services and DNS, in its on-premises data center.\nThe data center is connected to the AWS Cloud using AWS Direct Connect (DX). Additional AWS accounts are\nplanned that will require quick, cost-effective, and consistent access to these network services.\nWhat should a solutions architect implement to meet these requirements with the LEAST amount of operational\noverhead?",
    "options": {
      "D": "Configure AWS Transit Gateway between the accounts. Assign DX to the transit"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Configure AWS Transit Gateway between the accounts. Assign DX to the transit\ngateway and route network traffic to the on-premises servers.\nHere's a detailed justification:\nAWS Transit Gateway simplifies network architecture by acting as a central hub for routing traffic between\nVPCs and on-premises networks. Using Transit Gateway, you can create a single connection point to your on-\npremises network via Direct Connect (DX) and then share that connection with multiple AWS accounts and\nVPCs. This eliminates the need for separate DX connections or VPNs for each account, significantly reducing\noperational overhead and cost.\nOption A is incorrect because creating a DX connection in each new account is expensive and complex to\nmanage. Each connection would require individual configuration and management, adding significant\noperational overhead.\nOption B is incorrect. VPC endpoints are used to securely access AWS services without traversing the public\ninternet, but they are not relevant for routing traffic to on-premises resources. They don't facilitate\ncommunication between different AWS accounts and the on-premises data center.\nOption C is incorrect because setting up individual VPN connections for each account is more operationally\nintensive compared to using Transit Gateway. VPN connections, while functional, don't scale as efficiently or\nprovide the centralized management capabilities of Transit Gateway, resulting in higher overhead.\nFurthermore, VPNs might introduce more latency than Transit Gateway utilizing a DX connection.\nTransit Gateway provides a scalable, centralized, and cost-effective solution. It integrates seamlessly with\nDirect Connect, making it easy to extend your on-premises network connectivity to multiple AWS accounts.\nRouting policies can be centrally managed within the Transit Gateway, ensuring consistent network access\ncontrol across all connected VPCs. By associating the Direct Connect gateway to the Transit Gateway, all\naccounts can leverage the established DX connection to access on-premises resources, satisfying the\nrequirements of quick, cost-effective, and consistent access with minimal operational effort.\nHere are some authoritative links for further research:\nAWS Transit Gateway: https://aws.amazon.com/transit-gateway/\nAWS Direct Connect: https://aws.amazon.com/directconnect/\nTransit Gateway Routing: https://docs.aws.amazon.com/vpc/latest/tgw/tgw-routing.html",
    "links": [
      "https://aws.amazon.com/transit-gateway/",
      "https://aws.amazon.com/directconnect/",
      "https://docs.aws.amazon.com/vpc/latest/tgw/tgw-routing.html"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts its main public web application in one AWS Region across multiple Availability Zones. The\napplication uses an Amazon EC2 Auto Scaling group and an Application Load Balancer (ALB).\nA web development team needs a cost-optimized compute solution to improve the companys ability to serve\ndynamic content globally to millions of customers.\nWhich solution will meet these requirements?",
    "options": {
      "B": "Option C (S3 bucket with website hosting) is not suitable for dynamic content. S3 is primarily for serving static"
    },
    "answer": "A",
    "explanation": "The best solution is A, creating an Amazon CloudFront distribution with the existing ALB as the origin. Here's\nwhy:\nCloudFront is a Content Delivery Network (CDN) that caches content closer to users, reducing latency and\nimproving performance for globally distributed users. This directly addresses the requirement of serving\ndynamic content to millions of customers globally. By using the existing ALB as the origin, CloudFront will pull\ndynamic content from the ALB when it's not already cached in its edge locations.\nOption B (Route 53 geolocation routing) is less effective because it only routes users to the closest region, not\nthe closest edge location. Route 53 doesn't cache content, so users will still experience latency related to\nreaching the regional ALB.\nOption C (S3 bucket with website hosting) is not suitable for dynamic content. S3 is primarily for serving static\ncontent and while it can host a website, it doesn't handle the backend processing of dynamic elements.\nMigrating the entire web application to S3 would require significant re-architecting and won't inherently\nimprove global performance for dynamic requests.\nOption D (AWS Direct Connect) is for establishing a dedicated network connection between on-premises\ninfrastructure and AWS, not for content delivery to millions of customers. It's more suited for hybrid cloud\nscenarios and is not relevant to serving dynamic content globally. It is also significantly more expensive than a\nCDN.\nCloudFront's caching capabilities are key for cost optimization. By caching content at edge locations, it\nreduces the load on the origin server (ALB), potentially allowing for a smaller EC2 Auto Scaling group behind\nthe ALB, leading to cost savings.\nIn summary, CloudFront offers the best balance of improved global performance, cost optimization, and\nminimal disruption to the existing application architecture by utilizing the current ALB as the origin, making it\nthe ideal solution.\nAuthoritative links:\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
    "links": [
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/"
    ]
  },
  {
    "question": "CertyIQ\nA company stores user data in AWS. The data is used continuously with peak usage during business hours. Access\npatterns vary, with some data not being used for months at a time. A solutions architect must choose a cost-\neffective solution that maintains the highest level of durability while maintaining high availability.\nWhich storage solution meets these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The optimal storage solution is Amazon S3 Intelligent-Tiering. Here's why:\nThe requirement is a cost-effective solution with high durability and high availability for data with variable\naccess patterns, including infrequently accessed data. S3 Standard provides high durability and availability\nbut isn't cost-effective for data accessed infrequently.\nS3 Glacier Deep Archive is the cheapest option for long-term archival, but access times are slower (measured\nin hours), contradicting the requirement for continuous use and peak usage during business hours. Its\nintended for archival, not frequent retrieval.\nS3 One Zone-IA is cheaper than S3 Standard-IA but stores data in a single Availability Zone, which\ncompromises availability, especially when the goal is to maintain high availability. Its durability is also lower\nthan other S3 storage classes.\nS3 Intelligent-Tiering automatically moves data between frequent, infrequent, and archive access tiers based\non access patterns, optimizing costs. When data is accessed frequently, it resides in a tier comparable to S3\nStandard. When not accessed for a period, it transitions to infrequent access tiers like S3 Standard-IA or S3\nGlacier Instant Retrieval, and ultimately, archive tiers, reducing storage costs significantly without retrieval\nfees. Because Amazon S3 automatically manages the tiers, it removes the manual effort typically involved in\ndata lifecycle management. This achieves both cost-effectiveness and maintains high durability and\navailability due to S3's inherent design principles. It caters to the peak usage scenario without inflating costs\nduring periods of infrequent access.\nAuthoritative Links:\nAmazon S3 Storage Classes: https://aws.amazon.com/s3/storage-classes/\nAmazon S3 Intelligent-Tiering: https://aws.amazon.com/s3/intelligent-tiering/",
    "links": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://aws.amazon.com/s3/intelligent-tiering/"
    ]
  },
  {
    "question": "CertyIQ\nA company is testing an application that runs on an Amazon EC2 Linux instance. A single 500 GB Amazon Elastic\nBlock Store (Amazon EBS) General Purpose SSO (gp2) volume is attached to the EC2 instance.\nThe company will deploy the application on multiple EC2 instances in an Auto Scaling group. All instances require\naccess to the data that is stored in the EBS volume. The company needs a highly available and resilient solution\nthat does not introduce significant changes to the application's code.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D, provisioning an Amazon Elastic File System (EFS) file system configured for General\nPurpose performance mode. Here's why:\nThe core requirement is to provide a highly available and resilient shared storage solution that can be\naccessed by multiple EC2 instances within an Auto Scaling group without significant code changes.\nOption A (NFS server on a single EC2 instance) presents a single point of failure. If the NFS server instance\nfails, the entire application's data access is disrupted. This violates the high availability requirement. Also,\nmanaging an NFS server adds operational overhead.\nOption B (Amazon FSx for Windows File Server) is designed for Windows-based applications that require SMB\nprotocol support. The question specifies a Linux EC2 instance, making FSx for Windows inappropriate.\nOption C (Two Provisioned IOPS EBS volumes on a single EC2 instance) doesn't address the core requirement\nof shared storage accessible by multiple instances. EBS volumes are typically attached to a single instance at\na time. While you could potentially configure some software-based RAID and network sharing, it adds\ncomplexity and negates the \"no significant code changes\" requirement. Moreover, managing the redundancy\nand replication across these volumes increases operational overhead.\nOption D (Amazon EFS) is the optimal solution. EFS provides a fully managed, scalable, and highly available\nnetwork file system. It can be concurrently accessed by multiple EC2 instances across multiple Availability\nZones. Since the existing EBS volume is General Purpose (gp2) based, using EFS's General Purpose\nperformance mode is a suitable replacement, minimizing performance disruption. EFS's distributed\narchitecture inherently provides resilience and avoids a single point of failure. The instances within the Auto\nScaling group can mount the EFS file system using the NFS protocol, requiring minimal configuration and no\nsignificant application code changes. EFS automatically scales capacity as needed, simplifying storage\nmanagement.\nTherefore, EFS directly addresses the shared storage, high availability, and minimal code change\nrequirements.\nAuthoritative links:\nAmazon EFS: https://aws.amazon.com/efs/\nEBS vs. EFS: https://aws.amazon.com/premiumsupport/knowledge-center/ebs-vs-efs/",
    "links": [
      "https://aws.amazon.com/efs/",
      "https://aws.amazon.com/premiumsupport/knowledge-center/ebs-vs-efs/"
    ]
  },
  {
    "question": "CertyIQ\nA company recently launched a new application for its customers. The application runs on multiple Amazon EC2\ninstances across two Availability Zones. End users use TCP to communicate with the application.\nThe application must be highly available and must automatically scale as the number of users increases.\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": {
      "A": "Add a Network Load Balancer (NLB) in front of the EC2 instances:",
      "B": "Configure an Auto Scaling group for the EC2 instances:",
      "C": "Add an Application Load Balancer (ALB) in front of the EC2 instances: While ALBs can also provide high",
      "D": "Manually add more EC2 instances for the application: Manual scaling is time-consuming, error-prone, and"
    },
    "answer": "A",
    "explanation": "The most cost-effective solution for achieving high availability and automatic scaling for the application is a\ncombination of a Network Load Balancer (NLB) and an Auto Scaling group.\nA. Add a Network Load Balancer (NLB) in front of the EC2 instances:\nAn NLB is designed for high-performance and low-latency applications that use TCP or UDP protocols.\nBecause the application communicates using TCP, an NLB is a suitable choice compared to an Application\nLoad Balancer (ALB), which is intended for HTTP/HTTPS traffic and adds overhead. The NLB distributes traffic\nacross multiple EC2 instances in different Availability Zones, enhancing availability and fault tolerance. If one\ninstance fails, the NLB automatically redirects traffic to the remaining healthy instances. NLBs are also very\nefficient at handling fluctuating traffic volumes.\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\nB. Configure an Auto Scaling group for the EC2 instances:\nAn Auto Scaling group automatically adjusts the number of EC2 instances based on demand. This ensures\nthat the application can handle increased traffic without manual intervention. It scales out (adds more\ninstances) when the load increases and scales in (removes instances) when the load decreases. This dynamic\nscaling ensures optimal resource utilization and cost efficiency. Moreover, Auto Scaling groups automatically\nreplace unhealthy instances, contributing to high availability. It integrates seamlessly with the NLB,\nregistering and deregistering instances as they are launched or terminated.\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-group.html\nWhy other options are less suitable:\nC. Add an Application Load Balancer (ALB) in front of the EC2 instances: While ALBs can also provide high\navailability and scaling, they are optimized for HTTP/HTTPS traffic. Using an ALB for a TCP-based application\nintroduces unnecessary overhead and cost.\nD. Manually add more EC2 instances for the application: Manual scaling is time-consuming, error-prone, and\nnot cost-effective. It doesn't provide automatic fault tolerance and requires constant monitoring.\nE. Add a Gateway Load Balancer (GWLB) in front of the EC2 instances: GWLB is designed for deploying and\nmanaging virtual appliances (like firewalls and intrusion detection systems), not for general application load\nbalancing. It's not relevant for the described scenario and would be more complex and expensive than\nneeded.\nTherefore, the combination of an NLB and an Auto Scaling group is the most cost-effective approach to meet\nthe application's requirements for high availability and automatic scaling for TCP traffic.",
    "links": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-group.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is designing the architecture for a new mobile app that uses the AWS Cloud. The company uses\norganizational units (OUs) in AWS Organizations to manage its accounts. The company wants to tag Amazon EC2\ninstances with data sensitivity by using values of sensitive and nonsensitive. IAM identities must not be able to\ndelete a tag or create instances without a tag.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": {},
    "answer": "A",
    "explanation": "Here's a detailed justification for why options A and D are the correct choices, and why the others are\nincorrect, including supporting information and authoritative links:\nWhy A is correct:\nTag policies within AWS Organizations allow you to standardize tags across all accounts within your\norganization. This approach centralizes tag governance and ensures consistency. By defining a tag policy that\nspecifies the data sensitivity tag key and the allowed values (sensitive and nonsensitive), you enforce these\nvalues for EC2 instances. Attaching the tag policy to the relevant OU ensures that all accounts within that OU\nadhere to the tagging requirements. Tag policies can enforce required values. This addresses the requirement\nof instances having the correct tag values.\nWhy D is correct:\nService Control Policies (SCPs) provide centralized control over the AWS accounts in your organization. They\ndefine guardrails or boundaries for what IAM users and roles within member accounts can do. You can create\nan SCP that denies the creation of EC2 instances if the data sensitivity tag key is not specified. You can create\nanother SCP to prevent IAM identities from deleting tags. By attaching these SCPs to the appropriate OU, you\nensure that no one within the OU can bypass the tagging requirements. SCPs are powerful tools to enforce\ncompliance at the organization level.\nWhy B is incorrect:\nSCPs don't \"enforce tag values\" in the way tag policies do. They can prevent actions if a tag key is missing,\nbut they can't directly force a tag to have a specific value. Tag policies are designed for value validation.\nWhy C is incorrect:\nWhile tag policies can deny resources if a tag key is missing, creating two separate tag policies isn't as\nefficient as using a single tag policy to define the key and required values (option A). Also, tag policies are\nless effective than SCPs for preventing identities from deleting tags. SCPs provide a stronger, organization-\nwide preventative control.\nWhy E is incorrect:\nAWS Config rules and Lambda functions for remediation are reactive controls. They detect and fix non-\ncompliant resources after they have been created. The question specifies that IAM identities \"must not be\nable to create instances without a tag.\" This requires a preventative control, which Config + Lambda does not\nprovide. Also, while this approach can correct the problem, it is more complex than using SCPs and tag\npolicies.\nIn summary:\nThe best solution involves a combination of preventative controls. Tag policies enforce the presence and valid\nvalues of the tag (option A), while SCPs prevent the creation of resources if the required tag is missing and\nprevent tag deletion (option D). This layered approach provides comprehensive tag governance and ensures\ncompliance across the organization.\nAuthoritative Links:\nAWS Organizations Tag Policies:\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies.html\nAWS Organizations Service Control Policies (SCPs):\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html\nAWS Config: https://aws.amazon.com/config/",
    "links": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies.html",
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html",
      "https://aws.amazon.com/config/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs database workloads on AWS that are the backend for the company's customer portals. The\ncompany runs a Multi-AZ database cluster on Amazon RDS for PostgreSQL.\nThe company needs to implement a 30-day backup retention policy. The company currently has both automated\nRDS backups and manual RDS backups. The company wants to maintain both types of existing RDS backups that\nare less than 30 days old.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "C": "Configure the RDS backup retention policy to 30 days for automated backups."
    },
    "answer": "C",
    "explanation": "The correct answer is C. Configure the RDS backup retention policy to 30 days for automated backups.\nManually delete manual backups that are older than 30 days.\nHere's a detailed justification:\nAutomated Backups: RDS provides automated backups that create a point-in-time recovery of your DB\ninstance. Configuring the retention policy for automated backups directly within RDS is the most cost-\neffective and straightforward method. Setting this to 30 days ensures that RDS automatically manages these\nbackups, deleting those older than 30 days.\nManual Backups: Manual backups, on the other hand, are retained indefinitely until manually deleted.\nTherefore, a separate process is needed to manage their retention. Manually deleting backups older than 30\ndays is the simplest and most direct way to handle them, particularly if the number of manual backups is\nmanageable.\nCost-Effectiveness: AWS Backup is a service for centralizing and automating data protection across AWS\nservices. While it can manage RDS backups, it introduces additional cost and complexity compared to using\nRDS's built-in retention policy for automated backups.\nWhy other options are incorrect:\nOption A: It is same as option C and makes no sense.\nOption B: Disabling RDS automated backups is not a good practice as they are essential for disaster recovery.\nThe customer needs to maintain automated backups as mentioned in the prompt. The use of AWS\nCloudFormation for deleting backups (as in Option D) is overkill and adds unnecessary complexity when a\nsimple manual deletion process is sufficient.\nIn summary, leveraging RDS's native backup retention policy for automated backups and manually deleting\nolder manual backups provides the most cost-effective and efficient solution for maintaining a 30-day\nretention policy while preserving both types of existing RDS backups.\nSupporting Links:\nAmazon RDS Backups\nAWS Backup",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is planning to migrate a legacy application to AWS. The application currently uses NFS to\ncommunicate to an on-premises storage solution to store application data. The application cannot be modified to\nuse any other communication protocols other than NFS for this purpose.\nWhich storage solution should a solutions architect recommend for use after the migration?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C, Amazon Elastic File System (Amazon EFS). Here's a detailed justification:\nThe legacy application relies on NFS (Network File System) for accessing its storage. This means that it\nrequires a shared file system accessible over a network. Among the options, only Amazon EFS provides a fully\nmanaged NFS file system service in AWS.\nAmazon EFS is designed to provide scalable, elastic, and serverless file storage that can be mounted on\nmultiple EC2 instances or on-premises servers (via Direct Connect or VPN) simultaneously using the NFS\nprotocol. This makes it an ideal solution for applications that require a shared file system and rely on NFS,\nfulfilling the application's requirement without code modification.\nAWS DataSync (A) is a data transfer service used to move large amounts of data between on-premises\nstorage and AWS storage services. It's not a storage solution itself and doesn't provide NFS access to\napplications after the migration.\nAmazon Elastic Block Store (Amazon EBS) (B) provides block-level storage volumes for use with Amazon EC2\ninstances. While EBS volumes can be formatted with a file system, they are typically attached to a single EC2\ninstance at a time, making them unsuitable for applications requiring shared storage accessed through NFS.\nSetting up NFS server on an EC2 instance backed by EBS is possible, but adds operational overhead and\nnegates the fully-managed benefits of EFS.\nAmazon EMR File System (Amazon EMRFS) (D) is a file system implementation used with Amazon EMR\n(Elastic MapReduce) for processing big data. It's optimized for Hadoop-based workloads and is not generally\nused as a general-purpose file system for other applications. EMRFS is also often used with Amazon S3 as its\nunderlying storage, and not suited for direct NFS mounts.\nTherefore, because the legacy application requires NFS and needs a fully-managed solution for shared\nstorage, Amazon EFS is the most appropriate storage service for the migration.\nFurther Reading:\nAmazon EFS: https://aws.amazon.com/efs/\nNFS Protocol: https://en.wikipedia.org/wiki/Network_File_System",
    "links": [
      "https://aws.amazon.com/efs/",
      "https://en.wikipedia.org/wiki/Network_File_System"
    ]
  },
  {
    "question": "CertyIQ\nA company uses GPS trackers to document the migration patterns of thousands of sea turtles. The trackers check\nevery 5 minutes to see if a turtle has moved more than 100 yards (91.4 meters). If a turtle has moved, its tracker\nsends the new coordinates to a web application running on three Amazon EC2 instances that are in multiple\nAvailability Zones in one AWS Region.\nRecently, the web application was overwhelmed while processing an unexpected volume of tracker data. Data was\nlost with no way to replay the events. A solutions architect must prevent this problem from happening again and\nneeds a solution with the least operational overhead.\nWhat should the solutions architect do to meet these requirements?",
    "options": {
      "C": "Create an Amazon Simple Queue Service (Amazon SQS) queue to store the incoming",
      "A": "Amazon S3: S3 is primarily for object storage, not message queuing. Constantly scanning S3 for new data",
      "B": "This also puts a significant load on DynamoDB,",
      "D": "Amazon DynamoDB: DynamoDB is a NoSQL database, which can store location data. However, using it as a"
    },
    "answer": "C",
    "explanation": "The best solution is C. Create an Amazon Simple Queue Service (Amazon SQS) queue to store the incoming\ndata. Configure the application to poll for new messages for processing.\nHere's why:\nDecoupling: SQS decouples the GPS trackers (data producers) from the EC2 instances (data consumers). This\nmeans the trackers can send data to the queue regardless of the EC2 instance's availability or processing\ncapacity. This decoupling prevents the web application from being overwhelmed by surges in data volume.\nBuffering: SQS acts as a buffer. When there's a sudden spike in tracker data, SQS will queue the messages,\nensuring no data loss. The EC2 instances can then process the messages at their own pace. The queue\nprovides a holding place when the processing system cannot keep up.\nReliability: SQS provides reliable message delivery. Messages are stored redundantly across multiple\nAvailability Zones, ensuring durability and availability even in the event of hardware failures. This guarantees\nthat data isn't lost, addressing the critical requirement of avoiding data loss.\nScalability: SQS is highly scalable and can handle a massive volume of messages, adapting to the fluctuating\nneeds of GPS tracker data input. It dynamically scales to accommodate the workload without requiring\nmanual intervention.\nLeast Operational Overhead: SQS is a managed service, which means AWS handles the underlying\ninfrastructure, patching, and scaling. This minimizes the operational overhead for the solutions architect, as\nthey don't need to manage the queue infrastructure themselves.\nWhy other options are less suitable:\nA. Amazon S3: S3 is primarily for object storage, not message queuing. Constantly scanning S3 for new data\nis inefficient and introduces unnecessary complexity. The web application would need to implement polling\nlogic, which isn't ideal for real-time or near-real-time data processing.\nB. Amazon API Gateway and Lambda: While API Gateway and Lambda can handle incoming requests, Lambda\nfunctions have execution time limits. Processing complex GPS data transformations within a single Lambda\nexecution might exceed those limits. Furthermore, direct Lambda invocation might still overwhelm\ndownstream systems if there is a spike. Lambda is also stateless, making handling complex processing\nworkflows harder than using a dedicated queuing mechanism.\nD. Amazon DynamoDB: DynamoDB is a NoSQL database, which can store location data. However, using it as a\nqueue would be complex and inefficient. The application would need to implement its own queuing\nmechanism on top of DynamoDB, managing read/write consistency, polling, and deletion. Although TTL could\nbe used, this is not the primary purpose of DynamoDB. This also puts a significant load on DynamoDB,\nespecially if the application uses it for other purposes.\nAuthoritative links for further research:\nAmazon SQS: https://aws.amazon.com/sqs/\nDecoupling Applications with SQS: https://aws.amazon.com/blogs/architecture/queue-decoupling-pattern-\nfor-serverless-architectures/",
    "links": [
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/blogs/architecture/queue-decoupling-pattern-"
    ]
  },
  {
    "question": "CertyIQ\nA company's software development team needs an Amazon RDS Multi-AZ cluster. The RDS cluster will serve as a\nbackend for a desktop client that is deployed on premises. The desktop client requires direct connectivity to the\nRDS cluster.\nThe company must give the development team the ability to connect to the cluster by using the client when the\nteam is in the office.\nWhich solution provides the required connectivity MOST securely?",
    "options": {
      "B": "Here's why:",
      "C": "Supporting Links:"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Here's why:\nSecurity Posture: Exposing an RDS database directly to the internet, as implied by options A and D using\npublic subnets, is a significant security risk. Databases should reside in private subnets, isolated from direct\ninternet access.\nConnectivity Requirement: The desktop clients require direct connectivity from the company's office to the\nRDS cluster. This implies a need for a private, secure connection between the on-premises network and the\nAWS VPC.\nAWS Site-to-Site VPN: This service establishes an encrypted tunnel between the on-premises network\n(company office) and the AWS VPC. This ensures all traffic between the desktop clients and the RDS cluster\nis secure. The customer gateway is configured on the company's side of the VPN connection, providing the\nnecessary endpoint for the tunnel.\nPrivate Subnets: Placing the RDS cluster within private subnets reinforces security. Resources in private\nsubnets have no direct route to the internet. They can only be accessed from within the VPC or through\nservices like NAT gateways or, as in this case, a VPN connection.\nOption C's Flaw: While using private subnets is good, relying solely on RDS security groups with IP ranges is\nless secure than a VPN. IP ranges can be spoofed, and the traffic is not encrypted in transit without a VPN.\nOption D's Flaw: Creating separate database users is a good practice for access control, but it does nothing\nto solve the core problem of insecure internet exposure using public subnets.\nIn summary, option B provides the most secure solution by isolating the RDS cluster in private subnets and\nestablishing a secure, encrypted connection via AWS Site-to-Site VPN between the company's office and the\nVPC.\nSupporting Links:\nAWS Site-to-Site VPN: https://aws.amazon.com/vpn/site-to-site-vpn/\nAmazon VPC: https://aws.amazon.com/vpc/\nAmazon RDS Security: https://aws.amazon.com/rds/security/",
    "links": [
      "https://aws.amazon.com/vpn/site-to-site-vpn/",
      "https://aws.amazon.com/vpc/",
      "https://aws.amazon.com/rds/security/"
    ]
  },
  {
    "question": "CertyIQ\nA solutions architect is creating an application that will handle batch processing of large amounts of data. The\ninput data will be held in Amazon S3 and the output data will be stored in a different S3 bucket. For processing,\nthe application will transfer the data over the network between multiple Amazon EC2 instances.\nWhat should the solutions architect do to reduce the overall data transfer costs?",
    "options": {},
    "answer": "C",
    "explanation": "The most cost-effective approach to minimize data transfer costs when processing data between EC2\ninstances for batch processing, where data resides in S3, is to place all EC2 instances within the same\nAvailability Zone (AZ).\nHere's why:\nData Transfer Costs: AWS charges for data transferred between Availability Zones and Regions. Data\ntransfer within the same AZ is typically free or significantly less expensive. Since the application involves\nsubstantial data transfer between EC2 instances during processing, minimizing cross-AZ data movement\ndirectly reduces costs.\nOption A (Auto Scaling Group): While Auto Scaling is useful for managing EC2 instances and ensuring\navailability, it doesn't inherently reduce data transfer costs. The instances within the ASG could still be spread\nacross multiple AZs, negating any potential savings.\nOption B (Same Region): Keeping EC2 instances within the same Region is good practice for performance and\nreducing latency but doesn't eliminate data transfer costs. Inter-AZ data transfer charges still apply within a\nRegion.\nOption D (Private Subnets in Multiple AZs): While using private subnets adds a layer of security and\ndeploying across multiple AZs enhances availability, it actively increases data transfer costs due to cross-AZ\ndata movement. This option directly contradicts the goal of reducing data transfer expenses.\nTherefore, keeping all EC2 instances in the same Availability Zone ensures that data transfer occurs within\nthe AZ, thus avoiding costly cross-AZ charges. This is the most direct and effective method to reduce overall\ndata transfer expenses for the given scenario.\nRelevant AWS documentation:\nAmazon EC2 Pricing: https://aws.amazon.com/ec2/pricing/ (Review the \"Data Transfer\" section for pricing\ndetails)\nAWS Data Transfer Costs: https://aws.amazon.com/premiumsupport/knowledge-center/data-transfer-\nbetween-ec2-instances/",
    "links": [
      "https://aws.amazon.com/ec2/pricing/",
      "https://aws.amazon.com/premiumsupport/knowledge-center/data-transfer-"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage. The\napplication tier is hosted on Amazon EC2 instances. The company's IT security guidelines mandate that the\ndatabase credentials be encrypted and rotated every 14 days.\nWhat should a solutions architect do to meet this requirement with the LEAST operational effort?",
    "options": {},
    "answer": "A",
    "explanation": "The best solution is A because it leverages AWS Secrets Manager, designed specifically for managing,\nrotating, and retrieving secrets like database credentials. Secrets Manager integrates directly with Aurora\nand simplifies the rotation process.\nHere's a detailed justification:\nAWS Secrets Manager for Secret Management: Secrets Manager centralizes secrets, reducing the risk of\nhardcoding them in application code or configuration files, which is a security best practice.\nhttps://aws.amazon.com/secrets-manager/\nKMS Encryption: Using KMS to encrypt the secrets adds an extra layer of security. The secrets are encrypted\nat rest and in transit when retrieved. https://aws.amazon.com/kms/\nAurora Integration: Secrets Manager integrates directly with Aurora, simplifying credential management and\nrotation. You can associate the secret with the Aurora cluster, and Aurora will automatically use the\ncredentials from the secret. https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets-\naurora.html\nAutomated Rotation: Secrets Manager supports automated secret rotation. This means that you can\nconfigure Secrets Manager to automatically rotate the database credentials according to your schedule (in\nthis case, every 14 days). This reduces the operational burden and ensures that the credentials are always up-\nto-date.\nLeast Operational Effort: By using Secrets Manager's built-in rotation capabilities, you avoid the need to\nwrite and maintain custom Lambda functions for secret rotation. This simplifies the solution and reduces the\nrisk of errors.\nWhy other options are less suitable:\nOption B (SSM Parameter Store): While SSM Parameter Store can store secrets, Secrets Manager is a better\nchoice because it has built-in secret rotation capabilities. Implementing a Lambda function for rotation adds\ncomplexity.\nOption C (EFS with KMS): Storing credentials in a file on EFS and mounting it on EC2 instances is less secure\nand more complex than using Secrets Manager. Managing file access and ensuring proper encryption adds\noverhead.\nOption D (S3 with KMS): Similar to Option C, storing credentials in an S3 bucket and regularly downloading\nthem to the application is less secure and more complex than using Secrets Manager. It introduces latency in\ncredential updates and poses a risk of stale credentials.",
    "links": [
      "https://aws.amazon.com/secrets-manager/",
      "https://aws.amazon.com/kms/",
      "https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets-"
    ]
  },
  {
    "question": "CertyIQ\nA streaming media company is rebuilding its infrastructure to accommodate increasing demand for video content\nthat users consume daily.\nThe company needs to process terabyte-sized videos to block some content in the videos. Video processing can\ntake up to 20 minutes.\nThe company needs a solution that will scale with demand and remain cost-effective.\nWhich solution will meet these requirements?",
    "options": {
      "B": "Store video"
    },
    "answer": "B",
    "explanation": "The best solution is B because it offers a scalable, cost-effective, and robust approach for processing large\nvideo files.\nHere's a breakdown:\nAmazon ECS with Fargate: ECS, particularly with Fargate, provides a managed container orchestration\nservice. Fargate eliminates the need to manage underlying EC2 instances, simplifying operations and auto-\nscaling the resources required for processing videos based on demand. This aligns with the requirement for\nscalability.\nhttps://aws.amazon.com/ecs/fargate/\nMicroservices Architecture: Breaking down the video processing workflow into microservices allows for\nindependent scaling of different parts of the processing pipeline. This offers better resource utilization and\nfault isolation compared to a monolithic application.\nAmazon Aurora: Aurora is a MySQL and PostgreSQL-compatible relational database engine that combines\nthe speed and availability of high-end commercial databases with the simplicity and cost-effectiveness of\nopen-source databases. It's well-suited for storing video metadata due to its scalability and reliability.\nhttps://aws.amazon.com/rds/aurora/\nAmazon S3 Intelligent-Tiering: S3 Intelligent-Tiering automatically moves data to the most cost-effective\naccess tier based on access patterns without performance impact or operational overhead. Given the large\nsize of the videos and the likely variability in access frequency, this is a cost-effective choice.\nhttps://aws.amazon.com/s3/storage-classes/intelligent-tiering/\nWhy other options are less suitable:\nA (Lambda): AWS Lambda functions have execution time limits (maximum 15 minutes). Processing videos that\ncan take up to 20 minutes exceeds this limit, making Lambda unsuitable.\nC (EC2 Auto Scaling with SQS): While EC2 Auto Scaling provides scalability, it involves more operational\noverhead compared to Fargate. Managing EC2 instances (patching, scaling policies, etc.) increases the\nmanagement burden. Also, S3 Standard might not be as cost-effective as S3 Intelligent-Tiering for\ninfrequently accessed video content.\nD (EKS): EKS is a powerful container orchestration service, but it adds significant operational complexity, and\nit is probably overkill for this situation. It is not cost effective compared to using ECS with Fargate. Storing\nvideo metadata in Amazon RDS in a single Availability Zone provides limited availability and is not best\npractice. S3 Glacier Deep Archive is designed for very long-term archival and retrieval of data, which may not\nbe the most efficient storage class for this use case.\nIn summary, the combination of ECS with Fargate, Aurora for metadata, and S3 Intelligent-Tiering creates a\nscalable, cost-effective, and manageable solution that addresses all the requirements, particularly the video\nprocessing time and increasing demand.",
    "links": [
      "https://aws.amazon.com/ecs/fargate/",
      "https://aws.amazon.com/rds/aurora/",
      "https://aws.amazon.com/s3/storage-classes/intelligent-tiering/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an on-premises application on a Kubernetes cluster. The company recently added millions of new\ncustomers. The company's existing on-premises infrastructure is unable to handle the large number of new\ncustomers. The company needs to migrate the on-premises application to the AWS Cloud.\nThe company will migrate to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The company does not\nwant to manage the underlying compute infrastructure for the new architecture on AWS.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C because it offers the least operational overhead while fulfilling the requirements of\nmigrating the Kubernetes application to AWS without managing the underlying compute infrastructure. AWS\nFargate is a serverless compute engine for containers that works with EKS. This means the company doesn't\nneed to provision, scale, or manage EC2 instances for the Kubernetes worker nodes.\nOption A is incorrect as self-managed nodes require manual provisioning, scaling, patching, and management\nof EC2 instances, increasing operational overhead.\nOption B, using managed node groups, simplifies node management compared to self-managed nodes, but\nstill involves some degree of responsibility for the underlying EC2 instances, such as scaling and patching the\noperating system.\nOption D, combining managed node groups with Karpenter, automates node provisioning and scaling more\ndynamically than managed node groups alone. However, it still relies on EC2 instances and introduces the\ncomplexity of managing Karpenter itself, increasing operational overhead compared to Fargate.\nFargate eliminates the need to manage any EC2 instances, aligning with the requirement of minimal\noperational overhead. The company creates a Fargate profile, which specifies which pods should run on\nFargate based on namespace and selectors. EKS then automatically provisions and manages the necessary\ncompute resources when pods matching the profile are deployed. Thus, Fargate presents the most serverless,\nhands-off approach.\nFor further reading:\nAWS Fargate: https://aws.amazon.com/fargate/\nAmazon EKS: https://aws.amazon.com/eks/\nEKS Fargate Profiles: https://docs.aws.amazon.com/eks/latest/userguide/fargate-profile.html",
    "links": [
      "https://aws.amazon.com/fargate/",
      "https://aws.amazon.com/eks/",
      "https://docs.aws.amazon.com/eks/latest/userguide/fargate-profile.html"
    ]
  },
  {
    "question": "CertyIQ\nA company is launching a new application that requires a structured database to store user profiles, application\nsettings, and transactional data. The database must be scalable with application traffic and must offer backups.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "C",
    "explanation": "The most cost-effective solution is C, using Amazon Aurora Serverless. Aurora Serverless automatically\nscales database capacity based on application needs, eliminating the need to provision and manage database\ninstances. This eliminates the cost of idle capacity during periods of low activity. The automated backup\nfeature of Aurora provides data protection without extra effort.\nOption A involves managing a database on EC2, which requires manual patching, scaling, and backup\nconfiguration, increasing operational overhead and costs. While Spot Instances can be cheaper, their\navailability is unpredictable, risking application downtime.\nOption B, Amazon RDS with on-demand capacity, although easier to manage than self-managed EC2\ninstances, doesn't scale to zero like Aurora Serverless. It incurs costs even during periods of inactivity.\nOption D suggests using a NoSQL database, which doesn't fit the requirement for a structured database for\nuser profiles and transactional data. Using Reserved Instances provides cost savings only if database\nutilization is consistently high. Backing up to S3 Glacier Flexible Retrieval would be a more expensive option\nthan just using Aurora Serverless for this use case.\nTherefore, Aurora Serverless offers the best balance of scalability, cost optimization, automated backups, and\nsuitability for structured data, making it the most cost-effective solution.\nRelevant Links:\nAmazon Aurora Serverless v2\nAmazon RDS\nAmazon EC2 Spot Instances",
    "links": []
  },
  {
    "question": "CertyIQ\nA company runs its legacy web application on AWS. The web application server runs on an Amazon EC2 instance in\nthe public subnet of a VP",
    "options": {
      "C": "Configure the subnet route table to use the gateway VPC endpoint."
    },
    "answer": "A",
    "explanation": "The correct answer is A: Create a gateway VPC endpoint for the S3 bucket that has the necessary permissions\nfor the VPC. Configure the subnet route table to use the gateway VPC endpoint.\nHere's why:\nA gateway VPC endpoint allows resources within a VPC to privately access S3 without traversing the public\ninternet. This aligns directly with the requirement to avoid using the public endpoint for S3 traffic. By creating\na gateway endpoint, traffic destined for S3 from the EC2 instance within the VPC will be routed through the\nAWS network, remaining private and secure. The route table configuration ensures that any traffic destined\nfor S3 from the specified subnet utilizes this endpoint. This approach is cost-effective and straightforward for\nenabling private connectivity to S3.\nOption B is incorrect because S3 buckets are not placed \"inside\" VPCs. S3 is a global service. While you can\nrestrict access to an S3 bucket from a specific VPC, you can't physically relocate it.\nOption C is incorrect because an S3 access point manages access to data in S3 buckets. It doesn't inherently\nprovide private connectivity. While useful for other access control scenarios, it doesn't address the primary\nrequirement of avoiding the public internet.\nOption D is incorrect because AWS Direct Connect is for establishing a dedicated network connection\nbetween your on-premises infrastructure and AWS. This is overkill for the requirement of simply keeping S3\ntraffic within the AWS network. It's far more expensive and complex than using a VPC endpoint. Additionally,\nthe EC2 instance is already within AWS, so Direct Connect does not apply.\nTherefore, option A offers the most efficient and cost-effective solution to meet the requirements.\nRefer to the AWS documentation for more information on VPC endpoints:\nVPC Endpoints\nGateway VPC Endpoints",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is creating a prototype of an ecommerce website on AWS. The website consists of an Application Load\nBalancer, an Auto Scaling group of Amazon EC2 instances for web servers, and an Amazon RDS for MySQL DB\ninstance that runs with the Single-AZ configuration.\nThe website is slow to respond during searches of the product catalog. The product catalog is a group of tables in\nthe MySQL database that the company does not update frequently. A solutions architect has determined that the\nCPU utilization on the DB instance is high when product catalog searches occur.\nWhat should the solutions architect recommend to improve the performance of the website during searches of the\nproduct catalog?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B: Implement an Amazon ElastiCache for Redis cluster to cache the product catalog.\nUse lazy loading to populate the cache.\nHere's why:\nThe problem is high CPU utilization on the RDS for MySQL DB instance during product catalog searches,\nindicating the database is the bottleneck. Caching the catalog data is the most effective way to alleviate this.\nOption B suggests using Amazon ElastiCache for Redis, which is an in-memory data store ideal for caching\nfrequently accessed, relatively static data like a product catalog. Redis offers extremely fast read times\ncompared to querying a database.\nLazy loading ensures the cache is populated only when data is requested, reducing the initial load time and\ncache misses. Instead of loading everything at once, each item is loaded into the cache only when it's first\naccessed. This approach optimizes resource utilization and provides an immediate improvement in\nperformance.\nOption A (Migrate to Redshift) is not ideal for this use case. Amazon Redshift is designed for large-scale data\nwarehousing and analytics, not for serving frequent, low-latency requests like product catalog searches. The\noverhead of data transfer using the COPY command is also not ideal for this scenario.\nOption C (Adding EC2 instances) addresses the wrong bottleneck. The issue is database CPU, not web server\ncapacity. Adding more EC2 instances will only increase the load on the already overloaded database.\nOption D (Multi-AZ and throttling) improves availability, not performance. Multi-AZ provides failover capability\nbut doesn't reduce the load on the primary database instance during normal operation. Throttling queries will\nworsen the user experience by slowing down responses further.\nCaching with ElastiCache and lazy loading directly addresses the database bottleneck and provides a\nsignificant performance boost for product catalog searches.\nFurther Reading:\nAmazon ElastiCache: https://aws.amazon.com/elasticache/\nCaching Strategies: https://aws.amazon.com/caching/\nRedis: https://redis.io/",
    "links": [
      "https://aws.amazon.com/elasticache/",
      "https://aws.amazon.com/caching/",
      "https://redis.io/"
    ]
  },
  {
    "question": "CertyIQ\nA company currently stores 5 TB of data in on-premises block storage systems. The company's current storage\nsolution provides limited space for additional data. The company runs applications on premises that must be able\nto retrieve frequently accessed data with low latency. The company requires a cloud-based storage solution.\nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": {
      "B": "While it's suitable for file-based workloads, it might not provide the low-latency"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the best solution, along with supporting explanations and\nresources:\nThe company needs a cloud-based storage solution for 5 TB of data, requires low latency access to frequently\naccessed data, and aims for operational efficiency.\nOption B (AWS Storage Gateway Volume Gateway with cached volumes): This option provides a local cache\nof frequently accessed data on the on-premises environment while storing the entire dataset in Amazon S3.\nWhen an application requests data, Volume Gateway first checks the local cache. If the data is present (cache\nhit), it's served with low latency. If not (cache miss), it's retrieved from S3, written to the cache, and then\nserved to the application. This approach perfectly balances local performance with cloud storage capacity.\nCached volumes only store a subset of your data locally.\nOption A (Amazon S3 File Gateway): File Gateway is designed for storing files as objects in S3 using\nprotocols like NFS and SMB. While it's suitable for file-based workloads, it might not provide the low-latency\naccess required for block storage applications that expect consistent sub-millisecond response times.\nAlthough it caches recently used data locally, it's generally optimized for throughput over immediate latency\nfor random read/write operations, thus not optimal.\nOption C (AWS Storage Gateway Volume Gateway with stored volumes): Stored volumes store the entire\ndataset on-premises and asynchronously back it up to S3. While it offers local performance, it doesn't directly\nleverage cloud storage for the majority of the data. This approach negates the need for additional space,\nsince the company already has a on-premise solution with limited storage space. Moreover, this solution does\nnot help free up on-premise storage.\nOption D (AWS Storage Gateway Tape Gateway): Tape Gateway is designed for archival and backup\npurposes, emulating a tape library. It is unsuitable for frequently accessed data that requires low latency.\nWhy Option B is the most operationally efficient:\nReduced on-premises storage footprint: Only frequently accessed data is stored locally, minimizing the need\nfor large on-premises storage arrays.\nAutomated data tiering: Volume Gateway automatically manages the cache, moving frequently accessed\ndata to local storage and less frequently accessed data to S3, reducing manual intervention.\nCentralized management: AWS Storage Gateway is a managed service that simplifies storage management,\nmonitoring, and backup tasks.\nCost optimization: Cloud storage (S3) is generally more cost-effective for long-term storage compared to on-\npremises block storage.\nIn summary, AWS Storage Gateway Volume Gateway with cached volumes provides the best combination of\nlow-latency access, cloud-based storage capacity, and operational efficiency, addressing the company's\nspecific requirements.\nAuthoritative Links:\nAWS Storage Gateway: https://aws.amazon.com/storagegateway/\nCached Volumes: https://docs.aws.amazon.com/storagegateway/latest/userguide/cached-volumes.html\nVolume Gateway:\nhttps://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html",
    "links": [
      "https://aws.amazon.com/storagegateway/",
      "https://docs.aws.amazon.com/storagegateway/latest/userguide/cached-volumes.html",
      "https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html"
    ]
  },
  {
    "question": "CertyIQ\nA company operates a food delivery service. Because of recent growth, the company's order processing system is\nexperiencing scaling problems during peak traffic hours. The current architecture includes Amazon EC2 instances\nin an Auto Scaling group that collect orders from an application. A second group of EC2 instances in an Auto\nScaling group fulfills the orders.\nThe order collection process occurs quickly, but the order fulfillment process can take longer. Data must not be\nlost because of a scaling event.\nA solutions architect must ensure that the order collection process and the order fulfillment process can both\nscale adequately during peak traffic hours.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "D",
    "explanation": "The correct solution is D, which involves using Amazon SQS queues for both order collection and fulfillment,\nand scaling Auto Scaling groups based on the number of messages in each queue.\nHere's why:\nDecoupling: SQS decouples the order collection process from the order fulfillment process. This means the\norder collection EC2 instances can quickly add orders to the queue without waiting for fulfillment, preventing\nbottlenecks during peak hours. https://aws.amazon.com/sqs/\nAsynchronous Processing: SQS allows for asynchronous processing. Orders are placed in the queue and\nfulfilled later by the fulfillment EC2 instances. This is crucial since order fulfillment takes longer than order\ncollection.\nScalability: Both Auto Scaling groups can scale independently based on the number of messages in their\nrespective SQS queues. If the order collection queue has a large backlog, the order collection Auto Scaling\ngroup can scale up to handle the load. Similarly, the fulfillment Auto Scaling group can scale based on the\nnumber of messages in the fulfillment queue. https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-\nscaling-target.html\nData Durability: SQS is a fully managed message queuing service. It provides durable storage of messages\nuntil they are processed. This guarantees that no data is lost during scaling events.\nhttps://aws.amazon.com/sqs/features/\nReliable Scaling Trigger: Scaling based on the number of messages in the queue is a direct indicator of\nbacklog and processing needs. It's a more relevant scaling trigger than CPU utilization, which might not\naccurately reflect the queue's workload.\nWhy other options are incorrect:\nA and B: Scaling based only on CPU utilization might not be effective because the order collection process is\nquick and may not necessarily increase CPU load significantly, even during peak times. Option B suggests\ncreating new ASGs via SNS which is an unconventional and unnecessary approach.\nC: Scaling based on notifications that the queues send is not standard. Auto Scaling scales most effectively\nby monitoring metrics and the queue depth is a more accurate metric than general notifications.\nIn summary, using SQS and scaling based on queue depth ensures that the order collection and fulfillment\nprocesses are decoupled, scalable, and data is not lost during peak hours.",
    "links": [
      "https://aws.amazon.com/sqs/",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-",
      "https://aws.amazon.com/sqs/features/"
    ]
  },
  {
    "question": "CertyIQ\nAn online gaming company is transitioning user data storage to Amazon DynamoDB to support the company's\ngrowing user base. The current architecture includes DynamoDB tables that contain user profiles, achievements,\nand in-game transactions.\nThe company needs to design a robust, continuously available, and resilient DynamoDB architecture to maintain a\nseamless gaming experience for users.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {
      "D": "Here's why:"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Here's why:\nHigh Availability and Resilience: DynamoDB Global Tables inherently provide high availability and resilience\nby replicating data across multiple AWS Regions. This means that if one region experiences an outage, the\napplication can failover to another region with minimal disruption.\nSeamless Gaming Experience: Automatic multi-Region replication ensures that user data is synchronized\nacross all regions, providing a consistent and seamless gaming experience regardless of the user's location.\nCost-Effectiveness: While on-demand capacity mode (options A and C) might seem appealing for\nunpredictable workloads, provisioned capacity mode with auto-scaling (option D) can be more cost-effective\nin the long run if the workload has predictable patterns or if you are diligent about setting up proper auto-\nscaling rules. DynamoDB auto-scaling dynamically adjusts provisioned capacity based on actual workload,\noptimizing cost without sacrificing performance.\nGlobal Tables Superiority: Although option B uses DAX for caching and auto scaling, and option C attempts\nto replicate DynamoDB using DynamoDB Streams, these approaches require more manual configuration and\nare not as robust or efficient as using DynamoDB Global Tables. DynamoDB streams would require custom\ncode to handle conflict resolution and data consistency, while Global Tables are designed to handle this out-\nof-the-box. Manual cross-region replication is complex and error-prone.\nDynamoDB Global Tables Efficiency: Global tables provide very low latency access to data for a global user\nbase by allowing them to connect to the nearest AWS region with their replicated data.\nMulti-Region Deployment Necessity: The prompt states that the company needs to design a robust,\ncontinuously available, and resilient DynamoDB architecture, which inherently implies the use of multiple\nregions for fault tolerance. Therefore, options that only use a single region are not suitable.\nIn summary, using DynamoDB Global Tables with provisioned capacity mode and auto-scaling offers the best\ncombination of high availability, resilience, automatic replication, and cost-effectiveness for a global online\ngaming company.\nFurther Research:\nAmazon DynamoDB Global Tables\nAmazon DynamoDB Auto Scaling\nAmazon DynamoDB pricing",
    "links": []
  },
  {
    "question": "CertyIQ\nA company runs its media rendering application on premises. The company wants to reduce storage costs and has\nmoved all data to Amazon S3. The on-premises rendering application needs low-latency access to storage.\nThe company needs to design a storage solution for the application. The storage solution must maintain the\ndesired application performance.\nWhich storage solution will meet these requirements in the MOST cost-effective way?",
    "options": {
      "B": "Configure an Amazon S3 File Gateway to provide storage for the on-premises"
    },
    "answer": "B",
    "explanation": "The optimal solution is B. Configure an Amazon S3 File Gateway to provide storage for the on-premises\napplication.\nHere's why:\nS3 File Gateway: This service provides a local cache for frequently accessed data stored in S3. This local\ncache allows the on-premises rendering application to access data with low latency, meeting the application\nperformance requirement. The File Gateway acts as a bridge, allowing on-premises applications to access S3\ndata as if it were a local file system.\nCost-effectiveness: S3 File Gateway is cost-effective because it only caches frequently accessed data\nlocally. This minimizes the need for large, expensive local storage. The bulk of the data remains in the cost-\noptimized S3 storage tier.\nMountpoint for Amazon S3 (Option A): While Mountpoint allows direct access to S3, it does not provide local\ncaching. This can lead to higher latency for frequently accessed data compared to S3 File Gateway. This\nmakes it less suitable for applications requiring low latency.\nAmazon FSx for Windows File Server (Option C): This solution involves copying data from S3 to FSx, which\nincurs data transfer costs and increases storage costs due to FSx's higher storage prices. Adding an FSx File\nGateway adds unnecessary complexity and cost. It's a more expensive option than S3 File Gateway for simply\nproviding low-latency access to data stored in S3.\nOn-premises file server with S3 API (Option D): This approach requires managing and maintaining a local file\nserver, adding operational overhead. Furthermore, direct S3 API calls from the application may not provide\nthe low-latency access the application needs.\nIn summary, the S3 File Gateway provides a cost-effective balance between low-latency access for on-\npremises applications and leveraging the scalability and cost-effectiveness of S3. It minimizes the need for\nexpensive local storage by caching frequently accessed data.\nAuthoritative Links:\nAWS Storage Gateway - S3 File Gateway: https://aws.amazon.com/storagegateway/file-gateway/\nMountpoint for Amazon S3: https://aws.amazon.com/blogs/aws/mountpoint-for-amazon-s3-a-high-\nthroughput-file-client/",
    "links": [
      "https://aws.amazon.com/storagegateway/file-gateway/",
      "https://aws.amazon.com/blogs/aws/mountpoint-for-amazon-s3-a-high-"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts its enterprise resource planning (ERP) system in the us-east-1 Region. The system runs on\nAmazon EC2 instances. Customers use a public API that is hosted on the EC2 instances to exchange information\nwith the ERP system. International customers report slow API response times from their data centers.\nWhich solution will improve response times for the international customers MOST cost-effectively?",
    "options": {
      "B": "Set up an Amazon CloudFront distribution in front of the API. Configure the",
      "A": "AWS Direct Connect: Direct Connect provides dedicated network connections, which are expensive and",
      "C": "AWS Global Accelerator: Global Accelerator improves availability and performance for applications with",
      "D": "AWS Site-to-Site VPN: Setting up VPN tunnels to each customer's network is complex, expensive, and"
    },
    "answer": "B",
    "explanation": "The correct answer is B. Set up an Amazon CloudFront distribution in front of the API. Configure the\nCachingOptimized managed cache policy to provide improved cache efficiency.\nHere's why:\nCloudFront's Global Reach: CloudFront is a content delivery network (CDN) with edge locations distributed\nglobally. By caching API responses closer to international customers, it significantly reduces latency and\nimproves response times. This is a core function of CDNs.\nCost-Effectiveness: CloudFront is generally more cost-effective than options like Direct Connect or Global\nAccelerator for this specific use case. It leverages caching to reduce the load on the origin EC2 instances,\npotentially lowering compute costs.\nCachingOptimized Policy: The CachingOptimized managed cache policy is designed to improve cache\nefficiency, balancing cache hit ratio and time-to-live (TTL) to optimize content delivery. This is suitable\nbecause API responses, at least some, can often be cached.\nWhy other options are less suitable:\nA. AWS Direct Connect: Direct Connect provides dedicated network connections, which are expensive and\nprimarily used for hybrid cloud scenarios where high bandwidth and consistent network performance are\ncrucial between a customer's infrastructure and AWS, not for delivering API responses to multiple customers.\nSetting up Direct Connect for each customer is neither scalable nor cost-effective.\nC. AWS Global Accelerator: Global Accelerator improves availability and performance for applications with\nusers worldwide by routing traffic to the optimal endpoint (Region). While it can improve performance, it\nmainly optimizes TCP/UDP traffic and doesn't inherently cache content like CloudFront. It is less cost-\neffective than CloudFront for this specific caching scenario, and its primary benefit lies in failover and traffic\noptimization across Regions rather than caching closer to users.\nD. AWS Site-to-Site VPN: Setting up VPN tunnels to each customer's network is complex, expensive, and\ncreates unnecessary overhead. VPNs are designed for secure connectivity between networks, not for\ndelivering API responses. It does not provide caching benefits.\nIn summary, CloudFront leverages caching and a global network to improve API response times for\ninternational customers most cost-effectively.\nSupporting Documentation:\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nCloudFront Managed Cache Policies:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-managed-cache-policies.html\nAWS Global Accelerator: https://aws.amazon.com/global-accelerator/\nAWS Direct Connect: https://aws.amazon.com/directconnect/",
    "links": [
      "https://aws.amazon.com/cloudfront/",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-managed-cache-policies.html",
      "https://aws.amazon.com/global-accelerator/",
      "https://aws.amazon.com/directconnect/"
    ]
  },
  {
    "question": "CertyIQ\nA company tracks customer satisfaction by using surveys that the company hosts on its website. The surveys\nsometimes reach thousands of customers every hour. Survey results are currently sent in email messages to the\ncompany so company employees can manually review results and assess customer sentiment.\nThe company wants to automate the customer survey process. Survey results must be available for the previous 12\nmonths.\nWhich solution will meet these requirements in the MOST scalable way?",
    "options": {},
    "answer": "A",
    "explanation": "Option A provides the most scalable and cost-effective solution for automating the customer survey process\nwhile meeting the given requirements.\nHere's why:\nScalability: Amazon API Gateway and Amazon SQS are highly scalable services that can handle thousands of\nrequests per hour without manual intervention. API Gateway handles the initial influx of requests, while SQS\nacts as a buffer, decoupling the survey submission from the processing of results. This prevents any potential\noverload on the backend systems.\nSentiment Analysis: Amazon Comprehend is specifically designed for natural language processing tasks,\nincluding sentiment analysis. It offers accurate and efficient analysis of the survey results. Amazon\nRekognition (option C) is for image and video analysis, and Amazon Lex (option D) is for building\nconversational interfaces, neither of which are appropriate for this scenario.\nData Storage: Amazon DynamoDB is a NoSQL database that offers high performance and scalability. It's well-\nsuited for storing the survey results and sentiment analysis data. The TTL (Time To Live) attribute in\nDynamoDB is an efficient and automatic way to ensure data is only stored for the required 12 months (365\ndays).\nLambda Integration: AWS Lambda allows for serverless execution of code. Using Lambda to poll the SQS\nqueue, process survey results, and save data to DynamoDB ensures efficient resource utilization and\nautomatic scaling based on demand.\nDecoupling: Using SQS decouples the survey submission (via API Gateway) from the sentiment analysis and\ndata storage (performed by Lambda and DynamoDB). This makes the solution more resilient and able to\nhandle traffic spikes.\nOption B is less scalable as it relies on a single EC2 instance to handle API requests, potentially creating a\nbottleneck. It also introduces the overhead of managing an EC2 instance.\nOption C uses S3 for sentiment analysis, which is incorrect. Rekognition is for image and video analysis, not\ntext-based sentiment analysis.\nOption D uses Amazon Lex, which is for building conversational interfaces, not sentiment analysis of survey\ndata. Lex is not appropriate for analyzing text data.\nAuthoritative Links:\nAmazon API Gateway: https://aws.amazon.com/api-gateway/\nAmazon SQS: https://aws.amazon.com/sqs/\nAWS Lambda: https://aws.amazon.com/lambda/\nAmazon Comprehend: https://aws.amazon.com/comprehend/\nAmazon DynamoDB: https://aws.amazon.com/dynamodb/\nDynamoDB TTL: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html",
    "links": [
      "https://aws.amazon.com/api-gateway/",
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/comprehend/",
      "https://aws.amazon.com/dynamodb/",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html"
    ]
  },
  {
    "question": "CertyIQ\nA company uses AWS Systems Manager for routine management and patching of Amazon EC2 instances. The EC2\ninstances are in an IP address type target group behind an Application Load Balancer (ALB).\nNew security protocols require the company to remove EC2 instances from service during a patch. When the\ncompany attempts to follow the security protocol during the next patch, the company receives errors during the\npatching window.\nWhich combination of solutions will resolve the errors? (Choose two.)",
    "options": {
      "B": "While State Manager could periodically run a patching document, it doesn't"
    },
    "answer": "C",
    "explanation": "Let's break down why options C and D are the correct choices, and why the others are not.\nThe core issue is the company needs to patch EC2 instances behind an ALB while adhering to a new security\nprotocol that requires instances to be removed from service during the patching process. The errors they are\nexperiencing likely stem from interrupting the ALB's health checks when the instance is taken offline for\npatching. The ALB sees an unhealthy instance and might try to send traffic to it, or declare it out of service\nprematurely if the drain process isn't handled gracefully.\nWhy C is correct:\nAWSEC2-PatchLoadBalancerInstance is a pre-built AWS Systems Manager Automation document specifically\ndesigned to handle patching EC2 instances behind an ALB. It orchestrates the process of taking an instance\nout of service (by deregistering it from the target group), patching it, and then putting it back in service (by re-\nregistering it). This ensures minimal disruption to traffic flow.\nWhy D is correct:\nSystems Manager Maintenance Windows provide a scheduled time frame for performing tasks like patching.\nCrucially, you can configure tasks within a Maintenance Window to execute the AWSEC2-\nPatchLoadBalancerInstance Automation document described above, thus automating the entire process of\nremoving instances, patching them, and returning them to service within a defined timeframe. Also,\nMaintenance Windows allow you to configure pre- and post-tasks, so we can easily orchestrate the\nderegistration and registration of our instance.\nWhy A is incorrect:\nChanging the target type of the target group doesn't address the problem of patching. Whether the target\ntype is IP address or instance ID, the underlying issue of ALB health checks failing during patching remains.\nIt's more about how the patching process interacts with the load balancer.\nWhy B is incorrect:\nThe existing Systems Manager document isn't optimized to handle instances behind an ALB if it doesn't\ninclude steps to deregister and reregister instances. The prompt states that the company is experiencing\nerrors when trying to patch based on the security protocol.\nWhy E is incorrect:\nState Manager is for maintaining a desired configuration state and isn't the appropriate tool for orchestrating\npatching behind an ALB. While State Manager could periodically run a patching document, it doesn't\ninherently handle the graceful removal and re-addition of instances to the ALB target group during the\npatching process. Furthermore, the state manager is not suited for temporary task.\nIn summary, the correct solution combines the automation provided by AWSEC2-PatchLoadBalancerInstance\nand the scheduling and orchestration capabilities of Maintenance Windows to ensure patching is performed\ngracefully with minimal impact on application availability.\nAuthoritative Links:\nAWS Systems Manager Automation: https://docs.aws.amazon.com/systems-\nmanager/latest/userguide/systems-manager-automation.html\nAWSEC2-PatchLoadBalancerInstance Document: Look this document up within the AWS Management\nConsole under Systems Manager > Automation > Owned By Amazon\nAWS Systems Manager Maintenance Windows: https://docs.aws.amazon.com/systems-\nmanager/latest/userguide/systems-manager-maintenance.html",
    "links": [
      "https://docs.aws.amazon.com/systems-",
      "https://docs.aws.amazon.com/systems-"
    ]
  },
  {
    "question": "CertyIQ\nA medical company wants to perform transformations on a large amount of clinical trial data that comes from\nseveral customers. The company must extract the data from a relational database that contains the customer data.\nThen the company will transform the data by using a series of complex rules. The company will load the data to\nAmazon S3 when the transformations are complete.\nAll data must be encrypted where it is processed before the company stores the data in Amazon S3. All data must\nbe encrypted by using customer-specific keys.\nWhich solution will meet these requirements with the LEAST amount of operational effort?",
    "options": {},
    "answer": "C",
    "explanation": "The correct answer is C because it offers the least operational overhead while meeting all security and\nfunctional requirements. Here's why:\nData Extraction and Transformation: AWS Glue is designed for ETL (Extract, Transform, Load) operations. It\nprovides a serverless, fully managed environment for running data transformation jobs, which perfectly aligns\nwith the requirement to transform clinical trial data. Amazon EMR, while capable of ETL, is more suited for big\ndata processing and analytics, making it more complex and operationally heavier for simple data\ntransformations.\nCustomer-Specific Encryption: The requirement of customer-specific keys necessitates the use of KMS (Key\nManagement Service). CSE (Client-Side Encryption) allows the company to manage the encryption process\nwithin their environment, using KMS keys they control. This allows for granular control over encryption for\neach customer. Server-Side Encryption (SSE) options, like SSE-S3, use a single key managed by AWS S3,\nwhich is insufficient for the customer-specific encryption mandate. CSE-Custom requires managing the\nencryption process completely outside AWS KMS, adding operational burden.\nEncryption in Processing: CSE-KMS allows data to be encrypted before it's written to S3, ensuring encryption\nthroughout the processing phase. Server-side encryption only protects data at rest in S3. The question\nspecifically states data must be encrypted where it is processed, so options A and D are not suitable.\nLeast Operational Effort: Creating individual Glue jobs for each customer is operationally simpler than\nmanaging individual EMR clusters. Glue is serverless and managed, while EMR requires cluster provisioning,\nmanagement, and scaling. Using AWS KMS also reduces the burden of managing encryption keys outside\nAWS infrastructure.\nSecurity Configuration in Glue: Glue's security configurations enable associating KMS keys to each job for\ncustomer-specific encryption. By attaching a different security configuration (and hence KMS key) to each\nGlue job, the company can ensure each customer's data is encrypted with their dedicated key.\nIn summary, option C using AWS Glue with client-side encryption using KMS keys provides a managed,\nsecure, and scalable solution to meet all the requirements with the least operational effort.\nFurther research:\nAWS Glue: https://aws.amazon.com/glue/\nAWS KMS: https://aws.amazon.com/kms/\nEncryption in Amazon S3: https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html\nAWS Glue Security Configurations: https://docs.aws.amazon.com/glue/latest/dg/encryption-security-\nconfiguration.html",
    "links": [
      "https://aws.amazon.com/glue/",
      "https://aws.amazon.com/kms/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html",
      "https://docs.aws.amazon.com/glue/latest/dg/encryption-security-"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts a website analytics application on a single Amazon EC2 On-Demand Instance. The analytics\napplication is highly resilient and is designed to run in stateless mode.\nThe company notices that the application is showing signs of performance degradation during busy times and is\npresenting 5xx errors. The company needs to make the application scale seamlessly.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "D",
    "explanation": "The correct answer is D because it provides the most cost-effective and scalable solution for the described\nscenario.\nHere's a breakdown:\nRequirement of Seamless Scaling: The company needs the application to scale automatically based on\ndemand. Auto Scaling groups (ASGs) are designed precisely for this purpose. They automatically launch or\nterminate EC2 instances based on defined metrics (like CPU utilization).\nCost-Effectiveness: Spot Instances offer significant cost savings compared to On-Demand Instances. By\nconfiguring the launch template with Spot Fleet, the ASG can leverage these cheaper instances.\nStateless Application: The fact that the application is stateless is crucial. This allows instances to be\nlaunched and terminated without data loss or impact on application functionality.\nLoad Balancing: The Application Load Balancer (ALB) distributes incoming traffic evenly across the healthy\nEC2 instances within the ASG, preventing any single instance from becoming a bottleneck.\nAMIs and Launch Templates: Creating an AMI allows for consistent deployments. The launch template\ndefines the configuration of each instance launched by the ASG (AMI ID, instance type, security groups, etc.).\nWhy other options are less suitable:\nA & B: Manually creating a second EC2 instance and using either an ALB or Route 53 weighted routing\nprovides redundancy but doesn't scale automatically. It requires manual intervention to add or remove\ninstances as demand fluctuates, which isn't seamless.\nC: While this option attempts to address performance by changing the instance type, it's not ideal for several\nreasons. First, stopping and resizing an instance is disruptive. Second, it doesn't provide true scaling  it only\nmakes the existing instance larger. Third, it doesn't address the need for distributing the load across multiple\ninstances. The stopping of an instance is also a bad practice.\nIn summary: Option D utilizes Auto Scaling with Spot Instances and an ALB to achieve automatic scaling, cost\noptimization, and load distribution, effectively addressing the company's requirements in the most cost-\neffective manner.\nAuthoritative Links:\nAmazon EC2 Auto Scaling: https://aws.amazon.com/autoscaling/\nAmazon EC2 Spot Instances: https://aws.amazon.com/ec2/spot/\nApplication Load Balancer: https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\nLaunch Templates: https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-templates.html",
    "links": [
      "https://aws.amazon.com/autoscaling/",
      "https://aws.amazon.com/ec2/spot/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-templates.html"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an environment where data is stored in an Amazon S3 bucket. The objects are accessed\nfrequently throughout the day. The company has strict da ta encryption requirements for data that is stored in the\nS3 bucket. The company currently uses AWS Key Management Service (AWS KMS) for encryption.\nThe company wants to optimize costs associated with encrypting S3 objects without making additional calls to\nAWS KMS.\nWhich solution will meet these requirements?",
    "options": {},
    "answer": "B",
    "explanation": "The company needs to encrypt data at rest in S3 using KMS, but wants to optimize costs. The core issue is\nreducing the number of KMS calls since these calls incur charges.\nOption A (SSE-S3) doesn't satisfy the requirement of using AWS KMS for encryption. While it offers server-\nside encryption, it uses S3-managed keys, not KMS keys. Therefore, it is not the correct solution.\nOption B (SSE-KMS with S3 Bucket Keys) is the correct solution. S3 Bucket Keys are designed to reduce the\ncost of SSE-KMS. By enabling a bucket key, S3 reduces the request traffic to AWS KMS by generating a\nbucket-level key that is used to encrypt objects in the bucket. The bucket key reduces the calls to KMS to\nunwrap the data encryption key, which is a costly operation. It still uses KMS for encryption, fulfilling the\nencryption requirement, but optimizes cost by reducing KMS API calls.\nOption C (Client-side encryption with KMS) moves the encryption process to the client side. While it uses KMS,\nthe client is responsible for encrypting the data before uploading it to S3. This does not optimize costs\nbecause the client must still make KMS calls to encrypt the data, and might increase complexity. It also does\nnot leverage native S3 features for encryption at rest.\nOption D (SSE-C with keys in KMS) is incorrect because SSE-C requires the user to provide their own\nencryption keys. Although the user suggests storing it in KMS, the solution requires the user to manage the\nkeys. Amazon S3 does not store the encryption key you provide. Instead, Amazon S3 stores a keyed-hash\nmessage authentication code (HMAC) of the encryption key to validate future requests. This method will not\nreduce costs.\nTherefore, SSE-KMS with S3 Bucket Keys is the best solution because it offers the required encryption with\nKMS and optimizes costs by reducing KMS calls.\nRelevant documentation:\nUsing server-side encryption with AWS KMS keys (SSE-KMS)\nReducing the cost of SSE-KMS with S3 Bucket Keys",
    "links": []
  },
  {
    "question": "CertyIQ\nA company runs multiple workloads on virtual machines (VMs) in an on-premises data center. The company is\nexpanding rapidly. The on-premises data center is not able to scale fast enough to meet business needs. The\ncompany wants to migrate the workloads to AWS.\nThe migration is time sensitive. The company wants to use a lift-and-shift strategy for non-critical workloads.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "options": {
      "B": "Use AWS Application Migration Service. Install the AWS Replication Agent on the VMs. AWS MGN is",
      "C": "Complete the initial replication of the VMs. Launch test instances to perform acceptance tests on the",
      "D": "Stop all operations on the VMs. Launch a cutover instance. This step signifies the final migration stage. By",
      "A": "Use the AWS Schema Conversion Tool (AWS SCT) to collect data about the VMs. AWS SCT is designed"
    },
    "answer": "B",
    "explanation": "The chosen solution (BCD) effectively outlines a lift-and-shift migration strategy using AWS Application\nMigration Service (MGN) to meet the company's needs.\nHere's why:\nB. Use AWS Application Migration Service. Install the AWS Replication Agent on the VMs. AWS MGN is\nspecifically designed for lift-and-shift migrations, and it replicates on-premises servers to AWS. Installing the\nagent on the VMs is the first step to initiating replication. https://aws.amazon.com/application-migration-\nservice/\nC. Complete the initial replication of the VMs. Launch test instances to perform acceptance tests on the\nVMs. After initial replication, it's crucial to test the migrated instances in AWS to ensure they function as\nexpected. This verifies the migration process and allows for adjustments before the final cutover.\nD. Stop all operations on the VMs. Launch a cutover instance. This step signifies the final migration stage. By\nstopping operations on the original VMs, the company minimizes data loss and ensures consistency when\nlaunching the cutover instances in AWS. This minimizes downtime.\nHere's why the other options are incorrect:\nA. Use the AWS Schema Conversion Tool (AWS SCT) to collect data about the VMs. AWS SCT is designed\nfor database migrations, not VM migrations. Since the scenario describes a lift-and-shift of workloads on VMs,\nSCT is not applicable here.\nE. Use AWS App2Container (A2C) to collect data about the VMs. AWS App2Container is suitable for\nmodernizing applications by containerizing them. The company wants a lift-and-shift strategy, not\ncontainerization.\nF. Use AWS Database Migration Service (AWS DMS) to migrate the VMs. AWS DMS is for database\nmigration. The scenario involves the migration of whole VMs, rendering DMS an unsuitable tool for this\npurpose.\nThe chosen combination leverages the purpose-built AWS MGN service for efficient and rapid VM migration,\naligned with a lift-and-shift approach. Testing and a controlled cutover ensures a smooth transition.",
    "links": [
      "https://aws.amazon.com/application-migration-"
    ]
  },
  {
    "question": "CertyIQ\nA company hosts an application in a private subnet. The company has already integrated the application with\nAmazon Cognito. The company uses an Amazon Cognito user pool to authenticate users.\nThe company needs to modify the application so the application can securely store user documents in an Amazon\nS3 bucket.\nWhich combination of steps will securely integrate Amazon S3 with the application? (Choose two.)",
    "options": {},
    "answer": "A",
    "explanation": "Here's a detailed justification for why options A and C are the correct choices, and why the others are\nincorrect:\nWhy A is correct: Create an Amazon Cognito identity pool to generate secure Amazon S3 access tokens for\nusers when they successfully log in.\nAmazon Cognito identity pools (now part of Cognito Federated Identities) are specifically designed to grant\ntemporary AWS credentials to users, including those authenticated through Cognito user pools or other\nidentity providers. When a user successfully authenticates, the application can use the Cognito identity pool\nto obtain temporary credentials with specific permissions defined in IAM roles. These credentials can then be\nused to access the S3 bucket securely. This approach avoids embedding long-term credentials in the\napplication or directly using the user pool for S3 access, which is a security best practice.\nReference: https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html\nWhy C is correct: Create an Amazon S3 VPC endpoint in the same VPC where the company hosts the\napplication.\nSince the application is in a private subnet with no direct internet access, it needs a way to communicate with\nS3 without traversing the public internet. An S3 VPC endpoint provides a private connection between the VPC\nand S3. Traffic to S3 from within the VPC will then travel over the AWS network and remain within the AWS\ninfrastructure, enhancing security and reducing latency. Using a VPC endpoint ensures that the application\ncan access S3 even without a NAT gateway or internet gateway.\nReference: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\nWhy B is incorrect: Use the existing Amazon Cognito user pool to generate Amazon S3 access tokens for\nusers when they successfully log in.\nCognito user pools are primarily for authentication and user management. While user pools can be integrated\nwith identity pools to provide credentials for AWS resources, user pools themselves do not directly generate\nS3 access tokens. Identity pools are the AWS service component designed to fulfill this role.\nWhy D is incorrect: Create a NAT gateway in the VPC where the company hosts the application. Assign a\npolicy to the S3 bucket to deny any request that is not initiated from Amazon Cognito.\nWhile a NAT gateway allows instances in a private subnet to initiate outbound traffic to the internet (and\ntherefore S3), it doesn't inherently provide a secure, role-based access control mechanism to S3 based on\nCognito user identity. Option A, using an identity pool, is the correct way to leverage Cognito for generating\nsecure S3 access credentials. The second part of option D (deny any request that is not initiated from Cognito)\nis impractical. S3 bucket policies cannot directly filter based on the originator of a request being \"Amazon\nCognito\" itself. They grant or deny permissions based on the temporary credentials provided via the IAM roles\nassociated with the Cognito identity pool.\nWhy E is incorrect: Attach a policy to the S3 bucket that allows access only from the users' IP addresses.\nThis approach is highly impractical and insecure for several reasons. User IP addresses can change\nfrequently, making the policy difficult to maintain. More importantly, it's not reliable because the application\nis in a private subnet, so the source IP of the request seen by S3 would be the NAT gateway's public IP (if a\nNAT gateway were used) or the VPC endpoint's IP address. Finally, tying S3 access directly to user IP\naddresses doesn't provide a granular, role-based access control solution, which is what Cognito identity pools\noffer.",
    "links": [
      "https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has a three-tier web application that processes orders from customers. The web tier consists of\nAmazon EC2 instances behind an Application Load Balancer. The processing tier consists of EC2 instances. The\ncompany decoupled the web tier and processing tier by using Amazon Simple Queue Service (Amazon SQS). The\nstorage layer uses Amazon DynamoD",
    "options": {
      "B": "Caching"
    },
    "answer": "D",
    "explanation": "Here's a detailed justification for why option D is the best solution, along with supporting concepts and links:\nThe problem indicates the processing tier EC2 instances are overloaded during peak times, causing order\nprocessing delays and queue buildup in SQS. The key is to automatically scale the processing tier based on\nthe workload queued in SQS.\nOption D suggests using an EC2 Auto Scaling target tracking policy driven by the\nApproximateNumberOfMessages attribute of the SQS queue. This is the most effective solution because it\ndirectly addresses the bottleneck. The ApproximateNumberOfMessages attribute reflects the number of\nmessages waiting to be processed in the SQS queue. By configuring Auto Scaling to scale out the processing\ntier when this number exceeds a target value, the system proactively adds more processing capacity to\nhandle the backlog. This ensures timely processing of orders and prevents the queue from filling up\nexcessively. Target tracking policies automatically adjust the number of instances to maintain the desired\nmetric value, simplifying scaling management.\nOption A suggests scheduled scaling based on CPU utilization. While CPU utilization is a good indicator of\nload, scheduled scaling is reactive and relies on predictable patterns, which the problem states are\nunpredictable. Also scaling based on CPU after utilization reaches 100% will cause delay.\nOption B introduces ElastiCache for Redis in front of DynamoDB. While caching can improve performance for\nread-heavy workloads, the bottleneck is in the processing tier, not the data retrieval from DynamoDB. Caching\nwon't solve the CPU overload issue.\nOption C adds CloudFront to the web tier. CloudFront is a Content Delivery Network (CDN) that caches static\ncontent closer to users. Since the problem describes order processing delays, caching static web assets won't\naddress the root cause of the problem.\nIn summary, option D provides the most direct and adaptive solution by scaling the processing tier based on\nthe actual workload pending in the SQS queue, mitigating the processing delays and preventing excessive\nqueue buildup. It dynamically adjusts capacity to meet fluctuating demand.Supporting Links:\nAmazon EC2 Auto Scaling Target Tracking Scaling Policies:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\nAmazon SQS ApproximateNumberOfMessages Metric:\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-available-\ncloudwatch-metrics.html",
    "links": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-available-"
    ]
  },
  {
    "question": "CertyIQ\nA company's production environment consists of Amazon EC2 On-Demand Instances that run constantly between\nMonday and Saturday. The instances must run for only 12 hours on Sunday and cannot tolerate interruptions. The\ncompany wants to cost-optimize the production environment.\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": {},
    "answer": "A",
    "explanation": "Here's a detailed justification for why option A is the most cost-effective solution:\nThe core goal is to minimize costs while ensuring the production environment runs as required. The instances\nhave two distinct usage patterns: consistent operation Monday through Saturday, and limited operation on\nSunday.\nFor the Monday-Saturday usage, Standard Reserved Instances offer the best cost savings. They provide a\nsignificant discount compared to On-Demand pricing in exchange for a one- or three-year commitment.\nBecause the workload runs constantly during these days, committing to Reserved Instances is beneficial.\nFor the 12-hour Sunday usage, Scheduled Reserved Instances are the most appropriate. These instances\nallow you to reserve capacity on a recurring schedule, perfectly matching the need for specific time windows\neach week. They guarantee capacity availability and come with a discount compared to On-Demand, but\nwithout requiring you to pay for the instance for the entire day.\nOptions C and D are less desirable because Spot Instances are prone to interruption. The question explicitly\nstates that instances cannot tolerate interruptions. Therefore, relying on Spot Instances would violate a key\nrequirement of the problem. While Spot Instances are generally the cheapest option for compute, the\npotential for interruption makes them unsuitable for this use case.\nOption B is less ideal because Convertible Reserved Instances, while offering flexibility to change instance\ntypes and families, are more expensive than Standard Reserved Instances. For workloads with predictable\nresource requirements, as is the case for Monday-Saturday, the additional flexibility of Convertible Reserved\nInstances is unnecessary and introduces extra cost.\nIn summary, Option A strikes the best balance between cost optimization and reliability by using the most\ncost-effective reserved instance type for each distinct usage pattern. Scheduled Reserved Instances are used\nwhere availability is needed for only a defined time window and Standard Reserved Instances are used where\nthere is continuous usage.\nHere are some helpful links for further research:\nReserved Instances: https://aws.amazon.com/ec2/pricing/reserved-instances/\nScheduled Instances: https://aws.amazon.com/ec2/purchasing-options/scheduled-instances/\nSpot Instances: https://aws.amazon.com/ec2/spot/",
    "links": [
      "https://aws.amazon.com/ec2/pricing/reserved-instances/",
      "https://aws.amazon.com/ec2/purchasing-options/scheduled-instances/",
      "https://aws.amazon.com/ec2/spot/"
    ]
  },
  {
    "question": "CertyIQ\nA digital image processing company wants to migrate its on-premises monolithic application to the AWS Cloud.\nThe company processes thousands of images and generates large files as part of the processing workflow.\nThe company needs a solution to manage the growing number of image processing jobs. The solution must also\nreduce the manual tasks in the image processing workflow. The company does not want to manage the underlying\ninfrastructure of the solution.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "B",
    "explanation": "The correct answer is B because it offers the least operational overhead and best aligns with the described\nrequirements. Here's why:\nAWS Batch eliminates the need to manage underlying compute infrastructure for image processing. Batch\nautomatically provisions and scales compute resources based on job requirements. This is crucial as the\ncompany doesn't want to manage infrastructure. (https://aws.amazon.com/batch/)\nAWS Step Functions provides a serverless orchestration service to manage the complex image processing\nworkflow. Step Functions allows for defining workflows as state machines, simplifying coordination and\nmonitoring of image processing steps without manual intervention. Step Functions supports error handling\nand retries. (https://aws.amazon.com/step-functions/)\nAmazon S3 offers scalable, durable, and cost-effective object storage for storing the processed files. S3 is\nwell-suited for large files generated by image processing. It eliminates the need for managing file servers or\ncomplex file systems. (https://aws.amazon.com/s3/)\nOption A is less suitable because managing ECS clusters, even with Spot Instances, still incurs operational\noverhead related to container management and instance configuration. SQS provides messaging, but Step\nFunctions provides orchestration capabilities specifically designed for defining and managing complex\nworkflows. EFS, while a shared filesystem, isn't as cost-effective or scalable for storing a large number of\nfiles as S3.\nOption C is less suitable because while Lambda is serverless, it may have limitations in terms of execution\ntime and memory for large image processing tasks. It mentions EC2 Spot Instances together with Lambda\nfunctions which does not make sense. FSx introduces complexities in managing file systems.\nOption D is least suitable because it requires managing EC2 instances, which contradicts the requirement of\nminimizing operational overhead. EBS is attached to a single EC2 instance so it is not ideal for storing a very\nlarge number of images.",
    "links": [
      "https://aws.amazon.com/batch/)",
      "https://aws.amazon.com/step-functions/)",
      "https://aws.amazon.com/s3/)"
    ]
  },
  {
    "question": "CertyIQ\nA company's image-hosting website gives users around the world the ability to up load, view, and download images\nfrom their mobile devices. The company currently hosts the static website in an Amazon S3 bucket.\nBecause of the website's growing popularity, the website's performance has decreased. Users have reported\nlatency issues when they upload and download images.\nThe company must improve the performance of the website.\nWhich solution will meet these requirements with the LEAST implementation effort?",
    "options": {},
    "answer": "A",
    "explanation": "Option A is the most efficient solution because it leverages existing AWS services optimized for content\ndelivery and transfer acceleration with minimal configuration. Amazon CloudFront is a content delivery\nnetwork (CDN) that caches static content like images in edge locations worldwide, significantly reducing\nlatency for downloads. S3 Transfer Acceleration utilizes optimized network paths and the AWS backbone\nnetwork to accelerate uploads to S3 buckets, improving performance for users regardless of their location.\nOption B is overly complex and requires significant infrastructure management. Migrating the static website\nto EC2 instances and load balancing it across multiple regions involves setting up and maintaining servers,\nconfiguring application deployment, and managing networking, which adds operational overhead. AWS Global\nAccelerator, while beneficial, is an overkill for a simple static website hosted in S3.\nOption C introduces unnecessary complexity and cost. Creating S3 buckets in multiple regions and\nconfiguring replication rules adds administrative overhead and cost. The manual redirection of downloads to\nthe closest S3 bucket is complex to implement and maintain. CloudFront already provides global content\ndelivery and caching automatically.\nOption D's configuration is not the intended use case for AWS Global Accelerator. Global Accelerator is\ndesigned for dynamic applications, like gaming applications or streaming media, that need high availability\nand consistent performance. S3 Transfer Acceleration addresses the upload performance improvement\nneeded more effectively.\nTherefore, Option A provides the best balance of performance improvement, ease of implementation, and\ncost-effectiveness by using existing AWS services specialized for content delivery and transfer acceleration.\nRelevant Documentation:\nAmazon CloudFront: https://aws.amazon.com/cloudfront/\nAmazon S3 Transfer Acceleration: https://aws.amazon.com/s3/transfer-acceleration/\nAWS Global Accelerator: https://aws.amazon.com/global-accelerator/",
    "links": [
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/s3/transfer-acceleration/",
      "https://aws.amazon.com/global-accelerator/"
    ]
  },
  {
    "question": "CertyIQ\nA company runs an application in a private subnet behind an Application Load Balancer (ALB) in a VP",
    "options": {
      "C": "Authoritative Links:"
    },
    "answer": "B",
    "explanation": "The most cost-effective solution to ensure traffic from the application to Amazon S3 doesn't traverse the\ninternet while adhering to the security policy is option B: configuring an S3 gateway endpoint and updating\nthe VPC route table.\nHere's why:\nGateway Endpoints: S3 gateway endpoints are designed to provide private connectivity to S3 within your\nVPC. They are free to use and do not require an internet gateway, NAT device, or VPN connection to access\nS3. This aligns directly with the requirement of not traversing the internet.\nCost-Effectiveness: Gateway endpoints do not incur data processing or hourly charges, making them the\nmost cost-effective option for private S3 access. This is because the data transfer within the AWS network is\nalready paid for and gateway endpoints are free of charge.\nRoute Table Integration: Gateway endpoints work by adding a route to your VPC route table. This route\ndirects traffic destined for S3 to the gateway endpoint, ensuring traffic stays within the AWS network.\nOther Options:\nOption A (Interface Endpoint): While interface endpoints also provide private connectivity, they use AWS\nPrivateLink and incur hourly and data processing charges, making them less cost-effective than gateway\nendpoints for S3.\nOption C (Bucket Policy with NAT Gateway IP): This approach involves allowing traffic from the NAT\ngateway's Elastic IP to the S3 bucket. While it avoids internet traversal, it's less secure and manageable. If the\nNAT gateway is replaced, the IP changes, potentially disrupting access. Also, bucket policies can become\ncomplex to manage at scale.\nOption D (Second NAT Gateway): Creating a second NAT gateway increases costs and doesn't address the\ncore requirement of keeping S3 traffic within the AWS network. It's primarily for redundancy, not for private\nS3 access.\nIn summary, using an S3 gateway endpoint offers the optimal balance of security, cost-effectiveness, and\nease of management for accessing S3 privately from within a VPC.\nAuthoritative Links:\nAWS VPC Endpoints: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\nS3 Gateway Endpoints: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html",
    "links": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html"
    ]
  },
  {
    "question": "CertyIQ\nA company has an application that runs on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on\nAmazon EC2 instances. The application has a UI that uses Amazon DynamoDB and data services that use Amazon\nS3 as part of the application deployment.\nThe company must ensure that the EKS Pods for the UI can access only Amazon DynamoDB and that the EKS Pods\nfor the data services can access only Amazon S3. The company uses AWS Identity and Access Management (IAM).\nWhich solution meals these requirements?",
    "options": {
      "B": "The IAM role for the data services service",
      "D": "Here's why:"
    },
    "answer": "D",
    "explanation": "The correct answer is D. Here's why:\nThe requirement is to provide fine-grained access control for EKS pods to AWS services (DynamoDB and S3).\nPods running the UI should only access DynamoDB, and pods running data services should only access S3.\nThis necessitates a mechanism that allows granting IAM permissions to pods specifically, not just the\nunderlying EC2 instances.\nOption A is incorrect because attaching IAM policies to the EC2 instance profile grants access to all pods\nrunning on that instance. RBAC is for Kubernetes API access, not AWS resource access. Therefore, RBAC\ncan't be used to restrict a pod's access to DynamoDB or S3 at the IAM level.\nOption B is incorrect because it is not possible to attach IAM policies directly to Kubernetes pods. IAM policies\nare associated with IAM roles, which are assumed by entities like EC2 instances or, crucially, Kubernetes\nservice accounts.\nOption C is incorrect because it attaches AmazonS3FullAccess and AmazonDynamoDBFullAccess to the service\naccounts directly. However, the key problem is that it attempts to grant S3 access to the UI service account\nand DynamoDB access to the data services account, the opposite of what's needed. More importantly, without\nIRSA, the service accounts will not have the necessary permissions to assume an IAM role, hence access\nwon't be effective.\nOption D, using IAM Roles for Service Accounts (IRSA), is the correct approach. IRSA allows you to associate\nIAM roles with Kubernetes service accounts. You create separate service accounts for the UI and data\nservices pods. Each service account is then configured to assume a specific IAM role. The IAM role for the UI\nservice account has a policy allowing access only to DynamoDB. The IAM role for the data services service\naccount has a policy allowing access only to S3. When the pods are launched and associated with their\nrespective service accounts, they automatically inherit the permissions defined in the associated IAM roles.\nHere's a breakdown of the IRSA process:\n1. Create IAM roles: Two roles are created, one for UI pods (DynamoDB access) and one for data\nservices pods (S3 access). Each role includes a trust policy that allows the Kubernetes service\naccount to assume the role.\n2. Create Kubernetes service accounts: Two service accounts are created, one for UI pods and one for\ndata services pods.\n3. Associate IAM roles with service accounts: The service accounts are annotated to specify which IAM\nrole they can assume. This annotation securely links the Kubernetes service account to the AWS IAM\nrole.\n4. Deploy pods: The UI pods are configured to use the UI service account, and the data services pods\nare configured to use the data services service account.\nThis setup ensures that the UI pods can only access DynamoDB and the data services pods can only access\nS3, fulfilling the company's requirements.\nReferences:\nIAM roles for service accounts - Amazon EKS\nUse IAM roles for service accounts",
    "links": []
  },
  {
    "question": "CertyIQ\nA company needs to give a globally distributed development team secure access to the company's AWS resources\nin a way that complies with security policies.\nThe company currently uses an on-premises Active Directory for internal authentication. The company uses AWS\nOrganizations to manage multiple AWS accounts that support multiple projects.\nThe company needs a solution to integrate with the existing infrastructure to provide centralized identity\nmanagement and access control.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": {},
    "answer": "C",
    "explanation": "The best solution is C, using AD Connector with IAM Identity Center (successor to AWS SSO). Here's why:\nCentralized Identity Management: AD Connector allows you to leverage your existing on-premises Active\nDirectory as the identity provider, achieving centralized identity management without migrating your directory\nto AWS.\nIntegration with AWS Accounts: IAM Identity Center simplifies managing access across multiple AWS\naccounts within your organization. This aligns with the company's use of AWS Organizations.\nPermissions Sets: IAM Identity Center allows you to define permission sets, which are collections of IAM\npolicies, and assign them to AD groups. This provides granular access control to AWS resources based on\ngroup membership in Active Directory.\nLeast Operational Overhead: Compared to alternatives, this solution minimizes the need to create and\nmanage individual IAM users. It leverages existing AD groups, significantly reducing the operational burden.\nCreating a managed AD (option A) involves a significant operational overhead for managing and maintaining\nan entire directory service. IAM users (option B) are impractical for a large, dynamic team requiring frequent\npermission updates, increasing overhead. Cognito (option D) introduces complexity with identity federation\nand access token management, adding overhead.\nSecurity Compliance: AD Connector uses secure tunnels to connect to your on-premises Active Directory,\nwhile IAM Identity Center uses secure protocols for authentication and authorization. This helps meet security\npolicies.\nTherefore, AD Connector with IAM Identity Center streamlines authentication and authorization, centrally\nmanages access across AWS accounts, minimizes operational overhead, and complies with security policies.\nHere are some links for further research:\nAWS IAM Identity Center\nAWS Directory Service\nAD Connector",
    "links": []
  },
  {
    "question": "CertyIQ\nA company is developing an application in the AWS Cloud. The application's HTTP API contains critical information\nthat is published in Amazon API Gateway. The critical information must be accessible from only a limited set of\ntrusted IP addresses that belong to the company's internal network.\nWhich solution will meet these requirements?",
    "options": {
      "A": "Set up an API Gateway private integration to restrict access to a predefined set of IP addresses: Private",
      "C": "While you can control access within your",
      "B": "C. Directly deploy the API in a private subnet. Create a network ACL. Set up rules to allow the traffic from",
      "D": "Modify the security group that is attached to API Gateway to allow inbound traffic from only the trusted"
    },
    "answer": "B",
    "explanation": "Here's a detailed justification for why option B is the correct answer and why the others are not, along with\nsupporting concepts and links:\nJustification for Option B (Create a resource policy for the API that denies access to any IP address that is\nnot specifically allowed):\nAPI Gateway resource policies are specifically designed to control access to your APIs based on various\nfactors, including source IP addresses. This approach provides fine-grained control at the API level, making it\nideal for restricting access to a defined set of trusted IP addresses. By creating a resource policy that\nexplicitly denies access to all IP addresses except those on the allowed list, you create a robust security\nmeasure directly tied to the API itself. This mechanism works independently of your VPC configuration or\nnetwork ACLs. This means even if your VPC configuration is somehow misconfigured, the API Gateway policy\nwill still enforce the IP-based restriction.\nWhy other options are incorrect:\nA. Set up an API Gateway private integration to restrict access to a predefined set of IP addresses: Private\nintegrations primarily focus on routing requests within your VPC. While you can control access within your\nVPC, it doesn't directly address restricting public internet access based on source IP addresses. A private\nintegration routes requests to a VPC endpoint, but the endpoint itself needs further restrictions to ensure that\nonly the trusted IP addresses can access the API Gateway, and that's done through other mechanisms,\nmaking this option less direct than option B.\nC. Directly deploy the API in a private subnet. Create a network ACL. Set up rules to allow the traffic from\nspecific IP addresses: API Gateway is a managed service that doesn't directly \"deploy\" into a private subnet\nin the way that an EC2 instance would. Even if it did, network ACLs (NACLs) offer basic stateless traffic\nfiltering at the subnet level. While NACLs can filter based on IP address, they are not as granular or easily\nmanageable as API Gateway resource policies for controlling API access. Furthermore, deploying API in a\nprivate subnet alone doesn't inherently restrict access from the internet. You would require additional\nnetworking configuration (like NAT Gateway, VPC Endpoints) to make it work, increasing complexity.\nD. Modify the security group that is attached to API Gateway to allow inbound traffic from only the trusted\nIP addresses: API Gateway doesn't have a security group that you directly modify. API Gateway is a fully\nmanaged service, and its underlying infrastructure is managed by AWS. You don't have direct access to its\nsecurity groups.\nAuthoritative Links for further research:\nAPI Gateway Resource Policies:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-using-iam-\npolicies-to-invoke-api.html\nAPI Gateway Private Integrations:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-apis.html\nNetwork ACLs: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html",
    "links": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-using-iam-",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-apis.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html"
    ]
  }
]